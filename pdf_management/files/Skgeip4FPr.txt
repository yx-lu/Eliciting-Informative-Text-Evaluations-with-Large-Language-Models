Title:
```
Under review as a conference paper at ICLR 2020 NEURAL NETWORKS ARE a priori BIASED TOWARDS BOOLEAN FUNCTIONS WITH LOW ENTROPY
```
Abstract:
```
Understanding the inductive bias of neural networks is critical to explaining their ability to generalise. Here, for one of the simplest neural networks - a single-layer perceptron with n input neurons, one output neuron, and no threshold bias term - we prove that upon random initialisation of weights, the a priori probability P (t) that it represents a Boolean function that classifies t points in {0, 1} n as 1 has a remarkably simple form: P (t) = 2 −n for 0 ≤ t < 2 n . Since a perceptron can express far fewer Boolean functions with small or large values of t (low "entropy") than with intermediate values of t (high "entropy") there is, on average, a strong intrinsic a-priori bias towards individual functions with low entropy. Furthermore, within a class of functions with fixed t, we often observe a further intrinsic bias towards functions of lower complexity. Finally, we prove that, regardless of the distribution of inputs, the bias towards low entropy becomes monotonically stronger upon adding ReLU layers, and empirically show that increasing the variance of the bias term has a similar effect.
```

Figures/Tables Captions:
```
Figure 1: Probability P (f ) that a function obtains upon random choice of parameters versus Lempel Ziv complexity K LZ (f ) for (a) an n = 7 perceptron with b = 0 and weights sampled from a Gaussian distributions, (b) an n = 7 perceptron with b = 0 and weights sampled from a uniform distribution centred at 0 and (c) a 1-hidden layer neural network (with 64 neurons in the hidden layer). Weights w and the threshold bias terms are sampled from N (0, 1). For all cases 10 8 samples were taken and frequencies less than 2 were eliminated to reduce finite sampling effects. We present the graphs with the same scale for ease of comparison.
Figure 2: Probability P (f ) vs rank for functions for a perceptron with n = 7, σ b = 0, and weights sampled from independent Gaussian distributions. In Figures 2b and 2c the functions are ranked within their respective F t . The seven highest probability functions in Figure 2c are f = 0101 . . . and equivalent functions obtained by permuting the input dimensions - note that these are very simple functions (simpler than the simplest functions that satisfy T (f ) = 47).
Figure 3: Effect of adding a bias term sampled from N (0, σ b ) to a perceptron with weights sampled from N (0, 1). (a) Increasing σ b increases the bias against entropy, and with a particular strong bias towards t = 0 and t = 2 n . (b) P (t = 0) increases with σ b and asymptotes to 1/2 in the limit σ b → ∞.
Figure 4: P(T = t) becomes on average more biased towards low entropy for increasing number of layers or increasing σ b . Here we use n = 7 input layers, with input {0, 1} 7 (centered data) or {−1, 1} 7 (uncentered data) The hidden layers are of width 2 n−1 = 64 to guarantee full expressivity. σ w = 1.0 in all cases. The insets show how P (t = 0) asymptotes to 1 2 with increasing layers or σ b .
```

Main Content:
```

Section Title: INTRODUCTION
  INTRODUCTION In order to generalise beyond training data, learning algorithms need some sort of inductive bias. The particular form of the inductive bias dictates the performance of the algorithm. For one of the most important machine learning techniques, deep neural networks (DNNs) (LeCun et al., 2015), sources of inductive bias can include the architecture of the networks, e.g. the number of layers, how they are connected, say as a fully connected network (FCN) or as a convolutional neural net (CNN), and the type of optimisation algorithm used, e.g. stochastic gradient descent (SGD) versus full gradient descent (GD). Many further methods such as dropout (Srivastava et al., 2014), weight decay (Krogh & Hertz, 1992) and early stopping (Morgan & Bourlard, 1990) have been proposed as techniques to improve the inductive bias towards desired solutions that generalise well. What is particularly surprising about DNNs is that they are highly expressive and work well in the heavily overparameterised regime where traditional learning theory would predict poor generalisation due to overfitting (Zhang et al., 2016). DNNs must therefore have a strong intrinsic bias that allows for good generalisation, in spite of being in the overparameterised regime. Here we study the intrinsic bias of the parameter-function map for neural networks, defined in (Valle- Pérez et al., 2018) as the map between a set of parameters and the function that the neural net- work represents. In particular, we define the a-priori probability P (f ) of a DNN as the probability that a particular function f is produced upon random sampling (or initialisation) of the weight and threshold bias parameters. The prior at initialization, P (f ), should inform the inductive bias of SGD-trained neural networks, as long as SGD approximates Bayesian inference with P (f ) as prior sufficiently well Valle-Pérez et al. (2018). We explain this connection further, and give some ev- idence supporting this behavior of SGD, in Appendix L. This supports the idea studying neural networks with random parameters Poole et al. (2016); Lee et al. (2018); Schoenholz et al. (2017); Garriga-Alonso et al. (2018); Novak et al. (2018) is not just relevant to find good initializations for optimization, but also to understand their generalization. A naive null-model for P (f ) might suggest that without further information, one should expect that all functions are equally likely. However, recent very general arguments (Dingle et al., 2018) based on the coding theorem from Algorithmic Information Theory (AIT) (Li et al., 2008) have instead suggested that for a wide range of maps M that obey a number of conditions such as being simple Under review as a conference paper at ICLR 2020 (they have a low Kolmogorov complexity K(M )) and redundancy (multiple inputs map to the same output) then if they are sufficiently biased, they will be exponentially biased towards outputs of low Kolmogorov complexity. The parameter-function map of neural networks satisfies these conditions, and it was found empirically (Valle-Pérez et al., 2018) that, as predicted in (Dingle et al., 2018), the probability P (f ) of obtaining a function f upon random sampling of parameter weights satisfies the following simplicity-bias bound P (f ) 2 −(b K(f )+a) , (1) where K(f ) is a computable approximation of the true Kolmogorov complexity K(f ), and a and b are constants that depend on the network, but not on the functions. It is widely expected that real world data is highly structured, and so has a relatively low Kolmogorov complexity (Hinton & Van Camp, 1993; Schmidhuber, 1997). The simplicity bias described above may therefore be an important source of the inductive bias that allows DNNs to generalise so well (and not overfit) in the highly over-parameterised regime (Valle-Pérez et al., 2018). Nevertheless, this bound has limitations. Firstly, the only rigorously proven result is for the true Kol- mogorov complexity version of the bound in the case of large enough K(f ). Although it has been found to work remarkably well for small systems and computable approximations to Kolmogorov complexity (Valle-Pérez et al., 2018; Dingle et al., 2018), this success is not yet fully understood theoretically. Secondly, it does not explain why models like DNNs are biased; it only explains that, if they are biased, they should be biased towards simplicity. Also, the AIT bound is very general - it predicts a probability P (f ) that depends mainly on the function, and only weakly on the net- work. It may therefore not capture some variations in the bias that are due to details of the network architecture, and which may be important for practical applications. For these reasons it is of interest to obtain a finer quantitative understanding of the simplicity bias of neural networks. Some work has been done in this direction, showing that infinitely wide neural networks are biased towards functions which are robust to changes in the input (De Palma et al., 2018), showing that "flatness" is connected to function smoothness (Wu et al., 2016), or arguing that low Fourier frequencies are learned first by a ReLU neural network (Rahaman et al., 2018; Yang & Salman, 2019). All of these papers take some notion of "smoothness" as tractable proxy for the complexity of a function. One generally expects smoother functions to be simpler, although this is clearly a very rough measure of the Kolmogorov complexity.

Section Title: SUMMARY OF KEY RESULTS
  SUMMARY OF KEY RESULTS In this paper we study how likely different Boolean functions, defined as f : {0, 1} n → {0, 1}, are obtained upon randomly chosen weights of neural networks. Our key results are aimed at flesh- ing out with more precision and rigour what the inductive biases of (very) simple neural networks are, and how they arise For this, we study the prior distribution over functions P (f ), upon random initialization of the parameters, which reflects the inductive bias of training algorithms that approx- imate Bayesian inference (see Appendix L for a detailed explanation, and data on how well SGD follows this behaviour). We focus our study on a notion of complexity, namely the "entropy," H(f ), of a Boolean function f , defined as the binary entropy of the fraction of possible inputs to f that f maps to 1. This quantity essentially measures the amount of class imbalance of the function, and is complementary to previous works studying notions of smoothness as a proxy for complexity. 1. In Section 4 we study a simple perceptron with no threshold bias term, and with weights w sampled from a distribution which is symmetric under reflections along the coordinate axes. Let the random variable T correspond to the number of points in {0, 1} n which that fall above the decision boundary of the network (i.e. T=|{x ∈ {0, 1} n : w, x > 0}|) upon i.i.d. random initialisation of the weights. We prove that T is distributed uniformly, i.e. P (T = t) = 2 −n for 0 ≤ t < 2 n . Let F t be the set of all functions with T = t that the perceptron can produce and let |F t | be its size (cf. Definition 3.4). We expect |F t | for t ∼ 2 n−1 (high entropy) to be (much) larger than |F t | for extreme values of t (low entropy). The average probability of obtaining a particular function f which maps t inputs to 1 is 2 −n /|F t |. The perceptron therefore shows a strong bias towards functions with low entropy, in the sense that individual functions with low entropy have, on average, higher probability than individual functions with high entropy. Under review as a conference paper at ICLR 2020 2. In Section 4.3, we show that within the sets F t , there is a further bias, and in some cases this is clearly towards simple functions which correlates with Lempel-Ziv complexity (Lempel & Ziv, 1976; Dingle et al., 2018), as predicted in (Valle-Pérez et al., 2018). 3. In Section 4.4, we show that adding a threshold bias term to a perceptron significantly increases the bias towards low entropy. 4. In Section 5.1, we provide a new expressivity bound for Boolean functions: DNNs with input size n, l hidden layers each with width n + 2 n−1−log 2 l + 1 and a single output neuron can express all 2 2 n Boolean functions over n variables. 5. In Section 5.2 we generalise our results to neural networks with multiple layers, proving (in the infinite-width limit) that the bias towards low entropy increases with the number of ReLU-activated layers. In Appendix J, we also show some empirical evidence that the results derived in this paper seem to generalize beyond the assumptions of our theoretical analysis, to more complicated data distributions (MNIST, CIFAR) and architectures (CNNs). Finally, in Appendix M, we show preliminary results on the effect of entropy-like biases in P (f ) on learning class-imbalanced data. where 1(X) is the Heaviside step function defined as 1 if X > 0 and 0 otherwise, and σ is an activation function that acts element-wise. The w l ∈ R n l+l ×n l are the weights, and b l ∈ R n l+1 are the threshold bias weights at layer l, where n l is the number of hidden neurons in the l-th layer. n L+1 is the number of outputs (1 in this paper), and n 0 is the dimension of the inputs (which we will also refer to as n). We will refer to the whole set of parameters (w l and b l , 1 ≤ l ≤ L) as θ. In the case of perceptrons we use f θ (x) = σ( w, x + b) to specify a network. We define the parameter-function map as in (Valle-Pérez et al., 2018) below. Definition 3.2 (Parameter-function map). Consider a parameterised supervised model, and let the input space be X and the output space be Y. The space of functions the model can express is F ⊂ Y |X| . If the model has p real valued parameters, taking values within a set Θ ⊆ R p , the parameter function map M is defined M : Θ → F θ → f θ where f θ is the function corresponding to parameters θ. In this paper we are interested in the Boolean functions that neural networks express. We consider the 0-1 Boolean hypercube {0, 1} n as the input domain. Definition 3.3. The function T (f ) is defined as the number of points in the hypercube {0, 1} n that are mapped to 1 by the action of a neural network f . For example, for a perceptron this function is defined as, We will sometimes use T (w, b) if the neural network is a perceptron. Under review as a conference paper at ICLR 2020 Definition 3.4 (F t and P (t)). We define the set F t to be the set of functions expressible by some model M (e.g. a perceptron, a neural network) which all have the same value of T (f ), F t = {f ∈ F M |T (f ) = t} , where F M is the set of all functions expressible by M. Given a probability measure P on the weights θ, we define the probability measure We can also define T (f ) and P (T = t) in the natural way for sets of input points other than {0, 1} n , the context making clear what definition is being used. Definition 3.5. The entropy H(f ) of a Boolean function f : {0, 1} * → {0, 1} is defined as H(f ) = −p log 2 p − (1 − p) log 2 (1 − p), where p = T f /2 n . It is the binary entropy of the fraction p of possible inputs to f that f maps to 1 or equivalently, the binary entropy of the fraction of 1's in the right-hand column of the truth table of f . Definition 3.6. We define the Boolean complexity K Bool (f ) of a function f as the number of binary connectives in the shortest Boolean formula that expresses f . Note that Boolean complexity can be defined in other ways as well. For example, K Bool (f ) is sometimes defined as the number of connectives (rather than binary connectives) in the shortest formula that expresses f , or as the depth of the most shallow formula that expresses f . These definitions tend to give similar values for the complexity of a given function, and so they are largely interchangeable in most contexts. We use the definition above because it makes our calculations easier.

Section Title: INTRINSIC BIAS IN A PERCEPTRON'S PARAMETER-FUNCTION MAP
  INTRINSIC BIAS IN A PERCEPTRON'S PARAMETER-FUNCTION MAP In this section we study the parameter-function map of the perceptron (Rosenblatt, 1958), in many ways the simplest neural network. While it famously cannot express many Boolean functions - including XOR - it remains an important model system. Moreover, many DNN architectures include layers of perceptrons, so understanding this very basic architecture may provide important insight into the more complex neural networks used today.

Section Title: ENTROPY BIAS IN A SIMPLE PERCEPTRON WITH b = 0 (NO THRESHOLD BIAS TERM)
  ENTROPY BIAS IN A SIMPLE PERCEPTRON WITH b = 0 (NO THRESHOLD BIAS TERM) Here we consider perceptrons f θ (x) = 1( w, x + b) without threshold bias terms, i.e. b = 0. The following theorem shows that under certain conditions on the weight distribution, a perceptron with no threshold bias has a uniform P (θ : T (f θ ) = t). The class of weight distributions includes the commonly used isotropic multivariate Gaussian with zero mean, a uniform distribution on a centred cuboid, and many other distributions. The full proof of the theorem is in Appendix A. Theorem 4.1. For a perceptron f θ with b = 0 and weights w sampled from a distribution which is symmetric under reflections along the coordinate axes, the probability measure P (θ : T (f θ ) = t) is given by

Section Title: Proof sketch
  Proof sketch We consider the sampling of the normal vector w as a two-step process: we first sample the absolute values of the elements, giving us a vector w pos with positive elements, and then we sample the signs of the elements. Our assumption on the probability distribution implies that each of the 2 n sign assignments is equally probable, each happening with a probability 2 −n . The key of the proof is to show that for any w pos , each of the sign assignments gives a distinct value of T (and because there are 2 n possible sign assignments, for any value of T , there is exactly one sign assignment resulting in a normal vector with that value of T ). This implies that, provided all sign assignments of any w pos are equally likely, the distribution on T is uniform.

Section Title: Under review as a conference paper at ICLR 2020
  Under review as a conference paper at ICLR 2020 A consequence of Theorem 4.1 is that the average probability of the perceptron producing a partic- ular function f with T (f ) = t is given by P (f ) t = 2 −n |F t | , (3) where F t denotes the set of Boolean functions that the perceptron can express which satisfy T (f ) = t, and · t denotes the average (under uniform measure) over all functions f ∈ F t . We expect |F t | to be much smaller for more extreme values of t, as there are fewer distinct possible functions with extreme values of t. This would imply a bias towards low entropy functions. By way of an example, |F 0 | = 1 and |F 1 | = n (since the only Boolean functions f a perceptron can express which satisfy T (f ) = 1 have f (x) = 1 for a single one-hot x ∈ {0, 1} n ), implying that Nevertheless, the probability of functions within a set F t is unlikely to be uniform. We find that, in contrast to the overall entropy bias, which is independent of the shape of the distribution (as long as it satisfies the right symmetry conditions), the probability P (f ) of obtaining function f within a set F t can depend on distribution shape. Nevertheless, for a given distribution shape, the probabilities P (f ) are independent of scale of the shape, e.g. they are independent of the variance of the Gaussian, or the width of the uniform distribution. This is because the function is invariant under scaling all weights by the same factor (true only in the case of no threshold bias). We will address the probabilities of functions within a given F t further in Section 4.3.

Section Title: SIMPLICITY BIAS OF THE b = 0 PERCEPTRON
  SIMPLICITY BIAS OF THE b = 0 PERCEPTRON The entropy bias of Theorem 4.1 entails an overall bias towards low Boolean complexity. In Theo- rem B.1 in Appendix B we show that the Boolean complexity of a function f is bounded by 1 Using Theorem 4.1 and Equation (4), we have that the probability that a randomly initialised per- ceptron expresses a function f of Boolean complexity k or greater is upper bounded by Uniformly sampling functions would result in P (K Bool (f ) ≥ k) ≈ 1−2 k−2 n which for intermediate k is much larger than Equation (5). Thus from entropy bias alone, we see that the perceptron is much more likely to produce simple functions than complex functions: it has an inductive bias towards simplicity. This derivation is complementary to the AIT arguments from simplicity bias (Dingle et al., 2018; Valle-Pérez et al., 2018), and has the advantage that it also proves that bias exists, whereas AIT-based simplicity bias arguments presuppose bias. To empirically study the inductive bias of the perceptron with b = 0, we sampled over many random initialisations with weights drawn from Gaussian or uniform distributions and input size n = 7. As can be seen in Figure 1a and Figure 1b, the probability P (f ) that function f obtains varies over many orders of magnitude. Moreover, there is a clear simplicity bias upper bound on this probability, which, as predicted by Eq. 1, decreases with increasing Lempel-Ziv complexity (K LZ (f )) (using a version from (Dingle et al., 2018) applied to the Boolean functions represented as strings of bits, see Appendix E). Similar behaviour was observed in (Valle-Pérez et al., 2018) for a FCN network. Moreover it was also shown there that Lempel-Ziv complexity for these Boolean functions correlates with approximations to the Boolean complexity K Bool . A one-layer neural network ( Figure 1c) shows stronger bias than the perceptron, which may be expected because the former has a much larger expressivity. A rough estimate of the slope a in Eq. 1 from (Dingle et al., 2018) suggests that a ∼ log 2 (N O )/max f ∈O (K(f )) where O is the set of all Boolean functions the model can produce, and N O is the number of such functions. The maximum K(f ) may not differ that much between the one layer network and the perceptron, but N O will be much larger in former than in the latter. In Appendix D we also show rank plots for the networks from  Figure 1 . Interestingly, at larger rank, they all show a Zipf like power-law decay, which can be used to estimate N O , the total number of Under review as a conference paper at ICLR 2020 Boolean functions the network can express. We also note that the rank plots for the perceptron with b = 0 with Gaussian or uniform distributions of weights are nearly indistinguishable, which may be because the overall rank plot is being mainly determined by the entropy bias.

Section Title: BIAS WITHIN F t
  BIAS WITHIN F t In  Figure 2  we compare a rank plot for all functions expressed by an n = 7 perceptron with b = 0 to the rank plots for functions with T (f ) = 47 and T (f ) = 64. To define the rank, we order the functions by decreasing probability, and then the rank of a function f is the index of f under this ordering (so the most probable function has rank 1, the second rank 2 and so on). The highest probability functions in F 64 have higher probability than the highest in F 47 because the former allows for simpler functions (such as 0101..), but for both sets, the maximum probability is still considerably lower than the maximum probability functions overall. In Appendix E we present further empirical data that suggests that these probabilities are bounded above by Lempel-Ziv complexity (in agreement with (Valle-Pérez et al., 2018)). However, in con- trast to Theorem 4.1 which is independent of the parameter distribution (as long as they are symmet- ric), the distributions within F t are different for the Gaussian and uniform parameter distributions, with the latter showing less simplicity bias within a class of fixed t (see Appendix E.1).

Section Title: Under review as a conference paper at ICLR 2020
  Under review as a conference paper at ICLR 2020 In Appendix F, we give further arguments for simplicity bias, based on the set of constraints that needs to be satisfied to specify a function. Every function f can be specified by a minimal set of linear conditions on the weight vector of the perceptron, which correspond to the boundaries of the cone in weight space producing f . The Kolmogorov complexity of conditions should be close to that of the functions they produce as they are related to the functions in a one-to-one fashion, via a simple procedure. In Appendix F, we focus on conditions which involve more than two weights, and show that within each set F t there exists one function with as few as 1 such conditions, and that there exists a function with as many as n − 2 such conditions. We also compute the set of necessary conditions (up to permutations of the axes) explicitly for functions with small t, and find that the range in the number and complexity of the conditions appears to grow with t, in agreement, with what we observe in  Figure 2  for the range of complexities. More generally, we find that complex functions typically need more conditions than simple functions do. Intuitively, the more conditions needed to specify a function, the smaller the volume of parameters that can generate the function, so the lower its a-priori probability.

Section Title: EFFECT OF b (THE THRESHOLD BIAS TERM) ON P (t)
  EFFECT OF b (THE THRESHOLD BIAS TERM) ON P (t) We next study the behaviour of the perceptron when we include the threshold bias term b, sampled from N (0, σ b ), while still initialising the weights from N (0, 1), as in Section 4.1. We present results for n = 7 in  Figure 3 . Interestingly, for infinitesimal σ b , P (T = 0) is less than for b = 0 (See Appendix C), but then for increasing σ b it rapidly grows larger than 1/2 n and in the limit of large σ b asymptotes to 1/2 (see Figure 3b). It's not hard to see where this asymptotic behaviour comes from, a large positive or negative b means all inputs are mapped to true (1) or false (0) respectively.

Section Title: ENTROPY BIAS IN MULTI-LAYER NEURAL NETWORKS
  ENTROPY BIAS IN MULTI-LAYER NEURAL NETWORKS We next extend results from Section 4 to multi-layer neural networks, with the aim to comment on the behaviour of P (T = t) as we add hidden layers with ReLU activations. To study the bias in the parameter-function map of neural networks, it is important to first understand the expressivity of the networks. In Section 5.1, we produce a (loose) upper bound on the minimum size of a network with ReLU activations and l layers that is maximally expressive over Boolean functions. We comment on how sufficiently large expressivity implies a larger bias towards low entropy for models with similarly shaped distribution over T (when compared to the perceptron). In Section 5.2, we prove, in the limit of infinite width, that adding ReLU activated layers causes the moments of P (T = t) to increase, . This entails a lower expected entropy for neural networks with more hidden layers. We empirically observe that the distribution of T becomes convex (with input {0, 1} n ) with the addition of ReLU activated layers for neural networks with finite width.

Section Title: EXPRESSIVITY CONDITIONS FOR DNNS
  EXPRESSIVITY CONDITIONS FOR DNNS We provide upper bounds on the minimum size of a DNNs that can model all Boolean functions. We use the notation n 0 , n 1 , . . . , n L , n L+1 to denote a neural network with ReLU activations and of the form given in Definition 3.1. Lemma 5.1. A neural network with layer sizes n, 2 n−1 , 1 , threshold bias terms, and ReLU activa- tions can express all Boolean functions over n variables (also found in (Raj, 2018)). See Appendix G for proof. Lemma 5.2. A neural network with l hidden layers, layer sizes n, (n + 2 n−1 /l + 1), . . . , (n + 2 n−1 /l + 1), 1 , threshold bias terms, and ReLU activations can express all Boolean functions over n variables. See Appendix G for proof. Note that neither of these bounds are (known to be) tight. Lemma 5.1 says that a network with one hidden layer of size 2 n−1 can express all Boolean functions over n variables. We know that a perceptron with n input neurons (and a threshold bias term) can express at most 2 n 2 Boolean func- tions ((Anthony, 2001), Theorem 4.3), which is significantly less than the total number of Boolean functions over n variables, which is 2 2 n . Hence there is a very large number of Boolean functions that the network with a (sufficiently wide) hidden layer can express, but the perceptron cannot. The vast majority of these functions have high entropy (as almost all Boolean functions do). Moreover, we observe that the measure P (T = t) is convex in the case of the more expressive neural net- works, as discussed in section Section 5.2. This suggests that the networks with hidden layers have a much stronger relative bias towards low entropy functions than the perceptron does, which is also consistent with the stronger simplicity bias found in  Figure 1 . We further observe from Lemma 5.2 that the number of neurons can be kept constant and spread over multiple layers without loss of expressivity for a Boolean classifier (provided the neurons are evenly spread across the layers).

Section Title: HOW MULTIPLE LAYERS AFFECT THE BIAS
  HOW MULTIPLE LAYERS AFFECT THE BIAS We next consider the effect of addition of ReLU activated layers on the distribution P (t). Of course adding even just one layer hugely increases expressivity over a perceptron. Therefore, even if the distribution of P (t) would not change, the average probability of functions in a given F t could drop significantly due to the increase in expressivity. However, we observe that for inputs {0, 1} n , P (t) becomes more convex when more ReLU- activated hidden layers are added, see  Figure 4 . The distribution appears to be monotone on either side of t = 2 n−1 and relatively flat in the middle, even with the addition of 8 intermediate layers 2 . In particular, we show in  Figure 4  that for large number of layers, or large σ b , the probabilities for P (t = 0) (and by symmetry, in the infinite width limit, also P (t = 2 n )) each asymptotically reach 2 , and thus take up the vast majority of the probability weight. We now prove some properties of the distribution P (t) for DNNs with several layers. Lemma 5.3. The probability distribution on T for inputs in {0, 1} n of a neural network with linear activations and i.i.d. initialisation of the weights is independent of the number of layers and the layer widths, and is equal to the distribution of a perceptron. See Appendix H for proof. While it is trivial that such a linear network has the same expressivity as a perceptron, it may not be obvious that the entropy bias is identical. Lemma 5.4. Applying a ReLU function in between each layer produces a lower bound on P (T = 0) such that P (T = 0) ≥ 2 −n . See Appendix H for proof. This lemma shows that a DNN with ReLU functions is no less biased towards the lowest entropy function than a perceptron is. We prove a more general result in the following theorem which concerns the behaviour of the average entropy H(t) (where the average upon random sampling of parameters) as the number of layers grows. The theorem shows that the bias towards low entropy becomes stronger as we increase the number of layers, for any distribution of inputs. We rely Under review as a conference paper at ICLR 2020 (a) {0, 1} 7 , σ b = 0.0, number of layers varied (b) {0, 1} 7 , σ b = 1.0, number of layers varied (c) {0, 1} 7 , 1 layer, σ b varied (d) {−1, 1} 7 , σ b = 0.0, number of layers varied on previous work that shows that in the infinite width limit, neural networks approach a Gaussian process (Lee et al. (2018); Garriga-Alonso et al. (2018); Novak et al. (2018); Matthews et al. (2018); Yang (2019)), which for the case of fully-connected ReLU networks, has an analytic form (Lee et al., 2018). Theorem 5.5. Let S be a set of m = |S| input points in R n . Consider neural networks with i.i.d. Gaussian weights with variances σ 2 w / √ n and biases with variance σ b , in the limit where the width of all hidden layers n goes to infinity. Let N 1 and N 2 be such neural networks with L and L + 1 infinitely wide hidden layers, respectively, and no bias. Then, the following holds: H(T ) is smaller than or equal for N 2 than for N 1 . It is strictly smaller if there exist pairs of points in S with correlations less than 1. If the networks have sufficiently large threshold bias (σ b > 1 is a sufficient condition), the result above also holds. For smaller bias, the result holds only for a sufficiently large number of layers. See Appendix H for a proof of Theorem 5.5. Theorem 5.5 is complementary to Theorem 4.1, in that the former only proves that the bias towards low entropy increases with depth, and the later proves conditions on the data that guarantee bias toward low entropy on the "base case" of 0 layers. We show in  Figure 4  that when σ b = 0, the bias towards low entropy indeed becomes monotonically stronger as we increase the number of ReLU layers, for both inputs in {0, 1} n as well as for centered data {−1, 1} n . For centered inputs {−1, 1} n , the perceptron with b = 0 shows rather unusual behaviour. The dis- tribution is completely peaked around t = 2 n−1 because every input mapping to 1 has the opposite input mapping to 0. Not surprisingly, its expressivity is much lower than the equivalent perceptron with {0, 1} n (as can be seen in Figure 6a in Appendix D). Nevertheless, in Figure 4d we see that Under review as a conference paper at ICLR 2020 as the number of layers increases, the behaviour rapidly resembles that of uncentered data (In Ap- pendix K we also show that the bias toward low entropy is also recovered as we increase σ b ). So far this is the only exception we have found to the general bias to low entropy we observe for all other systems (see also Appendix J). We therefore argue that this is a singular result brought about by particular symmetries of the perceptron with zero bias term. The fact that there is an exception does not negate our general result which we find holds much more generally. The insets of in  Figure 4  show that the two trivial functions asymptotically dominate in the limit of large numbers of layers. We note that recent work ((Lee et al., 2018; Luther & Seung, 2019)) has also pointed out that for fully-connected ReLU networks in the infinite-width infinite-depth limit, all inputs become asymptotically correlated, so that the networks will tend to compute the constant function. Here we give a quantitative characterisation of this phenomenon for any number of layers. Some interesting recent work (Yang & Salman, 2019) has shown that certain choices of hyper- parameters lead to networks which are a priori unbiased, that is the P (f ) appears to be uniform. In Appendix I we show that this result is due to a choice of hyperparameters that lie deep in the chaotic region defined in (Poole et al., 2016). The effect therefore depends on the choice of activa- tion function (it can occur for say tanh and erf, but most likely not ReLU), and we are studying it further.

Section Title: DISCUSSION AND FUTURE WORK
  DISCUSSION AND FUTURE WORK In Section 4 Theorem 4.1, we have proven the existence of an intrinsic bias towards Boolean func- tions of low entropy in a perceptron with no threshold bias term, such that P (T = t) = 2 −n for 0 ≤ t < 2 n . This result puts an upper bound on the probability that a perceptron with no threshold bias term will be initialised to a Boolean function with at least a certain Boolean complexity. Adding a threshold term in general increases the bias towards low entropy. We also study how the entropy bias is affected by adding a threshold bias term or ReLU-activated hidden layers. One of our main results, Theorem 5.5, proves that adding layers to a feed-forward neural network with ReLU activations makes the bias towards low entropy stronger. We also show empirically that the bias towards low entropy functions is further increased when a threshold bias term with high enough variance is added. Recently, (Luther & Seung, 2019) have argued that batch normalisation (Ioffe & Szegedy, 2015) makes ReLU networks less likely to compute the constant function (which has also been experimentally shown in (Page, 2019)). If batch norm increases the probability of high entropy functions, it could help explain why batch norm improves generalisation for (typically class balanced) datasets. We leave further exploration of the effect of batch normali- sation on a-priori bias to future work. Simplicity bias within the set of constant t functions F t is affected by the choice of initialisation, even when the entropy bias is unaffected. This indicates that there are further properties of the parameter-function map that lead to a simplicity bias. In Section 4.3, we suggest that the complexity of the conditions on w producing a function should correlate with the complexity of the function, and we conjecture that more complex conditions correlate with a lower probability. We note that the a priori inductive bias we study here is for a randomly initialised network. If a network is trained on data, then the optimisation procedure (for example SGD) may introduce further biases. In Appendix L, we give some evidence that the bias at initialization is the main driver of the inductive bias on SGD-trained networks. Furthermore, in Appendix M, we show preliminary results on how bias in P (f ) can affect learning in class-imbalanced problems. This suggests that understanding properties of P (f ) (like those we study in this paper), can help design architectures with desired inductive biases. Simplicity bias in neural networks (Valle-Pérez et al., 2018) offers an explanation of why DNNs work in the highly overparameterised regime. DNNs can express an unimaginably large number of functions that will fit the training data, but almost all of these will give extremely poor generalisation. Simplicity bias, however, means that a DNN will preferentially choose low complexity functions, which should give better generalisation. Here we have shown some examples where changing hy- perparameters can affect the bias further. This raises the possibility of explicitly designing biases to optimise a DNN for a particular problem.
  The change in probability for other distributions can be more complex.

```
