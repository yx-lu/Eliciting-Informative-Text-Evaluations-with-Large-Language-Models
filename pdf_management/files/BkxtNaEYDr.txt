Title:
```
Under review as a conference paper at ICLR 2020 LEARNING BOOLEAN CIRCUITS WITH NEURAL NET- WORKS
```
Abstract:
```
Training neural-networks is computationally hard. However, in practice they are trained efficiently using gradient-based algorithms, achieving remarkable perfor- mance on natural data. To bridge this gap, we observe the property of local cor- relation: correlation between small patterns of the input and the target label. We focus on learning deep neural-networks with a variant of gradient-descent, when the target function is a tree-structured Boolean circuit. We show that in this case, the existence of correlation between the gates of the circuit and the target label determines whether the optimization succeeds or fails. Using this result, we show that neural-networks can learn the (log n)-parity problem for most product distri- butions. These results hint that local correlation may play an important role in differentiating between distributions that are hard or easy to learn.
```

Figures/Tables Captions:
```
Figure 1: Trainig ReLU networks with one hidden-layer of size 128 with Adam optimizer, on both instances of the k-Parity problem (k = 5, n = 128). The figure shows the accuracy on a test set.
```

Main Content:
```

Section Title: INTRODUCTION AND MOTIVATION
  INTRODUCTION AND MOTIVATION It is well known (e.g.  Livni et al. (2014) ) that while deep neural-networks can express any function that can be run efficiently on a computer, in the general case, training them is computationally hard. Despite this theoretic pessimism, in practice, deep neural networks are successfully trained on real world datasets. Bridging this theoretical-practical gap seems to be the holy grail of theoretical machine learning nowadays. Maybe the most natural direction to bridge this gap is to find a property of data distributions that determines whether training is computationally easy or hard. The goal of this paper is to propose such a property. To motivate this, we first recall the k-parity problem: the input is n bits, there is a subset of k relevant bits (which are unknown to the learner), and the output should be 1 if the number of 1's among the relevant bits is even and −1 otherwise. It is well known (e.g.  Shalev-Shwartz et al. (2017) ) that the parity problem can be expressed by a fully connected two layer network or by depth log(n) locally connected 1 network. We observe the behavior of a one hidden-layer neural network trained on the k-parity problem, in two different instances: first, when the underlying distribution is the uniform distribution (i.e. the probability to see every bit is 1 2 ); and second, when the underlying distribution is a slightly biased product distribution (the probability for every bit to be 1 is 0.6). As can be clearly seen in  figure 1 , adding a slight bias to the probability of each bit dramatically affects the behavior of the network: while on the uniform distribution the training process completely fails, in the biased case it converges to a perfect solution. This simple experiment shows that a small change in the underlying distribution can cause a dramatic change in the trainability of neural-networks. A key property that differentiates the uniform from the biased distribution is the correlation between input bits and the target label. While in the uniform distribution, the correlation between each bit and the label is zero, in the biased case every bit of the k bits in the parity has a non-negligible correlation to the label (we show this formally in section 5). So, local correlations between bits of the input and the target label seems to be a promising property which separates easy and hard distributions. In this paper, we analyze the problem of learning tree-structured Boolean circuits with neural- networks. The key property that we assume is having sufficient correlation between every gate in the circuit and the label. We show that a variant of gradient-descent can efficiently learn such Under review as a conference paper at ICLR 2020 0 0.5 1 ·10 4 0.5 0.6 0.7 0.8 0.9 1 iteration accuracy biased uniform circuits for some families of distributions, where at the same time, without correlation, gradient- descent is likely to fail. More concretely, we discuss specific target functions and distributions that satisfy the local correlation requirement. We show that for most product distributions, gradient- descent learns the (log n)-parity problem (parity on log n bits of an input with dimension n). We further show that for every circuit with AND/OR/NOT gates, there exists a generative distribution, such that gradient-descent recovers the Boolean circuit exactly. Admittedly, as the primary focus of this paper is on theoretical analysis, the distributions we study are synthetic in nature. However, to explain the empirical success of neural-networks, we need to verify whether the local correlation property holds for natural datasets as well. To confirm this, we perform the following simple experiment: we train a network with two hidden-layers on a single random patch from images in the ImageNet dataset. We observe that even on a complex task such as ImageNet, a network that gets only a 3 × 3 patch as an input, achieves 2.6% top-5 accuracy - much better than a random guess (0.5% top-5 accuracy). The full results of the experiment are detailed in the appendix. This experiment highlights that, to some extent, natural datasets display a local correlation property: even a few "bits" of the input already have some non-negligible information on the target label.

Section Title: RELATED WORK
  RELATED WORK In recent years, the success of neural-networks has inspired an ongoing theoretical research, trying to explain empirical observations about their behavior. Some theoretical works show failure cases of neural-networks. Other works give different guarantees on various learning algorithms for neural- networks. In this section, we cover the main works that are relevant to our paper.

Section Title: Failures of gradient-based algorithms
  Failures of gradient-based algorithms Various works have shown different examples demonstrat- ing failures of gradient-based algorithm. The work of  Shamir (2018)  shows failures of gradient descent, both in learning natural target functions and in learning natural distributions. The work of  Shalev-Shwartz et al. (2017)  shows that gradient-descent fails to learn parities and linear-periodic functions under the uniform distribution. In  Das et al. (2019) , a hardness result for learning random deep networks is shown. Other similar failure cases are also covered in  Abbe & Sandon (2018) ;  Malach & Shalev-Shwartz (2019) . While the details of these works differ, they all share the same key principal - if there is no local correlation, gradient-descent fails. Our work complements these results, showing that in some cases, when there are local correlations to the target, gradient-descent succeeds to learn the target function.

Section Title: Learning neural-networks with gradient-descent
  Learning neural-networks with gradient-descent Recently, a large number of papers have pro- vided positive results on learning neural-networks with gradient-descent. Generally speaking, most of these works show that over-parametrized neural-networks, deep or shallow, achieve performance that is competitive with kernel-SVM.  Daniely (2017)  shows that SGD learns the conjugate kernel associated with the architecture of the network, for a wide enough neural-network. The work of  Brutzkus et al. (2017)  shows that SGD learns a neural-network with good generalization, when the target function is linear. A growing number of works show that for a specific kernel induced Under review as a conference paper at ICLR 2020 by the network activation, called the Neural Tangent Kernel (NTK), gradient-descent learns over- parametrized networks, for target functions with small norm in the reproducing kernel Hilbert space (see the works of  Jacot et al. (2018) ;  Xie et al. (2016) ;  Oymak & Soltanolkotabi (2018) ;  Allen-Zhu et al. (2018a ;b);  Oymak & Soltanolkotabi (2019) ;  Arora et al. (2019) ;  Du et al. (2018) ; Ma et al. (2019);  Lee et al. (2019) ). While these results show that learning neural-networks with gradient- descent is not hopeless, they are in some sense disappointing - in practice, neural-networks achieve performance that are far better than SVM, a fact that is not explained by these works. A few results do discuss success cases of gradient-descent that go beyond the kernel-based analysis ( Brutzkus & Globerson, 2017 ; 2019;  Allen-Zhu & Li, 2019 ;  Yehudai & Shamir, 2019 ). However, these works still focus on very simple cases, such as learning a single neuron, or learning shallow neural-networks in restricted settings. In this work we deal with learning deep networks, going beyond the common reduction to linear classes of functions.

Section Title: Layerwise optimization algorithms
  Layerwise optimization algorithms In this paper, we analyze the behavior of layerwise gradient- descent - optimizing one layer at a time, instead of the common practice to optimize the full network end-to-end. We do so since such algorithm greatly simplifies our theoretical analysis. While layerwise training is not a common practice, recent works ( Belilovsky et al., 2018 ; 2019) have shown that such algorithms achieve performance that are competitive with the standard end-to-end approach, scaling up to the ImageNet dataset. We note that other theoretical works have studied iterative algorithms that learn neural-networks layer-by-layer ( Arora et al., 2014 ;  Malach & Shalev- Shwartz, 2018 ). However, our work focuses specifically on layerwise gradient-descent, considering the problem of learning Boolean circuits.

Section Title: Learning Boolean Circuits
  Learning Boolean Circuits The problem of learning Boolean circuits has been studied in the classical literature of theoretical machine learning. The work of  Kearns et al. (1987)  gives various positive and negative results on the learnability of Boolean Formulas, including Boolean circuits. The work of  Linial et al. (1989)  introduces an algorithm that learns a constant-depth circuit in quasi- polynomial time. Another work by  Kalai (2018)  discusses various properties of learning Boolean formulas and Boolean circuits. Our work differs from the above in various aspects. Our main focus is learning deep neural-networks with gradient descent, where the target function is implemented by a Boolean circuit, and we do not aim to study the learnability of Boolean circuits in general. Furthermore, we consider Boolean circuits where a gate can take any Boolean functions, and not only AND/OR/NOT, as is often considered in the literature of Boolean circuits. On the other hand, we restrict ourselves to the problem of learning circuits with a fixed structure of full binary trees. We are not aware of any work studying a problem similar to ours.

Section Title: PROBLEM SETTING
  PROBLEM SETTING We consider the problem of learning binary classification functions over the Boolean cube. So, let X = {±1} n be the instance space and Y = {±1} be the label set. Throughout the paper, we assume the target function is given by a Boolean circuit. In general, such assumption effectively does not limit the set of target functions, as any computable function can be implemented by a Boolean circuit. We define a circuit C to be a directed graph with n input nodes and a single output node, where each inner node has exactly two incoming edges, and is labeled by some arbitrary Boolean function f : {±1} 2 → {±1}, which we call a gate 2 . For each node v in C we denote by γ(v) ∈ f : {±1} 2 → {±1} its gate. We recursively define h v,C : {±1} n → {±1} to be: h v,C (x) = γ(v) (h u1,C (x), h u2,C (x)) where u 1 , u 2 are the two nodes with outcoming edges to v. Finally, define h C = h o,C , where o is the output node. We study the problem of learning the target function h C , when C is a full binary tree, and n = 2 d , where d is the depth of the tree. The leaves of the tree are the input bits, ordered by x 1 , . . . x n . Admittedly, such assumption greatly limits the set of target functions, but still gives a rather rich family of functions. For example, such circuit can calculate the parity function on any k bits of the input (the function calculated by f (x) = i∈I x i for some set of indexes I). We note that the total number of functions calculated by such tree grows like 6 n , as shown in  Farhoodi et al. (2019) .

Section Title: Under review as a conference paper at ICLR 2020
  Under review as a conference paper at ICLR 2020 We introduce a few notations that are used in the sequel. Fix some tree structured binary circuit C. This circuit has d levels, and we denote v i,j the j-th node in the i-th level of the tree, and denote γ i,j = γ(v i,j ). Fix some i ∈ [d], let n i := 2 i , and denote by Γ i : {±1} ni → {±1} ni/2 the function calculated by the i-th level of the circuit: For i < i , we denote: Γ i...i := Γ i • · · · • Γ i . So, the full circuit is given by h C (x) = Γ 1...d (x). As noted, our goal is to learn Boolean circuits with neural-networks. To do so, we use a network architecture that aims to imitate the Boolean circuits described above. We replace each Boolean gate with a neural-gate: a one hidden-layer ReLU network, with a hard-tanh 3 activation on its output. Formally, let σ be the ReLU activation, and let φ be the hard-tanh activation, so: Define a neural-gate to be a neural-network with one hidden layer, input dimension 2, with ReLU activation for the hidden-layer and hard-tanh for the output node. Namely, denote g w,v : R 2 → R such that: Notice that a neural-gate g w,v of width 4 or more can implement any Boolean gate. That is, we can replace any Boolean gate with a neural-gate, and maintain the same expressive power. To implement the full Boolean circuit defined above, we construct a deep network of depth d (the depth of the Boolean circuit), with the same structure as the Boolean circuit. We define d blocks, each block has neural-gates with the same structure and connectivity as the Boolean circuit. A block B W (i) ,V (i) : R 2 i → R 2 i−1 , is defined by: We consider the process of training neural-networks of the form N W,V = B W (1) ,V (1) • · · · • B W (d) ,V (d) . Notice that indeed, a network N W,V can implement any tree-structured Boolean cir- cuit of depth d. In practice, neural-networks are trained with gradient-based optimization algorithm, in an end-to-end fashion. That is, the weights of all the layers are optimized together, with gradient updates on a given sample. To simplify the analysis, we instead consider a layerwise optimization algorithm, that performs gradient updates layer-by-layer. While this approach is much less popu- lar, it has been recently shown to achieve performance that are comparable to end-to-end training, scaling up to the ImageNet dataset ( Belilovsky et al., 2018 ). Denote by P the average-pooling operator, defined by P (x 1 , . . . , x n ) = 1 n n i=1 x i . Denote the hinge-loss by (ŷ, y) = max(1 − yŷ, 0) and denote the loss on the distribution by L D (f ) = E (x,y)∼D [ (f (x), y)]. For a sample S ⊆ X × Y, denote the loss on the sample by L S (f ) = 1 |S| (x,y)∈S (f (x), y). The layerwise gradient-descent algorithm for learning deep networks is described in algorithm 1. For simplicity, we assume that the second layer of every neural-gate is fixed, such that v ∈ {±1}. Notice that this does not limit the expressive power of the network. Algorithm 1 iteratively opti- mizes the output of the network's layers, starting from the bottom-most layer. For each layer, the average-pooling operator is applied to reduce the output of the layer to a single bit, and this output is optimized with respect to the target label. Note that in fact, we can equivalently optimize each neural-gate separately and achieve the same algorithm. However, we present a layerwise training process to conform with algorithms used in practice.

Section Title: MAIN RESULTS
  MAIN RESULTS Our main result shows that algorithm 1 can learn a function implemented by the circuit C, when running on "nice" distributions, with the local correlation property. We start by describing the distributional assumptions needed for our main results. Let D be some distribution over X × Y. For some function f : X → X , we denote by f (D) the distribution of (f (x), y) where (x, y) ∼ D. Let D (i) be the distribution Γ (i+1)...d (D). Denote by c i,j the correlation between the output of the j-th gate in the i-th layer and the label, so: c i,j := E D (i) [x j y]. Define the influence of the j-th gate in the i-th layer with respect to the uniform distribution (U ) by: Now, we introduce the main assumption on the distribution D. We assume the following: Assumption 1. (local correlation) There exists some ∆ ∈ (0, 1) such that for every layer i ∈ [d] and for every gate j ∈ [2 i ] with I i,j = 0, the value of c i,j satisfies |c i,j | > |E D [y]| + ∆. Another technical assumption we need to make is the following: Assumption 2. (label bias) There exists some β ∈ (0, 1) such that |E D [y]| > β. Before we present the main results, we wish to discuss the distributional assumptions given above. Assumption 1 is the key assumption required for the algorithm to succeed in learning the target func- tion. Essentially, this assumption requires that the output of every gate in the circuit will "explain" the label slightly better then simply observing the bias between positive and negative examples. Clearly, gates that have no influence on the target function never satisfy this property, so we require it only for influencing gates. While this is a strong assumption, in section 5 we discuss examples of distributions where this assumption typically holds. Furthermore, the experiment described in section 1 hints that this assumption may hold for natural data. Assumption 2 is a simple technical assumption, that requires that the distribution of positive and negative examples is slightly biased. In a sense, we expect that "most" distributions would not be exactly balanced, so this assumption is easy to satisfy. Now, consider the case where D (limited to X ) is a product distribution: for every j = j , the variables x j and x j are independent, for (x, y) ∼ D. A simple argument shows that any product distribution D that satisfies assumptions 1, satisfies the following properties: Property 1. There exists some ∆ ∈ (0, 1) such that for every layer i ∈ [d] and for every gate j ∈ [2 i ], the output of the j-th gate in the i-th layer satisfies one of the following: • The value of the gate j is independent of the label y, and its influence is zero: I i,j = 0. • The value of c i,j satisfies |c i,j | > |E D [y]| + ∆. Property 2. For every layer i ∈ [d], and for every gate j ∈ [2 i−1 ], the value of (x 2j−1 , x 2j ) (i.e, the input to the j-th gate of layer i − 1) is independent of the label y given the output of the j-th gate: Under review as a conference paper at ICLR 2020 Property 1 is immediate from assumption 1. The following lemma shows that property 2 is satisfied as well for any product distribution: Lemma 1. Assume D (restricted to X ) is a product distribution (i.e., for every j = j we have that x j and x j are independent, for (x, y) ∼ D). Then D satisfies property 2. Notice that properties 1, 2 may hold for distributions that are not product distribution (as we show in the next section). Specifically, property 2 is a very common assumption in the field of Graphical Models (see Koller & Friedman (2009)). For our results to hold in a more general setting, we use properties 1 and 2, instead of assuming that D is a product distribution satisfying assumption 1. So, given a distribution satisfying properties 1, 2 and assumption 2, we show that algorithm 1 achieves an arbitrarily good approximation with high probability, with sample complexity and run-time quasi- polynomial in the dimension n: Theorem 1. Let D be a distribution satisfying properties 1, 2 and assumption 2. Assume that for every i we initialize W (i) 0 such that W (i) 0 max ≤ 1 4 √ 2k . Fix some , δ > 0 and as- sume that k ≥ log −1 ( 4 3 ) log( 2nd δ ), and that η ≤ 1 16k . Assume we sample S ∼ D, with |S| > 128 2 min{∆,2β} 2 n 11+4 log n−2 log min{∆,2β} log( 8nd δ ). Then, with probability at least 1 − δ, when running algorithm 1 on the sample S, the algorithm returns the a function such that: E (x,y)∼D [N 0 (x) = h C (x)] ≤ when running T > 3 √ 2 η min{∆,2β} n 6.5+2 log n−log min{∆,2β} steps for each layer. The above shows a learnability result in the standard PAC setting (given our distributional assump- tions), where we only guarantee approximation of the target function under the given distribution. In fact, we can get a stronger result, and show that the algorithm learns the function h C exactly, with run-time and sample complexity polynomial in n. To get this result, we need to require that there is no feasible pattern (pair of bits) in the Boolean circuit that is extremely rare: Assumption 3. There exists some ∈ (0, 1) such that for every layer i ∈ [d], for every gate j ∈ [2 i−1 ] and for every p ∈ {±1} 2 such that P (x,y)∼D (i) [(x 2j−1 , x 2j ) = p] = 0, it holds that: In section 5 we discuss distributions that satisfies assumption 3. Given all the above assumptions, we get the following: Theorem 2. Let D be a distribution satisfying properties 1, 2 and assumptions 2, 3. Assume that for every i we initialize W (i) 0 such that W (i) 0 max ≤ 1 4 √ 2k . Fix some δ > 0 and as- sume that k ≥ log −1 ( 4 3 ) log( 2nd δ ), and that η ≤ 1 16k . Assume we sample S ∼ D, with |S| > 128 2 min{∆,2β} 2 log( 8nd δ ). Then, with probability at least 1 − δ, when running algorithm 1 on the sample S, the algorithm returns a function such that N 0 (x) = h C (x) for all x ∈ X , when running T > 3 √ 2n η min{∆,2β} steps for each layer. We give the full proof of the theorems in the appendix, and give a sketch of the argument here. Observe that the input to the (i, j)-th neural-gate is a pattern of two bits. The target gate (the (i, j)-th gate in the circuit C) identifies each of the four possible patterns with a single output bit. For example, if the gate is OR, then the patterns {(1, 1), (−1, 1), (1, −1)} get the value 1, and the pattern (−1, −1) gets the value −1. Fix some pattern p ∈ {±1} 2 , and assume that the output of the (i, j)-th gate on the pattern p is 1. Since we assume the output of the gate is correlated with the label, the loss function draws the output of the neural-gate on the pattern p toward the correlation of the gate. In the case where the output of the gate on p is −1, the output of the neural-gate is drawn to the opposite sign of the correlation. All in all, the optimization separates the patterns that evaluate to 1 from the patterns that evaluate to −1. In other words, the neural-gate learns to implement the target gate. This way, we can show that the optimization process makes the network recover all the influencing gates, so that at the end of the optimization the network implements the circuit. Observe that when there is no correlation, the above argument fails immediately. Since the label is slightly biased, when there is no correlation the output of the neural-gate is drawn towards the bias of the label for all the input patterns, regardless of the value of the gate. If the gate is not influencing Under review as a conference paper at ICLR 2020 the target function (i.e. I i,j = 0), then this clearly doesn't effect the overall behavior. However, if there exists some influencing gate with no correlation to the label, then the output of the neural-gate will be constant on all its input patterns. Hence, the algorithm will fail to learn the target function. This shows that assumption 1 is in fact critical for the success of the algorithm.

Section Title: DISTRIBUTIONS
  DISTRIBUTIONS In the previous section we showed that algorithm 1 can learn tree-structured Boolean circuits in polynomial run-time and sample complexity. These results require some non-trivial distributional assumptions. In this section we study specific families of distributions, and show that they satisfy the above assumptions. First, we study the problem of learning a parity function on log n bits of the input, when the under- lying distribution is a product distribution. The problem of learning parities was studied extensively in the literature of machine learning theory ( Feldman et al., 2006 ; 2009;  Blum et al., 2003 ;  Shalev- Shwartz et al., 2017 ;  Brutzkus et al., 2019 ), and serves as a good case-study for the above results. In the (log n)-parity problem, we show that in fact most product distributions satisfy assumptions 1-3, hence our results apply to most product distributions. Next, we study distributions given by a generative model. We show that for every circuit with gates AND/OR and NOT, there exists a distribution that satisfies the above assumptions, so algorithm 1 can learn any such circuit exactly.

Section Title: PRODUCT DISTRIBUTIONS
  PRODUCT DISTRIBUTIONS We observe the k-Parity problem, where the target function is f (x) = j∈I x j some subset I ⊆ [n] of size |I| = k. A simple construction shows that f can be implemented by a tree structured circuit as defined previously. We define the gates of the first layer by: And for all other layers i < d − 1, we define: γ i,j (z 1 , z 1 ) = z 1 z 2 . Then we get the following: Lemma 2. Let C be a Boolean circuit as defined above. Then: h C (x) = j∈I x j = f (x). Now, let D X be some product distribution over X , and denote p j := P D X [x j = 1]. Let D be the distribution of (x, f (x)) where x ∼ D X . Then for the circuit defined above we get the following result: The above lemma shows that every product distribution that is far enough from the uniform distri- bution, or from a constant distribution, satisfies assumptions 1 and 2 with β, ∆ = (2ξ) k . Using the fact that at each layer, the output of each gate is an independent random variable (since the input distribution is a product distribution), we get that assumption 3 is satisfied with = ξ 2 . This gives us the following result: Corollary 1. Let D be a product distribution with p j ∈ (ξ, 1 2 − ξ) ∪ ( 1 2 + ξ, 1 − ξ) for every j, with the target function being the (log n)-Parity (i.e., k = log n). Then, when running algorithm 1 as described in Theorem 2, with probability 1 − δ the algorithm returns the true target function h C , with run-time and sample complexity polynomial in n.

Section Title: GENERATIVE MODELS
  GENERATIVE MODELS Next, we move beyond product distributions, and observe families of distributions given by a gener- ative model. We limit ourselves to circuits where each gate is chosen from the set {∧, ∨, ¬∧, ¬∨}. For every such circuit, we define a generative distribution as follows: we start by sampling a label Under review as a conference paper at ICLR 2020 for the example, from a slightly imbalanced distribution (to satisfy assumption 2). Then iteratively, for every gate, we sample uniformly at random a pattern from all the pattern that give the correct output. For example, if the label is 1 and the topmost gate is OR, we sample a pattern uniformly from {(1, 1), (1, −1), (−1, 1)}. The sampled pattern determines what should be the output of the second topmost layer. For every gate in this layer, we sample again a pattern that will result in the correct output. We continue in this fashion until reaching the bottom-most layer, which defines the observed example. Formally, for a given gate Γ ∈ {∧, ∨, ¬∧, ¬∨}, we denote the following sets of patterns: We recursively define D (0) , . . . , D (d) , where D (i) is a distribution over {±1} 2 i × {±1}: Then we have the following results: Lemma 4. For every i ∈ [d] and every j ∈ [2 i ], denote c i,j = E (x,y)∼D (i) [x j y]. Then we have: We also need the following simple observation: Lemma 5. For every i ∈ [d] we have Γ i (D (i) ) = D (i−1) . By definition, we have E [y] = 2ξ, so D (d) satisfies assumption 2 with β = ξ. Notice that from Lemma 4, the distribution D (d) satisfies property 1 with ∆ = 1 3 n log(2/3) (note that since we re- strict the gates to AND/OR/NOT, all gates have influence). By its construction, the distribution also satisfies property 2, and it satisfies assumption 3 with = 1 4 d = 1 n 2 . Therefore, we can apply The- orem 2 on the distribution D (d) , and get that algorithm 1 learns the circuit C exactly in polynomial time. This leads to the following nice corollary: Corollary 2. With the assumptions and notations of Theorem 2, for every circuit C with gates in {∧, ∨, ¬∧, ¬∨}, there exists a distribution D such that when running algorithm 1 on a sample from D, the algorithm returns h C with probability 1 − δ, in polynomial run-time and sample complexity. Note that the fact that for every circuit there exists a distribution that can be learned in the PAC setting is trivial: simply take a distribution that is concentrated on a single positive example, and approximating the target function on such distribution is achieved by a classifier that always returns a positive prediction. However, showing that there exists a distribution on which algorithm 1 exactly recovers the circuit, is certainly non-trivial.

Section Title: DISCUSSION
  DISCUSSION In this paper we suggested the property of local corrleation as a possible candidate for differenti- ating between hard and easy distributions. We showed that on the task of learning tree-structured Boolean circuits, the existence of local correlations between the gates and the target label allows layerwise gradient-descent to learn the target circuit. Furthermore, we showed specific tasks and distributions which satisfy the local correlation property. These results raise a few open questions, which we leave for future work. The most immediate research problem is showing similar results for more general structures of Boolean circuit, and on a wider range of distributions (beyond product distributions or generative models). More generally, we suggest that the local correlation property may be important in a broader context, beyond Boolean circuits. For example, examining whether an equivalent property exists when the target function is a convolutional network is an extremely inter- esting open problem. Needless to say, finding other properties of natural distribution that determine whether gradient-based algorithms succeed or fail is another promising research direction.
  We chose to use the hard-tanh activation over the more popular tanh activation since it simplifies our theoretical analysis. However, we believe the same results can be given for the tanh activation.

```
