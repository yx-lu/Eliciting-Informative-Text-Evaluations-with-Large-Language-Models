Title:
```
Under review as a conference paper at ICLR 2020 COMPRESSIVE RECOVERY DEFENSE: A DEFENSE
```
Abstract:
```
We provide recovery guarantees for compressible signals that have been corrupted with noise and extend the framework introduced in Bafna et al. (2018) to defend neural networks against 0 , 2 , and ∞ -norm attacks. In the case of 0 -norm noise, we provide recovery guarantees for Iterative Hard Thresholding (IHT) and Basis Pursuit (BP). For 2 -norm bounded noise, we provide recovery guarantees for BP, and for the case of ∞ -norm bounded noise, we provide recovery guarantees for a modified version of Dantzig Selector (DS). These guarantees theoretically bolster the defense framework introduced in Bafna et al. (2018) for defending neural networks against adversarial in- puts. Finally, we experimentally demonstrate the effectiveness of this defense framework against an array of 0 , 2 and ∞ -norm attacks.
```

Figures/Tables Captions:
```
Figure 1: The original image is shown in the first column, the adversarial image in the second column, and image reconstructed using IHT is shown in the third column.
Figure 2: Reconstruction quality of images using IHT and BP. The first column shows randomly selected original images from the test set, while the second and fifth column show the adversarial images. Reconstructions using IHT are labeled IHT-Rec and using BP are labeled BP-Rec. We show reconstructions in columns three, four, six, and seven.
Figure 3: Reconstruction quality of images using BP. The first columns shows the original images, while the adversarial images are shown in the second and fourth column. The reconstructions are shown in columns three and five.
Figure 4: Comparison of images reconstructed using Algorithm 3 (With Constraint) with images reconstructed using DS without the additional constraint (No Constraint).
Figure 5: Reconstruction quality of images using DS. The first column shows the original images, while the second columns shows adversarial images and the third columns shows reconstructions using Algorithm 3 respectively.
Table 1: Effectiveness of CRD against OPA. The first column lists the accuracy of the network on original images and the OPA Acc. column shows the network's accuracy on adversarial images. The IHT. Acc. column shows the accuracy of the network on images reconstructed using IHT.
Table 2: The t avg column lists the average adversarial budget for each attack. The Orig. Acc column lists the accuracy of the network on original test inputs, the Acc. columns under C&W 0 and JSMA list network accuracy on adversarial inputs. The IHT Acc. and the BP Acc. columns list the accuracy of the network on inputs that have been corrected using IHT and BP respectively.
Table 3: The 2avg column lists the average 2 -norm of the attack vector. The Orig. Acc column lists the accuracy of the network on original test inputs, while the Acc. columns under C&W 2 and DF columns report network accuracy on adversarial inputs. BP Acc. columns lists the accuracy of the network on inputs reconstructed using BP.
Table 4: The ∞avg column lists the ∞ -norm of each attack vector, Orig. Acc. and BIM Acc. columns list the accuracy of the network on the original and adversarial inputs respectively, and the Modified DS Acc. column lists the accuracy of the network on inputs reconstructed using Algorithm 3. We also show accuracy of the network on images reconstructed with DS (without the additional constraint) in the DS Acc. column.
```

Main Content:
```

Section Title: INTRODUCTION
  INTRODUCTION Signal measurements are often corrupted by noise. The theory of compressive sensing (Candes et al. (2006)) allows us to retrieve the original signal from a corrupted measurement, under some structural assumptions on the measurement mechanism and the signal. Let us consider the class of machine learning problems where the inputs are compressible (i.e., approximately sparse) in some domain. For instance, images and audio signals are known to be compressible in their frequency domain and machine learning algorithms have been shown to perform exceedingly well on clas- sification tasks that take such signals as input (Krizhevsky et al. (2012); Sutskever et al. (2014)). However, it was found in Szegedy et al. (2013) that neural networks can be easily forced into mak- ing incorrect predictions by adding adversarial perturbations to their inputs; see also Szegedy et al. (2014); Goodfellow et al. (2015); Papernot et al. (2016); Carlini & Wagner (2017). Further, the adversarial perturbations that led to incorrect predictions were shown to be very small (in either 0 , 2 , or ∞ -norm) and often imperceptible to human beings. For this class of machine learning tasks, we show how to approximately recover original inputs from adversarial inputs and thus defend the neural network 0 -norm, 2 -norm and ∞ -norm attacks. In the case of 0 -norm attacks on neural networks, the adversary can perturb a bounded number of coordinates in the input vector but has no restriction on how much each coordinate is perturbed in absolute value. In the case of 2 -norm attacks, the adversary can perturb as many coordinates of the input vector as they choose as long as the 2 -norm of the perturbation vector is bounded. Finally, in ∞ -norm attacks, the adversary is only constrained by the amount of noise added to each coordinate of the input vector. The contribution and structure of this paper is as follows. In Section 3.1, we describe the Compres- sive Recovery Defense (CRD) framework, a compressive-sensing-based framework for defending neural networks against adversarial inputs. This is essentially the same framework introduced in Bafna et al. (2018), though Bafna et al. (2018) considered only 0 attacks. In Section 3.2, we present the recovery algorithms which are used in the CRD framework to approximately recover original inputs from adversarial inputs. These algorithms include standard Basis Pursuit (BP), (k, t)-sparse Iterative Hard Thresholding (IHT) and Dantzig Selector (DS) with an additional constraint. In Sec- tion 3.3, we state recovery guarantees for the recovery algorithms in the presence of noise bounded in either 0 , 2 , or ∞ -norm. The guarantees apply to arbitrary 0 , 2 , and ∞ -norm attacks; they do not require prior knowledge of the adversary's attack strategy. The recovery guarantees are proved rigorously in Appendix A. In Section 4, we experimentally demonstrate the performance of Under review as a conference paper at ICLR 2020 the CRD framework in defending neural network classifiers on CIFAR-10, MNIST, and Fashion- MNIST datasets against state-of-the-art 0 , 2 and ∞ -norm attacks.

Section Title: Notation
  Notation Let x be a vector in C N . Let S ⊆ {1, . . . , N } and S = {1, . . . , N } \ S. The cardinality of S is |S|. If A ∈ C m×N is a matrix, then A S ∈ C m×|S| is the column submatrix of A consisting of the columns indexed by S. We denote by x S either the sub-vector in C S consisting of the entries indexed by S or the vector in C N that is formed by starting with x and setting the entries indexed by S to zero. For example, if x = [4, 5, −9, 1] T and S = {1, 3}, then x S is either [4, −9] T or [4, 0, −9, 0] T . It will always be clear from context which meaning is intended. Note that, under the second meaning, x S = x − x S . The support of x, denoted by supp(x), is the set of indices of the non-zero entries of x, i.e., supp(x) = {i ∈ {1, . . . , N } : x i = 0}. The 0 -quasinorm of x, denoted x 0 , is defined to be the number of non-zero entries of x, i.e. x 0 = card(supp(x)). We say that x is k-sparse if x 0 ≤ k. We use x h(k) to denote a k-sparse vector in C N consisting of the k largest (in absolute value) entries of x with all other entries zero. For example, if x = [4, 5, −9, 1] T then x h(2) = [0, 5, −9, 0] T . Note that x h(k) may not be uniquely defined. In contexts where a unique meaning for x h(k) is needed, we can choose x h(k) out of all possible candidates according to a predefined rule (such as the lexicographic order). We also define x t(k) = x − x h(k) . If x = [x 1 , x 2 ] T ∈ C 2n with x 1 , x 2 ∈ C n , and if x 1 is k-sparse and x 2 is t-sparse, then x is called (k, t)-sparse. We define x h(k,t) = [(x 1 ) h(k) , (x 2 ) h(t) ] T , which is a (k, t)-sparse vector in C 2n .

Section Title: THEORY
  THEORY

Section Title: COMPRESSIVE RECOVERY DEFENSE (CRD)
  COMPRESSIVE RECOVERY DEFENSE (CRD) Bafna et al. (2018) introduced a framework for defending machine learning classifiers against 0 - attacks. We extend the framework to 2 and ∞ attacks. The defense framework is based on the theory of compressive sensing, so we call it Compressive Recovery Defense (CRD). We explain the idea behind the CRD framework in the context of an image classifier. Suppose x ∈ C n is a (flattened) image vector we wish to classify. But suppose an adversary perturbs x with a noise vector e ∈ C n . We observe y = x + e, while x and e are unknown to us. Let F ∈ C n×n be the Discrete Fourier Transform (DFT) matrix. The Fourier coefficients of x arex = F x. It is well-known that natural images are approximately sparse in the frequency domain. So we expect thatx is approximately sparse, meaning thatx t(k) is small for some small k. We can write y = F −1x + e (1) If e 2 ≤ η or e ∞ ≤ η, with η small (as in a 2 or ∞ -attack), then we can use an appropriate sparse recovery algorithm with y and F −1 as input to compute a good approximation x # tox. Precise error bounds are given in Section 3.3. Then, since F is unitary, F −1 x # will be a good approximation (i.e., reconstruction) of x = F −1x . So we can feed F −1 x # into the classifier and expect to get the same classification as we would have for x. For an 0 -attack where e is t-sparse, the approach is only slightly different. We set A = [F −1 , I] and write so that [x h(k) , e] T is (k, t)-sparse. This structure lets us use a sparse recovery algorithm to compute a good approximation tox, as before. Note that the same idea can be applied with audio signals or other types of data instead of images. Moreover, the DFT can be replaced by any unitary transfor- mation F for whichx = F x is approximately sparse. For example, F may be the Cosine Transform, Sine Transform, Hadamard Transform, or another wavelet transform. We now describe the training and testing procedure for CRD. For each training image x, we computê x h(k) = (F x) h(k) , and then compute the compressed the image x = F −1x h(k) . We then add both x and x to the training set and train the network in the usual way. Given a (potentially adversarial) test image y, we first use a sparse recovery algorithm to compute an approximation x # tox, then we compute the reconstructed image y = F −1 x # and feed it into the network for classification.

Section Title: RECOVERY ALGORITHMS
  RECOVERY ALGORITHMS We provide the recovery algorithms used in this section. For 0 -attacks, we set A = [F −1 , I] as in (2). Against 2 or ∞ -attacks, we take A = F −1 as in (1). The IHT algorithm above is used to defend against 0 -norm attacks. For such attacks, according to (2), the vector we need to recover is (k, t)-sparse. Thus this IHT is adapted to the structure of our problem as it uses the thresholding operation h (k,t) that produces (k, t)-sparse vectors. This structured IHT was first considered in Baraniuk et al. (2010). It gives better theoretical guarantees and practical performance in our CRD application than the standard IHT, which would instead use the thresholding operation h (k+t) that produces (k + t)-sparse vectors. For 2 or ∞ attacks, the recovery error for IHT would (in general) be larger due to the need to include a term for the 2 norm of the tail of the noise vector e. This, in turn, produces worse expected performance of the recovery defense. Therefore we only use Algorithm 1 for 0 -norm attacks. We note that the results of Theorem 1 allow for values of k and t greater than or equal to Theorem 2.2. of Bafna et al. (2018). We utilize BP for 0 and 2 norm attacks. In the 0 norm case, BP allows us to provide recovery guarantees for larger values of k and t than IHT. For instance, in the case of MNIST and Fashion- MNIST, IHT (equation (4) of Theorem 1) allows us to set k = 4 and t = 3, whereas BP (equation (7) of Theorem 2) allows us to set k = 8 and t = 8. In the case of 2 norm attacks, BP is applied with A = F −1 , a unitary matrix. As unitary matrices are isometries in 2 norm, BP provides good recovery guarantees for such matrices, and hence against 2 norm attacks. Algorithm 3: Modified Dantzig Selector (DS). We utilize DS for ∞ norm attacks. The standard Dantzig Selector algorithm does not have the additional constraint Az − y ∞ ≤ η. Our modified Dantzig Selector includes this constraint for the following reason. In our application, A = F −1 and we want the reconstruction Ax # = F −1 x # to be close to the original image x, so that they are classified identically. Thus, we want to the search space for x # to be restricted to those z ∈ C N such that Az − x ∞ is small. Note, for any z ∈ C N , Az − x ∞ ≤ Az − y ∞ + x − y ∞ . In an ∞ -attack, x − y ∞ = e ∞ is already small. Thus it suffices to require Az − y ∞ is small. We experimentally illustrate the improvement in reconstruction due to the additional constraint in Section 4.3 ( Figure 4 ,  Table 4 ). Remarks on Reverse-Engineered Attacks. As observed in Bafna et al. (2018), x [0] in Algorithm 1, can be initialized randomly to defend against a reverse-engineered attack. In the case of Algorithm 2 and Algorithm 3, the minimization problems can be posed as semi-definite programming prob- lems. If solved with interior point methods, one can use random initialization of the central path parameter and add randomness to the stopping criterion. This makes recovery non-deterministic and consequently non-trivial to create a reverse-engineered attack.

Section Title: RECOVERY GUARANTEES
  RECOVERY GUARANTEES Let F ∈ C n×n be a unitary matrix and I ∈ C n×n be the identity matrix. Define A = [F, I] ∈ C n×2n and let y = A[x, e] T = Fx + e, wherex, e ∈ C n . Let 1 ≤ k, t ≤ n be integers. Moreover for any 0 < < 1 and any T ≥ log(1/ )+log( √ x h(k) 2 2 + e 2 2 ) log(1/ρ) , we get: Moreover for any 0 < < 1 and any T ≥ log(1/ )+log( x h(k) 2) log(1/ρ) , we get: Let us explain how to interpret the recovery guarantees provided by Theorem 1. The inequalities (3), (4), (5), (6) provide an upper bound on the size of x [T +1] −x h(k) 2 . Since F is a unitary matrix, x [T +1] −x h(k) 2 equals Fx [T +1] − Fx h(k) 2 , which is the difference between the reconstructed image Fx [T +1] and the compressed image Fx h(k) (which is a compressed version of the original image x). So the inequalities of Theorem 1 tell us how close the reconstructed image must be to the compressed image, and thus indicates how confident we should be that the classification of the reconstructed image will agree with the classification of the compressed image. In other words, the inequalities tell us how likely it is that the CRD scheme using IHT will be able to recover the correct class of the original image, and thus defend the classifier from the adversarial attack. The presence of the norm of the tailx t(k) in the upper bounds indicates that the CRD scheme should be more effective when the original image is closer to being perfectly k-sparse in the transformed basis. The ratio kt/n in the upper bounds (via ρ and τ ) suggests that smaller values of k and t relative to n (i.e., sparser transformed imagesx and error vectors e) will lead to the CRD being more effective. The experiments in Section 4 will demonstrate these phenomena. Let us compare Theorem 1 to the similar Theorem 2.2 of Bafna et al. (2018). We observe that (3) and (4) allow larger values of k and t than Theorem 2.2 of Bafna et al. (2018). This is because the authors of Bafna et al. (2018) prove their results using Theorem 4 of Baraniuk et al. (2010), which is more restrictive for the values of k and t. We do not use Theorem 4 of Baraniuk et al. (2010). Instead we use (a modified form of) Theorem 6.18 of Foucart & Rauhut (2017) to get (3) and (4). Both Theorem 4 of Baraniuk et al. (2010) (used by Bafna et al. (2018)) and Theorem 6.18 of Foucart & Rauhut (2017) (used by us here) take as input the Restricted Isometry Property (RIP) stated in Theorem 7. We and Bafna et al. (2018) both essentially prove the same RIP, although the proof methods are different. We use a standard Gershgorin disc theorem argument to bound eigenvalues, while Bafna et al. (2018) perform a direct estimation using the triangle inequality and AM-GM inequality. We turn now to (5) and (6), which provide recovery guarantees for larger values of k and t than (3) and (4), at the expense of the extra error term e 2 . Our proof of (5) and (6) is novel. It relies on explicitly expanding one iteration of IHT in matrix form and using the structure of the resulting matrix form to bound the approximation error at iteration T in terms of the error at iteration T − 2. We then use an inductive argument as in Theorem 6.18 of Foucart & Rauhut (2017) to get (5) and (6). Next, we consider the recovery error for 0 -norm bounded noise with BP instead of IHT. We note that since Algorithm 2 is not adapted to the (k, t)-sparse structure of vector to be recovered, we do not expect the guarantees to be particularly strong. However, providing bounds for BP is useful as there are cases when BP provides recovery guarantees for when recovering a larger number of coefficients (k) and a larger 0 noise budget (t) than IHT. Note that the recovery error in (7) is O(( √ k + t) x t(k) 2 ), which means that we should not expect recovery to be close when the attacker has a large 0 noise budget or whenx is not sparse. Also observe that the recovered vectorx # is not necessarily k-sparse. The recovery error still captures the difference in the original image Fx and the reconstructed image Fx # , where a smaller recovery error should once again indicate that our classifier would make the correct prediction. Our third result covers the case when the noise is bounded in 2 -norm. Theorem 3 ( 2 -norm BP). If e 2 ≤ η, then for x # = BP(y, F, η), we have the error bound Finally, we provide recovery guarantees when the noise is bounded in ∞ -norm. Theorem 4 ( ∞ -norm DS). If e ∞ ≤ η, then for x # = DS(y, F, η), we have the error bound The proofs of Theorem 3 and Theorem 4 are based on standard arguments in compressive sensing that rely on establishing the so-called robust null space property of the matrix. Note that the results of Theorem 3 and Theorem 4 also bound the norm difference of the original image Fx and the reconstructed image Fx # , wherex # has no sparsity guarantees. Next, observe that the results of Theorem 4 incur a factor of √ n in the error bounds due to the constraint A * (Az − y) ∞ ≤ √ nη in Algorithm 3 which is required to prove the robust null space property. Finally, we note that the additional constraint added to Algorithm 3 does not affect the proof of Theorem 4.

Section Title: RELATED WORK
  RELATED WORK The authors of Bafna et al. (2018) introduced the CRD framework which inspired this work. In fact, Theorem 2.2 of Bafna et al. (2018) also provides an approximation error bound for recovery via IHT. Note that a hypothesis t = O(n/k) has accidentally been dropped from their Theorem 2.2, though it appears in their Lemma 3.6. By making the implied constants explicit in the argument of Bafna et al. (2018), one sees that their Theorem 2.2 is essentially the same as (3) and (4) in Theorem 1 above. For more details, see the proof of Theorem 1 in Appendix A. Note that our recovery error bounds for IHT in (5) and (6) of Theorem 1 do not have analogs in Bafna et al. (2018). They hold for larger values of k and t at the expense of the additional error term e 2 . Other works that provide guarantees include (Hein & Andriushchenko (2017)) and (Cisse et al. (2017)) where the authors frame the problem as one of regularizing the Lipschitz constant of a network and give a lower bound on the norm of the perturbation required to change the classifier decision. The authors of Sinha et al. (2017) use robust optimization to perturb the training data and provide a training procedure that updates parameters based on worst case perturbations. A similar approach to (Sinha et al. (2017)) is (Wong & Kolter (2017)) in which the authors use robust optimization to provide lower bounds on the norm of adversarial perturbations on the training data. In Lecuyer et al. (2018), the authors use techniques from Differential Privacy (Dwork et al. (2014))in order to augment the training procedure of the classifier to improve robustness to adversarial inputs. Another approach using randomization is Li et al. (2018) in which the authors add i.i.d. Gaussian noise to the input and provide guarantees of maintaining classifier predictions as long as the 2 -norm of the attack vector is bounded by a function that depends on the output of the classifier. Most defenses against adversarial inputs do not come with theoretical guarantees. Instead, a large body of research has focused on finding practical ways to improve robustness to adversarial inputs by either augmenting the training data (Goodfellow et al. (2015)), using adversarial inputs from various networks (Tramèr et al. (2017)), or by reducing the dimensionality of the input (Xu et al. (2017)). For instance, Madry et al. (2017) use robust optimization to make the network robust to worst case adversarial perturbations on the training data. However, the effectiveness of their approach is determined by the amount and quality of training data available and its similarity to the distribution of the test data. An approach similar to ours but without any theoretical guarantees is (Samangouei et al. (2018)). In this work, the authors use Generative Adversarial Networks (GANs) to estimate the distribution of the training data and during inference, use a GAN to reconstruct a non-adversarial input that is most similar to a given test input. We now provide a brief overview on the field of compressive sensing. Though some component ideas originated earlier in other fields, the field of compressive sensing was initiated with the work of Candès et al. (2006) and Donoho et al. (2006) in which the authors studied the problem of reconstructing sparse signals using only a small number of measurements with the choice of a random matrix. The reconstruction was performed using 1 -minimization (i.e., Basis Pursuit) which was shown to produce sparse solutions even in presence of noise; see also Donoho & Elad (2003; 2006); Donoho & Huo (2001). Some of the earlier work in extending com- pressive sensing to perform stable recovery with deterministic matrices was done by Candes & Tao (2005) and Candes et al. (2006), where the authors showed that recovery of sparse vectors could be performed as long as the measurement matrix satisfied a restricted isometry hypothesis. Blumensath & Davies (2009) introduced IHT as an algorithm to recover sparse signals which was later modified in Baraniuk et al. (2010) to reduce the search space as long as the sparsity was structured. The standard DS algorithm was introduced in Candes et al. (2007) in order to perform stable recovery in the presence of ∞ noise.

Section Title: EXPERIMENTS
  EXPERIMENTS All of our experiments are conducted on CIFAR-10 (Krizhevsky (2009)), MNIST (LeCun), and Fashion-MNIST (Xiao et al. (2017)) datasets with pixel values of each image normalized to lie in [0, 1]. Each experiment is conducted on a set of 1000 points sampled uniformly at random from the test set of the respective dataset. For every experiment, we use the Discrete Cosine Transform (DCT) and the Inverse Discrete Cosine Transform (IDCT) denoted by the matrices F ∈ R n×n and F T ∈ R n×n respectively. That is, for an adversarial image y ∈ R √ n× √ n , such that, y = x+e, we let x = F x, and x = F Tx , where x,x ∈ R n and e ∈ R n is the noise vector. For an adversarial image y ∈ R √ n× √ n×c , that contains c channels, we perform recovery on each channel independently by considering y m = x m + e m , wherex m = F x m , x m = F Tx m for m = 1, . . . , c. The value k denotes the number of largest (in absolute value) DCT coefficients used for reconstruction of each channel, and the value t denotes the 0 noise budget for each channel. We implement Algorithm 2 and Algorithm 3 using the open source library CVXPY (Diamond & Boyd (2016)). We now outline the neural network architectures used for experiments in Section 4.1 and 4.2. For CIFAR-10, we use the network architecture of He et al. (2016) while the network architecture for MNIST and Fashion-MNIST datasets is provided in Table 5 of the Appendix. We train our networks using the Adam optimizer for CIFAR-10 and the AdaDelta optimizer for MNIST and Fashion- MNIST. In both cases, we use a cross-entropy loss function. We train the each neural network according to the CRD framework stated in Section 3.1. The code to reproduce our experiments is available here: https://github.com/anonymousiclrcompressive/iclr2020.

Section Title: DEFENSE AGAINST 0 -NORM ATTACKS
  DEFENSE AGAINST 0 -NORM ATTACKS This section is organized as follows: first we examine CRD against the One Pixel Attack (OPA) (Su et al. (2019)) for CIFAR-10. We only test the attack on CIFAR-10 as it is most effective against natural images and does not work well on MNIST or FASHION-MNIST. We note that this attack satisfies the theoretical constraints for t provided in Theorem 1, hence allowing us to test how well CRD works within existing guarantees. Once we establish the effectiveness of CRD against OPA, we then test it against two other 0 -norm bounded attacks: Carlini and Wagner (CW) 0 -norm attack (Carlini & Wagner (2017)) and the Jacobian based Saliency Map Attack (JSMA) (Papernot et al. (2016)).

Section Title: ONE PIXEL ATTACK
  ONE PIXEL ATTACK We first resize all CIFAR-10 images to 125 × 125 × 3 while maintaining aspect ratios to ensure that the data falls under the hypotheses of Theorem 1 even for large values of k. The OPA attack perturbs exactly one pixel of the image, leading to an 0 noise budget of t = 3 per image. The 0 noise budget of t = 3 per image allows us to use k = 275 per channel.  Table 1  shows that OPA is very effective against natural images and forces the network to mis-classify all previously correctly classified inputs. We test the performance of CRD in two ways: a) reconstruction quality b) network performance on reconstructed images. In order to analyse the reconstruction quality of Algorithm 1, we do the following: for each test image, we use OPA to perturb the image and then use Algorithm 1 to approximate its largest (in absolute value) k = 275 DCT co-efficients. We then perform the IDCT on these recovered co-efficients to generate reconstructed images. We illustrate reconstruction on a randomly selected image from the test set in  Figure 1 . Noting that Algorithm 1 leads to high quality reconstruction, we now test whether network accuracy improves on these reconstructed images. To do so, we feed these reconstructed images as input to the network and report its accuracy in  Table 1 . We note that network performance does indeed improve as network accuracy goes from 0.0% to 71.8% using Algorithm 1. Therefore, we conclude that CRD provides a substantial improvement in accuracy in against OPA.

Section Title: CW- 0 ATTACK AND JSMA
  CW- 0 ATTACK AND JSMA Having established the effectiveness of CRD against OPA, we move onto the CW 0 -norm attack and JSMA. We note that even when t is much larger than the hypotheses of Theorem 1 and Theorem 2, we find that Algorithms 1 and 2 are still able to defend the network. We hypothesize that this maybe related to the behavior of the RIP of a matrix for "most" vectors as opposed to the RIP for all vectors, and leave a more rigorous analysis for a follow up work. We follow the procedure described in Section 4.1.1 to analyze the quality of reconstructions for Algorithm 1 and Algorithm 2 in  Fig 2 . In each case it can be seen that both algorithms provide high quality reconstructions for values of t that are well outside the hypotheses required by Theorem 1 and Theorem 2. We report these t values and the improvement in network performance on reconstructed adversarial images using CRD in  Table 2 .

Section Title: DEFENSE AGAINST 2 -NORM ATTACKS
  DEFENSE AGAINST 2 -NORM ATTACKS In the case of 2 -norm bounded attacks, we use the CW 2 -norm attack (Carlini & Wagner (2017)) and the Deepfool attack (Moosavi-Dezfooli et al. (2016)) as they have been shown to be the most powerful. We note that Theorem 3 does not impose any restrictions on k or t and therefore the guarantees of equations (8) and (9) are applicable for recovery in all experiments of this section. The reconstruction quality is shown in  Figure 3 . It can be noted that reconstruction using Algorithm 2 is of high quality for all three datasets. In order to check whether this high quality reconstruction also leads to improved performance in network accuracy, we test each network on reconstructed images using Algorithm 2. We report the results in  Table 3  and note that Algorithm 2 provides a substantial improvement in network accuracy for each dataset and each attack method used.

Section Title: DEFENSE AGAINST ∞ -NORM ATTACKS
  DEFENSE AGAINST ∞ -NORM ATTACKS For ∞ -norm bounded attacks, we use the BIM attack (Kurakin et al. (2016)) as it is has been shown to be very effective and also allows us to control the ∞ -norm of the attack vector explicitly. We note that while the CW ∞ -norm attack (Carlini & Wagner (2017)) has the ability to create attack vectors with ∞ -norm less than or equal to BIM, it is computationally expensive and also does not allow one to pre-specify a value for the ∞ -norm of an attack vector. Therefore, we limit our experimental analysis to the BIM attack. Note that for any attack vector e, e 2 ≤ √ n e ∞ hence allowing ∞ - norm attacks to create attack vectors with large 2 -norm. Therefore, we could expect reconstruction quality and network accuracy to be lower when compared to 2 -norm attacks. In  figure 4 , we compare the reconstruction quality of images reconstructed with Algorithm 3 to those reconstructed using DS without the additional constraint. As can be noted from the figure, images reconstructed using DS without the additional constraint may not produce meaningful images. This is also reflected in  Table 4 , which shows that the accuracy of the network is roughly random on images reconstructed without the additional constraint. We show examples of original images, adversarial images, and their reconstructions using Algorithm 3 in  Figure 5 . Finally, we report the network performance on reconstructed inputs using Algorithm 3 in  Table 4  and also compare this to the performance on inputs reconstructed using DS without the additional constraint. We note that Algorithm 3 provides an increase in network performance against reconstructed adversarial inputs. However, the improvement in performance is not as substantial as it was against 0 or 2 -norm attacks.

Section Title: CONCLUSION
  CONCLUSION We provided recovery guarantees for corrupted signals in the case of 0 -norm, 2 -norm, and ∞ - norm bounded noise. We were able to utilize these results in CRD and improve the performance of neural networks substantially in the case of 0 -norm, 2 -norm and ∞ -norm bounded noise. While 0 -norm attacks don't always satisfy the constraints required by Theorem 1 and Theorem 2, we showed that CRD is still able to provide a good defense for values of t much larger than allowed in the guarantees. The guarantees of Theorem 3 and Theorem 4 were applicable in all experiments and CRD was shown to improve network performance for all attacks.

```
