Title:
```
Under review as a conference paper at ICLR 2020 VARIATIONAL INFORMATION BOTTLENECK FOR UN- SUPERVISED CLUSTERING: DEEP GAUSSIAN MIX- TURE EMBEDDING
```
Abstract:
```
In this paper, we develop an unsupervised generative clustering framework that combines variational information bottleneck and the Gaussian Mixture Model. Specifically, in our approach we use the variational information bottleneck method and model the latent space as a mixture of Gaussians. We derive a bound on the cost function of our model that generalizes the evidence lower bound (ELBO); and provide a variational inference type algorithm that allows to compute it. In the algorithm, the coders' mappings are parametrized using neural networks and the bound is approximated by Markov sampling and optimized with stochastic gradient descent. Numerical results on real datasets are provided to support the efficiency of our method.
```

Figures/Tables Captions:
```
Figure 1: Variational Information Bottleneck with Gaussian Mixtures.
Figure 3: Generative Network
Figure 4: Accuracy vs. number of epochs for the STL-10 dataset.
Figure 5: Information plane for the STL-10 dataset.
Figure 6: Visualization of the latent space before training; and after 1, 5 and 500 epochs.
Table 1: Comparison of clustering accuracy of various algorithms.
```

Main Content:
```

Section Title: INTRODUCTION
  INTRODUCTION Clustering consists in partitioning a given data set into various groups (clusters) based on some similarity metric, such as Euclidean distance, L 1 norm, L 2 norm, L ∞ norm, the popular logarithmic loss measure or others. The principle is that each cluster should contain elements of the data that are closer to each other than to any other element outside that cluster, in the sense of the defined similarity measure. If the joint distribution of the clusters and data is not known, one should operate blindly in doing so, i.e., using only the data elements at hand; and the approach is called unsupervised clustering. Unsupervised clustering is perhaps one of the most important tasks of unsupervised machine learning algorithms nowadays, due to a variety of application needs and connections with other problems. Examples of unsupervised clustering algorithms include the so-popular K-means ( Hartigan & Wong, 1979 ) and expectation maximization (EM) ( Dempster et al., 1977 ). The K-means algorithm partitions the data in a manner that the Euclidean distance among the members of each cluster is minimized. With the EM algorithm, the underlying assumption is that the data comprises a mixture of Gaussian samples, namely a Gaussian Mixture Model (GMM); and one estimates the parameters of each com- ponent of the GMM while simultaneously associating each data sample to one of those components. Although they offer some advantages in the context of clustering, these algorithms suffer from some strong limitations. For example, it is well known that the K-means is highly sensitive to both the order of the data and scaling; and the obtained accuracy depends strongly on the initial seeds (in addition to that it does not predict the number of clusters or K-value). The EM algorithm suffers mainly from low convergence, especially for high dimensional data. Recently, a new approach has emerged that seeks to perform inference on a transformed domain (generally referred to as latent space), not the data itself. The rationale is that because the latent space often has fewer dimensions it is more convenient computationally to perform inference (clustering) on it rather than on the high dimensional data directly. A key aspect then is how to design a latent space that is amenable to accurate low-complex unsupervised clustering, i.e., one that preserves only those features of the observed high dimensional data that are useful for clustering while removing out all redundant or non-relevant information. Along this line of work, we can mention ( Ding & He, 2004 ) which utilizes Principal Component Analysis (PCA) ( Wold et al., 1987 ) for dimensionality reduction followed by K-means for clustering the obtained reduced dimension data; or (Roweis, 1997) which uses a combination of PCA and the EM algorithm. Other works that use alternatives Under review as a conference paper at ICLR 2020 for the linear PCA include Kernel PCA ( Hofmann et al., 2008 ), which employs PCA in a non-linear fashion to maximize variance in the data. The usage of deep neural networks (DNN) for unsupervised clustering of high dimensional data on a lower dimensional latent space has attracted considerable attention, especially with the advent of autoencoder (AE) learning and the development of powerful tools to train them using standard backpropagation techniques ( Kingma & Welling, 2014 ;  Rezende et al., 2014 ). Advanced forms include Variational autoencoders (VAE) ( Kingma & Welling, 2014 ;  Rezende et al., 2014 ) which are generative variants of AE that regularize the structure of the latent space and the more general Variational Information Bottleneck (VIB) of (Alemi et al., 2017) which is a technique that is based on the Information Bottleneck method ( Tishby et al., 1999 ) and seeks a better trade-off between accuracy and regularization than VAE via the introduction of a Lagrange-type parameter s which controls that trade-off and whose optimization is similar to deterministic annealing ( Slonim, 2002 ) or stochastic relaxation. In this paper, we develop an unsupervised generative clustering framework that combines VIB and the Gaussian Mixture Model. Specifically, in our approach we use the variational information bottleneck method and model the latent space as a mixture of Gaussians. The encoder and decoder of the model are parametrized using neural networks (NN). The cost-function is calculated approximatively by Markov sampling and optimized with stochastic gradient descent. Furthermore, the application of our algorithm to the unsupervised clustering of various datasets, including the MNIST ( Lecun et al., 1998 ), REUTERS ( Lewis et al., 2004 ) and STL-10 ( Coates et al., 2011 ), allows a better clustering accuracy than previous state of the art algorithms. For instance, we show that our algorithm performs better than the variational deep embedding (VaDE) algorithm of ( Jiang et al., 2017 ) which is based on VAE and performs clustering by maximizes the ELBO and can be seen as a specific case of our algorithm (Section 3.1). Our algorithm also generalizes the VIB of (Alemi et al., 2017) which models the latent space as an isotropic Gaussian which is generally not expressive enough for the purpose of unsupervised clustering. Other related works, but which are of lesser relevance to the contribution of this paper, are the deep embedded clustering (DEC) of ( Xie et al., 2016 ), the improved deep embedded clustering (IDEC) of ( Guo et al., 2017 ) and ( Dilokthanakul et al., 2017 ). For a detailed survey of clustering with deep learning, the readers may refer to ( Min et al., 2018 ). To the best of our knowledge, our algorithm performs the best in terms of clustering accuracy by using deep neural networks without any prior knowledge regarding the labels (except the usual assumption regarding the number of the classes) compared to the state-of-the-art algorithms of this category. In order to achieve the aforementioned accuracy, i) we derive a cost-function that contains the IB hyper parameter s that controls the trade-off between over-fit and generalization of the model and we used an approximation of KL divergence that avoid assumptions which do not hold in the beginning of the learning process and lead to convergence issues; ii) evaluate the hyper-parameter s by following an annealing approach that improves both the convergence and the accuracy of the proposed algorithm.

Section Title: PROBLEM DEFINITION AND MODEL
  PROBLEM DEFINITION AND MODEL Consider a dataset that is composed of N samples {x i } N i=1 which we wish to partition into |C| ≥ 1 clusters. Let C = {1, . . . , |C|} be the set of all possible clusters; and C designate a categorical random variable that lies in C and stands for the index of the actual cluster. If X is a random variable that models elements of the dataset, given X = x i induces a probability distribution on C which the learner should learn. Thus, mathematically the problem is that of estimating the values of the Under review as a conference paper at ICLR 2020 unknown conditional probability P C|X (·|x i ) for all elements x i of the dataset. The estimates are sometimes referred to as the assignment probabilities. As mentioned previously, we use the VIB framework and model the latent space as a GMM. The resulting model is depicted in  Figure 1 , where the parameters π c , µ c , Σ c , for all values of c ∈ C, are to be optimized jointly with those of the employed NNs as instantiation of the coders. Also, the assignment probabilities are estimated based on the values of latent space vector instead of the observation themselves, i.e., P C|U = Q C|U . In the rest of this section, we elaborate on the inference and generative network models for our method, which are illustrated below.

Section Title: INFERENCE NETWORK MODEL
  INFERENCE NETWORK MODEL We assume that an observed data x is generated from a GMM with |C| components. Then, the latent representation u is inferred according the following procedure: 1. One of the components of the GMM is chosen according to a categorical variable C. 2. The data x is generated from the c-th competent of the GMM, i.e., P X|C ∼ N (x;μ c ,Σ c ). 3. Encoder maps x to a latent representation u according to P U|X ∼ N (µ θ , Σ θ ). 3.1. The encoder is modeled with a DNN f θ which maps x to the parameters of a Gaussian distribution, i.e., [µ θ , Σ θ ] = f θ (x). 3.2. The representation u is sampled from N (µ θ , Σ θ ). For the inference network, shown in Figure 2, the following Markov chain holds

Section Title: GENERATIVE NETWORK MODEL
  GENERATIVE NETWORK MODEL Since encoder extracts useful representations of the dataset and we assume that the dataset is generated from a GMM, we model our latent space also with a mixture of Gaussians. To do so, the categorical variable C is embedded with the latent variable U. The reconstruction of the dataset is generated according to the following procedure: 1. One of the components of the GMM is chosen according to a categorical variable C, with a prior distribution Q C . 2. The representation u is generated from the c-th component, i.e., Q U|C ∼ N (u; µ c , Σ c ). 3. The decoder maps the latent representation u tox which is the reconstruction of the source x by using the mapping Q X|U . 3.1. The decoder is modeled with a DNN g φ , that maps u to the estimatex, i.e., [x] = g φ (u). For the generative network, shown in  Figure 3 , the following Markov chain holds

Section Title: PROPOSED METHOD
  PROPOSED METHOD In this section we present our clustering method. First, we provide a general cost function for the problem of the unsupervised clustering that we study here based on the variational IB framework; and we show that it generalizes the ELBO bound developed in ( Jiang et al., 2017 ). We then parametrize our model using NNs whose parameters are optimized jointly with those of the GMM. Furthermore, we discuss the influence of the hyper-parameter s that controls optimal trade-offs between accuracy and regularization. As described in Chapter 2, the stochastic encoder P U|X maps the observed data x to a representation u. Similarly, the stochastic decoder Q X|U assigns an estimatex of x based on the vector u. As per the IB method ( Tishby et al., 1999 ) a suitable representation U should strike the right balance between capturing all information about the categorical variable C that is contained in the observation X and using the most concise representation for it. This leads to maximizing the following Lagrange problem L s (P) = I(C; U) − sI(X; U) , (3) where s ≥ 0 designates the Lagrange multiplier and for convenience P denotes the conditional distribution P U|X . Instead of equation 3 which is not always computable in our unsupervised clustering setting, we use a modified version of it (so-called unsupervised IB objective (Alemi et al., 2017)) given bỹ For a variational distribution Q U on U (instead of the unknown P U ) and a variational stochastic decoder Q X|U (instead of the unknown optimal decoder P X|U ), let Q := {Q X|U , Q U }. Also, let In addition, there exists a unique Q that achieves the maximum max Q L VB s (P, Q) =L s (P), and is given by Using Lemma 1, maximization of equation 4 can be written in term of the variational IB cost as follows max P L s (P) = max P max Q L VB s (P, Q) . (7) Remark 1. As we already mentioned in the beginning of this chapter, the related work ( Jiang et al., 2017 ) performs unsupervised clustering by combining VAE with GMM. Specifically, it maximizes the following ELBO bound Let, for an arbitrary non-negative parameter s, L VaDE s be a generalization of the ELBO bound in equation 8 of ( Jiang et al., 2017 ) given by Investigating the RHS of equation 9, we get Thus, by the non-negativity of relative entropy it is clear that L VaDE s is a lower bound on L VB s (P, Q). Also, if variational distribution Q is such that the conditional marginal Q C|U is equal to P C|X the bound is tight since the relative entropy term is zero in this case.

Section Title: PROPOSED ALGORITHM: VIB-GMM
  PROPOSED ALGORITHM: VIB-GMM In order to compute equation 7, we parametrize the distributions P U|X and Q X|U using DNNs. For instance, let the stochastic encoder P U|X be a DNN f θ and the stochastic decoder Q X|U be a DNN g φ . That is Under review as a conference paper at ICLR 2020 where θ and φ are the weight and bias parameters of the DNNs. Furthermore, the latent space is modeled as a GMM with |C| components with parameters ψ := {π c , µ c , Σ c } |C| c=1 , i.e., Using the parametrizations above, the optimization of equation 7 can be rewritten as max θ,φ,ψ L NN s (θ, φ, ψ) (13) where the cost function L NN s (θ, φ, ψ) given by Then, for a given observations of N samples, i.e., {x i } N i=1 , equation 13 can be approximated in terms of an empirical cost as follows max θ,φ,ψ 1 n n i=1 L emp s,i (θ, φ, ψ) , (15) where L emp s,i (θ, φ, ψ) is the empirical cost for the i-th observation x i , and given by Furthermore, the first term of the RHS of equation 16 can be computed using Monte Carlo sampling and the re-parametrization trick ( Kingma & Welling, 2014 ). In particular, P θ (u|x) can be sampled by first sampling a random variable Z with distribution P Z , i.e., P Z = N (0, I), then transforming the samples using some functionf θ : X × Z → U, i.e., u =f θ (x, z). Thus, where M is the number of samples for the Monte Carlo sampling step. The second term of the RHS of equation 16 is the KL divergence between a single component multivariate Gaussian and a Gaussian Mixture Model with |C| components. An exact closed-form solution for the calculation of this term does not exist. However, a variational lower bound approxi- mation ( Hershey & Olsen, 2007 ) of it can be obtained as In particular, in the specific case in which the covariance matrices are diagonal, i.e., Σ θ,i := diag({σ 2 θ,i,j } nu j=1 ) and Σ c := diag({σ 2 c,j } nu j=1 ), with n u denoting the latent space dimension, equa- tion 17 can be computed as follows where µ θ,i,j and σ 2 θ,i,j are the mean and variance of the i-th representation in the j-th dimension of the latent space. Furthermore, µ c,j and σ 2 c,j represent the mean and variance of the c-th component of the GMM in the j-th dimension of the latent space. Finally, we train NNs to maximize the cost function equation 14 over the parameters θ, φ, as well as those ψ of the GMM. For the training step, we use the ADAM optimization tool ( Kingma & Ba, 2015 ). The training procedure is detailed in Algorithm 1.

Section Title: Under review as a conference paper at ICLR 2020
  Under review as a conference paper at ICLR 2020 Once our model is trained, we assign the given dataset into the clusters. As mentioned in Section 2, we do the assignment from the latent representations, i.e., Q C|U = P C|X . Hence, the probability that the observed data x i belongs to the c-th cluster is computed as follows where indicates optimal values of the parameters as found at the end of the training phase. Finally, the right cluster is picked based on the largest assignment probability value. For the selected mini-batch, compute gradients of the empirical cost equation 15. until s does not exceed s max . As we already mentioned, the hyper-parameter s controls the trade-off between the relevance of the representation U and its complexity. As it can be seen from equation 14 for small val- ues of s, it is the cross-entropy term that dom- inates, i.e., the algorithm trains the parameters so as to reproduce X as accurate as possible. For large values of s, however, it is most im- portant for the NN to produce an encoded ver- sion of X whose distribution matches the prior distribution of the latent space, i.e., the term D KL (P θ (U|X) Q ψ (U)) is nearly zero. In the beginning of the training process, the GMM components are randomly selected; and so starting with a large value of the hyper-parameter s is likely to steer the solution towards an irrelevant prior. Hence, for the tunning of the hyper-parameter s in practice it is more efficient to start with a small value of s and gradually increase it with the number of epochs. This has the advantage to avoid possible local minimas, an aspect that is reminiscent of deterministic annealing ( Slonim, 2002 ), where s plays the role of the temperature parameter. The experiments that will be reported in the next section show that proceeding in the above described manner for the selection of the parameter s helps getting better accuracy results and better robustness to the initialization (i.e., no need for a strong pretraining). A pseudo-code for annealing is given in Algorithm 2. We note that tuning s is very critical, such that the step size s in update of s should be chosen carefully, otherwise phase transitions might be skipped that would cause a bad ACC score.

Section Title: EXPERIMENTS
  EXPERIMENTS

Section Title: DESCRIPTION OF USED DATASETS
  DESCRIPTION OF USED DATASETS In our empirical experiments, we apply our algorithm to the clustering of the following datasets. Under review as a conference paper at ICLR 2020 MNIST: A dataset of gray-scale images of 70000 handwritten digits of dimensions 28 × 28 pixel. STL-10: A dataset of color images collected from 10 categories. Each category consists of 1300 images of size of 96 × 96 (pixels) ×3 (rgb code). Hence, the original input dimension n x is 27648. For this dataset, we use a pretrained convolutional NN model, i.e., ResNet-50 ( He et al., 2016 ) to reduce the dimensionality of the input. This preprocessing reduces the input dimension to 2048. Then, our algorithm and other baselines are used for clustering. REUTERS10K: A dataset that is composed of 810000 English stories labeled with a category tree. As in ( Xie et al., 2016 ), 4 root categories (corporate/industrial, government/social, markets, economics) are selected as labels and all documents with multiple labels are discarded. Then, tf-idf features are computed on the 2000 most frequently occurring words. Finally, 10000 samples are taken randomly, which are referred to as REUTERS10K dataset.

Section Title: NETWORK SETTINGS AND OTHER PARAMETERS
  NETWORK SETTINGS AND OTHER PARAMETERS We use the following network architecture: the encoder is modeled with NNs with 3 hidden layers with dimensions n x −500−500−2000−J, where n x is the input dimension and n u is the dimension of the latent space. The decoder consists of NNs with dimensions n u − 2000 − 500 − 500 − n x . All layers are fully connected. For comparison purposes, we chose the architecture of the hidden layers as well as the dimension of the latent space n u = 10 to coincide with those made for the DEC algorithm of ( Xie et al., 2016 ) and the VaDE algorithm of ( Jiang et al., 2017 ). All except the last layers of the encoder and decoder are activated with ReLU function. For the last (i.e., latent) layer of the encoder we use a linear activation; and for the last (i.e., output) layer of the decoder we use sigmoid function for MNIST and linear activation for the remaining datasets. The batch size is 100 and the variational bound equation 15 is maximized by the Adam optimizer of ( Kingma & Ba, 2015 ). The learning rate is initialized with 0.002 and decreased gradually every 20 epochs with a decay rate of 0.9 until it reaches a small value (0.0005 is our experiments). The reconstruction loss is calculated by using the cross-entropy criterion for MNIST and mean squared error function for the other datasets.

Section Title: CLUSTERING ACCURACY
  CLUSTERING ACCURACY We evaluate the performance of our algorithm in terms of the so-called unsupervised clustering accuracy (ACC), which is a widely used metric in the context of unsupervised learning ( Min et al., 2018 ). For comparison purposes, we also present those of algorithms from previous art. For each of the aforementioned datasets, we run our VIB-GMM algorithm for various values of the hyper-parameter s inside an interval [s min , s max ], starting from the smaller valuer s 1 and gradually increasing the value of s every n epoch epochs. For the MNIST dataset, we set (s min , s max , n epoch ) = (1, 5, 500); and for the STL-10 dataset and the REUTERS10K datset we choose these parameters to be (1, 20, 500) and (1, 5, 100), respectively. The obtained ACC accuracy results are reported in the  Table 1  from which it can be seen that our algorithm outperforms significantly the DEC algorithm of ( Xie et al., 2016 ) as well as the VaDE algorithm of ( Jiang et al., 2017 ) and GMM on the same datsets. Important to note, for the MNIST dataset the reported ACC accuracy of 96.2% using our VIB- GMM algorithm is obtained as the best case run out of ten times run all with random initializations. For instance, we do not use any pretrained values for the initialization of our algorithm in sharp contrast with the VaDE of ( Jiang et al., 2017 ) and the DEC of ( Xie et al., 2016 ). For the STL-10 dataset, none of the compared algorithms use a pretrained network except the intimal ResNet-50 for dimensionality reduction. For REUTERS10K, we used the same pretrain parameters as DEC and VaDE.  Figure 4  depicts the evolution of the ACC accuracy with iterations (number of epochs) for the four compared algorithms.  Figure 5  shows the evolution of the reconstruction loss of our VIB-GMM algorithm for the STL-10 dataset, as a function of simultaneously varying values of the hyper-parameter s and the number of epochs (recall that, as per-the described methodology, we start with s = s 1 and we increase its value gradually every n epoch = 500 epochs). As it can be seen from the figure, the few first epochs are spent Under review as a conference paper at ICLR 2020 almost entirely on reducing the reconstruction loss (i.e., a fitting phase) and most of the remaining epochs are spent in making the found representation more concise (i.e., smaller KL-divergence). This is reminiscent of the two-phase (fitting v.s. compression) that was observed for supervised learning using VIB in (Schwartz-Ziv & Tishby, 2017).

Section Title: VISUALIZATION ON THE LATENT SPACE
  VISUALIZATION ON THE LATENT SPACE In this section, we investigate the evolution of the unsupervised clustering of the STL-10 dataset on the latent space using our VIB-GMM algorithm. For this purpose, we find it convenient to visualize the latent space through application of the t-SNE algorithm of ( van der Maaten & Hinton, 2008 ) in order to generate meaningful representations in a two-dimensional space.  Figure 6  shows 4000 randomly chosen latent representations before the start of the training process and respectively after 1, 5 and 500 epochs. The shown points (with a · marker in the figure) represent latent representations of data samples whose labels are identical. Colors are used to distinguish between clusters. Crosses (with an x marker in the figure) correspond to the centroids of the clusters. More specifically, Figure 6-(a) shows the initial latent space before the training process. If the clustering is performed on the initial representations it allows ACC accuracy of as small as 10%, i.e., as bad as a random assignment. Figure 6-(b) shows the latent space after one epoch, from which a partition of some of the points starts to be already visible. With five epochs, that partitioning is significantly sharper and the associated clusters can be recognized easily. Observe, however, that the cluster centers seem still not to have converged. With 500 epochs, the ACC accuracy of our algorithm reaches %91.6 and the clusters and their centroids are neater as visible from Figure 6-(d). Then, L VB s (P, Q) is defined as follows

```
