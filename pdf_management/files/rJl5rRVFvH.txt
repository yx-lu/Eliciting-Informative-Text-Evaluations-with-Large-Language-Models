Title:
```
Under review as a conference paper at ICLR 2020 WAY OFF-POLICY BATCH DEEP REINFORCEMENT LEARNING OF HUMAN PREFERENCES IN DIALOG
```
Abstract:
```
Most deep reinforcement learning (RL) systems are not able to learn effectively from off-policy data, especially if they cannot explore online in the environment. This is a critical shortcoming for applying RL to real-world problems where col- lecting data is expensive, and models must be tested offline before being deployed to interact with the environment - e.g. systems that learn from human interaction. Thus, we develop a novel class of off-policy batch RL algorithms which use KL- control to penalize divergence from a pre-trained prior model of probable actions. This KL-constraint reduces extrapolation error, enabling effective offline learn- ing, without exploration, from a fixed batch of data. We also use dropout-based uncertainty estimates to lower bound the target Q-values as a more efficient alter- native to Double Q-Learning. This Way Off-Policy (WOP) algorithm is tested on both traditional RL tasks from OpenAI Gym, and on the problem of open-domain dialog generation; a challenging reinforcement learning problem with a 20,000 dimensional action space. WOP allows for the extraction of multiple different re- ward functions post-hoc from collected human interaction data, and can learn ef- fectively from all of these. We test real-world generalization by deploying dialog models live to converse with humans in an open-domain setting, and demonstrate that WOP achieves significant improvements over state-of-the-art prior methods in batch deep RL.
```

Figures/Tables Captions:
```
Figure 1: In this example batch RL problem, the robot's goal is to travel the minimum distance around the black walls to get to the red flag. A trained behavior policy generated the batch data; the probability of each of the states appearing in the batch, p B (s), is in yellow (white locations are not contained in the batch). If the offline RL policy estimates the value of going up or left from the start position is high, it will have no way to refine this estimate using the batch data, or learn a good policy in this region of state space. The KL-constraint ensures that the RL policy will stay within the support of the batch data. However, the behavior policy is suboptimal, so using behavior cloning to directly imitate the batch data will result in suboptimal return. Instead, the KL-constrained model can learn to find the optimal policy, which is within the support of the batch.
Figure 2: Comparison of batch RL algorithms for different offline learning conditions. WOP con- sistently exceeds the performance of Batch Q-learning, Behavioral Cloning (BC), DBCQ, and the Behavior policy used to generate the batch data. Error bars show 95% CI of the mean over 50 trials.
Figure 3: KL-divergence of the policy from the prior is lower with KL-control throughout training. Bands show σ.
Figure 4: Z-scored reward. Red metrics were used in training rewards, green are post- hoc. Traditional RL methods like Batch Q exploit simple action-based rewards, like asking questions. In contrast, KL-control methods shift their distribution towards po- lite, supportive, and cheerful conversation, allowing them to elicit higher human reward (blue).
Table 1: Interactive human evaluation of batch RL techniques. KL-control models strongly outper- form other techniques. Ratings are Likert scale, votes and human reward are z-scores.
Table 2: Purely reward-maximizing methods like Batch Q (left) diverge away from realistic language (saying phrases like "where did you say to me?") in order to trivially exploit the reward function by asking a question every turn, and using the maximum number of tokens in every sentence. In contrast, KL-control methods (right) output plausible language by staying close to the prior, but shift to using polite, cheerful language to maximize implicit human reward.
Table 3: Interactive human evaluation of different reward functions (models trained with WOP).
```

Main Content:
```

Section Title: INTRODUCTION
  INTRODUCTION In order to scale deep reinforcement learning (RL) to safety-critical, real-world domains, two abil- ities are needed. First, since collecting real-world interaction data can be expensive and time- consuming, algorithms must be able to learn from off-policy data no matter how it was generated, or how little correlation between the data distribution and the current policy. Second, it is often necessary to carefully test a policy before deploying it to the real world; for example, to ensure its behavior is safe and appropriate for humans. Thus, the algorithm must be able to learn offline first, from a static batch of data, without the ability to explore. This off-policy, batch reinforcement learning (BRL) setting represents a challenging RL problem. Most deep RL algorithms fail to learn from data that is not heavily correlated with the current policy ( Fujimoto et al., 2018b ). Even models based on off-policy algorithms like Q-learning fail to learn in the offline, batch setting, when the model is not able to explore. If the batch data is not sufficient to cover the state-action space, BRL models can suffer from extrapolation error, learning unrealistic value estimates of state-action pairs not contained in the batch ( Fujimoto et al., 2018b ). It can be impossible to correct for extrapolation error when there is a mismatch in the distribution of state- actions pairs in the batch data, and the distribution induced by the learned policy. For example, if the policy learns to select actions which are not contained in the batch, it cannot learn a reasonable value function for those actions.  Figure 1  illustrates this concept, where the batch only covers a subset of possible policies. Extrapolation error is particularly problematic in high-dimensional state and action spaces (such as those inherent in language generation). We propose to resolve these issues by leveraging a pre-trained generative model of the state-action space, p(a|s), trained on known sequences of interaction data. While training with RL, we penalize divergence from this prior model with different forms of KL-control. This technique ensures that the RL model learns a policy that stays close the state-action distribution of the batch, combating Under review as a conference paper at ICLR 2020 extrapolation error. We also propose using dropout to obtain uncertainty estimates of the target Q- values, and use this lower bound to alleviate overestimation bias. We benchmark against a discrete adaptation of Batch Constrained Q-learning (BCQ) ( Fujimoto et al., 2018b ), a recently proposed state-of-the-art BRL algorithm for continuous domains, and show that our Way Off-Policy algorithm achieves superior performance in both a traditional RL domain, as well as in a challenging, under- explored, real-world reinforcement learning problem: using implicitly expressed human reactions in chat to improve open-domain dialog systems. When a machine learning system interacts with humans, ideally we would like to learn about the humans' preferences in order to improve its performance. Yet having humans manually indicate their preferences through explicit means like pressing a button (e.g.  Christiano et al. (2017) ) or submitting a feedback report, does not scale. Instead, we would like to be able to use humans' implicit reactions, such as the sentiment they express, or the length of the conversation, in order to improve the policy. However, applying off-policy batch RL to language generation is challenging because the number of potential combinations of words and sentences leads to a combinatorial explosion in the size of the state space. The action space - the set of frequent vocabulary words in the English language - is 20,000-dimensional. This compounds extrapolation error, making BRL even more difficult. However, when learning from human interactions in the wild, it is crucial to be able to learn offline and test the policy before deploying it, lest it learn inappropriate behaviors (e.g.  Horton (2016) ). To support this work, we developed an interactive online platform that allows humans to chat with deep neural network dialog models running on a GPU; the BRL models trained for this study are available live at https://neural.chat/rl/. Through this platform we collected human re- sponses to a set of over 40 different dialog models over the course of several months. Using our Way Off-Policy algorithm, we are able to effectively learn from this batch of data, in spite of the fact that it was generated with a vastly different set of model architectures, which were trained on different datasets. Further, we use the batch to learn from many different reward functions designed post-hoc to extract implicit human preferences, something that is only possible with effective off-policy BRL. In summary, the contributions of this paper are: • A novel algorithm, Way Off-Policy learning, which is the first to propose using KL-control from a pre-trained prior model as a way to reduce extrapolation error in batch RL. • Experiments showing the effectiveness of WOP above strong baselines based on prior work (e.g.  Fujimoto et al. (2018b) ), on both traditional RL tasks and on the challenging problem of open-domain dialog generation. • A set of novel conversation rewards based on how human preferences are implicitly ex- pressed in text. We are the first work to learn from implicit signals in conversation offline using batch RL.

Section Title: RELATED WORK
  RELATED WORK The approach we propose is based on KL-control, a branch of stochastic optimal control (SOC) ( Stengel, 1986 ) where the Kullback-Leibler (KL) divergence from some distribution is used to regularize an RL policy (e.g. ( Abdolmaleki et al., 2018 ;  Kappen et al., 2012 ;  Rawlik et al., 2012 ;  Todorov, 2007 )). Well-known examples include Trust Region Policy Optimization (TRPO) ( Schulman et al., 2015 ), and use conservative, KL-regularized policy updates to restrict the RL al- gorithm to stay close to its own prior policy (e.g. ( Haarnoja et al., 2018 ;  Kakade, 2002 ;  Peters et al., 2010 ;  Rawlik et al., 2012 )). KL-control can also be applied to entropy maximization (e.g. ( Ziebart et al., 2008 ;  Nachum et al., 2017 ;  Haarnoja et al., 2017 )); for example, G-learning penalizes KL- divergence from a simple uniform distribution in order to cope with overestimation of Q-values ( Fox et al., 2016 ).KL-control has also been used to improve transfer learning between maximum likelihood estimation (MLE) training on data, and training with RL ( Jaques et al., 2017 ). To the best of our knowledge, our work is the first to propose penalizing KL-divergence from a learned prior model of the state-action space as a way to improve offline batch RL. Other strategies to improve off-policy learning have been proposed, but differ from this work in key respects. Many focus on scenarios where the policy is able to explore and collect more data (e.g.  Degris et al. (2012) ;  Riedmiller (2005) ); for example, when learning online from an outdated replay buffer (e.g.  Munos et al. (2016) ). In contrast, we learn entirely offline, from a fixed batch of data, without the ability to explore. Methods proposed for this setting have often not been used in conjunction with modern function approximation techniques (e.g.  Thomas et al. (2015) ). Many other works focus on off-policy policy evaluation (rather than policy learning), for example using importance sampling or model estimation (e.g.  Farajtabar et al. (2018) ;  Jiang & Li (2016) ;  Precup (2000) ;  Thomas & Brunskill (2016) ). In the deep BRL setting,  Liu et al. (2019)  have proposed a correction to policy gradients,  Gelada & Bellemare (2019)  have proposed covariance-shift methods, and  Bhatt et al. (2019)  have proposed normalized feature representations. Kumar et al. (2019)  use maximum mean discrepancy to cope with extrapolation error in BRL, while  Agarwal et al. (2019)  use a Random Ensemble Mixture (REM) Q-network. Most similar to our work is Batch Constrained Q-learning (BCQ) ( Fujimoto et al., 2018b ), which tackles off-policy deep BRL in continuous action domains by training a generative model of the batch, p(a|s), sampling from this model, and selecting the best action based on a Q-estimate. Unlike our approach, this does not integrate information about the distribution p(a|s) directly into the policy, or allow the model to learn when to strategically deviate from the prior in order to obtain more reward. We propose using dropout to approximate model uncertainty of the target Q-network. The idea of using dropout to estimate uncertainty in neural networks was proposed by  Gal & Ghahramani (2016) . Different forms of uncertainty estimates have been used in RL (e.g.  Kahn et al. (2017) ;  Os- band et al. (2016) ); for example, Bayesian uncertainty estimates have been proposed as an alternative to double DQN ( Azizzadenesheli et al., 2018 ).

Section Title: RL FOR LANGUAGE GENERATION
  RL FOR LANGUAGE GENERATION Improving dialog systems with RL has largely been restricted to task-oriented dialog systems, which have a limited number of task-specific actions (e.g.  Fatemi et al. (2016) ;  Gašić et al. (2011) ;  Liu & Lane (2017) ;  Liu et al. (2018) ;  Su et al. (2017) ). These approaches may incorporate human input, usually through explicit, manual feedback (e.g.  Shah et al. (2018) ), but sometimes with more implicit signals, such as the user interrupting the system or starting over ( Shi & Yu, 2018 ). Efforts to expand RL to the open-domain dialog setting, such as those of  Li et al. (2016b ;  2017 ;  2018 ), are less numerous, and do not involve learning from human feedback. Even in the open-domain setting, authors may choose to use a highly restricted action space; for example, using RL to choose which scripted or MLE dialog model to invoke to answer a user's query ( Serban et al., 2017a ). Since the posting of the preprint of this paper,  Ziegler et al. (2019)  have used explicit human feed- back to improve the summarization and text continuation performance of a large-scale language model. Although they do not study dialog or the batch RL setting (instead learning online from a trained model of human feedback), they do make use of our proposal to penalize KL-divergence from a pre-trained language model, and find that this is important to achieving good performance.

Section Title: Under review as a conference paper at ICLR 2020
  Under review as a conference paper at ICLR 2020 Although implicit signals such as sentiment ( Hancock et al., 2019 ) and conversation length ( Zhou et al., 2018 ) have been used in MLE systems, the idea of using such signals as a reward for RL is relatively unexplored. Shin and colleagues uses on-policy learning in conjunction with a user- sentiment approximator to improve a seq2seq model ( Shin et al., 2019 ), but are unable to learn directly from user feedback. To the best of our knowledge, we are the first to use batch RL to train open-domain dialog models on implicit cues gained from real human interactions.

Section Title: METHODS
  METHODS We employ typical RL notation in which s t represents the environment state at time t, the agent takes action a t according to its policy π(a t |s t ), and receives reward r(s t , a t ). The agent's goal is to maximize reward over an episode trajectory τ , with a discount factor of γ applied to future rewards. Q-learning learns an action-value estimate of the total expected discounted future reward, Q π (a t , s t ) = E π [ T t =t γ t −t r(s t , a t )], through iterative updates based on the Bellman equation: In deep Q-learning ( Mnih et al., 2013 ), a Q-network approximates Q θπ (s t , a t ) and drives the policy π. A second target Q-network approximates the expected reward from the next state, Q θ T (s t+1 , a t+1 ) ( Van Hasselt et al., 2016 ).

Section Title: BATCH RL AND EXTRAPOLATION ERROR
  BATCH RL AND EXTRAPOLATION ERROR In batch RL, we are given a fixed batch of data B, and assume that no further interaction with the environment is possible. To train Q θπ , we sample (s t , a t , r t , s t+1 ) ∼ B, and update the weights of the Q-network to approximate Eq. 1. Because Q-learning is an off-policy algorithm, in principle it should be able to learn from data collected by any behavior policy. However, extrapolation error can occur if the BRL policy learns to favour a state-action pair (s, a) that is unlikely, or not contained, in the batch data. In this case, the estimate Q(s , π(s )) can be arbitrarily bad ( Fujimoto et al., 2018b ). Such errors can then accumulate through the Bellman backup operator ( Kumar et al., 2019 ). Experiments from  Fujimoto et al. (2018b)  show that extrapolation error can be highly detrimental to learning off-policy in BRL. These problems are compounded by the fact that algorithms based on the Bellman operator are inherently optimistic in the face of uncertainty. When value estimates for some region of the state- action space are noisy (because too few experience samples have been used to refine them), the maximum operation in Eq. 1 will lead to an overestimation of expected future reward. In a normal RL setting, this overestimation bias drives the model to explore areas of the state-action space for which the value estimates have the highest variance, thus enabling it to refine them; in essence, creating a built-in drive to explore. However, in a batch setting where exploration is not possible, the model is instead driven to value parts of the state-action space for which it has little to no data to learn a good policy (see  Figure 1 ).

Section Title: DROPOUT FOR UNCERTAINTY ESTIMATION OF TARGET Q-VALUES
  DROPOUT FOR UNCERTAINTY ESTIMATION OF TARGET Q-VALUES The overestimation of Q-values in the BRL setting necessitates other methods for estimating the future reward via the Target Q-network. Clipped Double Q-learning ( Fujimoto et al., 2018a ) main- tains two independent pairs of Q-networks, and takes the minimum of their estimates of future reward. This approach is computationally expensive and memory intensive. Instead, we leverage the fact that a network trained with dropout can be used to approximate a Bayesian uncertainty estimate of the output value ( Gal & Ghahramani, 2016 ). Given the target Q-network Q θ T , we com- pute Q(a t+1 , s t+1 ) using a Monte Carlo (MC) estimate of the lower-bound of Q θ T (a t+1 , s t+1 ) by running M stochastic forward passes of the network, each with a new dropout mask d i ∼ q W : Using the minimum operator penalizes high variance estimates and leads the algorithm to be pes- simistic in the face of uncertainty, rather than optimistic. Such a bias will push the model to favour actions that lead to states well covered by the batch data.

Section Title: DISCRETE BATCH CONSTRAINED Q
  DISCRETE BATCH CONSTRAINED Q Batch Constrained Q-learning (BCQ) ( Fujimoto et al., 2018b ) proposes to address the BRL problem by constraining the actions of the Q-network to be close to the data contained within the batch. This is accomplished by learning a generative model of the batch, G w = p(a|s), and sampling from this model during learning and inference. Because BCQ is designed for continuous action domains, it applies a learned perturbation model ξ(s, a; Φ) which is allowed to alter the action within the range [−Φ, Φ]. BCQ learns Q-estimates that incorporate the perturbation model, Q θ (s, a + ξ(s, a; Φ)). To act, n possible actions are sampled from the generative model, {a i ∼ G w (s)} n i=1 , perturbed, and the action with the maximum Q-value is selected, giving the BCQ policy: We propose an adaptation of BCQ to discrete action spaces (DBCQ) which does not use a continuous perturbation model. Since BCQ relies on Double Clipped Q-learning ( Fujimoto et al., 2018a ), here we use dropout-based uncertainty estimates as in Eq. 2. Thus the DBCQ policy is:

Section Title: KL-CONTROL FROM PRE-TRAINED PRIOR
  KL-CONTROL FROM PRE-TRAINED PRIOR Rather than simply sample from the prior, we would like the Q-learning algorithm to directly incor- porate the prior into the policy. Thus, we use KL-control to penalize divergence between the learned prior p(a|s), and the Q-network policy π θ , while still maximizing reward. Given a trajectory of ac- tions, τ = {a 1 , a 2 , ...a t−1 }, let q(τ ) = T t=1 π θ (a t , s t ) be the policy of our Q-learning algorithm at the trajectory level. Similarly, let p(τ ) = T t=1 p(a t |s t ) be the prior distribution over the trajectory, and r(τ ) be the return. We seek to maximize the following KL-regularized objective: Since D KL [q||p] = x q(x)(log q(x) − log p(x)), we can see that this is equivalent to maximizing the following expected value function of the policy π θ at the action level: The two terms introduced in Eq. 6 have clear motivations. The p(a|s) term rewards the model for choosing actions that have high probability under the prior, biasing the model to state-action pairs that are likely to be in the batch. The − log π(a|s) term is analogous to entropy regularization. Maintaining diversity in the action space through entropy regularization is important for generative models like dialog systems, which are known to collapse to an uninteresting, small number of re- peated samples ( Li et al., 2016a ). Re-stating Eq. 6 as an entropy-regularized Q-function, we obtain: One can derive a soft version of the entropy-regularized Q-function that uses a Boltzmann distri- bution to estimate future reward ( Haarnoja et al., 2017 ). We refer to it as a Ψ-function following previous work ( Jaques et al., 2017 ), which derived this function as a generalization of the Ψ-learning proposed by ( Rawlik et al., 2012 ). The optimal Ψ-function and policy are: Because it avoids taking a hard max over noisy estimates, Ψ-learning leads to less overestimation of future reward ( Abdolmaleki et al., 2018 ;  Haarnoja et al., 2017 ). This improves learning through more stable temporal-difference (TD) updates. Thus, it may be especially useful in the BRL setting for reducing optimism in the face of uncertainty. The Way Off-Policy (WOP) algorithm combines Monte Carlo (MC) target estimation, Ψ-learning, and KL-control from a pre-trained prior.

Section Title: TRADITIONAL RL EXPERIMENTS
  TRADITIONAL RL EXPERIMENTS To demonstrate the effectiveness of these techniques, we conduct a series of experiments in tra- ditional RL tasks using the OpenAI gym ( Brockman et al., 2016 ). Here we show results for the CartPole-v0 environment; more results are available in the Appendix. We first train an online Q- learning Behavior policy, and store all (s, a, r, s ) experience samples into a replay buffer. We use this buffer to train a prior model of p(a|s) using a Variational Auto-encoder (VAE) (details in Ap- pendix). This model is used as a part of both the DBCQ and WOP algorithms. We can use the prior for imitation learning, by sampling actions directly from p(a|s) to obtain Behavioral Cloning (BC). We benchmark all of these techniques against vanilla Q-learning on the batch data (Batch Q). We experiment with four different conditions which vary the quality of the Behavior policy and the replay buffer data: a) Full buffer: all experience samples experienced during online training are used for offline learning; b) Concurrent: the offline learning algorithms see a sliding window of experience samples in the same order that the online learner experienced them; c) Expert demon- strator: the buffer only contains experience generated by a fully trained online learner; and d) Noisy demonstrator: the online learner has a high probability of acting randomly ( = 0.3) and is thus a bad model of the optimal policy.  Figure 2  shows the results. Across conditions, we see that WOP is able to outperform Batch Q, imitation learning (BC), DBCQ, and the original behavior policy. As expected, Imitation learning (BC) underperforms other techniques when the batch contains noisy or inexpert experience samples. However, when the batch contains only expert trajectories, Batch Q fails to learn, because the batch does not cover the full state-action space well, increasing extrapolation error (as illustrated in  Figure 1 ). DBCQ matches or outperforms BC and Batch Q in all scenarios. However, because DBCQ acts by sampling from p(a|s) as learned by the BC model, its performance suffers when the batch data is noisy or imperfect. In contrast, WOP is able to learn to trade-off staying close to the prior and obtaining higher reward, and consistently outperforms all other algorithms in this environment.

Section Title: BATCH RL FOR LEARNING DIALOG FROM HUMAN FEEDBACK
  BATCH RL FOR LEARNING DIALOG FROM HUMAN FEEDBACK Here, we tackle the problem of training an open-domain dialog model from human feedback. We consider human interaction to represent the 'environment'. The response of a human to the bot's utterance is used to compute a reward signal to train the model. The state is the conversation history, composed of a series of conversation turns or utterances, u 1...t , where each utterance is composed of vocabulary tokens. The model attempts to construct a response utterance u π t+1 = [a 1 , a 2 , ..., a n ] by iteratively choosing an action a i as the next token. Applying RL to dialog generation is challenging due to the large state-action space. The number of tokens in the vocabulary of our pre-trained model is 20,000, making the action space very high-dimensional; this further compounds the problem of extrapolation error. We trained over 40 dialog models with different architectures (e.g.  Serban et al. (2017b) ), on dif- ferent datasets, generating models that varied significantly in terms of the distribution of language they learned. We deployed these models to users via a web server that hosts neural network dialog models on GPU for fast, real-time inference: https://neural.chat. The code for the models and the server is available in open-source at <redacted>. Using the server, we collected a batch Under review as a conference paper at ICLR 2020 of human interaction data containing 14232 pairs of user input and agent response. Because learn- ing language online from humans on the internet can result in inappropriate behavior (see  Horton (2016) ), learning offline using BRL is imperative. The batch data was used to train the RL models as described in Section 3. Here, we use a pre-trained language model to estimate p(a|s). We also initialize the weights of the Q-network and target Q-network are from the pre-trained model, to combat extrapolation error. The trained RL models were then re-deployed to the web. We recruited 90 Mechanical Turk workers to provide a total of 718 7-point Likert scale ratings of the bots' quality, fluency, diversity, contingency (relatedness), and empathy, after interacting with each bot for at least 3 turns. Participants also had the option to provide explicit feedback through upvoting or downvoting a particular utterance within the interface. We sum these manual votes to create an overall votes score. We note that using this platform to test our models "in the wild" with humans represents a more meaningful test of generalization than testing an RL model in the same limited (game) environment in which it was trained, since humans are not restricted in the text they can type as input to the model.

Section Title: LEARNING FROM IMPLICIT HUMAN PREFERENCES
  LEARNING FROM IMPLICIT HUMAN PREFERENCES We seek to improve a dialog model's ability to engage in natural conversation with a human by learning from the signals implicit in the way that the human responds. Rather than having the human manually label good performance - which we show in this work does not scale - the agent should recognize informative cues within the user's responses, like sentiment, and the amount of time they spend chatting. Essentially, we want to create an agent that is intrinsically motivated to produce positive reactions in its human conversation partner. To this end, we reward the model for: 1) eliciting positive sentiment, 2) eliciting longer conversations and more words typed (a sign of engagement), 3) eliciting laughter (in the form of typed 'ha's), 4) high semantic similarity between the human input and bot response, and 5) asking questions, since this is an important active listening skill ( Bodie et al., 2012 ). The total reward given to the agent is a combination of these, with details (and coefficients) in the Appendix. Note that the first 4 types of rewards depend on eliciting positive responses from a human user; we call these the implicit human reward. The 5th reward is easily exploitable by the agent itself. These rewards were designed and extracted post-hoc from the batch of human data, and thus learning from them is only possible with effective batch RL, since they had no effect on the policies used to generate the batch.

Section Title: DIALOG RESULTS
  DIALOG RESULTS To compare models, we not only look at human users' ratings and votes, but also consider the automatic signals detectable from the text itself. This implicit human reward metric aggregates the measures listed in items 1-4 in Section 5.1, and measures the ability to elicit positive responses from a human.  Table 1  shows the results of the human evaluation, comparing WOP to ablations of itself, Batch Q, and DBCQ. MC Target Q estimation leads to modest improvements in votes and human reward, but does not improve ratings. Using Ψ-learning improves all three. However, the most notable difference in performance comes from KL-control. The KL-control models show substantial gains over the baseline models across both ratings and human reward. We perform a one- way analysis of variance (ANOVA) comparing the KL-control models to the Batch Q baselines and DBCQ on the total human rating score, and find that the KL-control models are significantly better, F (x) = 4.781, p < .05. This validates the hypothesis that KL-control with a strong, pre-trained prior can be used to improve batch RL. As shown in  Figure 3 , without KL-regularization, the baseline RL models diverge quickly and continuously from the prior, losing information about realistic se- quences. This helps explain the poor performance of DBCQ in  Table 1 . The underlying Q-network in DBCQ does not directly integrate the prior. As Q-learning causes the model to diverge from the prior, the Q-estimates of language generated according to the prior become unreal- istic, and Eq. 4 selects unrealistic actions. This results in highly 'diverse' (random) generated utterances. Although DBCQ performed well in simple domains in Section 4, it does not scale effectively to dialog. The pre-trained prior may be especially important in a generative domain like dialog, where the true re- ward function is unknown, and so purely maximiz- ing a heuristic reward may lead to lower quality con- versations.  Table 2  shows examples of conversations with a Batch Q and KL-control model. Because the Batch Q model has no incentive to stay close to real- istic language, it learns to exploit the reward by ask- ing a question and outputting the maximum number of tokens (30) every utterance. These sentences con- tain implausible phrases that do not represent realis- tic language (e.g. "where did you say to me?"). In contrast, the KL-control model uses fluent language, but shifts its distribution towards cheerful and polite speech, presumably because this is what led to posi- tive human responses in the batch data. In fact, we noticed that all models trained with the implicit human rewards described in Section 5.1 learned to use more cheerful and supportive lan- guage. Therefore, we create post-hoc metrics to measure this effect (see the Appendix for details).  Figure 4  shows how these metrics, as well as the im- plicit rewards, differ across models. Without KL- control, baseline methods like Batch Q exploit sim- ple rewards like asking questions at the expense of realistic language, explaining their poor quality rat- ings. In contrast, KL-control models learn to rely more on realistic but polite, supportive, and cheerful dialog to elicit higher total human reward.  Table 3  presents the results of WOP models trained with only a single reward function, ordered from lowest to highest quality. Notably, extracting multiple different reward functions post-hoc from a batch of data and training on these independently is only possible with effective BRL. Investigating which rewards presented are most critical to achieving high-quality conversations with humans, we note that maximizing positive and minimizing negative sentiment in the user turns out to lead Under review as a conference paper at ICLR 2020

Section Title: CONCLUSION
  CONCLUSION This paper presents the Way Off-Policy (WOP) algorithm, which improves performance when learn- ing off-policy without the possibility to explore - i.e. batch RL (BRL). We are the first to propose using KL-control from a strong prior model pre-trained on data as a way to avoid extrapolation and instability in BRL. Our results on traditional RL tasks demonstrate that our WOP algorithm provides performance improvements over state-of-the-art BRL techniques, and the results in dialog generation show that KL-control is critical to achieving good performance in this real-world, high- dimensional setting. In a generative domain such as dialog, the true reward function is not known, and trivially exploiting the rewards can actually lead to worse performance. Thus, KL-control may be particularly necessary to ensure samples remain realistic and close to the data distribution. We propose several reward functions that could allow an open-domain dialog generation model to learn from rich cues implicit in human interaction, where learning from expressed sentiment was most promising. We find that maximizing implicit rewards leads to better performance than relying on explicit feedback. We hope that the techniques presented here can improve learning with RL from offline data, making it easier to apply RL to safety-critical settings such as human interaction.

```
