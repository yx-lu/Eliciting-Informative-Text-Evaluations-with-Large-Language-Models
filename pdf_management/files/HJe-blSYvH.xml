<article article-type="research-article"><front><article-meta><title-group><article-title /></title-group><abstract><p>We present an unsupervised method for learning speech representations based on a bidirectional contrastive predictive coding that implicitly discovers phonetic structure from large-scale corpora of unlabelled raw audio signals. The representa- tions, which we learn from up to 8000 hours of publicly accessible speech data, are evaluated by looking at their impact on the behaviour of supervised speech recognition systems. First, across a variety of datasets, we find that the features learned from the largest and most diverse pretraining dataset result in significant improvements over standard audio features as well as over features learned from smaller amounts of pretraining data. Second, they significantly improve sample efficiency in low-data scenarios. Finally, the features confer significant robustness advantages to the resulting recognition systems: we see significant improvements in out-of-domain transfer relative to baseline feature sets, and the features likewise provide improvements in four different low-resource African language datasets.</p></abstract></article-meta></front><body><sec><title>INTRODUCTION</title><p>The representation of the input to machine learning models strongly determines the difficulty faced by the learning algorithm, how much data the learner will require to find a good solution, and whether it will generalize out of sample and out of the domain of the training data. Representations (or features) that encode relevant information about data enable models to achieve good performance on downstream tasks, while representations that are invariant to factors that are not relevant to downstream tasks can further improve generalization and sample efficiency. Traditionally, many invariances were hard-coded in feature extraction methods. For example in image representations, geometric and photometric invariance has been investigated (<xref ref-type="bibr" rid="b32">Mundy et al., 1992</xref>; <xref ref-type="bibr" rid="b21">Van De Weijer et al., 2005</xref>). For acoustic representations, it is known that the standard MFCC features are sensitive to additive noise and many modifications have been proposed to overcome those limitations (<xref ref-type="bibr" rid="b7">Dev &amp; Bansal, 2010</xref>; <xref ref-type="bibr" rid="b28">Kumar et al., 2011</xref>).</p><p>In this paper, we investigate the potential of unsupervised representation learning for speech from large-scale, unlabelled and noisy data using bidirectional contrastive predictive coding (&#167;2-&#167;3). In previous work, <xref ref-type="bibr" rid="b21">van den Oord et al. (2018)</xref> showed that the unidirectional variant of this objective results in representations that are useful for frame-level phoneme classification (even with simple linear classifiers), but in this paper, we go further: we vastly increase the scale of the pretraining data to be larger and much noisier and more diverse, and we also evaluate the learned representations in a speech recognition (ASR) system, rather than on the less interpretable per-frame phone prediction.</p><p>In our experiments (&#167;4), we find that our acoustic representations improve ASR system performance relative to standard spectrogram and log-filterbank features, and also relative to features pretrained from smaller corpora that are more matched to the training/test domain. A previous attempt at integrating pretrained acoustic representations into an ASR system from <xref ref-type="bibr" rid="b36">Schneider et al. (2019)</xref> found that they were able to outperform mel-frequency features in both sample efficiency and overall performance, but their work used only clean, read English speech (from the LibriSpeech dataset) as pretraining data, and evaluated the features when used to recognize speech in a rather similar domain (read Wall Street Journal text). They found that increases in the amount of pretraining data led, in some cases, to diminished performance. However, we find the opposite: our best pretrained features are those learned from the largest and noisiest speech data, with performance far surpassing those obtained by pretraining only on the relatively clean LibriSpeech.</p></sec><sec><title>Under review as a conference paper at ICLR 2020</title><p>In addition to the standard ASR evaluation where the systems are evaluated on the same domain as they are trained on, we look at two further aspects of our features: their sample efficiency and whether their use confers robustness to domain shifts in the resulting system. Here again, we find compelling results. First, for a smaller speech recognition task (using a simpler convolutional architecture than is common), an ASR model trained with our representations only needs 10% of the labelled data to achieve the same result as a the same model trained on spectrogram features using 100% of the training data. Second, we evaluate the robustness of our representations by estimating how invariant they are to domain and language shifts. To do so, an ASR model is trained using our representations on one dataset but evaluated on the test sets of other datasets. In this experiment, we find that the representations derived from the large pretraining dataset lead the ASR model to be much more robust to domain shifts, compared to both log filterbank features as well as to pretraining just on LibriSpeech. Finally, we also evaluate the representations on four low-resource African languages (i.e. Amharic, Fongbe, Swahili, Wolof) and show that our representations significantly outperform both standard features and those pretrained only on clean English data.</p><p>In summary, we confirm several increasingly common patterns that may be discerned in the literature on unsupervised representation learning, across a variety of modalities. First, scale matters: good representation learning requires a large amount of data. Second, contextualized representations (that are sensitive to both past and future), are valuable. And finally, unsupervised representations consistently improve sample efficiency, performance, and robustness of downstream tasks.</p></sec><sec><title>CONTRASTIVE PREDICTIVE CODING: CPC</title><p>Unsupervised representation learning methods rely on differentiable objectives which quantify the degree to which representations have succeeded at capturing the relevant characteristics in data. Mutual information measures relationships between random variables (<xref ref-type="bibr" rid="b13">Fano &amp; Hawkins, 1961</xref>). Mutual information maximization techniques, that learn representations that describe data by maximizing mutual information between data and representation variables, have been explored for a long time in unsupervised representation learning (<xref ref-type="bibr" rid="b30">Linsker, 1988</xref>; <xref ref-type="bibr" rid="b4">Bell &amp; Sejnowski, 1995</xref>). However, since the exact computation of mutual information is not tractable for continuous variables, many estimators have been proposed for enabling unsupervised representation learning with neural networks (<xref ref-type="bibr" rid="b3">Belghazi et al., 2018</xref>; <xref ref-type="bibr" rid="b21">van den Oord et al., 2018</xref>; <xref ref-type="bibr" rid="b2">Hjelm et al., 2019</xref>).</p><p>Contrastive predictive coding (<xref ref-type="bibr" rid="b21">van den Oord et al., 2018</xref>, CPC) is a mutual information maximization method that has been successfully applied to many modalities such as images and speech (<xref ref-type="bibr" rid="b21">H&#233;naff et al., 2019</xref>; <xref ref-type="bibr" rid="b2">Bachman et al., 2019</xref>; <xref ref-type="bibr" rid="b36">Schneider et al., 2019</xref>). The objective is designed to extract features that allow the model to make long-term predictions about future observations. This is done by maximizing the mutual information of these features with those extracted from future timesteps. The intuition is that the representations capture different levels of structure dependent on how far ahead the model predicts. For example, if the model only predicts a few steps ahead, the resulting representations can capture local structures. On the other hand, if the model predicts further in the future, the representations will need to infer "slow features" (<xref ref-type="bibr" rid="b0">Wiskott &amp; Sejnowski, 2002</xref>); more global structures such as phonemes, words and utterances in speech.</p><p>The overall unsupervised learning process is visualized in <xref ref-type="fig" rid="fig_0">Figure. 1</xref>. Given a raw audio signal of length L (x = x 1 , x 2 , . . . , x L , x i &#8712; R where x i represents the acoustic amplitude at time i), a function g enc encodes the audio signals into vector representations (z = z 1 , z 2 . . . , z M , z &#8712; R dz ). Next, an auto-regressive function g ar , such as a recurrent neural network, summarizes the past representations and produces context vectors (c = c 1 , c 2 . . . , c M , c &#8712; R dc ). The representations are learned to maximize mutual information between context vectors (c) and future latent representations (z) as follows:</p><p>Since the mutual information is not tractable for high dimensional data, it is common to use a lower-bound on the mutual information such as InfoNCE (<xref ref-type="bibr" rid="b21">van den Oord et al., 2018</xref>) which is a loss function based on noise contrastive estimation (<xref ref-type="bibr" rid="b20">Gutmann &amp; Hyv&#228;rinen, 2010</xref>). Given a set Z = {z 1 , . . . z N } which contains one positive sample from p(z t+k |c t ) and N &#8722; 1 negative samples Under review as a conference paper at ICLR 2020 from a "noise" distribution p(z), the approximated lower-bound is written as I(c t , z t+k ) &#8805; E Z log f k (c t , z t+k ) 1 N z&#8712;Z f k (c t ,z) = L NCE tk , (1) where f k (c t , z t+k ) is a scoring function. We used the standard log-bilinear model as follows:</p><p>The loss function we maximize is a sum of the InfoNCE loss for each step, L NCE = t k L NCE tk and the negatives are uniformly sampled from representations in the same audio signal (z) and/or mini-batch.</p></sec><sec><title>METHODS</title><p>In this section, we describe our models and objectives for unsupervised representation learning and downstream speech recognition. First, an acoustic feature extractor is trained with a bidirectional variant of contrastive predictive coding (CPC) on an unlabeled audio dataset. Next, the parameters of this model are frozen and its output representations are used as input to train various speech recognition models, potentially on a different or smaller labeled dataset (<xref ref-type="fig" rid="fig_0">Figure 1</xref>).</p></sec><sec><title>UNSUPERVISED LEARNING WITH BI-DIRECTIONAL CONTRASTIVE PREDICTIVE CODING</title><p>Following the success of bidirectional models in representation learning (<xref ref-type="bibr" rid="b35">Peters et al., 2018</xref>; <xref ref-type="bibr" rid="b8">Devlin et al., 2019</xref>), we extend the original CPC method explained above with bidirectional context networks. The encoder function g enc is shared for both directions but there are two auto-regressive models (g fwd ar and g bwd ar ) which read encoded observations (z) from the forward and backward contexts, respectively. The forward and backward context representations c fwd t , c bwd t are learned with separate InfoNCE losses. When they are used for downstream tasks, a concatenation of two representations c t = [c fwd t ; c bwd t ] is used. A similar technique has been used in image representation learning where representations are learned along different spatial dimensions (<xref ref-type="bibr" rid="b21">H&#233;naff et al., 2019</xref>).</p><p>All audio signals have a sampling rate of 16kHz and we normalize the mean and variance of the input signals over each utterance in order to mitigate volume differences between samples. For architectures, we use encoder and auto-regressive models similar to <xref ref-type="bibr" rid="b36">Schneider et al. (2019)</xref>. The encoder function g enc , is a stack of causal convolutions with kernel sizes (10, 8, 4, 4, 4, 1, 1) and stride sizes (5, 4, 2, 2, 2, 1, 1), corresponding to a receptive field of 10 ms of audio. For auto-regressive functions, we use a 13 layer causal convolution architecture with kernel sizes (1, 2, . . . , 12, 13) and stride size 1, for both forward and backward functions. Layer-normalization across the temporal and Under review as a conference paper at ICLR 2020 feature dimensions is applied to every layer. Also, each layer has dense skip connections with layers below as in DenseNet (<xref ref-type="bibr" rid="b26">Huang et al., 2017</xref>). The objective function we optimize is the sum of the forward and backward InfoNCE losses (eq.1).</p></sec><sec><title>SEMI-SUPERVISED SPEECH RECOGNITION</title><p>Once the acoustic representations are trained, the resulting context vectors (c) are used as inputs to character-level speech recognition models which predict transcriptions of audio-signals character by character. The model first predicts frame-level character probabilities with a series of convolution layers while the CTC forward algorithm (<xref ref-type="bibr" rid="b19">Graves et al., 2006</xref>) calculates conditional probabilities of a transcription given an audio signal. The model parameters are trained to maximize likelihood of the data. The training is terminated when the word error rate on development set stops improving or the model is trained more than certain epochs. The models are evaluated on standard word error rate on test data. During training, the parameters in speech recognition models are trained with supervision but the parameters of the pretrained models are fixed. For decoding, we use greedy CTC decoding without a language model (LM) in order to focus on discerning the effects of the acoustic representations, although we also include results with a 4-gram LM to facilitate comparisons with published results.</p><p>Common practice in unsupervised representation learning is to evaluate learned representations using a linear classifier. However, we found a simple linear layer followed by a CTC decoder does not have enough capacity to recognize speech. Thus, for our first set of experiments we use a smaller version of DeepSpeech2 (<xref ref-type="bibr" rid="b1">Amodei et al., 2016</xref>) to predict the frame-level character probabilities. The model has two 2d-convolutions with kernel sizes (11, 41) and (11, 21) and stride sizes (2, 2) and (1, 2) and one recurrent neural network (GRU) on top of the output from the convolution layers. A linear transformation and a softmax function are applied to predict frame-level character probabilities. We refer to DeepSpeech2 small for the model specifics (<xref ref-type="bibr" rid="b1">Amodei et al., 2016</xref>). In order to further investigate how the representations interact with larger speech recognition models, we use the time-delay neural networks (TDNN) that are commonly used in speech recognition (<xref ref-type="bibr" rid="b6">Collobert et al., 2016</xref>; <xref ref-type="bibr" rid="b27">Kuchaiev et al., 2018</xref>). These consist of 17 layers of 1d-convolutions followed by 2 fully connected layers. Refer to OpenSeq2Seq for a detailed description. 1 These large models have enough capacity to learn to recognize speech from log-filterbank features with purely supervised learning on sufficient data, so they represent a challenging test case for learned representations.</p></sec><sec><title>EXPERIMENTS AND RESULTS</title></sec><sec><title>DATASET</title><p>We collected nine publicly available speech datasets which cover a variety of types of speech (e.g. read and spoken), noise conditions and languages. For unsupervised pretraining we used a combination of datasets, removing speech transcriptions for datasets that included those. For semi-supervised learning on top of the reprensenations we used the transcribed datasets following their standard train-test splits. A full overview of all datasets and their statistics can be found in Appendix A. Since we will ultimately compare ASR systems trained on one dataset but evaluated on the test set of another, we must ensure that the representation of the transcription is consistent. To do so, we use the format of the LibriSpeech dataset, which also ensure our results are comparable with standard speech recognition systems (<xref ref-type="bibr" rid="b27">Kuchaiev et al., 2018</xref>). For the other datasets, the transcriptions are lower-cased and unpronounced symbols are removed. We also removed examples that contain numbers as there are multiple ways for transcribing them.</p></sec><sec><title>Large-scale unlabeled speech</title><p>We collected large-scale unlabelled speech data from two existing datasets, a subset of Audio Set (Gemmeke et al., 2017) and the audio part of AVSpeech (<xref ref-type="bibr" rid="b12">Ephrat et al., 2018</xref>). Among many annotated acoustic events in Audio Set, we only used examples labeled as "speech." We also included the Common Voice (CV) 2 dataset in all the 29 languages available. Note Under review as a conference paper at ICLR 2020 that there are many examples with background noise (e.g. music, conversations etc.) and a variety of languages. We used an opportunistic strategy to gather our 8000h pretraining dataset, and it is conceivable that more careful curation of this dataset can be used to manipulate the properties of the learned representations, but we leave such considerations for future work.</p></sec><sec><title>Read English</title><p>In speech recognition research, it is common to train models on read English corpora such as LibriSpeech (<xref ref-type="bibr" rid="b33">Panayotov et al., 2015</xref>), the Wall Street Journal (<xref ref-type="bibr" rid="b34">Paul &amp; Baker, 1992</xref>) and TIMIT (<xref ref-type="bibr" rid="b14">Garofolo, 1993</xref>). We use standard train / test splits for speech recognition evaluation. We also use the Speech Accent Archive (<xref ref-type="bibr" rid="b26">Weinberger &amp; Kunath, 2009</xref>, SSA).</p></sec><sec><title>Spoken English</title><p>We also evaluate on conversational speech and public speaking datasets. Switch- board (<xref ref-type="bibr" rid="b18">Godfrey et al., 1992</xref>) is a standard conversational speech recognition dataset consisting of two-sided telephone conversations. Since the data was recorded more than 10 years ago, it is relatively noisy compared to recent English corpora. Tedlium-3 (<xref ref-type="bibr" rid="b22">Hernandez et al., 2018</xref>) is a large spoken English dataset which contains 450 hours of speech extracted from TED conference talks. The recordings are clear but there are some environmental noise (reverberation).</p></sec><sec><title>Low-resource languages</title><p>We use speech recognition datasets in four African languages collected in the ALFFA project 3 : Amharic (<xref ref-type="bibr" rid="b1">Tachbelie et al., 2014</xref>), Fongbe (<xref ref-type="bibr" rid="b0">A. A Laleye et al., 2016</xref>), Swahili (<xref ref-type="bibr" rid="b16">Gelas et al., 2012</xref>), Wolof (<xref ref-type="bibr" rid="b15">Gauthier et al., 2016</xref>) for evaluation. These languages have unique phonetic properties (e.g. height harmony) and phonemic inventories, making them a good contrast to English. These languages are low-resource, each with 20 hours or less of transcribed speech.</p></sec><sec><title>UNSUPERVISED REPRESENTATION LEARNING</title><p>We train the model described above (&#167;3.1) with a combination of the large-scale unlabelled data and for the speech recognition part we use the training portions of the transcribed speech recognition datasets (&#167;4.1). Similarly to <xref ref-type="bibr" rid="b36">Schneider et al. (2019)</xref>), audio signals are randomly cropped with a window size 149,600 observations (9.35 seconds) and encoded with the model. The bidirectional contrastive predictive coding objective (e.q.,1) with prediction steps (k) 12 and negatives (N ) 10 is optimized with the Adam optimizer with learning rate 0.0001. A batch size of 128 is used as well as a polynomial learning rate scheduler with power 2 and gradient clipping with maximum norm 5.0. Training was terminated at 4.2 million steps based on speech recognition performance on the development set of the LibriSpeech corpus.</p></sec><sec><title>SAMPLE EFFICIENCY</title><p>We evaluate the pretrained representations by looking at sample complexity on downstream speech recognition tasks. 10% and 100% portions of the Wall Street Journal and LibriSpeech dataset are used to train speech recognition models. We use small and large speech recognition models (DeepSpeech2 small, TDNN) to see how much information is contained in the representations. The speech recognition models are trained in the similar way as heavily tuned state-of-the-art models (<xref ref-type="bibr" rid="b6">Collobert et al., 2016</xref>; <xref ref-type="bibr" rid="b27">Kuchaiev et al., 2018</xref>). We refer to Appendix B for a detailed description of training configurations. <xref ref-type="table" rid="tab_0">Table 1</xref> shows results with DeepSpeech2 small models trained on different sizes of the Wall Street Journal corpus. Since the original DeepSpeech2 (<xref ref-type="bibr" rid="b1">Amodei et al., 2016</xref>) model uses spectrogram features, we use the same features to train our baselines. CPC-LibriSpeech and CPC-8k indicate representations are learned from LibriSpeech and 8000h of speech datasets listed above respectively. The ablation experiments show the effectiveness of bidirectional models. The bidirectional models obtain consistent improvements, and unsupervised pretraining with the larger uncurated noisy dataset also shows large improvements. We would like to highlight the comparison with a model trained on spectrogram features with 100% of training data and a model trained on learned representations with 10% of the training data. Interestingly, when a model is small (i.e. models need to rely more on the features since they are capable of less complex transformations of the signal), the learned representations can be as much as ten times more sample efficient compared to spectrogram features. Under review as a conference paper at ICLR 2020 <xref ref-type="table" rid="tab_1">Tables 2</xref> and 3 summarize the results with TDNN models trained on different sizes of the Wall Street Journal and LibriSpeech corpora. Following the well-tuned open source models (<xref ref-type="bibr" rid="b6">Collobert et al., 2016</xref>; <xref ref-type="bibr" rid="b27">Kuchaiev et al., 2018</xref>), our baseline is trained using log-filterbank features. Even if the speech recognition models have a large number of parameters and are trained on plenty of supervised data, the learned representations still provide significant improvements. The pattern continues to hold if we use beam search decoding with a language model. 4 Our + LM decoding results are comparable to the OpenSeq2Seq benchmark, since we used the exact same LM and decoding algorithm as they used (<xref ref-type="bibr" rid="b27">Kuchaiev et al., 2018</xref>).</p></sec><sec><title>ROBUSTNESS</title><p>Robustness to shifts in domain, recording conditions, and the intonation of speech is an important desideratum for a good ASR system, and we hypothesized that the diversity of our largest pretraining regime would improve robustness along these dimensions. In contrast, standard MFCC features have been tested in terms of noise robustness and it is known that such representations are sensitive to additive noise (<xref ref-type="bibr" rid="b0">Zhao &amp; Wang, 2013</xref>). Moreover, speech recognition systems developed on top of such features are not robust when they are evaluated on out-of-domain datasets (<xref ref-type="bibr" rid="b1">Amodei et al., 2016</xref>).</p><p>To test whether our pretraining approach improves robustness, we evaluate speech recognition models trained on the learned representations on many different datasets so as to investigate benefit of using the representations learned from large-scale data. We compare ASR systems on all of the Wall Street Journal and LibriSpeech corpora with the same optimization as explained above and evaluate word error rate on different evaluation sets, such as phone call conversations (Switchboard). <xref ref-type="table" rid="tab_3">Table 4</xref> summarizes the results on models trained on the Wall Street Journal, LibriSpeech or the Tedlium corpora and evaluated on different evaluation sets. The features trained on large-scale data consistently outperform other representations across different evaluation sets. The speech recognition models trained on the Wall Street Journal perform badly on phone call data in general. However, the representations learned on large datasets are more robust than the representations trained only on read English data (LibriSpeech).</p></sec><sec><title>LOW-RESOURCE LANGUAGE</title><p>Thus far, all our experiments have compared our representations in terms of their impacts on En- glish recognition tasks (although we know that the pretraining dataset contains samples from many languages). We now turn to the question of whether these representations are suitable for driving recognition different languages with substantially different phonetic properties than English has. Specifically, we look at the performance on four languages-Amharic, Fongbe, Swahili, and Wolof- which manifest a variety of interesting phonological properties that are quite different from English. Evaluating on such languages will provide insights into the phonetic space learned in the representa- tions 7 . Moreover, our non-English languages are low-resource in terms of speech recognition data, but have 2-20 million native speakers each. It is therefore valuable if the representations learned from large-scale unlabelled data can improve low-resource speech recognition. Although there is a small chance that the large-scale pretraining dataset may contain some examples from those languages, we did not add any extra data specifically to improve representations for those languages.</p></sec><sec><title>Under review as a conference paper at ICLR 2020</title><p>To test the cross-linguistic value of these features, we trained speech recognition models on low- resource languages (&#167;4.1) and compare how much difference we obtain from using standard features and the learned representations. As these are very small datasets, we trained DeepSpeech2 small models with the Adam optimizer with a fixed learning rate of 0.0002 and gradient clipping with maximum norm 25.0. Note that we did not tune architectures and learning methods for particular configurations so as to keep the comparisons fair. <xref ref-type="table" rid="tab_4">Table 5</xref> summarizes results. Again, we find that the representations trained on large-scale data outperforms other features by a large margin and that the models trained on the representations trained on (English-only) LibriSpeech do not perform as well as standard features. This suggests that the representations learned on large-scale data capture a phonetic space that generalizes across different languages.</p></sec><sec><title>RELATED WORK</title><p>Unsupervised learning of representations was an active area of research in deep neural networks. Representations learned with deep Boltzmann machines and auto-encoders trained to reconstruct inputs had initial successes in image and speech recognition (<xref ref-type="bibr" rid="b23">Hinton et al., 2006</xref>; <xref ref-type="bibr" rid="b3">Bengio et al., 2007</xref>; <xref ref-type="bibr" rid="b22">Vincent et al., 2010</xref>; <xref ref-type="bibr" rid="b23">Hinton et al., 2012</xref>). After a period of focusing on supervised techniques, unsupervised representation learning has recently seen a resurgence in a variety of modalities (Doersch &amp; Zisserman, 2017; <xref ref-type="bibr" rid="b21">van den Oord et al., 2018</xref>; <xref ref-type="bibr" rid="b10">Donahue &amp; Simonyan, 2019</xref>; <xref ref-type="bibr" rid="b2">Bachman et al., 2019</xref>) and has led to improved results, especially in low-data regimes (<xref ref-type="bibr" rid="b21">H&#233;naff et al., 2019</xref>; <xref ref-type="bibr" rid="b36">Schneider et al., 2019</xref>). In natural language processing, pretrained representations can outperform state-of-the-art system even in high data regimes (<xref ref-type="bibr" rid="b31">Mikolov et al., 2013</xref>; <xref ref-type="bibr" rid="b8">Devlin et al., 2019</xref>).</p><p>There have been considerable work on unsupervised speech representation learning. The most frequent aim is to learn representations that correspond to phonetic structure. In order to measure the quality of learned structures many evaluations have been proposed. The Zerospeech challenges explicitly look at correlations between learned representations and phonetic structures (<xref ref-type="bibr" rid="b11">Dunbar et al., 2019</xref>). <xref ref-type="bibr" rid="b21">van den Oord et al. (2018)</xref> used a frame-level phoneme classification task with a linear classifier to investigate how well the representations can discriminate phonemes. Although the task is easy to evaluate, the results do not correspond to existing downstream tasks. <xref ref-type="bibr" rid="b36">Schneider et al. (2019)</xref> applied learned representations to speech recognition and showed the effectiveness of learned representations. However, as we discussed in the paper, many important aspects such as invariances to domain shift and language shift were not carefully assessed.</p></sec><sec><title>CONCLUSION</title><p>We presented an unsupervised speech representation learning method that discovers acoustic repre- sentations from up to 8000 hours of publicly accessible speech data. We have shown, for the first time, that such pretrained representations lead speech recognition systems to be sample efficient and more robust to domain shifts compared to standard acoustic represenations, and compared to representations trained on smaller and more domain-narrow pretraining datasets. The representations are evaluated on a standard speech recognition setup where the models are trained and evaluated on in-domain data and also on a transfer tasks where the models are evaluated on out-of-domain data. We obtained consistent improvements on different English corpora as well as four low-resource African languages. This suggests we are making progress toward models that implicitly discovers phonetic structure from large-scale unlabelled audio signals.</p><p>Under review as a conference paper at ICLR 2020</p></sec><sec id="figures"><title>Figures</title><fig id="fig_0"><object-id>fig_0</object-id><label>Figure 1:</label><caption><title>Figure 1:</title><p>Left, unsupervised representation learning with forward contrastive predictive coding. The learned representations are fixed and used as inputs to a speech recognition model (Right).</p></caption><graphic /><graphic /><graphic /><graphic /><graphic /><graphic /><graphic /><graphic /><graphic /><graphic /><graphic /><graphic /><graphic /><graphic /><graphic /><graphic /><graphic /><graphic /><graphic /><graphic /><graphic /><graphic /><graphic /><graphic /><graphic /><graphic /><graphic /><graphic /><graphic /><graphic /><graphic /><graphic /><graphic /><graphic /><graphic /><graphic /><graphic /><graphic /><graphic /><graphic /><graphic /><graphic /><graphic /><graphic /><graphic /><graphic /><graphic /><graphic /><graphic /><graphic /><graphic /><graphic /><graphic /><graphic /><graphic /><graphic /><graphic /><graphic /><graphic /><graphic /><graphic /><graphic /><graphic /><graphic /><graphic /><graphic /><graphic /><graphic /><graphic /><graphic /><graphic /><graphic /><graphic /><graphic /><graphic /><graphic /><graphic /><graphic /><graphic /><graphic /><graphic /><graphic /><graphic /><graphic /></fig><table-wrap id="tab_0"><label>Table 1:</label><caption><title>Table 1:</title><p>Sample efficiency experiments with DeepSpeech2 small trained and evaluated on the Wall Street Journal. The numbers are word error rate on development and evaluation sets and the second row indicates the amount of training data used to train each model.</p></caption><table><tbody><tr><td /></tr></tbody></table></table-wrap><table-wrap id="tab_1"><label>Table 2:</label><caption><title>Table 2:</title><p>Sample efficiency experiments with the TDNN on the Wall Street Journal. The metrics are word error rate on the development and evaluation sets and the second row indicates the amount of training data.</p></caption><table><tbody><tr><td /></tr></tbody></table></table-wrap><table-wrap id="tab_2"><label>Table 3:</label><caption><title>Table 3:</title><p>Sample efficiency experiments with the TDNN trained and evaluated on LibriSpeech. The results are word error rate on the LibriSpeech development and evaluation sets. 10% vs. 100% indicates the amount of training data used. The section in + LM decoding contain results with beamsearch decoding with a 4-gram language model. The underlined (OpenSeq2Seq) scores are taken from public benchmarks.</p></caption><table><tbody><tr><td /></tr></tbody></table></table-wrap><table-wrap id="tab_3"><label>Table 4:</label><caption><title>Table 4:</title><p>Domain transfer experiments to test the robustness of the representations to domain shifts. The models are trained on the Wall Street Journal, LibriSpeech or Tedlium and evaluated on different evaluation sets.</p></caption><table><tbody><tr><td /></tr></tbody></table></table-wrap><table-wrap id="tab_4"><label>Table 5:</label><caption><title>Table 5:</title><p>Speech recognition on low-resource languages. Models are trained and evaluated on each language based on different features.</p></caption><table><tbody><tr><td /></tr></tbody></table></table-wrap></sec></body><back><sec><p>The universality of these representations is important since spectrograms, whatever their limitations, are language agnostic.</p></sec><ref-list id="ref-list-1"><ref id="b0"><element-citation publication-type="journal"><article-title>First automatic fongbe continuous speech recognition system: Development of acoustic models and language models</article-title><source>Proc. FedCSIS</source><year>2016</year><person-group person-group-type="author"><name><surname>Fr&#233;jus</surname><given-names>A</given-names></name><name><surname>Laleye</surname><given-names>Laurent</given-names></name><name><surname>Besacier</surname><given-names /></name><name><surname>Eug&#232;ne</surname><given-names>C</given-names></name><name><surname>Ezin</surname><given-names>Cina</given-names></name><name><surname>Motamed</surname><given-names /></name></person-group></element-citation></ref><ref id="b1"><element-citation publication-type="journal"><article-title>Deep speech 2: End-to-end speech recognition in english and mandarin</article-title><source>Proc. ICML</source><year>2016</year><person-group person-group-type="author"><name><surname>Amodei</surname><given-names>Dario</given-names></name><name><surname>Sundaram Ananthanarayanan</surname><given-names>Rishita</given-names></name><name><surname>Anubhai</surname><given-names>Jingliang</given-names></name><name><surname>Bai</surname><given-names>Eric</given-names></name><name><surname>Battenberg</surname><given-names>Carl</given-names></name><name><surname>Case</surname><given-names>Jared</given-names></name><name><surname>Casper</surname><given-names>Bryan</given-names></name><name><surname>Catanzaro</surname><given-names>Qiang</given-names></name><name><surname>Cheng</surname><given-names>Guoliang</given-names></name><name><surname>Chen</surname><given-names /></name></person-group></element-citation></ref><ref id="b2"><element-citation publication-type="journal"><article-title>Learning representations by maximizing mutual information across views</article-title><source>Proc. NeurIPS</source><year>2019</year><person-group person-group-type="author"><name><surname>Bachman</surname><given-names>Philip</given-names></name><name><surname>Hjelm</surname><given-names>Devon</given-names></name><name><surname>Buchwalter</surname><given-names>William</given-names></name></person-group></element-citation></ref><ref id="b3"><element-citation publication-type="journal"><article-title>Mine: mutual information neural estimation</article-title><source>Proc. ICML</source><year>2018</year><person-group person-group-type="author"><name><surname>Belghazi</surname><given-names>Mohamed Ishmael</given-names></name><name><surname>Baratin</surname><given-names>Aristide</given-names></name><name><surname>Rajeswar</surname><given-names>Sai</given-names></name><name><surname>Ozair</surname><given-names>Sherjil</given-names></name><name><surname>Bengio</surname><given-names>Yoshua</given-names></name><name><surname>Courville</surname><given-names>Aaron</given-names></name><name><surname>Hjelm</surname><given-names>R Devon</given-names></name></person-group></element-citation></ref><ref id="b4"><element-citation publication-type="journal"><article-title>An information-maximization approach to blind separation and blind deconvolution</article-title><source>Neural computation</source><year>1995</year><volume>7</volume><issue>6</issue><fpage>1129</fpage><lpage>1159</lpage><person-group person-group-type="author"><name><surname>Anthony</surname><given-names>J</given-names></name><name><surname>Bell</surname><given-names>Terrence J</given-names></name><name><surname>Sejnowski</surname><given-names /></name></person-group></element-citation></ref><ref id="b5"><element-citation publication-type="journal"><article-title>Greedy layer-wise training of deep networks</article-title><source>Proc. NeurIPS</source><year>2007</year><person-group person-group-type="author"><name><surname>Bengio</surname><given-names>Yoshua</given-names></name><name><surname>Lamblin</surname><given-names>Pascal</given-names></name><name><surname>Popovici</surname><given-names>Dan</given-names></name><name><surname>Larochelle</surname><given-names>Hugo</given-names></name></person-group></element-citation></ref><ref id="b6"><element-citation publication-type="journal"><article-title>Wav2letter: an end-to-end convnet-based speech recognition system</article-title><source>arXiv preprint arXiv:1609.03193</source><year>2016</year><person-group person-group-type="author"><name><surname>Collobert</surname><given-names>Ronan</given-names></name><name><surname>Puhrsch</surname><given-names>Christian</given-names></name><name><surname>Synnaeve</surname><given-names>Gabriel</given-names></name></person-group></element-citation></ref><ref id="b7"><element-citation publication-type="journal"><article-title>Robust features for noisy speech recognition using mfcc computation from magnitude spectrum of higher order autocorrelation coefficients</article-title><source>International Journal of Computer Applications</source><year>2010</year><volume>10</volume><issue>8</issue><fpage>36</fpage><lpage>38</lpage><person-group person-group-type="author"><name><surname>Dev</surname><given-names>Amita</given-names></name><name><surname>Bansal</surname><given-names>Poonam</given-names></name></person-group></element-citation></ref><ref id="b8"><element-citation publication-type="journal"><article-title>Bert: Pre-training of deep bidirectional transformers for language understanding</article-title><source>Proc. of NAACL</source><year>2019</year><person-group person-group-type="author"><name><surname>Devlin</surname><given-names>Jacob</given-names></name><name><surname>Chang</surname><given-names>Ming-Wei</given-names></name><name><surname>Lee</surname><given-names>Kenton</given-names></name><name><surname>Toutanova</surname><given-names>Kristina</given-names></name></person-group></element-citation></ref><ref id="b9"><element-citation publication-type="journal"><article-title>Multi-task self-supervised visual learning</article-title><source>Proc. ICCV</source><year>2017</year><fpage>2051</fpage><lpage>2060</lpage><person-group person-group-type="author"><name><surname>Doersch</surname><given-names>Carl</given-names></name><name><surname>Zisserman</surname><given-names>Andrew</given-names></name></person-group></element-citation></ref><ref id="b10"><element-citation publication-type="journal"><article-title>Large scale adversarial representation learning</article-title><year>2019</year><person-group person-group-type="author"><name><surname>Donahue</surname><given-names>Jeff</given-names></name><name><surname>Simonyan</surname><given-names>Karen</given-names></name></person-group></element-citation></ref><ref id="b11"><element-citation publication-type="journal"><article-title>The zero resource speech challenge 2019: Tts without t</article-title><source>Proc. INTERSPEECH</source><year>2019</year><person-group person-group-type="author"><name><surname>Dunbar</surname><given-names>Ewan</given-names></name><name><surname>Algayres</surname><given-names>Robin</given-names></name><name><surname>Karadayi</surname><given-names>Julien</given-names></name><name><surname>Bernard</surname><given-names>Mathieu</given-names></name><name><surname>Benjumea</surname><given-names>Juan</given-names></name><name><surname>Cao</surname><given-names>Xuan-Nga</given-names></name><name><surname>Miskic</surname><given-names>Lucie</given-names></name><name><surname>Dugrain</surname><given-names>Charlotte</given-names></name><name><surname>Ondel</surname><given-names>Lucas</given-names></name><name><surname>Black</surname><given-names>Alan W</given-names></name></person-group></element-citation></ref><ref id="b12"><element-citation publication-type="journal"><article-title>Looking to listen at the cocktail party: A speaker-independent audio-visual model for speech separation</article-title><source>arXiv preprint arXiv:1804.03619</source><year>2018</year><person-group person-group-type="author"><name><surname>Ephrat</surname><given-names>A</given-names></name><name><surname>Mosseri</surname><given-names>I</given-names></name><name><surname>Lang</surname><given-names>O</given-names></name><name><surname>Dekel</surname><given-names>T</given-names></name><name><surname>Wilson</surname><given-names>A</given-names></name><name><surname>Hassidim</surname><given-names>W T</given-names></name><name><surname>Freeman</surname><given-names>M</given-names></name><name><surname>Rubinstein</surname><given-names /></name></person-group></element-citation></ref><ref id="b13"><element-citation publication-type="journal"><article-title>Transmission of information: A statistical theory of communica- tions</article-title><source>American Journal of Physics</source><year>1961</year><volume>29</volume><fpage>793</fpage><lpage>794</lpage><person-group person-group-type="author"><name><surname>Robert</surname><given-names>M</given-names></name><name><surname>Fano</surname><given-names>David</given-names></name><name><surname>Hawkins</surname><given-names /></name></person-group></element-citation></ref><ref id="b14"><element-citation publication-type="journal"><article-title>Timit acoustic phonetic continuous speech corpus</article-title><source>Linguistic Data Consortium</source><year>1993</year><person-group person-group-type="author"><name><surname>John S Garofolo</surname><given-names /></name></person-group></element-citation></ref><ref id="b15"><element-citation publication-type="journal"><article-title>Collect- ing resources in sub-saharan african languages for automatic speech recognition: a case study of wolof</article-title><source>LREC</source><year>2016</year><person-group person-group-type="author"><name><surname>Gauthier</surname><given-names>Elodie</given-names></name><name><surname>Besacier</surname><given-names>Laurent</given-names></name><name><surname>Voisin</surname><given-names>Sylvie</given-names></name><name><surname>Melese</surname><given-names>Michael</given-names></name><name><surname>Elingui</surname><given-names>Uriel Pascal</given-names></name></person-group></element-citation></ref><ref id="b16"><element-citation publication-type="journal"><article-title>Developments of swahili resources for an automatic speech recognition system</article-title><source>Workshop Proc. SLTU</source><year>2012</year><person-group person-group-type="author"><name><surname>Gelas</surname><given-names>Hadrien</given-names></name><name><surname>Besacier</surname><given-names>Laurent</given-names></name><name><surname>Pellegrino</surname><given-names>Francois</given-names></name></person-group></element-citation></ref><ref id="b17"><element-citation publication-type="journal"><article-title>Audio set: An ontology and human-labeled dataset for audio events</article-title><source>Proc. ICASSP</source><year>2017</year><person-group person-group-type="author"><name><surname>Jort</surname><given-names>F</given-names></name><name><surname>Gemmeke</surname><given-names /></name><name><surname>Daniel</surname><given-names>P W</given-names></name><name><surname>Ellis</surname><given-names>Dylan</given-names></name><name><surname>Freedman</surname><given-names>Aren</given-names></name><name><surname>Jansen</surname><given-names>Wade</given-names></name><name><surname>Lawrence</surname><given-names>R</given-names></name><name><surname>Moore</surname><given-names>Manoj</given-names></name><name><surname>Plakal</surname><given-names>Marvin</given-names></name><name><surname>Ritter</surname><given-names /></name></person-group></element-citation></ref><ref id="b18"><element-citation publication-type="journal"><article-title>Understanding the difficulty of training deep feedforward neural networks</article-title><source>Proc. ICASSP</source><year>2010</year><person-group person-group-type="author"><name><surname>Glorot</surname><given-names>Xavier</given-names></name><name><surname>Bengio</surname><given-names>Yoshua</given-names></name><name><surname>John</surname><given-names>J</given-names></name><name><surname>Godfrey</surname><given-names /></name><name><surname>Edward</surname><given-names>C</given-names></name><name><surname>Holliman</surname><given-names>Jane</given-names></name><name><surname>Mcdaniel</surname><given-names /></name></person-group></element-citation></ref><ref id="b19"><element-citation publication-type="journal"><article-title>Connectionist temporal classification: labelling unsegmented sequence data with recurrent neural networks</article-title><source>Proc. ICML</source><year>2006</year><person-group person-group-type="author"><name><surname>Graves</surname><given-names>Alex</given-names></name><name><surname>Fern&#225;ndez</surname><given-names>Santiago</given-names></name><name><surname>Gomez</surname><given-names>Faustino</given-names></name><name><surname>Schmidhuber</surname><given-names>J&#252;rgen</given-names></name></person-group></element-citation></ref><ref id="b20"><element-citation publication-type="journal"><article-title>Noise-contrastive estimation: A new estimation principle for unnormalized statistical models</article-title><source>Proc. AISTATS</source><year>2010</year><person-group person-group-type="author"><name><surname>Gutmann</surname><given-names>Michael</given-names></name><name><surname>Hyv&#228;rinen</surname><given-names>Aapo</given-names></name></person-group></element-citation></ref><ref id="b21"><element-citation publication-type="journal"><article-title>Data-efficient image recognition with contrastive predictive coding</article-title><source>arXiv preprint arXiv:1905.09272</source><year>2019</year><person-group person-group-type="author"><name><surname>Olivier</surname><given-names>J</given-names></name><name><surname>H&#233;naff</surname><given-names>Ali</given-names></name><name><surname>Razavi</surname><given-names>Carl</given-names></name><name><surname>Doersch</surname><given-names /></name><name><surname>Sm Eslami</surname><given-names>Aaron</given-names></name><name><surname>Van Den Oord</surname><given-names /></name></person-group></element-citation></ref><ref id="b22"><element-citation publication-type="journal"><article-title>Ted-lium 3: twice as much data and corpus repartition for experiments on speaker adaptation</article-title><source>Proc. SPECOM</source><year>2018</year><person-group person-group-type="author"><name><surname>Hernandez</surname><given-names>Fran&#231;ois</given-names></name><name><surname>Nguyen</surname><given-names>Vincent</given-names></name><name><surname>Ghannay</surname><given-names>Sahar</given-names></name><name><surname>Tomashenko</surname><given-names>Natalia</given-names></name><name><surname>Est&#232;ve</surname><given-names>Yannick</given-names></name></person-group></element-citation></ref><ref id="b23"><element-citation publication-type="journal"><article-title>Deep neural networks for acoustic modeling in speech recognition</article-title><source>IEEE Signal processing magazine</source><year>2012</year><volume>29</volume><person-group person-group-type="author"><name><surname>Hinton</surname><given-names>Geoffrey</given-names></name><name><surname>Deng</surname><given-names>Li</given-names></name><name><surname>Yu</surname><given-names>Dong</given-names></name><name><surname>Dahl</surname><given-names>George</given-names></name><name><surname>Mohamed</surname><given-names>Abdel-Rahman</given-names></name><name><surname>Jaitly</surname><given-names>Navdeep</given-names></name><name><surname>Senior</surname><given-names>Andrew</given-names></name><name><surname>Vanhoucke</surname><given-names>Vincent</given-names></name><name><surname>Nguyen</surname><given-names>Patrick</given-names></name><name><surname>Kingsbury</surname><given-names>Brian</given-names></name></person-group></element-citation></ref><ref id="b24"><element-citation publication-type="journal"><article-title>A fast learning algorithm for deep belief nets</article-title><source>Neural computation</source><year>2006</year><volume>18</volume><issue>7</issue><fpage>1527</fpage><lpage>1554</lpage><person-group person-group-type="author"><name><surname>Hinton</surname><given-names>Geoffrey E</given-names></name><name><surname>Osindero</surname><given-names>Simon</given-names></name><name><surname>Teh</surname><given-names>Yee-Whye</given-names></name></person-group></element-citation></ref><ref id="b25"><element-citation publication-type="journal"><article-title>Learning deep representations by mutual information estimation and maximization</article-title><source>Proc. ICLR</source><year>2019</year><person-group person-group-type="author"><name><surname>R Devon Hjelm</surname><given-names>Alex</given-names></name><name><surname>Fedorov</surname><given-names>Samuel</given-names></name><name><surname>Lavoie-Marchildon</surname><given-names>Karan</given-names></name><name><surname>Grewal</surname><given-names>Phil</given-names></name><name><surname>Bachman</surname><given-names>Adam</given-names></name><name><surname>Trischler</surname><given-names>Yoshua</given-names></name><name><surname>Bengio</surname><given-names /></name></person-group></element-citation></ref><ref id="b26"><element-citation publication-type="journal"><article-title>Densely connected convolutional networks</article-title><source>Proc. CVPR</source><year>2017</year><person-group person-group-type="author"><name><surname>Huang</surname><given-names>Gao</given-names></name><name><surname>Liu</surname><given-names>Zhuang</given-names></name><name><surname>Van Der Maaten</surname><given-names>Laurens</given-names></name><name><surname>Weinberger</surname><given-names>Kilian</given-names></name></person-group></element-citation></ref><ref id="b27"><element-citation publication-type="journal"><article-title>Mixed-precision training for nlp and speech recognition with openseq2seq</article-title><source>arXiv preprint arXiv:1805.10387</source><year>2018</year><person-group person-group-type="author"><name><surname>Kuchaiev</surname><given-names>Oleksii</given-names></name><name><surname>Ginsburg</surname><given-names>Boris</given-names></name><name><surname>Gitman</surname><given-names>Igor</given-names></name><name><surname>Lavrukhin</surname><given-names>Vitaly</given-names></name><name><surname>Li</surname><given-names>Jason</given-names></name><name><surname>Nguyen</surname><given-names>Huyen</given-names></name><name><surname>Case</surname><given-names>Carl</given-names></name><name><surname>Micikevicius</surname><given-names>Paulius</given-names></name></person-group></element-citation></ref><ref id="b28"><element-citation publication-type="journal"><article-title>Delta-spectral cepstral coefficients for robust speech recognition</article-title><source>Proc. ICASSP</source><year>2011</year><person-group person-group-type="author"><name><surname>Kumar</surname><given-names>Kshitiz</given-names></name><name><surname>Kim</surname><given-names>Chanwoo</given-names></name><name><surname>Stern</surname><given-names>Richard M</given-names></name></person-group></element-citation></ref><ref id="b29"><element-citation publication-type="journal"><article-title>Jasper: An end-to-end convolutional neural acoustic model</article-title><source>In Proc. INTERSPEECH</source><year>2019</year><person-group person-group-type="author"><name><surname>Li</surname><given-names>Jason</given-names></name><name><surname>Lavrukhin</surname><given-names>Vitaly</given-names></name><name><surname>Ginsburg</surname><given-names>Boris</given-names></name><name><surname>Leary</surname><given-names>Ryan</given-names></name><name><surname>Kuchaiev</surname><given-names>Oleksii</given-names></name><name><surname>Cohen</surname><given-names>Jonathan M</given-names></name><name><surname>Nguyen</surname><given-names>Huyen</given-names></name><name><surname>Teja Gadde</surname><given-names>Ravi</given-names></name></person-group></element-citation></ref><ref id="b30"><element-citation publication-type="journal"><article-title>An application of the principle of maximum information preservation to linear systems</article-title><source>Proc. NeurIPS</source><year>1988</year><person-group person-group-type="author"><name><surname>Linsker</surname><given-names>Ralph</given-names></name></person-group></element-citation></ref><ref id="b31"><element-citation publication-type="journal"><article-title>Distributed representations of words and phrases and their compositionality</article-title><source>Proc. NeurIPS</source><year>2013</year><person-group person-group-type="author"><name><surname>Mikolov</surname><given-names>Tomas</given-names></name><name><surname>Sutskever</surname><given-names>Ilya</given-names></name><name><surname>Chen</surname><given-names>Kai</given-names></name><name><surname>Corrado</surname><given-names>Greg S</given-names></name></person-group></element-citation></ref><ref id="b32"><element-citation publication-type="journal"><source>Geometric invariance in computer vision</source><year>1992</year><volume>92</volume><person-group person-group-type="author"><name><surname>Joseph</surname><given-names>L</given-names></name><name><surname>Mundy</surname><given-names>Andrew</given-names></name><name><surname>Zisserman</surname><given-names /></name></person-group></element-citation></ref><ref id="b33"><element-citation publication-type="journal"><article-title>Librispeech: an asr corpus based on public domain audio books</article-title><source>Proc. ICASSP</source><year>2015</year><person-group person-group-type="author"><name><surname>Panayotov</surname><given-names>Vassil</given-names></name><name><surname>Chen</surname><given-names>Guoguo</given-names></name><name><surname>Povey</surname><given-names>Daniel</given-names></name><name><surname>Khudanpur</surname><given-names>Sanjeev</given-names></name></person-group></element-citation></ref><ref id="b34"><element-citation publication-type="journal"><article-title>The design for the wall street journal-based csr corpus</article-title><source>Proc. ACL</source><year>1992</year><person-group person-group-type="author"><name><surname>Douglas</surname><given-names>B</given-names></name><name><surname>Paul</surname><given-names>Janet M</given-names></name><name><surname>Baker</surname><given-names /></name></person-group></element-citation></ref><ref id="b35"><element-citation publication-type="journal"><article-title>Deep contextualized word representations</article-title><source>Proc. of NAACL</source><year>2018</year><person-group person-group-type="author"><name><surname>Peters</surname><given-names>Matthew E</given-names></name><name><surname>Neumann</surname><given-names>Mark</given-names></name><name><surname>Iyyer</surname><given-names>Mohit</given-names></name><name><surname>Gardner</surname><given-names>Matt</given-names></name><name><surname>Clark</surname><given-names>Christopher</given-names></name><name><surname>Lee</surname><given-names>Kenton</given-names></name><name><surname>Zettlemoyer</surname><given-names>Luke</given-names></name></person-group></element-citation></ref><ref id="b36"><element-citation publication-type="journal"><article-title>wav2vec: Unsupervised pre-training for speech recognition</article-title><source>Proc. INTERSPEECH</source><year>2019</year><person-group person-group-type="author"><name><surname>Schneider</surname><given-names>Steffen</given-names></name><name><surname>Baevski</surname><given-names>Alexei</given-names></name><name><surname>Collobert</surname><given-names>Ronan</given-names></name><name><surname>Auli</surname><given-names>Michael</given-names></name></person-group></element-citation></ref></ref-list></back></article>