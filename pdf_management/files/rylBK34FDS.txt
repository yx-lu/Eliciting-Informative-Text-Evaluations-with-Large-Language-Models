Title:
```
Published as a conference paper at ICLR 2020 DEEPHOYER: LEARNING SPARSER NEURAL NETWORK WITH DIFFERENTIABLE SCALE-INVARIANT SPARSITY MEASURES
```
Abstract:
```
In seeking for sparse and efficient neural network models, many previous works investigated on enforcing 1 or 0 regularizers to encourage weight sparsity during training. The 0 regularizer measures the parameter sparsity directly and is invariant to the scaling of parameter values. But it cannot provide useful gradients and therefore requires complex optimization techniques. The 1 regularizer is almost everywhere differentiable and can be easily optimized with gradient descent. Yet it is not scale-invariant and causes the same shrinking rate to all parameters, which is inefficient in increasing sparsity. Inspired by the Hoyer measure (the ratio between 1 and 2 norms) used in traditional compressed sensing problems, we present DeepHoyer, a set of sparsity-inducing regularizers that are both differentiable almost everywhere and scale-invariant. Our experiments show that enforcing DeepHoyer regularizers can produce even sparser neural network models than previous works, under the same accuracy level. We also show that DeepHoyer can be applied to both element-wise and structural pruning. The codes are available at https://github.com/yanghr/DeepHoyer.
```

Figures/Tables Captions:
```
Figure 1: Comparing the 1 and the Hoyer regularizer of a 2-D vector. Their contours are shown in the left 2 subplots (darker color corresponds to a lower value). The right 2 subplots compare their negative gradients.
Figure 2: Minimization path of Hoyer-Square regularizer during gradient descent, with W ∈ R 20 initialized as i.i.d. N (0, 1). The figure shows the path of each element w i during the minimization, with the black dash line showing the induced trimming threshold.
Figure 3: Comparisons of accuracy-#FLOPs tradeoff on ImageNet and CIFAR-10, black dash lines mark the Pareto frontiers. The exact data for the points are listed in Appendix C.3.
Table 1: Element-wise pruning results on LeNet-300-100 model @ accuracy 98.4%
Table 2: Element-wise pruning results on LeNet-5 model @ accuracy 99.2%
Table 3: Element-wise pruning results on AlexNet model.
Table 4: Structural pruning results on LeNet-300-100 model
Table 5: Structural pruning result on LeNet-5 model
```

Main Content:
```

Section Title: INTRODUCTION
  INTRODUCTION The use of deep neural network (DNN) models has been expanded from handwritten digit recogni- tion ( LeCun et al., 1998 ) to real-world applications, such as large-scale image classification ( Simonyan & Zisserman, 2014 ), self driving ( Makantasis et al., 2015 ) and complex control problems ( Mnih et al., 2013 ). However, a modern DNN model like AlexNet ( Krizhevsky et al., 2012 ) or ResNet ( He et al., 2016 ) often introduces a large number of parameters and computation load, which makes the deployment and real-time processing on embedded and edge devices extremely difficult (Han et al., 2015b; a ;  Wen et al., 2016 ). Thus, model compression techniques, especially pruning methods that increase the sparsity of weight matrices, have been extensively studied to reduce the memory consumption and computation cost of DNNs (Han et al., 2015b; a ;  Wen et al., 2016 ;  Guo et al., 2016 ;  Louizos et al., 2017b ;  Luo et al., 2017 ;  Zhang et al., 2018 ;  Liu et al., 2015 ). Most of the previous works utilize some form of sparsity-inducing regularizer in searching for sparse neural networks. The 1 regularizer, originally proposed by  Tibshirani (1996) , can be easily optimized through gradient descent for its convex and almost everywhere differentiable property. Therefore it is widely used in DNN pruning:  Liu et al. (2015)  directly apply 1 regularization to all the weights of a DNN to achieve element-wise sparsity;  Wen et al. (2016 ; 2017) present structural sparsity via group lasso, which applies an 1 regularization over the 2 norms of different groups of parameters. However, it has been noted that the value of the 1 regularizer is proportional to the scaling of parameters (i.e. ||αW || 1 = |α|·||W || 1 ), so it "scales down" all the elements in the weight matrices with the same speed. This is not efficient in finding sparsity and may sacrifice the flexibility of the trained model. On the other hand, the 0 regularizer directly reflects the real sparsity of weights and is scale invariant (i.e. ||αW || 0 = ||W || 0 , ∀α = 0), yet the 0 norm cannot provide useful gradients. Han et al. (2015b) enforce an element-wise 0 constraint by iterative pruning a fixed percentage of smallest weight elements, which is a heuristic method and therefore can hardly achieve optimal compression rate. Some recent works mitigate the lack of gradient information by integrating 0 regularization with stochastic approximation ( Louizos et al., 2017b ) or more complex optimization methods (e.g. Published as a conference paper at ICLR 2020 ADMM) ( Zhang et al., 2018 ). These additional measures brought overheads to the optimization process, making the use of these methods on larger networks difficult. To achieve even sparser neural networks, we argue to move beyond 0 and 1 regularizers and seek for a sparsity-inducing regularizer that is both almost everywhere differentiable (like 1 ) and scale-invariant (like 0 ). Beyond the 1 regularizer, plenty of non-convex sparsity measurements have been used in the field of feature selection and compressed sensing ( Hurley & Rickard, 2009 ;  Wen et al., 2018 ). Some popular regularizers like SCAD ( Fan & Li, 2001 ), MDP ( Zhang et al., 2010 ) and Trimmed 1 ( Yun et al., 2019 ) use a piece-wise formulation to mitigate the proportional scaling problem of 1 . The piece-wise formulation protects larger elements by having zero penalty to elements greater than a predefined threshold. However, it is extremely costly to manually seek for the optimal trimming threshold, so it is hard to obtain optimal result in DNN pruning by using these regularizers. The transformed 1 regularizer formulated as N i=1 (a+1)|wi| a+|wi| manages to smoothly interpolate between 1 and 0 by tuning the hyperparameter a ( Ma et al., 2019 ). However, such an approximation is close to 0 only when a approaches infinity, so the practical formulation of the transformed 1 (i.e. a = 1) is still not scale-invariant. Particularly, we are interested in the Hoyer regularizer ( Hoyer, 2004 ), which estimates the sparsity of a vector with the ratio between its 1 and 2 norms. Comparing to other sparsity-inducing regularizers, Hoyer regularizer achieves superior performance in the fields of non-negative matrix factorization ( Hoyer, 2004 ), sparse reconstruction ( Esser et al., 2013 ;  Tran et al., 2018 ) and blend deconvolution ( Krishnan et al., 2011 ;  Repetti et al., 2015 ). We note that Hoyer regularizer is both almost everywhere differentiable and scale invariant, satisfying the desired property of a sparsity- inducing regularizer. We therefore propose DeepHoyer, which is the first Hoyer-inspired regularizers for DNN sparsification. Specifically, the contributions of this work include: • Hoyer-Square (HS) regularizer for element-wise sparsity: We enhance the original Hoyer regularizer to the HS regularizer and achieve element-wise sparsity by applying it in the training of DNNs. The HS regularizer is both almost everywhere differentiable and scale invariant. It has the same range and minima structure as the 0 norm. Thus, the HS regularizer presents the ability of turning small weights to zero while protecting and maintaining those weights that are larger than an induced, gradually adaptive threshold; • Group-HS regularizer for structural sparsity, which is extended from the HS regularizer; • Generating sparser DNN models: Our experiments show that the proposed regularizers beat state-of-the-arts in both element-wise and structural weight pruning of modern DNNs.

Section Title: RELATED WORK ON DNN PRUNING
  RELATED WORK ON DNN PRUNING It is well known that high redundancy pervasively exists in DNNs. Consequently, pruning methods have been extensively investigated to identify and remove unimportant weights. Some heuristic pruning methods (Han et al., 2015b;  Guo et al., 2016 ) simply remove weights in small values to generate sparse models. These methods usually require long training time without ensuring the optimality, due to the lack of theoretical understanding and well-formulated optimization ( Zhang et al., 2018 ). Some works formulate the problem as a sparsity-inducing optimization problem, such as 1 regularization ( Liu et al., 2015 ;  Park et al., 2016 ) that can be optimized using standard gradient- based algorithms, or 0 regularization ( Louizos et al., 2017b ;  Zhang et al., 2018 ) which requires stochastic approximation or special optimization techniques. We propose DeepHoyer regularizers in this work, which belong to the line of sparsity-inducing optimization research. More specific, the proposed Hoyer-Square regularizer for element-wise pruning is scale-invariant and can serve as an differentiable approximation to the 0 norm. Furthermore, it can be optimized by gradient-based optimization methods in the same way as the 1 regularization. With these properties, the Hoyer- Square regularizer achieves a further 38% and 63% sparsity improvement on LeNet-300-100 model and LeNet-5 model respectively comparing to previous state-of-the-arts, and achieves the highest sparsity on AlexNet without accuracy loss. Structurally sparse DNNs attempt to create regular sparse patterns that are friendly for hardware execution. To achieve the goal,  Li et al. (2016)  propose to remove filters with small norms;  Wen et al. (2016)  apply group Lasso regularization based methods to remove various structures (e.g., filters, channels, layers) in DNNs and the similar approaches are used to remove neurons ( Alvarez & Published as a conference paper at ICLR 2020  Salzmann, 2016);  Liu et al. (2017)  and  Gordon et al. (2018)  (MorphNet) enforce sparsity-inducing regularization on the scaling parameters within Batch Normalization layers to remove the corre- sponding channels in DNNs; ThiNet ( Luo et al., 2017 ) removes unimportant filters by minimizing the reconstruction error of feature maps; and He et al. (2017) incorporate both Lasso regression and reconstruction error into the optimization problem. Bayesian optimization methods have also been applied for neuron pruning ( Louizos et al., 2017a ;  Neklyudov et al., 2017 ), yet these methods are not applicable in large-scale problems like ImageNet. We further advance the DeepHoyer to learn structured sparsity (such as reducing filters and channels) with the newly proposed "Group-HS" regularization. The Group-HS regularizer further improves the computation reduction of the LeNet-5 model by 8.8% from the 1 based method ( Wen et al., 2016 ), and by 110.6% from the 0 based method ( Louizos et al., 2017b ). Experiments on ResNet models reveal that the accuracy-speedup tradeoff induced by Group-HS constantly stays above the Pareto frontier of previous methods. More detailed results can be found in Section 5.

Section Title: MEASURING SPARSITY WITH THE HOYER MEASURE
  MEASURING SPARSITY WITH THE HOYER MEASURE Sparsity measures provide tractable sparsity constraints for enforcement during problem solving and therefore have been extensively studied in the compressed sensing society. In early non-negative matrix factorization (NMF) research, a consensus was that a sparsity measure should map a n- dimensional vector X to a real number S ∈ [0, 1], such that the possible sparsest vectors with only one nonzero element has S = 1, and a vector with all equal elements has S = 0 ( Hoyer, 2004 ). Unders the assumption, the Hoyer measure was proposed as follows It can be seen that Thus, the normalization in Equation (1) fits the measure S(X) into the [0, 1] interval. According to the survey by  Hurley & Rickard (2009) , among the six desired heuristic criteria of sparsity measures, the Hoyer measure satisfies five, more than all other commonly applied sparsity measures. Given its success as a sparsity measure in NMF, the Hoyer measure has been applied as a sparsity-inducing regularizer in optimization problems such as blind deconvolution ( Repetti et al., 2015 ) and image deblurring ( Krishnan et al., 2011 ). Without the range constraint, the Hoyer regularizer in these works adopts the form R(X) = i |xi| √ i x 2 i directly, as the ratio of the 1 and 2 norms.  Figure 1  compares the Hoyer regularizer and the 1 regularizer. Unlike the the 1 norm with a single minimum at the origin, the Hoyer regularizer has minima along axes, the structure of which is very similar to the 0 norm's. Moreover, the Hoyer regularizer is scale-invariant, i.e. R(αX) = R(X), because both the 1 norm and the 2 norm are proportional to the scale of X. The gradients of the Hoyer regularizer are purely radial, leading to "rotations" towards the nearest axis. These features make the Hoyer regularizer outperform the 1 regularizer on various tasks ( Esser et al., 2013 ;  Tran Published as a conference paper at ICLR 2020 et al., 2018 ;  Krishnan et al., 2011 ;  Repetti et al., 2015 ). The theoretical analysis by  Yin et al. (2014)  also proves that the Hoyer regularizer has a better guarantee than the 1 norm on recovering sparse solutions from coherent and redundant representations.

Section Title: MODEL COMPRESSION WITH DEEPHOYER REGULARIZERS
  MODEL COMPRESSION WITH DEEPHOYER REGULARIZERS Inspired by the Hoyer regularizer, we propose two types of DeepHoyer regularizers: the Hoyer-Square regularizer (HS) for element-wise pruning and the Group-HS regularizer for structural pruning.

Section Title: HOYER-SQUARE REGULARIZER FOR ELEMENT-WISE PRUNING
  HOYER-SQUARE REGULARIZER FOR ELEMENT-WISE PRUNING Since the process of the element-wise pruning is equivalent to regularizing each layer's weight with the 0 norm, it is intuitive to configure the sparsity-inducing regularizer to have a similar behavior as the 0 norm. As shown in Inequality (2), the value of the original Hoyer regularizer of a N - dimensional nonzero vector lies between 1 and √ N , while its 0 norm is within the range of [1, N ]. Thus we propose to apply the square of Hoyer regularizer, namely Hoyer-Square (HS), to the weights W of a layer, like The proposed HS regularizer behaves as a differentiable approximation to the 0 norm. First, both regularizers now have the same range of [1, N ]. Second, H S is scale invariant as H S (αW ) = H S (W ) holds for ∀α = 0, so as the 0 norm. Moreover, as the squaring operator monotonously increases in the range of [1, √ N ], the Hoyer-Square regularizer's minima remain along the axes as the Hoyer regularizer's do (see  Figure 1 ). In other words, they have similar minima structure as the 0 norm. At last, the Hoyer-Square regularizer is also almost everywhere differentiable and Equation (4) formulates the gradient of H S w.r.t. an element w j in the weight matrix W : Very importantly, this formulation induces a trimming effect: when H S (W ) is being minimized through gradient descent, w j moves towards 0 if |w j |< i w 2 i i |wi| , otherwise moves away from 0. In other words, unlike the 1 regularizer which tends to shrink all elements, our Hoyer-Square regularizer will turn weights in small value to zero meanwhile protecting large weights. Traditional trimmed regularizers ( Fan & Li, 2001 ;  Zhang et al., 2010 ;  Yun et al., 2019 ) usually define a trimming threshold as a fixed value or percentage. Instead, the HS regularizer can gradually extend the scope of pruning as more weights coming close to zero. This behavior can be observed in the gradient descent path shown in  Figure 2 .

Section Title: GROUP-HS REGULARIZER FOR STRUCTURAL PRUNING
  GROUP-HS REGULARIZER FOR STRUCTURAL PRUNING Beyond element-wise pruning, structural pruning is often more preferred because it can construct the sparsity in a structured way and therefore achieve higher computation speed-up on general computation platforms ( Wen et al., 2016 ). The structural pruning is previously empowered by the group lasso ( Yuan & Lin, 2006 ;  Wen et al., 2016 ), which is the sum (i.e. 1 norm) of the 2 norms of all the groups within a weight matrix like R G (W ) = G g=1 ||w (g) || 2 , (5) where ||W || 2 = i w 2 i represents the 2 norm, w (g) is a group of elements in the weight matrix W which consists of G such groups. Following the same approach in Section 4.1, we use the Hoyer-Square regularizer to replace the 1 regularizer in the group lasso formulation and define the Group-HS (G H ) regularizer in Equation (6): Note that the second equality holds when and only when the groups cover all the elements of W without overlapping with each other. Our experiments in this paper satisfy this requirement. However, the Group-HS regularizer can always be used in the form of the first equality when overlapping exists across groups. The gradient and the descent path of the Group-HS regularizer are very similar to those of the Hoyer-Square regularizer, and therefore we omit the detailed discussion here. The derivation of the Group-HS regularizer's gradient shall be found in Appendix A.

Section Title: APPLY DEEPHOYER REGULARIZERS IN DNN TRAINING
  APPLY DEEPHOYER REGULARIZERS IN DNN TRAINING The deployment of the DeepHoyer regularizers in DNN training follows the common layer-based regularization approach ( Wen et al., 2016 ;  Liu et al., 2015 ). For element-wise pruning, we apply the Hoyer-Square regularizer to layer weight matrix W (l) for all L layers, and directly minimize it alongside the DNN's original training objective L(W (1:L) ). The 2 regularizer can also be added to the objective if needed. Equation (7) presents the training objective with H S defined in Equation (3). Here, α and β are pre-selected weight decay parameters for the regularizers. For structural pruning, we mainly focus on pruning the columns and rows of fully connected layers and the filters and channels of convolutional layers. More specific, we group a layer in filter-wise and channel-wise fashion as proposed by  Wen et al. (2016)  and then apply the Group-HS regularizer to the layer. The resulted optimization objective is formulated in Equation (8). Here N l is the number of filters and C l is the number of channels in the l th layer if it is a convolutional layer. If the l th layer is fully connected, then N l and C l is the number of rows and columns respectively. α n , α c and β are pre-selected weight decay parameters for the regularizers. The recent advance in stochastic gradient descent (SGD) method provides satisfying results under large-scale non-convex settings ( Sutskever et al., 2013 ; Kingma & Ba, 2014), including DNNs with non-convex objectives ( Auer et al., 1996 ). So we can directly optimize the DeepHoyer regularizers with the same SGD optimizer used for the original DNN training objective, despite their nonconvex formulations. Our experiments show that the tiny-bit nonconvexity induced by DeepHoyer does not affect the performance of DNNs. The pruning is conducted by following the common three-stage operations: (1) train the DNN with the DeepHoyer regularizer, (2) prune all the weight elements smaller than a predefined small threshold, and (3) finetune the model by fixing all the zero elements and removing the DeepHoyer regularizer.

Section Title: EXPERIMENT RESULT
  EXPERIMENT RESULT The proposed DeepHoyer regularizers are first tested on the MNIST benchmark using the LeNet- 300-100 fully connected model and the LeNet-5 CNN model ( LeCun et al., 1998 ). We also conduct tests on the CIFAR-10 dataset ( Krizhevsky & Hinton, 2009 ) with ResNet models ( He et al., 2016 ) in various depths, and on ImageNet ILSVRC-2012 benchmark ( Russakovsky et al., 2015 ) with the AlexNet model ( Krizhevsky et al., 2012 ) and the ResNet-50 model ( He et al., 2016 ). All the models are implemented and trained in the PyTorch deep learning framework ( Paszke et al., 2017 ), where we match the model structure and the benchmark performance with those of previous works for the fairness of comparison. The experiment results presented in the rest of this section show that the proposed DeepHoyer regularizers consistently outperform previous works in both element-wise and structural pruning. Detailed information on the experiment setups and the parameter choices of our reported results can be found in Appendix B.

Section Title: ELEMENT-WISE PRUNING
  ELEMENT-WISE PRUNING   Table 1  and  Table 2  summarize the performance of the proposed Hoyer-square regularizer on the MNIST benchmark, with comparisons against state of the art (SOTA) element-wise pruning methods. Without losing the testing accuracy, training with the Hoyer-Square regularizer reduces the number of nonzero weights by 54.5× on the LeNet-300-100 model and by 122× on the LeNet-5 model. Among all the methods, ours achieves the highest sparsity: it is a 38% improvement on the LeNet-300-100 model and a 63% improvement on the LeNet-5 model comparing to the best available methods. Additional results in Appendix C.1 further illustrates the effect of the Hoyer-Square regularizer on each layer's weight distribution during the training process. The element-wise pruning performance on the AlexNet model testing on the ImageNet benchmark is presented in  Table 3 . Without losing the testing accuracy, the Hoyer-Square regularizer improves Published as a conference paper at ICLR 2020 We perform the ablation study for performance comparison between the Hoyer-Square regularizer and the original Hoyer regularizer. The results in  Tables 1 , 2 and 3 all show that the Hoyer-Square regularizer always achieves a higher compression rate than the original Hoyer regularizer. The layer-wise compression results show that the Hoyer-Square regularizer emphasizes more on the layers with more parameters (i.e. FC1 for the MNIST models). This corresponds to the fact that the value of the Hoyer-Square regularizer is proportional to the number of non-zero elements in the weight. These observations validate our choice to use the Hoyer-Square regularizer for DNN compression.

Section Title: STRUCTURAL PRUNING
  STRUCTURAL PRUNING This section reports the effectiveness of the Group-HS regularizer in structural pruning tasks. Here we mainly focus on the number of remaining neurons (output channels for convolution layers and rows for fully connected layers) after removing the all-zero channels or rows in the weight matrices. The comparison is then made based on the required float-point operations (FLOPs) to inference with the remaining neurons, which indeed represents the potential inference speed of the pruned model. As shown in  Table 4 , training with the Group-HS regularizer can reduce the number of FLOPs by 16.2× for the LeNet-300-100 model with a slight accuracy drop. This is the highest speedup among all existing methods achieving the same testing accuracy.  Table 5  shows that the Group-HS regularizer can reduce the number of FLOPs of the LeNet-5 model by 12.4×, which outperforms most of the existing work-an 8.8% increase from the 1 based method ( Wen et al., 2016 ) and a 110.6% increase from the 0 based method ( Louizos et al., 2017b ). Only the Bayesian compression (BC) method with the group-horseshoe prior (BC-GHS) ( Louizos et al., 2017a ) achieves a slightly higher speedup on the LeNet-5 model. However, the complexity of high dimensional Bayesian inference limits BC's capability. It is difficult to apply BC to ImageNet-level problems and large DNN models like ResNet. In contrast, the effectiveness of the Group-HS regularizer can be easily extended to deeper models and larger datasets, which is demonstrated by our experiments. We apply the Group-HS regularizer to ResNet models ( He et al., 2016 ) on the CIFAR-10 and the ImageNet datasets. Pruning ResNet has long been considered difficult due to the compact structure of the ResNet model. Since previous works usually report the compression rate at different accuracy, we use the "accuracy-#FLOPs" plot to represent the tradeoff. The tradeoff between the accuracy and the FLOPs are explored in this work by changing the strength of the Group-HS regularizer used in training.  Figure 3  shows the performance of DeepHoyer constantly stays above the Pareto frontier of previous methods.

Section Title: CONCLUSIONS
  CONCLUSIONS In this work, we propose DeepHoyer, a set of sparsity-inducing regularizers that are both scale- invariant and almost everywhere differentiable. We show that the proposed regularizers have similar range and minima structure as the 0 norm, so it can effectively measure and regularize the sparsity of the weight matrices of DNN models. Meanwhile, the differentiable property enables the proposed regularizers to be simply optimized with standard gradient-based methods, in the same way as the 1 regularizer is. In the element-wise pruning experiment, the proposed Hoyer-Square regularizer achieves a 38% sparsity increase on the LeNet-300-100 model and a 63% sparsity increase on the LeNet-5 model without accuracy loss comparing to the state-of-the-art. A 21.3× model compression rate is achieved on AlexNet, which also surpass all previous methods. In the structural pruning experiment, the proposed Group-HS regularizer further reduces the computation load by 24.4% from the state-of-the-art on LeNet-300-100 model. It also achieves a 8.8% increase from the 1 based method and a 110.6% increase from the 0 based method of the computation reduction rate on the LeNet-5 model. For CIFAR-10 and ImageNet dataset, the accuracy-FLOPs tradeoff achieved by training ResNet models with various strengths of the Group-HS regularizer constantly stays above the Pareto frontier of previous methods. These results prove that the DeepHoyer regularizers are effective in achieving both element-wise and structural sparsity in deep neural networks, and can produce even sparser DNN models than previous works.
  We implement the transformed 1 regularizer in ( Ma et al., 2019 ) ourselves because the experiments in the original paper are under different settings. Implementation details can be found in Appendix B.

```
