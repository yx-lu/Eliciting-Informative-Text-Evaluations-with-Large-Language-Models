Title:
```
Under review as a conference paper at ICLR 2020 BLENDING DIVERSE PHYSICAL PRIORS WITH NEU- RAL NETWORKS
```
Abstract:
```
Rethinking physics in the era of deep learning is an increasingly important topic. This topic is special because, in addition to data, one can leverage a vast library of physical prior models (e.g. kinematics, fluid flow, etc) to perform more ro- bust inference. The nascent sub-field of physics-based learning (PBL) studies this problem of blending neural networks with physical priors. While previous PBL algorithms have been applied successfully to specific tasks, it is hard to generalize existing PBL methods to a wide range of physics-based problems. Such general- ization would require an architecture that can adapt to variations in the correctness of the physics, or in the quality of training data. No such architecture exists. In this paper, we aim to generalize PBL, by making a first attempt to bring neural ar- chitecture search (NAS) to the realm of PBL. We introduce a new method known as physics-based neural architecture search (PhysicsNAS) that is a top-performer across a diverse range of quality in the physical model and the dataset.
```

Figures/Tables Captions:
```
Figure 1: Generalizing PBL across a range of sparsity in training data and correctness in the physical model.
Figure 2: An overview of proposed NAS-based blending approach. Our PhysicsNAS takes ad- vantage of all the existing methods on blending physical prior, and is capable of generating new hybrid architectures for tasks under diversified physical environments. With the augmented search space and knowledge from prior information, it is possible for the proposed PhysicsNAS to gener- alize its performance with limited number of training samples.
Figure 3: Search space of our PhysicsNAS. In the proposed PhysicsNAS, all the nodes are densely connected by mixed operators from predefined candidate operation sets. The hidden nodes can obtain information from the original inputs or from previous hidden nodes within this search setup. The training process is supervised by both ground truth and physical constraints.
Figure 4: We evaluate our method on a simulator of classical tasks. The first task (Left) is predicting the trajectory of a ball being tossed, and the second task (Right) is estimating the velocities of two objects after collision.
Figure 5: PhysicsNAS has lower errors compared with the best PBL methods over a range of quality conditions in physics and data. The left figure shows comparison between best PBL methods and PhyisicsNAS along different physical mismatch levels. The physical mismatch levels are from extreme to low, respectively. Here (r : ±i, k : j) refers to the mismatch level of a initial acceleration range [−im/s 2 , im/s 2 ] and a damping factor j. Analogously, the right figure shows comparison along different data amounts. Results in the left figure are all trained with 32 samples and the results in the right figure are trained at low physical mismatch level.The plots show error; lower curves are preferred.
Figure 6: Utilization of physical operations in PhysicsNAS. The selection of physics-inspired operation depends on its accuracy. PhysicsNAS tends to utilize the physical operations when they are more accurate (like the elastic collision model), and prefers a residual connection when they are inaccurate (like the parabola equation).
Figure 7: Failure case. In rare situations, a single-stream network could be preferred. PhysicsNAS is unable to converge to single-stream architectures due to the edge selection mechanism.
Table 1: Testing performance on tossing task. We adopt the average Euclidean distance be- tween the ground truth and the predicted loca- tions as the evaluation metric (lower distance is better). The low mismatch level corresponds to a small random initial acceleration range [−1m/s 2 , 1m/s 2 ] and a small damping factor 0.2. The high mismatch level corresponds to a large acceleration range [−3m/s 2 , 3m/s 2 ] and a large damping factor 0.5. The best model is marked in red and the sub-optimal is in blue.
Table 2: Testing performance on collision task. We use similar Euclidean distance be- tween the estimated speed and the ground-truth speed as the metric (lower distance is better). The low mismatch level corresponds to a ran- dom initial friction coefficient in range [0.28, 0.32], and the high mismatch level corresponds to a friction coefficient in range [0.45, 0.55]. The best model is marked in red and the sub- optimal is in blue.
```

Main Content:
```

Section Title: INTRODUCTION
  INTRODUCTION Advances in machine learning can transform the way physical calculations are performed. Many physical models are idealized and do not precisely match real-world data. An elementary exam- ple would be equations for projectile motion which do not account for air resistance. Using these idealized equations, a completely physics-driven approach would have large errors on real-world data. A separate approach is completely data-driven, e.g., one could repeatedly record real-world projectile tosses and use a regression model to estimate a future trajectory; unfortunately, this ap- proach requires large datasets and lacks interpretability. To bridge this gap, the field of physics-based learning (PBL) aims to blend physical priors with data-driven inference, to combine the best of both worlds. Previous PBL architectures have achieved competitive performance on a wide variety of tasks in microscopy ( Rivenson et al., 2019 ;  Nehme et al., 2018 ;  Nguyen et al., 2018 ;  Sinha et al., 2017 ;  Goy et al., 2018 ), low level and high level computer vision ( Ba et al., 2019 ;  Sun et al., 2019 ), medical imaging ( Jin et al., 2017 ;  Kang et al., 2017 ), and robot control ( Zeng et al., 2019 ;  Ajay et al., 2019 ). These seemingly diverse problem statements share a common thread: the presence of a partially known physical prior that can be blended with a neural network. Unfortunately, existing PBL methods are typically designed for a specific task. Generalization would (as a first step) require a PBL architecture capable of adapting to variations in the correctness of physics or the quality of training data. Our experiments show that no such architecture exists ( Fig- ure 1  and Section 4.3). Having a general recipe for blending physics and learning is an important step in adopting physics-based learning to encompass the wide range of physical problems, where priors are only approximate and training data can be sparse. In this work, we approach the problem of PBL from a different angle. Inspired by work in neural architecture search (NAS) ( Zoph & Le, 2016 ;  Baker et al., 2016 ;  Liu et al., 2018 ;  Cai et al., 2018 ), we propose a first attempt to automatically find the optimal PBL architecture, taking into account characteristics of not just data, but also physics. To incorporate physical models into NAS, we find that three modifications must be made to the existing NAS framework: (1) the inclusion of physical inputs; (2) the inclusion of physical operation sets; and (3) edge weights to normalize variations in the degrees of freedom introduced by the inclusion of physical operators. As these modifications are specific to the PBL problem, we refer to our algorithm as PhysicsNAS. As shown in  Figure 1 , the Under review as a conference paper at ICLR 2020 goal of PhysicsNAS is to handle a diverse range of quality in the physical prior or data. Experiments in Section 4.3 offer support for this goal, where PhysicsNAS outperforms previous PBL methods on multiple physical tasks across a range of physical prior and dataset conditions. The performance improvement over existing PBL methods ranges from 3% to 60%. Our contributions to physics-based learning can be summarized as follows: • We make a first attempt at bringing neural architecture search (NAS) into the realm of physics- based learning (PBL), introducing PhysicsNAS as a new method for PBL. • We show in our experiments that PhysicsNAS generalizes to a wider range of diversity in data and physical priors, as compared to previous PBL methods; • We interpret the converged architectures of PhysicsNAS in context of prior PBL work, to provide general evidence that: (a) Accurate physical operations can be embedded into the network design if there is enough training data available; (b) Residual Physics is a preferred alternative to inaccu- rate physical operations; (c) Physical Fusion is a general strategy that can be adopted in various physical environments. Separate from our work, these insights can help lay a foundation for how to explain the choice of PBL models in future work. Although our primary contributions are to PBL, it is worth noting that conventional differentiable NAS approaches ( Liu et al., 2018 ) are not designed to incorporate physical priors; in developing this paper we found it necessary to modify such approaches to incorporate physical priors as both inputs and candidate operations.

Section Title: CATEGORIZING PRIOR WORK IN PHYSICS-BASED LEARNING
  CATEGORIZING PRIOR WORK IN PHYSICS-BASED LEARNING There has been remarkable progress in blending physical priors with neural networks, over the past few years. Here, we make a first attempt to group previous methods into the four categories as illustrated in  Figure 2 : • Physical Fusion feeds the solution from physics-based models as part of the input ( Karpatne et al., 2017 ;  Ba et al., 2019 ). The solutions can be stacked with the original input, or additionalidentical network branches can be used to extract features separately; • Residual Physics is another way to improve the model-based solutions with deployments in robot control ( Zeng et al., 2019 Continuing to propose new models for PBL is a viable direction, however this may not address adaptability to diverse scenarios of physical model mismatch and sparsity in training data. Physic- sNAS is a different tack, where we design basic operation sets inspired by PBL strategies, and allow networks to customize their architectures during training.

Section Title: PHYSICSNAS
  PHYSICSNAS In what follows, we describe the PhysicsNAS algorithm. In Section 3.1, we discuss the problem setup. We then describe the search algorithm in Section 3.2 and the detailed features of PhysicsNAS in Section 3.3.

Section Title: PROBLEM SETUP
  PROBLEM SETUP In the PBL problem, we have access to a training set D train = {(x i , y i )} N i=1 and a partially known physical operator A phy . Each sample within the training set is a data pair (x i , y i ) formed by an input instance x i ∈ X and the corresponding output (label) y i ∈ Y , and the objective is to learn a function f (·) that maps input space to output space X → Y . f (·) is approximated by a physics network from a search space H with hypothesesf (ω, α, A phy ), where ω denotes network parameters and α denotes architecture parameters. The learning algorithm searches inside H and tries to find {ω, α} that parameterizes the optimalf (ω, α, A phy ) for D train . The challenge for these problems lies in finding a suitable method to incorporate A phy into the network design under diverse regimes of physical model mismatch.

Section Title: SEARCH ALGORITHM
  SEARCH ALGORITHM We develop PhysicsNAS based on differentiable NAS techniques ( Liu et al., 2018 ;  Cai et al., 2018 ). With learnable architecture parameters α and continuity relaxation, both network architectures and Under review as a conference paper at ICLR 2020 parameters can be updated using gradient descent. In contrast to NAS for complicated vision tasks, we do not search the cell structures and apply these searched cells to predefined meta-architectures in PhysicsNAS. As such, PhysicsNAS tries to learn an architecture that links the network input and network output directly. The search space of PhysicsNAS is illustrated in  Figure 3 , where the whole architecture is represented by a directed acyclic graph with nodes {N i } N i=1 and edges {E m } M m=1 . Each edge connects two nodes (N i , N j ) through a mixed operator, and each node corresponds to a type of input or a feature vector extracted from previous nodes through the mixed operators. The output of the mixed operator between (N i , N j ) is the gated sum of all candidate operations {o k } K k=1 : m ij (n i ) = K k=1 g o k o k (n i ), (1) where m ij is the output of this mixed operator, n i is the feature vector of node N i , g o is the binarized operation mask based on the softmax probability of architecture parameters α o in ( Cai et al., 2018 ), and K is the number of operations inside a edge, which depends on the properties of node pair (N i , N j ). The nodes are densely connected, so that n j , the output features of node N j , is the gated sum of features from all its previous nodes: n j = j−1 i=0 g ei m ij (n i ) = j−1 i=0 g ei K k=1 g o k o k (n i ), (2) where g e is the binarized edge mask based on the softmax probability of architecture parameters α e , and N j can either be an intermediate node or the output node. During training, we retain two incoming edges for each node and one operation for each edge through the binary gate sampling in ( Cai et al., 2018 ). While for inference, we pick two candi- date edges with the largest edge probabilities, and select the operation with maximum operation probability for each of the two edges. We choose two edges for each node to leave the potential for PhysicsNAS to learn complicated structures, like skip connection and multi-stream encoding. In order to learn both the network weights and the associated architecture parameters, we update these two sets of parameters alternately. In the architecture step, we freeze the network weights ω and minimize the validation loss L val (ω, α) by updating α. In the network step, we update ω to minimize the training loss L train (ω, α) with frozen α.

Section Title: PHYSICSNAS FEATURES
  PHYSICSNAS FEATURES To incorporate priors into existing differentiable NAS ( Liu et al., 2018 ;  Cai et al., 2018 ), we make three unique modifications into the search process of PhysicsNAS.

Section Title: Physical Inputs
  Physical Inputs As a first step in blending physics into PhysicsNAS, we need to prepare unique input nodes that take into account four categories of input information: 1) the data input X; 2) the duplicated data input X dup to verify whether physical information is indeed necessary since each Under review as a conference paper at ICLR 2020

Section Title: What is the trajectory after tossing?
  What is the trajectory after tossing? node has to pick two edges; 3) the estimated solution from physicsŶ phy = A phy (X); and 4) the concatenation of X andŶ phy to test which stage to conduct the physical fusion.

Section Title: Physical Operations
  Physical Operations To merge physical models inside the network, we create physics-informed operation sets O = {o N N1 , ..., o N N L , o phy }, where o N Ni denotes the neural network operations (e.g.,fully-connected layer, skip connection) and o phy denotes the physical forward operation. Specifically, for the physical forward module, we also use a light-weight network layer, such as a fully-connected (FC) layer, to make the size of its input consistent with the parameter size required by the physical module. Physical forward modules are only included in the edges that connect to the output node in our implementation.

Section Title: Edge Weights
  Edge Weights In PhysicsNAS, not all edges are created with the same amount of operations, since they are used to connect different types of nodes. Consequently, if we select the edges purely based on the operation probabilities, edges with fewer operations are naturally preferred due to the softmax probability, which causes a biased architecture selection. We solve this issue by introducing the edge weight as described in Equation 2. After searching, we first pick a desired edge according to the edge weights, and then select the desired operation for that edge based on the operation weights.

Section Title: EXPERIMENTS AND RESULTS
  EXPERIMENTS AND RESULTS To comprehensively evaluate PhysicsNAS, we simulate two representative physical tasks for which we can vary the model mismatch: 1) predicting trajectories of an object being tossed; and 2) esti- mating the speed of rigid objects after collision.  Figure 4  illustrates these tasks; further details are provided in Section 4.1. Comparison PBL architectures are described in Section 4.2. Finally, we evaluate PhysicsNAS and provide a detailed analysis on the searched architectures in Section 4.3.

Section Title: DESCRIPTION OF TASKS
  DESCRIPTION OF TASKS For the TOSSING TRAJECTORY PREDICTION task (see  Figure 4  for visualization), the initial three locations of the object {l 1 , l 2 , l 3 } are given as input X, and our objective is to predict locations of this object in the following 15 time stamps, {l 4 , l 5 , ..., l 18 }. We only consider the displacement within a 2D plane, therefore, the coordinates of each location can be represented by two numbers, i.e. l i = (l xi , l yi ). We adopt the following elementary free-falling equations as the prior, and examine different methods under this inadequate physical prior: Y phy : l xi = l x1 + v x t i l yi = l y1 + v y t i − 1 2 gt 2 i , (3) where l xi and l yi denote the object location at time t i , l x1 and l y1 are the initial location of the object, v x and v y denote the initial velocities along horizontal and vertical directions respectively, and g is the fixed gravitational acceleration of 9.8m/s 2 . We introduce two model mismatches: the random acceleration as the winds and an additional damping factor based on F air = k × v 2 to simulate the air resistance. The future locations estimated according to mismatched prior are used as the physical inputŶ phy . As to the physical modules in Embedded Physics and PhysicsNAS, we Under review as a conference paper at ICLR 2020 estimate parameters {l x1 ,l y1 ,v x ,v y }, and substitute these parameters into Equation 3 as the physical operation. In the COLLISION SPEED ESTIMATION task (see  Figure 4  for visualization), we use the speed of two objects at the initial two time stamps, object mass, and the distance between these two objects {v a1 , v a2 , v b1 , v b2 , m a , m b , D} as the input X to estimate the speed after their collision {v a f , v b f }. We assume the objects have the different mass, and can only move along one direction. Based on energy conservation and momentum conservation for perfectly elastic collision, we adopt the following prior:Ŷ phy : We add sliding friction to the system as intentional model mismatch, where conservation prior in Equation 4 does not hold. Solutions without the consideration of friction are used asŶ phy , and {m a ,m b ,v a1 ,v b1 } are estimated for the physical modules.

Section Title: MANUALLY DESIGNED PBL METHODS
  MANUALLY DESIGNED PBL METHODS For the sake of comparison, several manually designed architectures from Section 2 are also eval- uated. We use a three-layer multilayer perceptron (MLP) as the naive data-driven baseline, since it has sufficient expressiveness to fit any continuous function, especially the elementary physical tasks we have chosen ( Csáji, 2001 ). Network structures for the Physical Regularization model and the Residual Physics model are the same as the naive model. The output of the Residual Physics model is a summation ofŶ phy and the learned residual from the network, while there is an additional regu- larization term in loss function of the Physical Regularization model. Since only a partially correct physical prior is used, directly using physical solution as the regularization will in turn aggravate the error. Thus, we introduce an ReLU-based regularization similar to ( Karpatne et al., 2017 ). The reg- ularization loss penalizes the network solution based on the assumption that the object moves along one direction in the horizontal axis for the trajectory prediction task, and the total kinetic energy is less than the initial kinetic energy for the speed estimation task. In the Physical Fusion approach, two separate branches are utilized to extract features from X andŶ phy respectively, and each of them is a two-layer MLP. The extracted features will then be concatenated and fed into the output layer. The Embedded Physics model first estimates necessary parameters in Equation 3 and Equa- tion 4 with a three-layer MLP, and then produces trajectory estimation based on the fixed physical process. All the above models are supervised by the ground-truth future locations with mean square error (MSE) loss, and the hidden dimension for FC layers are 128.

Section Title: RESULTS ANALYSIS
  RESULTS ANALYSIS

Section Title: Training Details
  Training Details To evaluate the importance and success of the proposed approach, we vary the physical model mismatch and sparsity in training data in a controlled manner. When training Physic- sNAS, we split the training set into two subsets of the same size to update architecture variables and network variables respectively. We limit the number of learnable nodes in PhysicsNAS to be 5, and retrain the searched architectures with full training sets after searching. The models are implemented in PyTorch ( Paszke et al., 2017 ), and are trained using the Adam optimizer ( Kingma & Ba, 2014 ). Moreover, for all baseline approaches we compare in this paper, we fine tune their hyperparameters in order to make fair comparisons. We choose three hyperparameter sets for each scenario and run five times for each method. We finally pick out the best result for each method.

Section Title: Performance Comparison
  Performance Comparison We apply the proposed PhysicsNAS to learn architectures embedded in the search space. The testing results of PhysicsNAS and other existing PBL methods (as detailed in Section 4.2) are summarized in  Table 1  and  Table 2 . As shown in these two tables, the perfor- mance of different PBL models varies based on the disparity of mismatch levels and training data sizes, while PhysicsNAS is capable of generating architectures that outperform these manual PBL models consistently. Results in  Figure 5  further demonstrate this capability along data dimension and physics dimension in a fine-grained scale. We also conduct an ablation study on PhysicsNAS by removing task-specific adaptations such as physical inputs and operations. Results in Appendix D show the task-specific adaptations improve the performance of PhysicsNAS at different mismatch levels, demonstrating that having physical inputs and physical operations in PhysicsNAS's search Under review as a conference paper at ICLR 2020 space is necessary. Our experiments also show that PhysicsNAS is able to perform inference on small training datasets: the physical prior reduces the demand for high-fidelity training samples. We find that PhysicsNAS only requires less than 64 training samples to reach the same testing performance of a naive MLP with 256 training samples. Moreover, the performance gap between PhysicsNAS and naive MLP method minimizes as the number of training samples increases. This suggests that PhysicsNAS is more favorable in scenarios where training data is not rich enough on the other hand. A detailed discussion about how PhysicsNAS reduces the demand on training data is provided in Appendix E.

Section Title: Utilization of Physical Inputs
  Utilization of Physical Inputs The physical inputs are always selected in our searched architectures for these two tasks, which verifies the importance of physical information during learning. Please re- fer to Appendix B for the illustration of a range of searched architectures corresponding to scenarios in  Table 1  and  Table 2 .

Section Title: Utilization of Physical Operations
  Utilization of Physical Operations The selection of physics-inspired operations are more nuanced, depending on the accuracy of physical information encoded in the physical operations as well as the amount of training data.  Figure 6  shows two examples, one where physical operations are selected and the other where they are not selected. In particular, the inaccurate physical operation in the Under review as a conference paper at ICLR 2020 trajectory task is preferred at early training epochs. However, as training proceeds, the learned FC modules achieve higher accuracy and the network thus discards the physical operations. As a result, the Residual Physics strategy is adopted in the final searched architecture. For the collision task, the physical operation could model the perfectly elastic collision completely, and the estimated physical solutions are precise if the estimated physical parameters are accurate. Therefore, physical operations are selected when there exists a robust estimation of physical parameters. However, it usually requires sufficient training samples to obtain this robust estimation, which might be a reason why physical operations are only selected in the cases with 128 training samples. More details about the changes of network architectures during searching can be found in Appendix F.

Section Title: Failure Case
  Failure Case In differentiable NAS, the training algorithm aims to optimize the over-parameterized network with all the edges and operations. Therefore, it is necessary to prune the redundant edges and operations after training. We adopt the pruning mechanism in ( Liu et al., 2018 ), where each node has to retain two incoming edges. For extreme cases where single-stream architectures are optimal, PhysicsNAS may generate sub-optimal architectures due to this arrangement. As shown in  Figure 7 , we make a toy comparison between two lightweight architectures on the collision task with the friction coefficient range [0.15, 0.25] and 32 training samples. It is notable that by simply adding an additional stream to the input x, the new searchable architecture deteriorates the result. Introducing an adaptive edge selection mechanism might be a meaningful future work. This limitation could also be overcome by resorting to other NAS frameworks, such as those based on reinforcement learning ( Pham et al., 2018 ).

Section Title: CONCLUSION
  CONCLUSION In conclusion, our experiments show that PhysicsNAS can handle a wider range of input physical models and data, as compared to existing PBL methods. This is only a first attempt at increasing the diversity of PBL through architecture search. Ultimately, our hope is to apply PhysicsNAS to problems as diverse as microscopy( Barbastathis et al., 2019 ), computer vision ( Velten et al., 2012 ), sensor fusion ( Eitel et al., 2015 ;  Xu et al., 2018 ) and astrophysics ( Bouman et al., 2016 ;  Akiyama et al., 2019 ), where it is important to handle variations in model mismatch and dataset quality across these problem domains. Under review as a conference paper at ICLR 2020

```
