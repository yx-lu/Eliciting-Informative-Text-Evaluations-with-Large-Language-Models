<article article-type="research-article"><front><article-meta><title-group><article-title>Under review as a conference paper at ICLR 2020 MINCUT POOLING IN GRAPH NEURAL NETWORKS</article-title></title-group><abstract><p>The advance of node pooling operations in Graph Neural Networks (GNNs) has lagged behind the feverish design of new message-passing techniques, and pool- ing remains an important and challenging endeavor for the design of deep archi- tectures. In this paper, we propose a pooling operation for GNNs that leverages a differentiable unsupervised loss based on the minCUT optimization objective. For each node, our method learns a soft cluster assignment vector that depends on the node features, the target inference task (e.g., a graph classification loss), and, thanks to the minCUT objective, also on the connectivity structure of the graph. Graph pooling is obtained by applying the matrix of assignment vectors to the ad- jacency matrix and the node features. The proposed method can also be used as a stand-alone module to cluster vertexes in annotated graphs and solve unsupervised problems. We validate the effectiveness of the proposed pooling method on down- stream tasks, including supervised graph classification and a set of unsupervised tasks, which reveal the limitations of existing GNN pooling approaches.</p></abstract></article-meta></front><body><sec><title>INTRODUCTION</title><p>A fundamental component in deep convolutional neural networks is the pooling operation, which re- places the output of convolutions with local summaries of nearby points and is usually implemented by maximum or average operations (<xref ref-type="bibr" rid="b16">Lee et al., 2016</xref>). State-of-the-art architectures alternate convo- lutions, which extrapolate local patterns irrespective of the specific location on the input signal, and pooling, which lets the ensuing convolutions capture aggregated patterns. Pooling allows to learn abstract representations in deeper layers of the network by discarding information that is superflu- ous for the task, and keeps model complexity under control by limiting the growth of intermediate features.</p><p>Graph Neural Networks (GNNs) extend the convolution operation from regular domains, such as images or time series, to data with arbitrary topologies and unordered structures described by graphs (<xref ref-type="bibr" rid="b0">Battaglia et al., 2018</xref>). The development of pooling strategies for GNNs, however, has lagged behind the design of newer and more effective message-passing (MP) operations (<xref ref-type="bibr" rid="b10">Gilmer et al., 2017</xref>), such as graph convolutions, mainly due to the difficulty of defining an aggregated version of the original graph that supports the pooled signal.</p><p>A na&#239;ve pooling strategy in GNNs is to average all nodes features (<xref ref-type="bibr" rid="b18">Li et al., 2016</xref>), but it has lim- ited flexibility since it does not extract local summaries of the graph structure, and no further MP operations can be applied afterwards. An alternative approach consists in pre-computing coarsened versions of the original graph and then fit the data to these deterministic structures (<xref ref-type="bibr" rid="b2">Bruna et al., 2013</xref>). While this aggregation accounts for the connectivity of the graph, it ignores task-specific objectives as well as the node features.</p><p>In this paper, we propose a differentiable pooling operation implemented as a neural network layer, which can be seamlessly combined with other MP layers (see <xref ref-type="fig" rid="fig_0">Fig. 1</xref>). The parameters in the pool- ing layer are learned by combining the task-specific loss with an unsupervised regularization term, which optimizes a continuous relaxation of the normalized minCUT objective. The minCUT iden- tifies dense graph components, where the nodes features become locally homogeneous after the message-passing. By gradually aggregating these components, the GNN learns to distil global prop- erties from the graph. The proposed minCUT pooling operator (minCUTpool) yields partitions that 1) cluster together nodes which have similar features and are strongly connected on the graph, and 2) take into account the objective of the downstream task.</p></sec><sec><title>BACKGROUND</title></sec><sec><title>MINCUT AND SPECTRAL CLUSTERING</title><p>Given a graph G = {V, E}, |V| = N , and the associated adjacency matrix A &#8712; R N &#215;N , the K-way normalized minCUT (simply referred to as minCUT) aims at partitioning V in K disjoint subsets by removing the minimum volume of edges. The problem is equivalent to maximizing 1 K K k=1 links(V k ) degree(V k ) = 1 K K k=1 i,j&#8712;V k E i,j i&#8712;V k ,j&#8712;V\V k E i,j , (1) where the numerator counts the edge volume within each cluster, and the denominator counts the edges between the nodes in a cluster and the rest of the graph (<xref ref-type="bibr" rid="b26">Shi &amp; Malik, 2000</xref>). Let C &#8712; R N &#215;K be a cluster assignment matrix, so that C i,j = 1 if node i belongs to cluster j, and 0 otherwise. The minCUT problem can be expressed as maximize 1 K K k=1 C T k AC k C T k DC k , s.t. C &#8712; {0, 1} N &#215;K , C1 K = 1 N , (2) where D = diag(A1 N ) is the degree matrix (<xref ref-type="bibr" rid="b6">Dhillon et al., 2004</xref>). Since problem (2) is NP-hard, it is usually recast in a relaxed formulation that can be solved in polynomial time and guarantees a near-optimal solution (<xref ref-type="bibr" rid="b16">Yu &amp; Shi, 2003</xref>):</p><p>While the optimization problem (3) is still non-convex, there exists an optimal solution Q * = U K O, where U K &#8712; R N &#215;K contains the eigenvectors of A corresponding to the K largest eigenvalues, and O &#8712; R K&#215;K is an orthogonal transformation (<xref ref-type="bibr" rid="b13">Ikebe et al., 1987</xref>).</p><p>Since the elements of Q * are real values rather than binary cluster indicators, the spectral clustering (SC) approach can be used to find discrete cluster assignments. In SC, the rows of Q * are treated as node representations embedded in the eigenspace of the Laplacian, and are clustered together with standard algorithms such as k-means (<xref ref-type="bibr" rid="b31">Von Luxburg, 2007</xref>). One of the main limitations of SC lies in the computation of the spectrum of A, which has a memory complexity of O(N 2 ) and a computational complexity of O(N 3 ). This prevents its applicability to large datasets.</p><p>To deal with such scalability issues, the constrained optimization in (3) can be solved by gradient descent algorithms that refine the solution by iterating operations whose individual complexity is O(N 2 ), or even O(N ) (<xref ref-type="bibr" rid="b11">Han &amp; Filippone, 2017</xref>). Those algorithms search the solution on the manifold induced by the orthogonality constraint on the columns of Q, by performing gradient updates along the geodesics (<xref ref-type="bibr" rid="b32">Wen &amp; Yin, 2013</xref>; <xref ref-type="bibr" rid="b3">Collins et al., 2014</xref>). Alternative approaches rely on the QR factorization to constrain the space of feasible solutions (<xref ref-type="bibr" rid="b4">Damle et al., 2016</xref>), and alleviate the cost O(N 3 ) of the factorization by ensuring that orthogonality holds only on one minibatch at a time (<xref ref-type="bibr" rid="b24">Shaham et al., 2018</xref>).</p></sec><sec><title>Under review as a conference paper at ICLR 2020</title><p>Other works based on neural networks include an autoencoder trained to map the ith row of the Laplacian to the ith components of the first K eigenvectors, to avoid the spectral decomposi- tion (<xref ref-type="bibr" rid="b29">Tian et al., 2014</xref>). <xref ref-type="bibr" rid="b33">Yi et al. (2017)</xref> use a soft orthogonality constraint to learn spectral em- beddings as a volumetric reparametrization of a precomputed Laplacian eigenbase. <xref ref-type="bibr" rid="b24">Shaham et al. (2018)</xref>; <xref ref-type="bibr" rid="b14">Kampffmeyer et al. (2019)</xref> propose differentiable loss functions to partition generic data and process out-of-sample data at inference time. <xref ref-type="bibr" rid="b20">Nazi et al. (2019)</xref> generate balanced node partitions with a GNN, but adopt an optimization that does not encourage cluster assignments to be orthogonal.</p></sec><sec><title>GRAPH NEURAL NETWORKS</title><p>Many approaches have been proposed to process graphs with neural networks, including recurrent architectures (<xref ref-type="bibr" rid="b23">Scarselli et al., 2009</xref>; <xref ref-type="bibr" rid="b18">Li et al., 2016</xref>) or convolutional operations inspired by filters used in graph signal processing (<xref ref-type="bibr" rid="b5">Defferrard et al., 2016</xref>; <xref ref-type="bibr" rid="b1">Bianchi et al., 2019</xref>). Since our focus is on graph pooling, we base our GNN implementation on a simple MP operation, which combines the features of each node with its 1st-order neighbors. To account for the initial node features, it is possible to introduce self-loops by adding a (scaled) identity matrix to the diagonal of A (<xref ref-type="bibr" rid="b15">Kipf &amp; Welling, 2017</xref>). Since our pooling will modify the structure of the adjacency matrix, we prefer a MP implementation that leaves the original A unaltered and accounts for the initial node features by means of skip connections.</p><p>Let&#195; = D &#8722; 1 2 AD &#8722; 1 2 &#8712; R N &#215;N be the symmetrically normalized adjacency matrix and X &#8712; R N &#215;F the matrix containing the node features. The output of the MP layer is X (t+1) = M P (X (t) ,&#195;) = ReLU(&#195;X (t) W m + X (t) W s ), (4) where &#920; M P = {W m , W s } are the trainable weights relative to the mixing and skip component of the layer, respectively.</p></sec><sec><title>PROPOSED METHOD</title><p>The minCUT pooling strategy computes a cluster assignment matrix S &#8712; R N &#215;K by means of a multi-layer perceptron, which maps each node feature x i into the ith row of S: S = sof tmax(ReLU(XW 1 )W 2 ), (5) where &#920; P ool = {W 1 &#8712; R F &#215;H , W 2 &#8712; R H&#215;K } are trainable parameters. The softmax function guarantees that s i,j &#8712; [0, 1] and enforces the constraints S1 K = 1 N inherited from the optimization problem in (2). The parameters &#920; M P and &#920; P ool are jointly optimized by minimizing the usual task-specific loss, as well as an unsupervised loss L u , which is composed of two terms</p><p>where &#183; F indicates the Frobenius norm.</p><p>The cut loss term, L c , evaluates the minCUT given by the cluster assignment S, and is bounded by &#8722;1 &#8804; L c &#8804; 0. Minimizing L c encourages strongly connected nodes to be clustered together, since the inner product s i , s j increases when&#227; i,j is large. L c has a single maximum, reached when the numerator T r(S T&#195; S) = 1 K K k=1 S T k&#195; S k = 0. This occurs if, for each pair of connected nodes (i.e.,&#227; i,j &gt; 0), the cluster assignments are orthogonal (i.e., s i , s j = 0). L c reaches its minimum, &#8722;1, when T r(S T&#195; S) = T r(S TD S). This occurs when in a graph with K disconnected com- ponents the cluster assignments are equal for all the nodes in the same component and orthogonal to the cluster assignments of nodes in different components. However, L c is a non-convex func- tion and its minimization can lead to local minima or degenerate solutions. For example, given a connected graph, a trivial optimal solution is the one that assigns all nodes to the same cluster. As a consequence of the continuous relaxation, another degenerate minimum occurs when the cluster assignments are all uniform, that is, all nodes are equally assigned to all clusters. This problem is exacerbated by prior message-passing operations, which make the node features more uniform.</p></sec><sec><title>Under review as a conference paper at ICLR 2020</title><p>The orthogonality loss term, L o , penalizes the degenerate minima of L c by encouraging the cluster assignments to be orthogonal and the clusters to be of similar size. Since the two matrices in L o have unitary norm it is easy to see that 0 &#8804; L o &#8804; 2. Therefore, L o does not dominate over L c and the two terms can be safely summed directly (see <xref ref-type="fig" rid="fig_3">Fig. 4</xref> for an example). I K can be interpreted as a (rescaled) clustering matrix I K =&#348; T&#348; , where&#348; assigns exactly N/K points to each cluster. The value of the Frobenius norm between clustering matrices is not dominated by the performance on the largest clusters (<xref ref-type="bibr" rid="b16">Law et al., 2017</xref>) and, thus, can be used to optimize intra-cluster variance. Contrarily to SC methods that search for feasible solutions only within the space of orthogonal matrices, L o only introduces a soft constraint that could be violated during the learning procedure. Since L c is non-convex, the violation compromises the theoretical guarantee of convergence to the optimum of (3). However, we note that:</p><p>1. the cluster assignments S are well initialized: after the MP operation, the features of the connected vertices become similar and, since the MLP is a smooth function (<xref ref-type="bibr" rid="b21">Nelles, 2013</xref>), it yields similar cluster assignments for those vertices;</p><p>2. in the GNN architecture, the minCUT objective is a regularization term and, therefore, a solution which is sub-optimal for (3) could instead be adequate for the specific objective of the downstream task;</p><p>3. optimizing the task-specific loss helps the GNN to avoid the degenerate minima of L c .</p></sec><sec><title>COARSENING</title><p>The coarsened version of the adjacency matrix and the graph signal are computed as A pool = S T&#195; S; X pool = S T X, (7) where the entry x pool i,j in X pool &#8712; R K&#215;F is the weighted average value of feature j among the elements in cluster i. A pool &#8712; R K&#215;K is a symmetric matrix, whose entries a pool i,i are the total number of edges between the nodes in the cluster i, while a pool i,j is the number of edges between cluster i and j. Since A pool corresponds to the numerator of L c in (7), the trace maximization yields clusters with many internal connections and weakly connected to each other. Hence, A pool will be a diagonal-dominant matrix, which describes a graph with self-loops much stronger than any other connection. Because self-loops hamper the propagation across adjacent nodes in the MP operations following the pooling layer, we compute the new adjacency matrix&#195; pool by zeroing the diagonal and by applying the degree normalization A = A pool &#8722; I K diag(A pool );&#195; pool =D &#8722; 1 2&#194;D &#8722; 1 2 . (8) where diag(&#183;) returns the matrix diagonal.</p></sec><sec><title>DISCUSSION AND RELATIONSHIP WITH SPECTRAL CLUSTERING</title><p>The proposed method is straightforward to implement: the cluster assignments, the loss, graph coarsening, and feature pooling are all computed with standard linear algebra operations. There are several differences between minCUTpool and classic SC methods. SC partitions the graph based on the Laplacian, but does not account for the node features. Instead, the cluster assignments s i found by minCUTpool depend on x i , which works well if connected nodes have similar features. This is a reasonable assumption in GNNs since, even in disassortative graphs (i.e., networks where dissimilar nodes are likely to be connected (<xref ref-type="bibr" rid="b22">Newman, 2003</xref>)), the features tend to become similar due to the MP operations.</p><p>Another difference is that SC handles a single graph and is not conceived for tasks with multiple graphs to be partitioned independently. Instead, thanks to the independence of the model parameters from the number of nodes N and from the graph spectrum, minCUTpool can generalize to out- of-sample data. This feature is fundamental in problems such as graph classification, where each sample is a graph with a different structure, and allows to train the model on small graphs and process larger ones at inference time. Finally, minCUTpool directly uses the soft cluster assignments rather than performing k-means afterwards.</p></sec><sec><title>RELATED WORK ON POOLING IN GNNS</title></sec><sec><title>Trainable pooling methods</title><p>Similarly to our method, these approaches learn how to generate coarsened version of the graph through differentiable functions, which take as input the nodes fea- tures X and are parametrized by weights optimized on the task at hand.</p><p>Diffpool (<xref ref-type="bibr" rid="b4">Ying et al., 2018</xref>) is a pooling module that includes two parallel MP layers: one to compute the new node features X (t+1) and another to generate the cluster assignments S. Diff- pool implements an unsupervised loss that consists of two terms. First, the link prediction term A &#8722; SS T F minimizes the Frobenius norm of the difference between the adjacency and the Gram matrix of the cluster assignments, encouraging nearby nodes to be clustered together. The sec- ond term 1 N N i=1 H(S i ) minimizes the entropy of the cluster assignments to make them alike to one-hot vectors. Like minCUTpool, Diffpool clusters the vertices of annotated graphs, but yields completely different partitions, since it computes differently the clustering assignments, the coars- ened adjacency matrix and, most importantly, the unsupervised loss. In Diffpool, such a loss shows pathological behaviors that are discussed later in the experiments.</p><p>The approach dubbed Top-K pooling (<xref ref-type="bibr" rid="b12">Hongyang Gao, 2019</xref>; <xref ref-type="bibr" rid="b16">Lee et al., 2019</xref>), learns a projection vector that is applied to each node feature to obtain a score. The nodes with the K highest scores are retained, the others are dropped. Since the top-K selection is not differentiable, the scores are also used as a gate/attention for the node features, letting the projection vector to be trained with backpropagation. Top-K is memory efficient as it avoids generating cluster assignments. To prevent A from becoming disconnected after nodes removal, Top-K drops the rows and the columns from A 2 and uses it as the new adjacency matrix. However, computing A 2 costs O(N 2 ) and it is inefficient to implement with sparse operations.</p></sec><sec><title>Topological pooling methods</title><p>These methods pre-compute a pyramid of coarsened graphs, only taking into account the topology (A), but not the node features (X). During training, the node features are pooled with standard procedures and are fit into these deterministic graph structures. These methods are less flexible, but provide a stronger bias that can prevent degenerate solutions (e.g., coarsened graphs collapsing in a single node).</p><p>The approach proposed by <xref ref-type="bibr" rid="b2">Bruna et al. (2013)</xref>, which has been adopted also in other GNN ar- chitectures (<xref ref-type="bibr" rid="b5">Defferrard et al., 2016</xref>; <xref ref-type="bibr" rid="b8">Fey et al., 2018</xref>), exploits GRACLUS (<xref ref-type="bibr" rid="b6">Dhillon et al., 2004</xref>), a hierarchical algorithm based on SC. At each pooling level l, GRACLUS indetifies the pairs of max- imally similar nodes i l and j l to be clustered together into a new vertex k (l+1) . At inference phase, max-pooling is used to determine which node in the pair is kept. Fake vertices are added so that the number of nodes can be halved each time, but this injects noisy information in the graph.</p><p>Node decimation is a method originally proposed in graph signal processing literature (<xref ref-type="bibr" rid="b27">Shuman et al., 2016</xref>), which as been adapted also for GNNs (<xref ref-type="bibr" rid="b28">Simonovsky &amp; Komodakis, 2017</xref>). The nodes are partitioned in two sets, according to the signs of the Laplacian eigenvector associated to the largest eigenvalue. One of the two sets is dropped, reducing the number of nodes each time approx- imately by half. Kron reduction is used to compute a pyramid of coarsened Laplacians from the remaining nodes.</p><p>A procedure proposed in <xref ref-type="bibr" rid="b9">Gama et al. (2018)</xref> diffuses a signal from designated nodes on the graph and stores the observed sequence of diffused components. The resulting stream of information is interpreted as a time signal, where standard CNN pooling is applied. We also mention a pooling operation for coarsening binary unweighted graphs by aggregating maximal cliques (<xref ref-type="bibr" rid="b19">Luzhnica et al., 2019</xref>). Nodes assigned to the same clique are summarized by max or average pooling and become a new node in the coarsened graph.</p></sec><sec><title>EXPERIMENTS</title><p>We consider both supervised and unsupervised tasks, and compare minCUTpool with other GNN pooling strategies. The Appendix provides further details on the experiments and a schematic de- piction of the architectures used in each task. In addition, the Appendix reports two additional experiments: i) graph reconstruction by means of an Auto Encoder with bottleneck, implemented with pooling and un-pooling layers, ii) an architecture with pooling for graph regression.</p></sec><sec><title>CLUSTERING THE GRAPH NODES</title><p>To study the effectiveness of the proposed loss, we perform different node clustering tasks with a simple GNN composed of a single MP layer followed by a pooling layer. The GNN is trained by minimizing L u only, so that its effect is evaluated without the "interference" of a supervised loss.</p></sec><sec><title>Clustering on synthetic networks</title><p>We consider two simple graphs: the first is a network with 6 communities and the second is a regular grid. The adjacency matrix A is binary and the features X are the 2-D node coordinates. <xref ref-type="fig" rid="fig_1">Fig. 2</xref> depicts the node partitions generated by SC (a, d), Diffpool (b, e), and minCUTpool (c, f). Cluster indexes for Diffpool and minCUTpool are obtained by taking the argmax of S row-wise. Compared to SC, Diffpool and minCUTpool leverage the information contained in X. minCUTpool generates very accurate and balanced partitions, demonstrating that the cluster assignment matrix S is well formed. On the other hand, Diffpool assigns some nodes to the wrong community in the first example, and produces an imbalanced partition of the grid.</p></sec><sec><title>Image segmentation</title><p>Given an image, we build a Region Adjacency Graph (<xref ref-type="bibr" rid="b30">Tr&#233;meau &amp; Colan- toni, 2000</xref>) using as nodes the regions generated by an oversegmentation procedure (<xref ref-type="bibr" rid="b7">Felzenszwalb &amp; Huttenlocher, 2004</xref>). The SC technique used in this example is the recursive normalized cut (<xref ref-type="bibr" rid="b26">Shi &amp; Malik, 2000</xref>), which recursively clusters the nodes until convergence. For Diffpool and min- CUTpool, we include node features consisting of the average and total color in each oversegmented region. We set the number of desired clusters to K = 4. The results in <xref ref-type="fig" rid="fig_2">Fig. 3</xref> show that minCUTpool yields a more precise segmentation. On the other hand, SC and Diffpool aggregate wrong regions and, in addition, SC finds too many segments.</p></sec><sec><title>Clustering on citation networks</title><p>We cluster the nodes of three popular citation networks: Cora, Citeseer, and Pubmed. The nodes are documents represented by sparse bag-of-words feature vectors stored in X and the binary undirected edges in A indicate citation links between documents. Each node i is labeled with the document class y i . Once the training is over, to test the quality of the partitions generated by each method we check the agreement between the cluster assignments and the true class labels. <xref ref-type="table" rid="tab_0">Tab. 1</xref> reports the Completeness Score CS(&#7929;, y) = 1 &#8722; H(&#7929;|y) H(&#7929;) and Normalized Mutual Information NMI(&#7929;, y) = H(&#7929;)&#8722;H(&#7929;|y) &#8730; H(&#7929;)&#8722;H(y) , where H(&#183;) is the entropy.</p><p>The GNN architecture configured with minCUTpool achieves a higher NMI score than SC, which does not account for the node features X when generating the partitions. Our pooling operation Under review as a conference paper at ICLR outperforms also Diffpool, since the minimization of the unsupervised loss in Diffpool yields de- generate solutions. The pathological behavior is shown in <xref ref-type="fig" rid="fig_3">Fig. 4</xref>, which depicts the evolution of the NMI scores as the unsupervised losses in Diffpool and minCUTpool are minimized in training. <xref ref-type="table" rid="tab_1">Tab. 2</xref> reports the classification results, highlighting those that are significantly better (p-value &lt; 0.05 w.r.t. the method with the highest mean accuracy). The comparison with Flat helps to understand if a pooling operation is useful or not. The results of Dense, instead, help to quantify how much additional information is brought by the graph structure, with respect to the node features alone. It can be seen that minCUTpool obtains always equal or better results with respect to every other GNN architecture. On the other hand, some pooling procedures do not always improve the performance compared to the Flat baseline, making them not advisable to use in some cases. The WL kernel generally performs worse than the GNNs, except for the Mutagenicity dataset. This is probably because Mutagenicity has smaller graphs than the other datasets, and the adopted GNN architecture is overparametrized for this task. Interestingly, in some dataset such as Proteins and COLLAB it is possible to obtain fairly good classification accuracy with the Dense architecture, meaning that the graph structure only adds limited information. <xref ref-type="fig" rid="fig_4">Fig. 5</xref> reports a comparison of the execution time per training epoch for each pooling algorithm. Graclus and Decimation are understandably the fastest methods, since the coarsened graphs are pre- computed. Among the differentiable pooling methods, minCUTpool is faster than Diffpool, which uses a slower MP layer rather than a MLP to compute cluster assignments, and than Top-K, which computes the square of A at every forward pass.</p></sec><sec><title>CONCLUSIONS</title><p>We proposed a pooling layer for GNNs that coarsens a graph by taking into account both the the connectivity structure and the node features. The layer optimizes a regularization term based on the minCUT objective, which is minimized in conjunction with the task-specific loss to produce node partitions that are optimal for the task at hand.</p><p>We tested the effectiveness of our pooling strategy on unsupervised node clustering tasks, by op- timizing only the unsupervised clustering loss, as well as supervised graph classification tasks on several popular benchmark datasets. Results show that minCUTpool performs significantly better than existing pooling strategies for GNNs.</p><p>Under review as a conference paper at ICLR 2020</p></sec><sec id="figures"><title>Figures</title><fig id="fig_0"><object-id>fig_0</object-id><label>Figure 1:</label><caption><title>Figure 1:</title><p>A deep GNN architecture where message-passing is followed by minCUT pooling.</p></caption><graphic /></fig><fig id="fig_1"><object-id>fig_1</object-id><label>Figure 2:</label><caption><title>Figure 2:</title><p>Node clustering on a community network (K=6) and on a grid graph (K=5).</p></caption><graphic /></fig><fig id="fig_2"><object-id>fig_2</object-id><label>Figure 3:</label><caption><title>Figure 3:</title><p>Image segmentation by clustering the nodes of the Region Adjacency Graph.</p></caption><graphic /><graphic /><graphic /><graphic /><graphic /><graphic /></fig><fig id="fig_3"><object-id>fig_3</object-id><label>Figure 4:</label><caption><title>Figure 4:</title><p>Unsupervised losses and NMI of Diffpool and minCUTpool on Cora.</p></caption><graphic /></fig><table-wrap id="tab_0"><label>Table 1:</label><caption><title>Table 1:</title><p>NMI and CS obtained by clustering the nodes on citation networks over 10 different runs. The number of clusters K is equal to the number of node classes.</p></caption><table><tbody><tr><td /></tr></tbody></table></table-wrap><table-wrap id="tab_1"><label>Table 2:</label><caption><title>Table 2:</title><p>Graph classification accuracy. Significantly better results (p &lt; 0.05) are in bold.</p></caption><table><tbody><tr><td /></tr></tbody></table></table-wrap><fig id="fig_4"><object-id>fig_4</object-id><label>Figure 5:</label><caption><title>Figure 5:</title><p>Average duration of one epoch using the same GNN with different pooling operations. Times were computed with an Nvidia GeForce GTX 1050, on the DD dataset with batch size of 1.</p></caption><graphic /></fig></sec></body><back><ref-list id="ref-list-1"><ref id="b0"><element-citation publication-type="journal"><article-title>Relational inductive biases, deep learning, and graph networks</article-title><source>arXiv preprint arXiv:1806.01261</source><year>2018</year><person-group person-group-type="author"><name><surname>Peter</surname><given-names>References</given-names></name><name><surname>Battaglia</surname><given-names>W</given-names></name><name><surname>Hamrick</surname><given-names>Jessica B</given-names></name><name><surname>Bapst</surname><given-names>Victor</given-names></name><name><surname>Sanchez-Gonzalez</surname><given-names>Alvaro</given-names></name><name><surname>Zambaldi</surname><given-names>Vinicius</given-names></name><name><surname>Malinowski</surname><given-names>Mateusz</given-names></name><name><surname>Tacchetti</surname><given-names>Andrea</given-names></name><name><surname>Raposo</surname><given-names>David</given-names></name><name><surname>Santoro</surname><given-names>Adam</given-names></name><name><surname>Faulkner</surname><given-names>Ryan</given-names></name></person-group></element-citation></ref><ref id="b1"><element-citation publication-type="journal"><article-title>Graph neural networks with con- volutional arma filters</article-title><source>arXiv preprint arXiv:1901.01343</source><year>2019</year><person-group person-group-type="author"><name><surname>Filippo</surname><given-names>Maria</given-names></name><name><surname>Bianchi</surname><given-names>Daniele</given-names></name><name><surname>Grattarola</surname><given-names /></name><name><surname>Livi</surname><given-names>C</given-names></name><name><surname>Alippi</surname><given-names /></name></person-group></element-citation></ref><ref id="b2"><element-citation publication-type="journal"><article-title>Spectral networks and locally connected networks on graphs</article-title><source>arXiv preprint arXiv:1312.6203</source><year>2013</year><person-group person-group-type="author"><name><surname>Bruna</surname><given-names>Joan</given-names></name><name><surname>Zaremba</surname><given-names>Wojciech</given-names></name><name><surname>Szlam</surname><given-names>Arthur</given-names></name><name><surname>Lecun</surname><given-names>Yann</given-names></name></person-group></element-citation></ref><ref id="b3"><element-citation publication-type="journal"><source>European Conference on Computer Vision</source><year>2014</year><fpage>282</fpage><lpage>298</lpage><person-group person-group-type="author"><name><surname>Maxwell</surname><given-names>D</given-names></name><name><surname>Collins</surname><given-names>Ji</given-names></name><name><surname>Liu</surname><given-names>Jia</given-names></name><name><surname>Xu</surname><given-names>Lopamudra</given-names></name><name><surname>Mukherjee</surname><given-names>Vikas</given-names></name><name><surname>Singh</surname><given-names /></name></person-group></element-citation></ref><ref id="b4"><element-citation publication-type="journal"><article-title>Robust and efficient multi-way spectral clustering</article-title><source>arXiv preprint arXiv:1609.08251</source><year>2016</year><person-group person-group-type="author"><name><surname>Damle</surname><given-names>Anil</given-names></name><name><surname>Minden</surname><given-names>Victor</given-names></name><name><surname>Ying</surname><given-names>Lexing</given-names></name></person-group></element-citation></ref><ref id="b5"><element-citation publication-type="journal"><article-title>Convolutional neural networks on graphs with fast localized spectral filtering</article-title><source>Advances in Neural Information Processing Systems</source><year>2016</year><fpage>3844</fpage><lpage>3852</lpage><person-group person-group-type="author"><name><surname>Defferrard</surname><given-names>Micha&#235;l</given-names></name><name><surname>Bresson</surname><given-names>Xavier</given-names></name><name><surname>Vandergheynst</surname><given-names>Pierre</given-names></name></person-group></element-citation></ref><ref id="b6"><element-citation publication-type="journal"><article-title>Kernel k-means: spectral clustering and nor- malized cuts</article-title><source>Proceedings of the tenth ACM SIGKDD international conference on Knowledge discovery and data mining</source><year>2004</year><fpage>551</fpage><lpage>556</lpage><person-group person-group-type="author"><name><surname>Inderjit S Dhillon</surname><given-names>Yuqiang</given-names></name><name><surname>Guan</surname><given-names>Brian</given-names></name><name><surname>Kulis</surname><given-names /></name></person-group></element-citation></ref><ref id="b7"><element-citation publication-type="journal"><article-title>Efficient graph-based image segmentation</article-title><source>Inter- national journal of computer vision</source><year>2004</year><volume>59</volume><issue>2</issue><fpage>167</fpage><lpage>181</lpage><person-group person-group-type="author"><name><surname>Pedro</surname><given-names>F</given-names></name><name><surname>Felzenszwalb</surname><given-names>Daniel P</given-names></name><name><surname>Huttenlocher</surname><given-names /></name></person-group></element-citation></ref><ref id="b8"><element-citation publication-type="journal"><article-title>Splinecnn: Fast geomet- ric deep learning with continuous b-spline kernels</article-title><source>Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</source><year>2018</year><fpage>869</fpage><lpage>877</lpage><person-group person-group-type="author"><name><surname>Fey</surname><given-names>Matthias</given-names></name><name><surname>Lenssen</surname><given-names>Jan Eric</given-names></name><name><surname>Weichert</surname><given-names>Frank</given-names></name><name><surname>M&#252;ller</surname><given-names>Heinrich</given-names></name></person-group></element-citation></ref><ref id="b9"><element-citation publication-type="journal"><article-title>Convolutional neural network architectures for signals supported on graphs</article-title><source>IEEE Transactions on Signal Processing</source><year>2018</year><volume>67</volume><issue>4</issue><fpage>1034</fpage><lpage>1049</lpage><person-group person-group-type="author"><name><surname>Gama</surname><given-names>Fernando</given-names></name><name><surname>Antonio</surname><given-names>G</given-names></name><name><surname>Marques</surname><given-names>Geert</given-names></name><name><surname>Leus</surname><given-names>Alejandro Ribeiro</given-names></name></person-group></element-citation></ref><ref id="b10"><element-citation publication-type="journal"><article-title>Neural message passing for quantum chemistry</article-title><year>2017</year><volume>70</volume><fpage>1263</fpage><lpage>1272</lpage><person-group person-group-type="author"><name><surname>Gilmer</surname><given-names>Justin</given-names></name><name><surname>Samuel</surname><given-names>S</given-names></name><name><surname>Schoenholz</surname><given-names /></name><name><surname>Patrick</surname><given-names>F</given-names></name><name><surname>Riley</surname><given-names>Oriol</given-names></name><name><surname>Vinyals</surname><given-names>George E</given-names></name><name><surname>Dahl</surname><given-names /></name></person-group></element-citation></ref><ref id="b11"><element-citation publication-type="journal"><article-title>Mini-batch spectral clustering</article-title><source>International Joint Conference on Neural Networks (IJCNN)</source><year>2017</year><fpage>3888</fpage><lpage>3895</lpage><person-group person-group-type="author"><name><surname>Han</surname><given-names>Yufei</given-names></name><name><surname>Filippone</surname><given-names>Maurizio</given-names></name></person-group></element-citation></ref><ref id="b12"><element-citation publication-type="journal"><article-title>Graph u-nets</article-title><source>Proceedings of the 36th International conference on Machine learning (ICML)</source><year>2019</year><person-group person-group-type="author"><name><surname>Shuiwang</surname><given-names>Ji Hongyang</given-names></name><name><surname>Gao</surname><given-names /></name></person-group></element-citation></ref><ref id="b13"><element-citation publication-type="journal"><article-title>The monotonicity theorem, cauchy's interlace theorem, and the courant-fischer theorem</article-title><source>The American Mathematical Monthly</source><year>1987</year><volume>94</volume><issue>4</issue><fpage>352</fpage><lpage>354</lpage><person-group person-group-type="author"><name><surname>Ikebe</surname><given-names>Yasuhiko</given-names></name><name><surname>Inagaki</surname><given-names>Toshiyuki</given-names></name><name><surname>Miyamoto</surname><given-names>Sadaaki</given-names></name></person-group></element-citation></ref><ref id="b14"><element-citation publication-type="journal"><article-title>Deep divergence-based approach to clustering</article-title><source>Neural Networks</source><year>2019</year><volume>113</volume><fpage>91</fpage><lpage>101</lpage><person-group person-group-type="author"><name><surname>Kampffmeyer</surname><given-names>Michael</given-names></name><name><surname>L&#248;kse</surname><given-names>Sigurd</given-names></name><name><surname>Filippo</surname><given-names>M</given-names></name><name><surname>Bianchi</surname><given-names>Lorenzo</given-names></name><name><surname>Livi</surname><given-names>Arnt-B&#248;rre</given-names></name><name><surname>Salberg</surname><given-names>Robert Jenssen</given-names></name></person-group></element-citation></ref><ref id="b15"><element-citation publication-type="journal"><article-title>Semi-supervised classification with graph convolutional net- works</article-title><source>International Conference of Learning Representations (ICLR)</source><year>2017</year><person-group person-group-type="author"><name><surname>Thomas</surname><given-names>N</given-names></name><name><surname>Kipf</surname><given-names>Max</given-names></name><name><surname>Welling</surname><given-names /></name></person-group></element-citation></ref><ref id="b16"><element-citation publication-type="journal"><article-title>Generalizing pooling functions in convo- lutional neural networks: Mixed, gated, and tree</article-title><source>Artificial Intelligence and Statistics</source><year>2017</year><volume>70</volume><fpage>1985</fpage><lpage>1994</lpage><person-group person-group-type="author"><name><surname>Marc</surname><given-names>T</given-names></name><name><surname>Law</surname><given-names>Raquel</given-names></name><name><surname>Urtasun</surname><given-names>Richard S</given-names></name><name><surname>Zemel</surname><given-names /></name><name><surname>Lee</surname><given-names>Chen-Yu</given-names></name><name><surname>Gallagher</surname><given-names>Patrick W</given-names></name><name><surname>Tu</surname><given-names>Zhuowen</given-names></name></person-group></element-citation></ref><ref id="b17"><element-citation publication-type="journal"><article-title>Self-attention graph pooling</article-title><source>Proceedings of the 36th International conference on Machine learning (ICML-19)</source><year>2019</year><person-group person-group-type="author"><name><surname>Lee</surname><given-names>Junhyun</given-names></name><name><surname>Lee</surname><given-names>Inyeop</given-names></name><name><surname>Kang</surname><given-names>Jaewoo</given-names></name></person-group></element-citation></ref><ref id="b18"><element-citation publication-type="journal"><article-title>Gated graph sequence neural networks</article-title><source>International Conference of Learning Representations (ICLR)</source><year>2016</year><person-group person-group-type="author"><name><surname>Li</surname><given-names>Yujia</given-names></name><name><surname>Tarlow</surname><given-names>Daniel</given-names></name><name><surname>Brockschmidt</surname><given-names>Marc</given-names></name><name><surname>Zemel</surname><given-names>Richard</given-names></name></person-group></element-citation></ref><ref id="b19"><element-citation publication-type="journal"><article-title>Clique pooling for graph classification</article-title><source>International Conference of Learning Representations (ICLR) - Representation Learning on Graphs and Man- ifolds workshop</source><year>2019</year><person-group person-group-type="author"><name><surname>Luzhnica</surname><given-names>Enxhell</given-names></name><name><surname>Day</surname><given-names>Ben</given-names></name><name><surname>Lio</surname><given-names>Pietro</given-names></name></person-group></element-citation></ref><ref id="b20"><element-citation publication-type="journal"><article-title>Gap: Generalizable approximate graph partitioning framework</article-title><source>arXiv preprint arXiv:1903.00614</source><year>2019</year><person-group person-group-type="author"><name><surname>Nazi</surname><given-names>Azade</given-names></name><name><surname>Hang</surname><given-names>Will</given-names></name><name><surname>Goldie</surname><given-names>Anna</given-names></name><name><surname>Ravi</surname><given-names>Sujith</given-names></name><name><surname>Mirhoseini</surname><given-names>Azalia</given-names></name></person-group></element-citation></ref><ref id="b21"><element-citation publication-type="journal"><source>Nonlinear system identification: from classical approaches to neural networks and fuzzy models</source><year>2013</year><person-group person-group-type="author"><name><surname>Nelles</surname><given-names>Oliver</given-names></name></person-group></element-citation></ref><ref id="b22"><element-citation publication-type="journal"><article-title>Mixing patterns in networks</article-title><source>Physical Review E</source><year>2003</year><volume>67</volume><issue>2</issue><fpage>026126</fpage><lpage>026126</lpage><person-group person-group-type="author"><name><surname>Mark Ej Newman</surname><given-names /></name></person-group></element-citation></ref><ref id="b23"><element-citation publication-type="journal"><article-title>The graph neural network model</article-title><source>IEEE Transactions on Neural Networks</source><year>2009</year><volume>20</volume><issue>1</issue><fpage>61</fpage><lpage>80</lpage><person-group person-group-type="author"><name><surname>Scarselli</surname><given-names>Franco</given-names></name><name><surname>Gori</surname><given-names>Marco</given-names></name><name><surname>Chung Tsoi</surname><given-names>Ah</given-names></name><name><surname>Hagenbuchner</surname><given-names>Markus</given-names></name><name><surname>Monfardini</surname><given-names>Gabriele</given-names></name></person-group></element-citation></ref><ref id="b24"><element-citation publication-type="journal"><article-title>Spectralnet: Spectral clustering using deep neural networks</article-title><source>arXiv preprint arXiv:1801.01587</source><year>2018</year><person-group person-group-type="author"><name><surname>Shaham</surname><given-names>Uri</given-names></name><name><surname>Stanton</surname><given-names>Kelly</given-names></name><name><surname>Li</surname><given-names>Henry</given-names></name><name><surname>Nadler</surname><given-names>Boaz</given-names></name><name><surname>Basri</surname><given-names>Ronen</given-names></name><name><surname>Kluger</surname><given-names>Yuval</given-names></name></person-group></element-citation></ref><ref id="b25"><element-citation publication-type="journal"><article-title>Weisfeiler-lehman graph kernels</article-title><source>Journal of Machine Learning Research</source><year>2011</year><volume>12</volume><fpage>2539</fpage><lpage>2561</lpage><person-group person-group-type="author"><name><surname>Shervashidze</surname><given-names>Nino</given-names></name><name><surname>Schweitzer</surname><given-names>Pascal</given-names></name><name><surname>Van Leeuwen</surname><given-names>Erik Jan</given-names></name><name><surname>Mehlhorn</surname><given-names>Kurt</given-names></name><name><surname>Borg- Wardt</surname><given-names>Karsten M</given-names></name></person-group></element-citation></ref><ref id="b26"><element-citation publication-type="journal"><article-title>Normalized cuts and image segmentation</article-title><source>Departmental Papers (CIS)</source><year>2000</year><fpage>107</fpage><lpage>107</lpage><person-group person-group-type="author"><name><surname>Shi</surname><given-names>Jianbo</given-names></name><name><surname>Malik</surname><given-names>Jitendra</given-names></name></person-group></element-citation></ref><ref id="b27"><element-citation publication-type="journal"><article-title>A multiscale pyramid trans- form for graph signals</article-title><source>IEEE Transactions on Signal Processing</source><year>2016</year><volume>64</volume><issue>8</issue><fpage>2119</fpage><lpage>2134</lpage><person-group person-group-type="author"><name><surname>David I Shuman</surname><given-names>Mohammad Javad</given-names></name><name><surname>Faraji</surname><given-names>Pierre</given-names></name><name><surname>Vandergheynst</surname><given-names /></name></person-group></element-citation></ref><ref id="b28"><element-citation publication-type="journal"><article-title>Dynamic edgeconditioned filters in convolutional neural networks on graphs</article-title><source>Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</source><year>2017</year><person-group person-group-type="author"><name><surname>Simonovsky</surname><given-names>Martin</given-names></name><name><surname>Komodakis</surname><given-names>Nikos</given-names></name></person-group></element-citation></ref><ref id="b29"><element-citation publication-type="journal"><article-title>Learning deep representations for graph clustering</article-title><source>AAAI</source><year>2014</year><fpage>1293</fpage><lpage>1299</lpage><person-group person-group-type="author"><name><surname>Tian</surname><given-names>Fei</given-names></name><name><surname>Gao</surname><given-names>Bin</given-names></name><name><surname>Cui</surname><given-names>Qing</given-names></name><name><surname>Chen</surname><given-names>Enhong</given-names></name><name><surname>Liu</surname><given-names>Tie-Yan</given-names></name></person-group></element-citation></ref><ref id="b30"><element-citation publication-type="journal"><article-title>Regions adjacency graph applied to color image segmenta- tion</article-title><source>IEEE Transactions on image processing</source><year>2000</year><volume>9</volume><issue>4</issue><fpage>735</fpage><lpage>744</lpage><person-group person-group-type="author"><name><surname>Tr&#233;meau</surname><given-names>Alain</given-names></name><name><surname>Colantoni</surname><given-names>Philippe</given-names></name></person-group></element-citation></ref><ref id="b31"><element-citation publication-type="journal"><article-title>A tutorial on spectral clustering</article-title><source>Statistics and computing</source><year>2007</year><volume>17</volume><issue>4</issue><fpage>395</fpage><lpage>416</lpage><person-group person-group-type="author"><name><surname>Von Luxburg</surname><given-names>Ulrike</given-names></name></person-group></element-citation></ref><ref id="b32"><element-citation publication-type="journal"><article-title>A feasible method for optimization with orthogonality constraints</article-title><source>Mathematical Programming</source><year>2013</year><volume>142</volume><issue>1-2</issue><fpage>397</fpage><lpage>434</lpage><person-group person-group-type="author"><name><surname>Wen</surname><given-names>Zaiwen</given-names></name><name><surname>Yin</surname><given-names>Wotao</given-names></name></person-group></element-citation></ref><ref id="b33"><element-citation publication-type="journal"><article-title>Syncspeccnn: Synchronized spectral cnn for 3d shape segmentation</article-title><source>Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</source><year>2017</year><fpage>2282</fpage><lpage>2290</lpage><person-group person-group-type="author"><name><surname>Yi</surname><given-names>Li</given-names></name><name><surname>Su</surname><given-names>Hao</given-names></name><name><surname>Guo</surname><given-names>Xingwen</given-names></name><name><surname>Guibas</surname><given-names>Leonidas J</given-names></name></person-group></element-citation></ref><ref id="b34"><element-citation publication-type="journal"><article-title>Hi- erarchical graph representation learning with differentiable pooling</article-title><source>Advances in Neural Infor- mation Processing Systems</source><year>2018</year><fpage>4800</fpage><lpage>4810</lpage><person-group person-group-type="author"><name><surname>Ying</surname><given-names>Zhitao</given-names></name><name><surname>You</surname><given-names>Jiaxuan</given-names></name><name><surname>Morris</surname><given-names>Christopher</given-names></name><name><surname>Ren</surname><given-names>Xiang</given-names></name><name><surname>Hamilton</surname><given-names>Will</given-names></name><name><surname>Leskovec</surname><given-names>Jure</given-names></name></person-group></element-citation></ref><ref id="b35"><element-citation publication-type="journal"><article-title>Multiclass spectral clustering</article-title><source>Proceedings Ninth IEEE International Conference on Computer Vision</source><year>2003</year><fpage>313</fpage><lpage>319</lpage></element-citation></ref></ref-list></back></article>