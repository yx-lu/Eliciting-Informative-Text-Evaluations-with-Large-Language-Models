Title:
```
Under review as a conference paper at ICLR 2020 BUZZ: BUFFER ZONES FOR DEFENDING ADVERSAR- IAL EXAMPLES IN IMAGE CLASSIFICATION
```
Abstract:
```
We propose a novel defense against all existing gradient based adversarial attacks on deep neural networks for image classification problems. Our defense is based on a combination of deep neural networks and simple image transformations. While straight forward in im- plementation, this defense yields a unique security property which we term buffer zones. In this paper, we formalize the concept of buffer zones. We argue that our defense based on buffer zones is secure against state-of-the-art black box attacks. We are able to achieve this security even when the adversary has access to the entire original training data set and unlimited query access to the defense. We verify our security claims through experimen- tation using FashionMNIST, CIFAR-10 and CIFAR-100. We demonstrate < 10% attack success rate - significantly lower than what other well-known defenses offer - at only a price of a 15-20% drop in clean accuracy. By using a new intuitive metric we explain why this trade-off offers a significant improvement over prior work.
```

Figures/Tables Captions:
```
Figure 1. a describes a 2D snapshot of the landscape of a normal classifier. Three different regions with class labels A, B and C are depicted. Clearly, for any image x which is close to the boundaries between the regions, the adversary can produce an adversarial example x by adding a small noise η to x. The resulting adversarial image is what we would consider a true adversarial image. Here we say true in the sense that the difference between x and x is almost visually imperceptible to humans, but it makes the classifier produce a wrong label.
Figure 1: (a) Landscape without buffer zone, and (b) Landscape with buffer zone ⊥.
Figure 2: General Design of BUZz.
Table 1: Attacker's success rate of black-box attacks for state-of-the-art defenses (b) To the best of our knowledge, there are no papers on defenses against adversarial machine learn- ing based on our concept of buffer zones. This new concept offers a conceptually simple and efficient defense strategy with rigorous security argument.
Table 2: Mixed black-box attack on CIFAR-10
Table 3: δ values for mixed black-box attacks on FashionMNIST, CIFAR-10, and CIFAR-100
```

Main Content:
```

Section Title: INTRODUCTION
  INTRODUCTION There are many applications based on Convolution Neural Networks (CNNs) such as image classifi- cation ( Krizhevsky et al. (2012) ;  Simonyan & Zisserman (2015) ), object detection ( Girshick (2015) ;  Ren et al. (2015) ), semantic segmentation ( Shelhamer et al. (2017) ) and visual concept discovery ( Wang et al. (2017) ). However, it is well-known that CNNs are highly susceptible to small pertur- bations which are added to benign input images. As shown in ( Szegedy et al. (2013) ;  Goodfellow et al. (2014) ), by adding visually imperceptible perturbations to the original image, adversarial ex- amples can be created. These adversarial examples are misclassifed by the CNN network with high confidence. Hence, making CNNs secure against this type of attack is a significantly important task. The strength of an attack is relative to the considered adversarial model. In the white-box setting parameters of the target/defense model f are given to the adversary - white-box attacks use this information to model access to an oracle that returns gradients ∇f (·). From our analysis of previous literature, it is clear that a secure white-box defense is extremely challenging to design. We can also question the realism of having an adversary that knows the model's weights and architecture. Many online machine learning services by default only allow black-box access to their models and do not publish their model parameters ( Papernot et al. (2017) ). Therefore in this paper, we keep the classifier defense parameters secret. This disallows white-box attacks and so we focus exclusively on black-box adversaries in this paper. In a black-box setting, the adversary may know some general features about the classifier (i.e. that a CNN is being used) and how the target model is trained. Most importantly, the adversary has query access to the target model itself and/or access to (part of the) training dataset. The adversary uses this information to train a synthetic model g. Using the synthetic model, adversarial examples can be created. The underlying assumption in the black-box setting is that a large percent of the adversarial examples created with the synthetic model will also fool the target model f . Our proposed defense assumes a black-box adversary with two important attributes. First, the ad- versary has unlimited access to an oracle (target model) which returns the final class label F (f (x)). Here f (x) indicates the score vector enumerating confidence scores for each possible class label and F (.) computes the class label with the maximum confidence score. The second important at- tribute our adversary has is access to the entire original training dataset. In this sense, we model the strongest known black-box adversary which has not yet been studied in the literature.

Section Title: Defense based on Buffer Zones
  Defense based on Buffer Zones We first explain the concept of buffer zones. Next, we argue how "wide" buffer zones force the black-box adversary to produce a "sufficient large" noise η: As Under review as a conference paper at ICLR 2020 discussed in  Papernot et al. (2016a)  we count an attack successful only if the adversarial noise η has small magnitude, say η ≤ , which cannot be recognized by human beings. Forcing noise η > accomplishes our security goal as either the human eye or our defense detects η. To force the adversary to use a large perturbation η, we create buffer zones ⊥ between all the regions as presented in Figure 1.b. Now the classifier outputs one of the class labels (A, B, or C) or ⊥, where ⊥ means "no class is assigned". We can think of ⊥ as the class label of the buffer zones. In other words, the adversarial noise η (i.e., perturbation η = x − x) must cross over the buffer zones ⊥ in order to modify label l to label l . To cross from A to B in  Figure 1 .b is initially not visible by the human eye as long as one remains in the original A-region of  Figure 1 .a and starts to transition into the B-region of  Figure 1 .a. However, to cross over into the smaller B-region in  Figure 1 .b we need an additional perturbation which we expect will become visible. Since we only focus on small noise which is not recognized by human beings, the buffer zones ⊥ allow us to defend against adversarial images with small noise. Only large adversarial noise, which is out of the interest of the attacker as it can be detected by the human eye, can possibly fool our defense. If the buffer zones are wide, then we accomplish our security goal since small adversarial noise cannot cross over the buffer zones, however, the prediction/clean accuracy of the defense will decrease (as it becomes more noise intolerant to both clean and adversarial examples). Notice that it is well-known in the security community that to provide security to any system, there is an associated cost; we believe the buffer zone concept is reasonable as experiments for our proposed techniques for creating mostly wide buffer zones show that the clean accuracy suffers only about 15-20% while reducing the attacker's success rate to about 5-10% (a 90% drop in success). Being able to reach such small attacker's success rates is a main contribution of our paper.

Section Title: Performance of a defense
  Performance of a defense We introduce a new metric to properly understand the combined effect of (1) a drop γ in clean accuracy from an original clean accuracy p to clean accuracy p d = p − γ for the defense and (2) a small attacker's success rate α against the defense. The attacker's success rate is defined as the fraction of adversarial examples that produce an adversarial label for images that are properly classified by the defense. So, in a non-malicious environment we have the original clean accuracy p while in the malicious environment the probability of proper/accurate classification by the defense model is (p−γ)(1−α) (since the defense properly labels a fraction p−γ if no adversary is present and out of these images a fraction α is successfully attacked if an adversary is present). We call (p − γ)(1 − α) the effective clean accuracy of the defense. Going from a non-malicious environment to a malicious environment with defense gives a drop in the effective clean accuracy of 1 1 As an example of the usefulness of δ suppose one wishes to classify a new object by taking say n images and submit these images to a registration service which implements a classifier with defense. In a malicious environment the camera which is used for taking pictures or any man-in-the-middle can attempt to transform the images into adversarial examples (with a targeted new label) which cannot be detected by the human eye. Nevertheless, the service will see agreement among the produced labels of (in expectation) (p − δ)n images that are correct labeled with in the worst case the remaining images having the adversarial targeted label. This is a drop of δn compared to a registration of an object without adversary. The smaller δ, the better the defense. In order to trust a majority vote among the n image labels we need p − δ > 0.5.

Section Title: Under review as a conference paper at ICLR 2020
  Under review as a conference paper at ICLR 2020 In order to minimize this drop δ, it turns out to be very important to have α small enough, which is accomplished in this paper. We use subscripts t and u in δ t and δ u when differentiating for targeted attacks and untargeted attacks (since untargeted attacks are easier to pull off, δ t ≤ δ u ).

Section Title: Contributions
  Contributions (a) We focus on state-of-the-art (pure and oracle-based) black box attacks where we strengthen the adversary in that we allow the adversary access to the entire original training data with query access to the target classifier. Related work on defense strategies is detailed in Supplemental Material B, where we argue and/or experimentally demonstrate into what extent this strong adversary can successfully attack well-known defense strategies; a summary is given in  Table 1 . These defenses typically (except  Tramèr et al. (2017) ) have an attacker's success rate of ≥ 50% with a relative small drop in clean accuracy of < 0.05 with often almost no (close to zero percent) drop, for data sets such as MNIST and CIFAR-10. Precise calculations show that this corresponds to δ values of at least 0.29 ( Tramèr et al. (2017) ), 0.43 ( Srisakaokul et al. (2018) ), and ≥ 0.65 for the other defenses. It turns out that for our strong adversary a vanilla network as defense - i.e., we implement no defense at all against the black-box attacker - has δ = 0.63 for CIFAR-10. The other considered defenses for CIFAR-10 do not do better than implementing no defense at all. (c) We realize wide buffer zones by combining multiple classifiers with a majority vote based on a threshold together with image transformations that are unique for each of the classifiers. In Sec- tion 4 we verify our security claims through experimentation using FashionMNIST, CIFAR-10 and CIFAR-100 and show for untargeted attacks a drop in clean accuracy of 0.158, 0.200, and 0.170 in return for small attacker's success rates α < 9%, < 7%, and < 10%, respectively. This gives δ u values 0.226, 0.247, and 0.216 showing a significant improvement over prior work - it makes sense to sacrifice some clean accuracy in return for a much smaller attacker's success rate. 2 For CIFAR-10 we conclude that BUZz with δ = 0.247 (and α = 7%) improves over  Tramèr et al. (2017)  with δ = 0.29 (and α = 34%); both are far better than any other defense, and since both use complimen- tary techniques we expect to be able to combine both to improve δ in future work. (All the above and this observation demonstrate the usefulness of our new δ-metric.) Finally, we will make all our code, including replicated defenses and attacks, available online.

Section Title: Outline
  Outline We give an overview of known attacks in Section 2 and a mathematical formulation of a black-box adversary is given in Supplemental Material A. Related work on white-box and black-box defenses is given in Supplemental Material B where we also analyze (by argument and experiment) white-box defenses against black-box adversaries - this benchmarks our work. In Section 3 we detail BUZz, our defense based on buffer zones. Section 4 has experiments and simulations showing the attacker's success rates versus clean rates. Supplemental Material C enumerates pseudo code for the implemented attacks and more experimental results are given in Supplemental Material D. We conclude in Section 5.

Section Title: ATTACKS
  ATTACKS Adversarial Examples in an image classification task. See ( Yuan et al. (2017) ), the general scheme of a successful attack can be described as follows. The adversary is given a trained im- 2 BUZz does better for targeted attacks with attacker's success rates α < 3.5%, < 5%, and < 7% leading to δt values 0.144, 0.161, and 0.04. Here, in order to achieve δt = 0.0423 for CIFAR-100 it turns out to be best not to implement any defense (no BUZz) - the data set must have made it already hard for a successful targeted state-of-the-art black-box attack! Under review as a conference paper at ICLR 2020 age classifier (e.g, CNN network) f which outputs a class label l for a given input data (i.e., image) x. The adversary will add a perturbation η to the original input x to get an adversarial example (or a modified data input) x , i.e., x = x + η. Normally, η should be small to make the adversar- ial example barely recognizable by humans. Yet, the adversary may be able to fool the classifier f to produce any class label l ( = l) as she wants. Assume that f (x) = (s 1 , s 2 , . . . , s k ) is a k- dimensional vector of confidence scores s j of class labels j. We call f (x) the score vector with 0 ≤ s j ≤ 1, k j=1 s j = 1, and k the number of class labels. The class label l is computed as Given x ∈ [0, 1] d and l = l = F (f (x)), the attacker wishes to ideally solve min x ∈[0,1] d x − x such that F (f (x )) = l = l = F (f (x)), (2) where l and l are the output label of x and x , · denotes the distance between two data samples, and d is the number of dimensions of x. η = x − x is the perturbation added on x. In this optimization problem, we minimize the perturbation η while the label l is fixed (this represents a targeted attack). This problem becomes easier when the attacker has more information about f (·): In some adversarial models, the adversary may know parameters/weights that define the target model f . Some classification applications may directly output vector f (x) instead of F (f (x)) and this gives more information about the target model. By adding a sufficient large noise η to any given benign input image, we can fool any existing image classifier. However, as discussed in ( Papernot et al. (2016a) ) we count an attack successful only if the adversarial noise η has small magnitude, say η ≤ , which cannot be recognized by human beings ( indicates this transition from noise not being recognized to noise that is visually perceptible). In this sense the attacker is already successful as soon as a sufficiently small perturbation η ( η ≤ ) is found that realizes label l . That is, finding the minimal possible perturbation η in (2) is not necessary.

Section Title: Attack Methodologies
  Attack Methodologies Attacks can be partitioned into two main categories based on their ap- proach. The first kind are white-box attacks where the adversary knows the parameters of de- fense/target model/classifier f and uses these parameters to compute gradients. One can think of this scenario as having oracle access to gradients ∇f (x) for input images x. The attacker only uses this type of oracle access to compute adversarial examples. The second kind are black-box attacks where the adversary does not know the parameters of f , but does have black-box access to the target model itself. One can think of this scenario as having oracle access to class labels F (f (x)) or score vector values f (x) (the latter gives more information and models a stronger attacker). In addition to having black-box access to the target model, the adver- sary may know and use (part of) the original training data (this can be used to train an adversarial synthetic model which resembles the target model). Since the oracles given to the white-box and black-box attackers are different/complimentary, white-box defenses and black-box defenses deal with different attack methodologies. White-box Attacks. ( Yuan et al. (2017) ) constructs perturbation η with the help of gradient ∇f (·): for example, η = × sign(∇ x L(x, l; θ) in the Fast Gradient Sign Method (FGSM) by ( Goodfellow et al. (2014) ), where θ represents the parameters of f , L is a loss function (e.g, cross entropy) of model f . ( can be thought of as relating to the maximum amount of noise which is not visually perceptible.) Black-box Attacks. Black-box attacks use non-gradient information of classifier f such as (part of) the original training data set X 0 ( Papernot et al. (2016b) ) and/or a set X 1 of adaptively chosen queries to f (i.e., {(x, f (x)) : x ∈ X 1 } or {(x, l = F (f (x))) : x ∈ X 1 }) ( Papernot et al. (2017) ) - querries in X 1 are not in the training data set X 0 . These type of attacks exploit the transferability property of adversarial examples ( Papernot et al. (2016b) ; Liu et al. (2017)): Based on information X 0 and X 1 the adversary trains its own copy of the proposed defense. This is called the adversarial synthetic network/model and is used to create adversarial examples for the target model. (Liu et al. (2017)) shows that the transferability property of adversarial examples between different models which have the same topology/architecture and are trained over the same dataset is very high, i.e., nearly 100% for ImageNet ( Russakovsky et al. (2015) ). This explains why the adversarial examples Under review as a conference paper at ICLR 2020 generated for the synthetic network can often be successful adversarial examples for the defense network. Black-box attacks can be partitioned into three following categories: • Pure black-box attack ( Szegedy et al. (2014) ;  Papernot et al. (2016b) ;  Athalye et al. (2018a) ; Liu et al. (2017)). The adversary is only given knowledge of a training data set X 0 . Based on this information, the adversary builds his own classifier g which is used to produce adversarial examples using an existing white-box attack methodology. These adversarial examples of g may also be the adversarial examples of f due to the transferability property between f and g. • Oracle based black-box attack ( Papernot et al. (2017) ). The adversary is allowed to adaptively query target classifier f , which gives information X 1 . Based on this information, the adversary builds his own classifier g which is used to produce adversarial examples using an existing white-box attack methodology. Again, the generated adversarial examples for g may also be able fool classifier f due to the transferability property between f and g. Compared to the native (pure) black box attack, this attack is supposed to be more efficient because g is intentionally trained to be similar to f . Hence, the transferability between f and g may be significantly higher. • Zeroth Order Optimization based black-box attack ( Chen et al. (2017) ). The adversary does not build any assistant classifier g as done in the previous black-box attacks. Instead, the adversary adaptively queries {x, f (x), F (f (x))} to approximate the gradient ∇f based on a derivative-free optimization approach. Using the approximated ∇f , the adversary can build adversarial examples by directly working with the classifier f . In this paper, we analyze a mixed black-box attack where the synthetic network g is built based on the training data set X 0 of the target model f and is based on adaptively chosen queries X 1 . Our mixed black-box attack is more powerful than both the pure black-box attack and oracle based black-box attack. Supplemental material C provides pseudo code. As explained and motivated in the introduction, we restrict ourselves to the black-box setting where we keep secret the parameters of our defense classifier, called BUZz (see next section), and we do not reveal score vectors - this disallows white-box attacks and zeroth order optimization based black-box attacks.

Section Title: BUZZ: A DEFENSE BASED ON BUFFER ZONES
  BUZZ: A DEFENSE BASED ON BUFFER ZONES

Section Title: Design Philosophy
  Design Philosophy Since each single network gives a classifier with aligned boundaries (i.e. no buffer zones), we propose to combine multiple classifiers (each with its own aligned boundaries) to produce a composed classifier which will provide a non-empty buffer zone. To create a buffer zone we use majority voting among the individual classifiers with a threshold mechanism. E.g., the threshold may be such that only if all individual classifiers agree on the same label l, then the composed classifier outputs l, otherwise, it outputs ⊥. Because the transferability among all the individual classifiers is not perfect, they will disagree if an image has 'too much' noise and this leads to an output ⊥. The area where they disagree is the buffer zone. Although majority voting based on a threshold leads to the existence of a buffer zone, the resulting buffer zone may not be wide enough to prevent a successful attack using small adversarial noise. If we are able to decrease/diminish the transferability among the different classifiers, then this leads to a wider buffer zone. To decrease the transferability, we must make the individual classifiers more unique. This can be done by, for each CNN network, first uniquely transforming the inputted image x. Since the transformations are different for each of the classifiers that participate in the majority 3 The buffer zone concept offers a first immediate insight: A defense with only one single classifier with buffer zones may be hard to develop because of its nature - a single network does not produce buffer zones between regions unless (1) another class label ⊥ for the buffer zone can be trained, but how does one construct proper training data? or (2) the score vector is used to mathematically define a subspace of score vectors that should map to ⊥, but how can one achieve acceptable clean accuracy at the same time? For this reason our technique for creating a non-empty buffer zone uses multiple classifiers. vote, transferability will be reduced. We will discuss what kind of transformations reduce the clean accuracy the least. The created buffer zones may be mostly wide, but there will exist narrow parts. Since the narrow parts allow a successful attack based on small adversarial noise, the attacker will want to search for a narrow part around a given image x. In order to make such a search less efficient and less often successful, the distinct transformations are kept secret. That is, only the distribution (or set) from which the transformations are drawn is known to the adversary (the concrete transformation selections/parameters are kept secret). We will make the adversarial model explicit once we have detailed our defense mechanism:

Section Title: BUZz using multiple classifiers and secret image transformations
  BUZz using multiple classifiers and secret image transformations Our defense is composed of multiple CNNs as depicted in Figure 2b. Each CNN has two unique image transformations as shown in Figure 2a. The first is a fixed randomized linear transformation c(x) = Ax + b, where A is a matrix and b is a vector. After the linear transformation a resizing operation i is applied to the image before it is fed into the CNN. The CNN corresponding to c and i is trained on clean data {i(c(x))}. This results in a weight vector w. The m protected layers in Figure 2b are described by 'parameters' (c j , i j , w j ) m j=1 . When a user wants to query the defense, input x is submitted to each layer which computes its corresponding image transformation and executes its CNN. The outputs of the layers are class labels (l j ) m j=1 . The final class label of BUZz, i.e., the composition of the m protected layers, is a majority vote based on a threshold κ. In our experiments we use unanimous voting, i.e., if the networks do not all output the same class label then the adversarial/undetermined class label ⊥ is given as the output (i.e., κ = m).

Section Title: Adversarial Model
  Adversarial Model In our adversarial model we assume that no score vectors are revealed (which makes the attacker incapable of executing a zeroth order optimization based black-box attack) and we assume that the parameters (c j , i j , w j ) m j=1 are kept secret, i.e., the attacker has no direct knowl- edge about the weights of the CNN networks, matrices A and vectors b, and the amount of image resizing for each layer (this makes the attacker incapable of executing a white-box attack). Our adversarial model allows a mixed black-box attacker having more capabilities (is stronger) than the strongest black-box adversary in literature: Just like in ( Papernot et al. (2017) ), the adversary is allowed to query the defense as many times as they desire and is practically possible, they may generate synthetic data X 1 and they can train a synthetic network. Based on this synthetic network they can carry out white-box attacks and test their efficiency (attack success rate) on the defense. We give the adversary one additional and extremely powerful ability that ( Papernot et al. (2017) ) does not. We allow the adversary access to the entire original training data set X 0 as an initial set. This gives the adversary access to a huge amount of training data, an order of magnitude higher than what ( Papernot et al. (2017) ) gives in the original attack. In supplemental material A we mathematically formalize the adversary (as is done in crypto/security literature) as an adversarial algorithm in order to make precise the adversary's capabilities.

Section Title: Image Transformations
  Image Transformations From ( Guo et al. (2017) ) we know adversarial examples are sensitive to image transformations which either distort the value of the pixels in the image or change the original spatial location of the pixels. However, it is well established ( Goodfellow et al. (2016) ) that CNNs rely on certain spatial patterns being present in the data in order to classify clean (non-adversarial) data. Hence, we want an image transformation that keeps such patterns 'invariant' while introducing distortions that make the attacker's task less likely to succeed.

Section Title: Under review as a conference paper at ICLR 2020
  Under review as a conference paper at ICLR 2020 In the literature, previous defenses with only a single network have suggested multiple different image transformations (Meng & Chen (2017);  Xie et al. (2018) ). Through experimentation we decided to employ two image transformations for each protected layer. The first transformation is a simple resizing operation, where we resize the image before giving it as input to the CNN. Resizing to a smaller dimension than the original image may result in lost pixel information and by extension hurt the network's performance on clean data. Therefore we only consider resizing operations which increase the size of the image. The other transformation we use is a linear transformation: c(x) = Ax + b where x is an n by n pixels input image, A is a fixed n by n matrix and b a fixed n by n pixels 'noise' sample. Depending on the data set we can control the trade off between the attacker's success rate and clean accuracy using the linear transformation. For example if only b is random (with small magnitude) and A is identity, it results in less image distortion (so higher clean accuracy) but also less security (more adversarial samples bypass the defense).

Section Title: Security Argument
  Security Argument In the context of Papernot's oracle based black-box attack and pure black-box attack, the adversarial noise η is created based on a white-box attack for a synthetic network g of BUZz. It means that the noise η is specifically developed for g. Since the x = x + η is inputted into every protected layer of BUZz, the j-th layer will apply its CNN network on a noisy image i j (c j (x )) = i j (c j (x + η)), which due to the linearity of i j (c j (·)) is equal to i j (c j (x)) + i j (c j (η)). Therefore, layers receive different inputted noises i j (c j (η)) = η. Hence, the protected layers have different behavior from one another and from synthetic network g for any given adversarial example x = x + η. This widens the buffer zones as it is less likely that each protected layer reacts the same to η in terms of miss-classification.

Section Title: EXPERIMENTAL RESULTS
  EXPERIMENTAL RESULTS In this section we discuss our experiments for CIFAR-10; Supplemental Material D has also exper- iments for FashionMNIST and CIFAR-100. Supplemental material C provides exact details in the form of pseudo codes and tables with attack parameters; below is a concise summary in words. We implemented the mixed black-box attack, i.e., the state-of-the-art oracle based black-box attack of  Papernot et al. (2017)  where in addition the adversary has access to the entire training data set. The attack first generates a synthetic network by taking the initial training data set X 0 and learning the parameters θ g of a single vanilla network g - for the training we use Adam ( Kingma & Ba (2014) ) with learning rate 0.0001, batch size 64, 100 epochs, and no data augmentation. During the first iteration the Jacobian matrix of the score vector g is computed in each image x ∈ X 0 . The signs of the entries in the Jacobian matrix that correspond to x's class label (according to the target model) form a vector which is scaled with λ = 0.1 and added to x. This leads to an augmented data set X 1 which consists of these new images together with X 0 . We use black-box access to the target model to learn the labels for the images in X 1 . During next iterations we double the data set 5 times and learn a more accurate synthetic network g. Second, the attacker uses Carlini's single synthetic network g ( Carlini & Wagner (2016) ) and ap- plies a targeted/non-targeted iterative/non-iterative FGSM attack (we use the cleverhans library, see https://github.com/tensorflow/cleverhans) to produce an adversarial example x for image x. (We do not use the C&W attack ( Carlini & Wagner (2016) ) because our experiments show a much lower attacker's success rate.) During each FGSM iteration the black-box adversary has the capability to use black-box access to the target model to verify whether the produced x in that iteration has a desired label l . We use 10 iterations in iterative FGSM with = 10/256 (giving a 1/256 scaling vector in each of the 10 iterations). In non-iterative FGSM we pick = 1/20. The CNN networks in the vanilla network and each of the protected layers of BUZz (the target model) each have the VGG16 architecture (see https://neurohive.io/en/ popular-networks/vgg16/8) and are trained on clean data {(i j (c j (x)), l)}, where l is the to-be-learned class label for x. BUZz uses anonymous voting with κ equal to the number of net- works. The image transformations are selected from mappings c(x) = x + b (we use the identity matrix for A) where the entries of b are non-zero with probability p = 0.35 and non-zero entries Under review as a conference paper at ICLR 2020 We tested three BUZz designs: One with 8 networks each using a different image resizing operation from 32 to 32, 40, 48, 64, 72, 80, 96, 104. The second with 4 networks being the subset of the 8 networks that use image resizing operations from 32 to 32, 48, 72, 96. The third with 2 networks being a subset of the 8 networks that use image resizing operations from 32 to 32 and 104.  Table 2  depicts the results of the four possible target/untargeted iterative/non-iterative FGSM attacks against a plain vanilla network without image transformation and BUZz with 2, 4, and 8 networks respectively. The defense rate is defined as 1 − α, where α is the attacker success rate computed as the fraction of 1000 test data (with the property that the target model produces the correct label) for which the adversary produces a successful adversarial example x that changes the label to a desired label l . The clean rate and defense rate 5 are an average over 5 runs where each run anew selects random image transformations in BUZz. The experiments show that the min and max values over runs are not far apart indicating that most random image transformation selections according to the recipe described above give similar results. For BUZz with 4 networks for CIFAR-10 we see a 0.2003 drop in clean accuracy from 0.8835 to 0.6832. In return for this drop in clean accuracy we see a defense rate of ≥ 0.931, i.e., an attacker's success rate < 7%. Also for FashionMNIST and CIFAR-100 (for BUZz with 8 resp. 2 networks) we see drops in clean accuracy of 0.1577 and 0.1703 in order to achieve an attacker's success rate of < 10% and < 9% (see supplemental material D). This leads to the δ values reported in the introduction.  Table 3  shows the best BUZz configurations with targeted and untargeted δ values.

Section Title: CONCLUSION
  CONCLUSION We introduced a new concept called buffer zones which is at the core of a new adversarial ML de- fense, coined BUZZ. BUZz defends against black-box adversaries with oracle access to the target model (BUZz) and knowledge of the entire training data set. BUZz uses threshold voting over mul- tiple networks that each are preceeded with a secret/hidden image transformation. Experiments for FashionMNIST, CIFAR-10, and CIFAR-100 for carefully designed classes of image transformations in BUZz show that at the cost of drop in clean accuracy of 15-20% the attacker's success rate is only 5-10% - much less than the best attacker's success rates ≥ 34% achieved by other well known de- fenses for these data sets. We have argued this to be an acceptable trade-off for better security by using a new metric (called δ value).
  We note that the positive results reported in this paper open the door for the exploration of many more pos- sible image transformations. Other transformations such as the affine transformation, zero padding images and pixel shuffling could yield even better trade off between security and clean accuracy in multi model defenses. We leave these possibilities as future work.

```
