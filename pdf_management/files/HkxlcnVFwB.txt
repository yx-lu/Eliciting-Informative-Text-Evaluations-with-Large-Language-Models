Title:
```
Published as a conference paper at ICLR 2020 GENDICE: GENERALIZED OFFLINE ESTIMATION OF STATIONARY VALUES
```
Abstract:
```
An important problem that arises in reinforcement learning and Monte Carlo methods is estimating quantities defined by the stationary distribution of a Markov chain. In many real-world applications, access to the underlying transition opera- tor is limited to a fixed set of data that has already been collected, without addi- tional interaction with the environment being available. We show that consistent estimation remains possible in this challenging scenario, and that effective esti- mation can still be achieved in important applications. Our approach is based on estimating a ratio that corrects for the discrepancy between the stationary and em- pirical distributions, derived from fundamental properties of the stationary distri- bution, and exploiting constraint reformulations based on variational divergence minimization. The resulting algorithm, GenDICE, is straightforward and effec- tive. We prove its consistency under general conditions, provide an error analysis, and demonstrate strong empirical performance on benchmark problems, including off-line PageRank and off-policy policy evaluation.
```

Figures/Tables Captions:
```
Figure 1: Stationary Distribution Estimation on BA and real-world graphs. Each plot shows the log KL- divergence of GenDICE and model- based method towards the number of samples.
Figure 2: Results on Taxi Domain. The plots show log MSE of the tabular estimator across different trajectory lengths, different discount factors and different behavior policies (x-axis).
Figure 3: Results on Reacher. The left three plots in the first row show the log MSE of estimated av- erage per-step reward over different numbers of trajectories, truncated lengths, and behavior policies (M1 and M2 mean off-policy set collected by multiple behavior policies with α = [0.0, 0.33] and α = [0.0, 0.33, 0.66]). The right two figures show the loss curves towards the optimization steps. Each plot in the second row shows the average reward case.
Figure 4: Results on HalfCheetah. Plots from left to the right show the log MSE of estimated average per-step reward over different truncated lengths, numbers of trajectories, and behavior policies in discounted and average reward cases.
Figure 5: Results of ablation study with different learning rates and activation functions. The plots show the log MSE of estimated average per-step reward over training and different behavior policies.
Figure 6: Results of ablation study with constraint penalty and discount factors. The left two figures show the effect of ratio constraint on estimating average per-step reward. The right three figures show the log MSE for average per-step reward over training and different discount factor γ.
Figure 7: Results of ablation study with (a) different divergence and (b) weight of penalty λ. The plots show the log KL-Divergence of OPR on Barabasi-Albert graph.
```

Main Content:
```

Section Title: INTRODUCTION
  INTRODUCTION Estimation of quantities defined by the stationary distribution of a Markov chain lies at the heart of many scientific and engineering problems. Famously, the steady-state distribution of a random walk on the World Wide Web provides the foundation of the PageRank algorithm (Langville & Meyer, 2004). In many areas of machine learning, Markov chain Monte Carlo (MCMC) methods are used to conduct approximate Bayesian inference by considering Markov chains whose equilibrium dis- tribution is a desired posterior (Andrieu et al., 2002). An example from engineering is queueing theory, where the queue lengths and waiting time under the limiting distribution have been exten- sively studied (Gross et al., 2018). As we will also see below, stationary distribution quantities are of fundamental importance in reinforcement learning (RL) (e.g., Tsitsiklis & Van Roy, 1997). Classical algorithms for estimating stationary distribution quantities rely on the ability to sample next states from the current state by directly interacting with the environment (as in on-line RL or MCMC), or even require the transition probability distribution to be given explicitly (as in PageR- ank). Unfortunately, these classical approaches are inapplicable when direct access to the environ- ment is not available, which is often the case in practice. There are many practical scenarios where a collection of sampled trajectories is available, having been collected off-line by an external mech- anism that chose states and recorded the subsequent next states. Given such data, we still wish to estimate a stationary quantity. One important example is off-policy policy evaluation in RL, where we wish to estimate the value of a policy different from that used to collect experience. Another ex- ample is off-line PageRank (OPR), where we seek to estimate the relative importance of webpages given a sample of the web graph. Motivated by the importance of these off-line scenarios, and by the inapplicability of classical meth- ods, we study the problem of off-line estimation of stationary values via a stationary distribution corrector. Instead of having access to the transition probabilities or a next-state sampler, we assume only access to a fixed sample of state transitions, where states have been sampled from an unknown distribution and next-states are sampled according to the Markov chain's transition operator. The Published as a conference paper at ICLR 2020 off-line setting is indeed more challenging than its more traditional on-line counterpart, given that one must infer an asymptotic quantity from finite data. Nevertheless, we develop techniques that still allow consistent estimation under general conditions, and provide effective estimates in prac- tice. The main contributions of this work are: • We formalize the problem of off-line estimation of stationary quantities, which captures a wide range of practical applications. • We propose a novel stationary distribution estimator, GenDICE, for this task. The resulting algo- rithm is based on a new dual embedding formulation for divergence minimization, with a carefully designed mechanism that explicitly eliminates degenerate solutions. • We theoretically establish consistency and other statistical properties of GenDICE, and empir- ically demonstrate that it achieves significant improvements on several behavior-agnostic off- policy evaluation benchmarks and an off-line version of PageRank. The methods we develop in this paper fundamentally extend recent work in off-policy policy evalu- ation (Liu et al., 2018; Nachum et al., 2019) by introducing a new formulation that leads to a more general, and as we will show, more effective estimation method.

Section Title: BACKGROUND
  BACKGROUND We first introduce off-line PageRank (OPR) and off-policy policy evaluation (OPE) as two motivat- ing domains, where the goal is to estimate stationary quantities given only off-line access to a set of sampled transitions from an environment.

Section Title: Off-line PageRank (OPR)
  Off-line PageRank (OPR) The celebrated PageRank algorithm (Page et al., 1999) defines the ranking of a web page in terms of its asymptotic visitation probability under a random walk on the (augmented) directed graph specified by the hyperlinks. If we denote the World Wide Web by a directed graph G = (V, E) with vertices (web pages) v ∈ V and edges (hyperlinks) (v, u) ∈ E, PageRank considers the random walk defined by the Markov transition operator v → u: P (u|v) = (1−η) |v| 1 (v,u)∈E + η |V | , (1) where |v| denotes the out-degree of vertex v and η ∈ [0, 1) is a probability of "teleporting" to any page uniformly. Define d t (v) := P (s t = v|s 0 ∼ µ 0 , ∀i < t, s i+1 ∼ P(·|s i )), where µ 0 is the initial distribution over vertices, then the original PageRank algorithm explicitly iterates for the limit The classical version of this problem is solved by tabular methods that simulate Equation 1. How- ever, we are interested in a more scalable off-line version of the problem where the transition model is not explicitly given. Instead, consider estimating the rank of a particular web page v from a large web graph, given only a sample D = {(v, u) i } N i=1 from a random walk on G as specified above. We would still like to estimate d(v ) based on this data. First, note that if one knew the distribution p by which any vertex v appeared in D, the target quantity could be re-expressed by a simple importance ratio d (v ) = E v∼p d(v) p(v) 1 v=v . Therefore, if one had the correction ratio function τ (v) = d(v) p(v) , an estimate of d (v ) can easily be recovered via d (v ) ≈p (v ) τ (v ), wherep (v ) is the empirical probability of v estimated from D. The main attack on the problem we investigate is to recover a good estimate of the ratio function τ .

Section Title: Policy Evaluation
  Policy Evaluation An important generalization of this stationary value estimation problem arises in RL in the form of policy evaluation. Consider a Markov Decision Process (MDP) M = S, A, P, R, γ, µ 0 (Puterman, 2014), where S is a state space, A is an action space, P (s |s, a) de- notes the transition dynamics, R is a reward function, γ ∈ (0, 1] is a discounted factor, and µ 0 is the initial state distribution. Given a policy, which chooses actions in any state s according to the prob- ability distribution π(·|s), a trajectory β = (s 0 , a 0 , r 0 , s 1 , a 1 , r 1 , . . .) is generated by first sampling the initial state s 0 ∼ µ 0 , and then for t ≥ 0, a t ∼ π(·|s t ), r t ∼ R(s t , a t ), and s t+1 ∼ P(·|s t , a t ). The value of a policy π is the expected per-step reward defined as: In the above, the expectation is taken with respect to the randomness in the state-action pair P (s |s, a) π (a |s ) and the reward R (s t , a t ). Without loss of generality, we assume the limit exists for the average case, and hence R(π) is finite.

Section Title: Behavior-agnostic Off-Policy Evaluation (OPE)
  Behavior-agnostic Off-Policy Evaluation (OPE) An important setting of policy evaluation that often arises in practice is to estimate R γ (π) or R (π) given a fixed dataset D = {(s, a, r, s ) i } N i=1 ∼ P (s |s, a) p (s, a), where p (s, a) is an unknown distribution induced by multiple unknown behavior policies. This problem is different from the classical form of OPE, where it is assumed that a known behavior policy π b is used to collect transitions. In the behavior-agnostic scenario, however, typical importance sampling (IS) estimators (e.g., Precup et al., 2000) do not apply. Even if one can assume D consists of trajectories where the behavior policy can be estimated from data, it is known that that straightforward IS estimators suffer a variance exponential in the trajectory length, known as the "curse of horizon" (Jiang & Li, 2016; Liu et al., 2018). Let d π t (s, a) = P (s t = s, a t = a|s 0 ∼ µ 0 , ∀i < t, a i ∼ π (·|s i ) , s i+1 ∼ P(·|s i , a i )). The station- ary distribution can then be defined as With this definition, R(π) and R γ (π) can be equivalently re-expressed as Here we see once again that if we had the correction ratio function τ (s, a) = µ π γ (s,a) p(s,a) a straightfor- ward estimate of R γ (π) could be recovered via R γ (π) ≈ Ep [τ (s, a) R (s, a)], wherep (s, a) is an empirical estimate of p (s, a). In this way, the behavior-agnostic OPE problem can be reduced to estimating the correction ratio function τ , as above. We note that Liu et al. (2018) and Nachum et al. (2019) also exploit Equation 5 to reduce OPE to stationary distribution correction, but these prior works are distinct from the current proposal in different ways. First, the inverse propensity score (IPS) method of Liu et al. (2018) assumes the transitions are sampled from a single behavior policy, which must be known beforehand; hence that approach is not applicable in behavior-agnostic OPE setting. Second, the recent DualDICE algo- rithm (Nachum et al., 2019) is also a behavior-agnostic OPE estimator, but its derivation relies on a change-of-variable trick that is only valid for γ < 1. This previous formulation becomes unstable when γ → 1, as shown in Section 6 and Appendix E. The behavior-agnostic OPE estimator we derive below in Section 3 is applicable both when γ = 1 and γ ∈ (0, 1). This connection is why we name the new estimator GenDICE, for GENeralized stationary DIstribution Correction Estimation.

Section Title: GENDICE
  GENDICE As noted, there are important estimation problems in the Markov chain and MDP settings that can be recast as estimating a stationary distribution correction ratio. We first outline the conditions that characterize the correction ratio function τ , upon which we construct the objective for the GenDICE estimator, and design efficient algorithm for optimization. We will develop our approach for the more general MDP setting, with the understanding that all the methods and results can be easily specialized to the Markov chain setting.

Section Title: ESTIMATING STATIONARY DISTRIBUTION CORRECTION
  ESTIMATING STATIONARY DISTRIBUTION CORRECTION The stationary distribution µ π γ defined in Equation 4 can also be characterized via At first glance, this equation shares a superficial similarity to the Bellman equation, but there is a fundamental difference. The Bellman operator recursively integrates out future (s , a ) pairs to characterize a current pair (s, a) value, whereas the distribution operator T defined in Equation 6 operates in the reverse temporal direction. When γ < 1, Equation 6 always has a fixed-point solution. For γ = 1, in the discrete case, the fixed- point exists as long as T is ergodic; in the continuous case, the conditions for fixed-point existence become more complicated (Meyn & Tweedie, 2012) and beyond the scope of this paper. The development below is based on a divergence D and the following default assumption.

Section Title: Assumption 1 (Markov chain regularity)
  Assumption 1 (Markov chain regularity) For the given target policy π, the resulting state-action transition operator T has a unique stationary distribution µ that satisfies D(T • µ µ) = 0.

Section Title: Published as a conference paper at ICLR 2020
  Published as a conference paper at ICLR 2020 In the behavior-agnostic setting we consider, one does not have direct access to P for element-wise evaluation or sampling, but instead is given a fixed set of samples from P (s |s, a) p (s, a) with respect to some distribution p (s, a) over S × A. Define T p γ,µ0 to be a mixture of µ 0 π and T p ; i.e., let Obviously, conditioning on (s, a, s ) one could easily sample a ∼ π (a |s ) to form (s, a, s , a ) ∼ T p ((s , a ) , (s, a)); similarly, a sample (s , a ) ∼ µ 0 (s ) π (a |s ) could be formed from s . Mixing such samples with probability γ and 1 − γ respectively yields a sample (s, a, s , a ) ∼ T p γ,µ0 ((s , a ) , (s, a)). Based on these observations, the stationary condition for the ratio from Equa- tion 6 can be re-expressed in terms of T p γ,µ0 as where τ * (s, a) := µ(s,a) p(s,a) is the correction ratio function we seek to estimate. One natural approach to estimating τ * is to match the LHS and RHS of Equation 8 with respect to some divergence D (· ·) over the empirical samples. That is, we consider estimating τ * by solving the optimization problem Although this forms the basis of our approach, there are two severe issues with this naive formulation that first need to be rectified: i) Degenerate solutions: When γ = 1, the operator T p γ=1,µ0 is invariant to constant rescaling: if τ * = T p γ=1,µ0 • τ * then cτ * = T p γ=1,µ0 • (cτ * ) for any c ≥ 0. Therefore, simply minimizing the divergence D T p γ=1,µ0 • τ p · τ cannot provide a desirable estimate of τ * . In fact, in this case the trivial solution τ * (s, a) = 0 cannot be eliminated. ii) Intractable objective: The divergence D T p γ,µ0 • τ p · τ involves the computation of T p γ,µ0 • τ , which in general involves an intractable integral. Thus, evaluation of the exact objective is intractable, and neglects the assumption that we only have access to samples from T p γ,µ0 and are not able to evaluate it at arbitrary points. We address each of these two issues in a principled manner.

Section Title: ELIMINATING DEGENERATE SOLUTIONS
  ELIMINATING DEGENERATE SOLUTIONS To avoid degenerate solutions when γ = 1, we ensure that the solution is a proper density ratio; that is, the property τ ∈ Ξ := {τ (·) ≥ 0, E p [τ ] = 1} must be true of any τ that is a ratio of some density to p. This provides an additional constraint that we add to the optimization formulation min With this additional constraint, it is obvious that the trivial solution τ (s, a) = 0 is eliminated as an infeasible point of Eqn (10), along with other degenerate solutions τ (s, a) = cτ * (s, a) with c = 1. Unfortunately, exactly solving an optimization with expectation constraints is very complicated in general (Lan & Zhou, 2016), particularly given a nonlinear parameterization for τ . The penalty method (Luenberger & Ye, 2015) provides a much simpler alternative, where a sequence of regular- ized problems are solved min τ ≥0 J (τ ) := D T p γ,µ0 • τ p · τ + λ 2 (E p [τ ] − 1) 2 , (11) with λ increasing. The drawback of the penalty method is that it generally requires λ → ∞ to ensure the strict feasibility, which is still impractical, especially in stochastic gradient descent. The infinite λ may induce unbounded variance in the gradient estimator, and thus, divergence in optimization. However, by exploiting the special structure of the solution sets to Equation 11, we can show that, remarkably, it is unnecessary to increase λ. Theorem 1 For γ ∈ (0, 1] and any λ > 0, the solution to Equation 11 is given by τ * (s, a) = u(s,a) p(s,a) . The detailed proof for Theorem 1 is given in Appendix A.1. By Theorem 1, we can estimate the desired correction ratio function τ * by solving only one optimization with an arbitrary λ > 0. Published as a conference paper at

Section Title: EXPLOITING DUAL EMBEDDING
  EXPLOITING DUAL EMBEDDING The optimization in Equation 11 involves the integrals T p γ,µ0 • τ and E p [τ ] inside nonlinear loss functions, hence appears difficult to solve. Moreover, obtaining unbiased gradients with a naive approach requires double sampling (Baird, 1995). Instead, we bypass both difficulties by applying a dual embedding technique (Dai et al., 2016; 2018). In particular, we assume the divergence D is in the form of an f -divergence (Nowozin et al., 2016) D φ T p γ,µ0 • τ p · τ := p · τ (s, a) φ (T p γ,µ 0 •τ )(s,a) p·τ (s,a) ds da where φ (·) : R + → R is a convex, lower-semicontinuous function with φ (1) = 0. Plugging this into J (τ ) in Equation 11 we can easily check the convexity of the objective Theorem 2 For an f -divergence with valid φ defining D φ , the objective J (τ ) is convex w.r.t. τ . The detailed proof is provided in Appendix A.2. Recall that a suitable convex function can be represented as φ (x) = max f x·f −φ * (f ), where φ * is the Fenchel conjugate of φ (·). In particular, we have the representation 1 2 x 2 = max u ux − 1 2 u 2 , which allows us to re-express the objective as Applying the interchangeability principle (Shapiro et al., 2014; Dai et al., 2016), one can replace the inner max in the first term over scalar f to maximize over a function f (·, ·) : This yields the main optimization formulation, which avoids the aforementioned difficulties and is well-suited for practical optimization as discussed in Section 3.4.

Section Title: Remark (Other divergences)
  Remark (Other divergences) In addition to f -divergence, the proposed estimator Equation 11 is compatible with other divergences, such as the integral probability metrics (IPM) (Müller, 1997; Sriperumbudur et al., 2009), while retaining consistency. Based on the definition of the IPM, these divergences directly lead to min-max optimizations similar to Equation 13 with the identity func- tion as φ * (·) and different feasible sets for the dual functions. Specifically, maximum mean discrep- ancy (MMD) (Smola et al., 2006) requires f H k ≤ 1 where H k denotes the RKHS with kernel k; the Dudley metric (Dudley, 2002) requires f BL ≤ 1 where f BL := f ∞ + ∇f 2 ; and Wasserstein distance (Arjovsky et al., 2017) requires ∇f 2 ≤ 1. These additional requirements on the dual function might incur some extra difficulty in practice. For example, with Wasserstein distance and the Dudley metric, we might need to include an extra gradient penalty (Gulrajani et al., 2017), which requires additional computation to take the gradient through a gradient. Meanwhile, the consistency of the surrogate loss under regularization is not clear. For MMD, we can obtain a closed-form solution for the dual function, which saves the cost of the inner optimization (Gretton et al., 2012), but with the tradeoff of requiring two independent samples in each outer optimization update. Moreover, MMD relies on the condition that the dual function lies in some RKHS, which introduces additional kernel parameters to be tuned and in practice may not be sufficiently flexible compared to neural networks.

Section Title: A PRACTICAL ALGORITHM
  A PRACTICAL ALGORITHM We have derived a consistent stationary distribution correction estimator in the form of a min-max saddle point optimization Equation 13. Here, we present a practical instantiation of GenDICE with a concrete objective and parametrization. We choose the χ 2 -divergence, which is an f -divergence with The objective becomes There two major reasons for adopting χ 2 -divergence: i) In the behavior-agnostic OPE problem, we mainly use the ratio correction function for estimating E p [τ (s, a) R (s, a)], which is an expectation. Recall that the error between the estimate and ground-truth can then be bounded by total variation, which is a lower bound of χ 2 -divergence. ii) For the alternative divergences, the conjugate of the KL-divergence involves exp (·), which may lead to instability in optimization; while the IPM variants introduce extra constraints on dual function, which may be difficult to be optimized. The conjugate function of χ 2 -divergence en- joys suitable numerical properties and provides squared regularization. We have provided an empirical ablation study that investigates the alternative divergences in Section 6.3. To parameterize the correction ratio τ and dual function f we use neural networks, τ (s, a) = nn wτ (s, a) and f (s, a) = nn w f (s, a), where w τ and w f denotes the parameters of τ and f respec- tively. Since the optimization requires τ to be non-negative, we add an extra positive neuron, such as exp (·), log (1 + exp (·)) or (·) 2 at the final layer of nn wτ (s, a). We empirically compare the different positive neurons in Section 6.3. For these representations, and unbiased gradient estimator ∇ (wτ ,u,w f ) J (τ, u, f ) can be obtained straightforwardly, as shown in Appendix B. This allows us to apply stochastic gradient descent to solve the saddle-point problem Equation 14 in a scalable manner, as illustrated in Algorithm 1.

Section Title: THEORETICAL ANALYSIS
  THEORETICAL ANALYSIS We provide a theoretical analysis for the proposed GenDICE algorithm, following a similar learning setting and assumptions to (Nachum et al., 2019).

Section Title: Assumption
  Assumption where E [·] is w.r.t. the randomness in D and in the optimization algorithms, opt is the optimization error, and approx (F, H) is the approximation induced by (F, H) for parametrization of (τ, f ). The theorem shows that the suboptimality of GenDICE's solution, measured in terms of the objective function value, can be decomposed into three terms: (1) the approximation error approx , which is controlled by the representation flexibility of function classes; (2) the estimation error due to sample randomness, which decays at the order of 1/ √ N ; and (3) the optimization error, which arises from the suboptimality of the solution found by the optimization algorithm. As discussed in Appendix C, in special cases, this suboptimality can be bounded below by a divergence betweenτ and τ * , and therefore directly bounds the error in the estimated policy value. There is also a tradeoff between these three error terms. With more flexible function classes (e.g., neural networks) for F and H, the approximation error approx becomes smaller. However, it may increase the estimation error (through the constant in front of 1/ √ N ) and the optimization error (by solving a harder optimization problem). On the other hand, if F and H are linearly parameterized, estimation and optimization errors tend to be smaller and can often be upper-bounded explicitly in Appendix C.3. However, the corresponding approximation error will be larger.

Section Title: RELATED WORK
  RELATED WORK Off-policy Policy Evaluation Off-policy policy evaluation with importance sampling (IS) has has been explored in the contextual bandits (Strehl et al., 2010; Dudík et al., 2011; Wang et al., 2017), and episodic RL settings (Murphy et al., 2001; Precup et al., 2001), achieving many empirical successes (e.g., Strehl et al., 2010; Dudík et al., 2011; Bottou et al., 2013). Unfortunately, IS- based methods suffer from exponential variance in long-horizon problems, known as the "curse of horizon" (Liu et al., 2018). A few variance-reduction techniques have been introduced, but still cannot eliminate this fundamental issue (Jiang & Li, 2016; Thomas & Brunskill, 2016; Guo et al., 2017). By rewriting the accumulated reward as an expectation w.r.t. a stationary distribution, Liu et al. (2018); Gelada & Bellemare (2019) recast OPE as estimating a correction ratio function, which significantly alleviates variance. However, these methods still require the off-policy data to be collected by a single and known behavior policy, which restricts their practical applicability. The only published algorithm in the literature, to the best of our knowledge, that solves agnostic-behavior off-policy evaluation is DualDICE (Nachum et al., 2019). However, DualDICE was developed for Published as a conference paper at ICLR 2020 discounted problems and its results become unstable when the discount factor approaches 1 (see below). By contrast, GenDICE can cope with the more challenging problem of undiscounted reward estimation in the general behavior-agnostic setting. Note that standard model-based methods (Sutton & Barto, 1998), which estimate the transition and reward models directly then calculate the expected reward based on the learned model, are also applicable to the behavior-agnostic setting considered here. Unfortunately, model-based methods typically rely heavily on modeling assumptions about rewards and transition dynamics. In practice, these assumptions do not always hold, and the evaluation results can become unreliable. Markov Chain Monte Carlo Classical MCMC (Brooks et al., 2011; Gelman et al., 2013) aims at sampling from µ π by iteratively simulting from the transition operator. It requires continuous inter- action with the transition operator and heavy computational cost to update many particles. Amor- tized SVGD (Wang & Liu, 2016) and Adversarial MCMC (Song et al., 2017; Li et al., 2019) alleviate this issue via combining with neural network, but they still interact with the transition operator di- rectly, i.e., in an on-policy setting. The major difference of our GenDICE is the learning setting: we only access the off-policy dataset, and cannot sample from the transition operator. The proposed GenDICE leverages stationary density ratio estimation for approximating the stationary quantities, which distinct it from classical methods.

Section Title: Density Ratio Estimation
  Density Ratio Estimation Density ratio estimation is a fundamental tool in machine learning and much related work exists. Classical density ratio estimation includes moment matching (Gretton et al., 2008), probabilistic classification (Bickel et al., 2007), and ratio matching (Nguyen et al., 2008; Sugiyama et al., 2008; Kanamori et al., 2009). These classical methods focus on estimating the ratio between two distributions with samples from both of them, while GenDICE estimates the density ratio to a stationary distribution of a transition operator, from which even one sample is difficult to obtain. PageRank Yao & Schuurmans (2013) developed a reverse-time RL framework for PageRank via solving a reverse Bellman equation, which is less sensitive to graph topology and shows faster adap- tation with graph change. However, Yao & Schuurmans (2013) still considers the online manner, which is different with our OPR setting.

Section Title: EXPERIMENTS
  EXPERIMENTS In this section, we evaluate GenDICE on OPE and OPR problems. For OPE, we use one or multiple behavior policies to collect a fixed number of trajectories at some fixed trajectory length. This data is used to recover a correction ratio function for a target policy π that is then used to estimate the average reward in two different settings: i) average reward; and ii) discounted reward. In both settings, we compare with a model-based approach and step-wise weighted IS (Precup et al., 2000). We also compare to Liu et al. (2018) (referred to as "IPS" here) in the Taxi domain with a learned behavior policy 1 . We specifically compare to DualDICE (Nachum et al., 2019) in the discounted reward setting, which is a direct and current state-of-the-art baseline. For OPR, the main comparison is with the model-based method, where the transition operator is empirically estimated and stationary distribution recovered via an exact solver. We validate GenDICE in both tabular and continuous cases, and perform an ablation study to further demonstrate its effectiveness. All results are based on 20 random seeds, with mean and standard deviation plotted. Our code is publicly available at https://github.com/zhangry868/GenDICE.

Section Title: TABULAR CASE
  TABULAR CASE Offline PageRank on Graphs One direct application of GenDICE is off-line PageRank (OPR). We test GenDICE on a Barabasi-Albert (BA) graph (synthetic), and two real-world graphs, Cora and Citeseer. Details of the graphs are given in Appendix D. We use the log KL-divergence between estimated stationary distribution and the ground truth as the evaluation metric, with the ground truth computed by an exact solver based on the exact transition operator of the graphs. We compared GenDICE with model-based methods in terms of the sample efficiency. From the results in  Fig- ure 1 , GenDICE outperforms the model-based method when limited data is given. Even with 20k samples for a BA graph with 100 nodes, where a transition matrix has 10k entries, GenDICE still Published as a conference paper at ICLR 2020 shows better performance in the offline setting. This is reasonable since GenDICE directly esti- mates the stationary distribution vector or ratio, while the model-based method needs to learn an entire transition matrix that has many more parameters.

Section Title: Off-Policy Evaluation with Taxi
  Off-Policy Evaluation with Taxi We use a similar taxi domain as in Liu et al. (2018), where a grid size of 5 × 5 yields 2000 states in total (25 × 16 × 5, corresponding to 25 taxi locations, 16 passenger appearance status and 5 taxi status). We set the target policy to a final policy π after running tabular Q-learning for 1000 iterations, and set another policy π + after 950 iterations as the base pol- icy. The behavior policy is a mixture controlled by α as π b = (1 − α)π + απ + . For the model-based method, we use a tabular representation for the reward and transi- tion functions, whose entries are estimated from behavior data. For IS and IPS, we fit a policy via behavior cloning to estimate the policy ratio. In this specific setting, our methods achieve better results compared to IS, IPS and the model-based method. Interestingly, with longer hori- zons, IS cannot improve as much as other methods even with more data, while GenDICE consistently improve and achieves much better results than the baselines. DualDICE only works with γ < 1. GenDICE is more stable than DualDICE when γ becomes larger (close to 1), while still showing competitive performance for smaller discount factors γ.

Section Title: CONTINUOUS CASE
  CONTINUOUS CASE We further test our method for OPE on three control tasks: a discrete-control task Cartpole and two continuous-control tasks Reacher and HalfCheetah. In these tasks, observations (or states) are continuous, thus we use neural network function approximators and stochastic optimization. Since DualDICE (Nachum et al., 2019) has shown the state-of-the-art performance on discounted OPE, we mainly compare with it in the discounted reward case. We also compare to IS with a learned policy via behavior cloning and a neural model-based method, similar to the tabular case, but with neural network as the function approximator. All neural networks are feed-forward with two hidden layers of dimension 64 and tanh activations. More details can be found in Appendix D. Due to limited space, we put the discrete control results in Appendix E and focus on the more challenging continuous control tasks. Here, the good performance of IS and model-based methods in Section 6.1 quickly deteriorates as the environment becomes complex, i.e., with a continuous action space. Note that GenDICE is able to maintain good performance in this scenario, even when using function approximation and stochastic optimization. This is reasonable because of the difficulty of fitting to the coupled policy-environment dynamics with a continuous action space. Here we also empirically validate GenDICE with off-policy data collected by multiple policies. As illustrated in  Figure 3 , all methods perform better with longer trajectory length or more trajecto- ries. When α becomes larger, i.e., the behavior policies are closer to the target policy, all methods performs better, as expected. Here, GenDICE demonstrates good performance both on average- reward and discounted reward cases in different settings. The right two figures in each row show the log MSE curve versus optimization steps, where GenDICE achieves the smallest loss. In the discounted reward case, GenDICE shows significantly better and more stable performance than the Published as a conference paper at ICLR 2020 strong baseline, DualDICE.  Figure 4  also shows better performance of GenDICE than all baselines in the more challenging HalfCheetah domain.

Section Title: ABLATION STUDY
  ABLATION STUDY Finally, we conduct an ablation study on GenDICE to study its robustness and implementation sen- sitivities. We investigate the effects of learning rate, activation function, discount factor, and the specifically designed ratio constraint. We further demonstrate the effect of the choice of divergences and the penalty weight.

Section Title: Effects of the Learning Rate
  Effects of the Learning Rate Since we are using neural network as the function approxi- mator, and stochastic optimization, it is necessary to show sensitivity to the learning rate with {0.0001, 0.0003, 0.001, 0.003}, with results in  Figure 5 . When α = 0.33, i.e., the OPE tasks are relatively easier and GenDICE obtains better results at all learning rate settings. However, when α = 0.0, i.e., the estimation becomes more difficult and only GenDICE only obtains reasonable results with the larger learning rate. Generally, this ablation study shows that the proposed method is not sensitive to the learning rate, and is easy to train.

Section Title: Activation Function of Ratio Estimator
  Activation Function of Ratio Estimator We further investigate the effects of the activation func- tion on the last layer, which ensure the non-negative outputs required for the ratio. To better un- derstand which activation function will lead to stable trainig for the neural correction estimator, we empirically compare using i) (·) 2 ; ii) log(1 + exp(·)); and iii) exp(·). In practice, we use the (·) 2 since it achieves low variance and better performance in most cases, as shown in  Figure 5 .

Section Title: Effects of Discount Factors
  Effects of Discount Factors We vary γ ∈ {0.95, 0.99, 0.995, 0.999, 1.0} to probe the sensitiv- ity of GenDICE. Specifically, we compare to DualDICE, and find that GenDICE is stable, while DualDICE becomes unstable when the γ becomes large, as shown in  Figure 6 . GenDICE is also more general than DualDICE, as it can be applied to both the average and discounted reward cases.

Section Title: Effects of Ratio Constraint
  Effects of Ratio Constraint In Section 3, we highlighted the importance of the ratio constraint. Here we investigate the trivial solution issue without the constraint. The results in  Figure 6  demon- strate the necessity of adding the constraint penalty, since a trivial solution prevents an accurate corrector from being recovered (green line in left two figures).

Section Title: Effects of the Choice of Divergences
  Effects of the Choice of Divergences We empiri- cally test the GenDICE with several other alternative di- vergences, e.g., Wasserstein-1 distance, Jensen-Shannon divergence, KL-divergence, Hellinger divergence, and MMD. To avoid the effects of other factors in the estima- tor, e.g., function parametrization, we focus on the offline PageRank task on BA graph with 100 nodes and 10k of- fline samples. All the experiments are evaluated with 20 random trials. To ensure the dual function to be 1-Lipchitz, we add the gradient penalty. Besides, we use a learned Gaussian kernel in MMD, similar to Li et al. (2017). As we can see in Figure 7(a), the GenDICE estimator is com- patible with many different divergences. Most of the di- vergences, with appropriate extra techniques to handle the difficulties in optimization and carefully tuning for extra parameters, can achieve similar performances, consistent with phenomena in the variants of GANs (Lucic et al., 2018). However, KL-divergence is an outlier, performing notice- ably worse, which might be caused by the ill-behaved exp (·) in its conjugate function. The χ 2 - divergence and JS-divergence are better, which achieve good performances with fewer parameters to be tuned.

Section Title: Effects of the Penalty Weight
  Effects of the Penalty Weight The results of different penalty weights λ are illustrated in Fig- ure 7(b). We vary the λ ∈ [0.1, 5] with χ 2 -divergence. Within a large range of λ, the performances of the proposed GenDICE are quite consistent, which justifies Theorem 1. The penalty multiplies with λ. Therefore, with λ increases, the variance of the stochastic gradient estimator also increases, which explains the variance increasing in large λ in Figure 7(b). In practice, λ = 1 is a reasonable choice for general cases.

Section Title: CONCLUSION
  CONCLUSION In this paper, we proposed a novel algorithm GenDICE for general stationary distribution correction estimation, which can handle both the discounted and average stationary distribution given multiple behavior-agnostic samples. Empirical results on off-policy evaluation and offline PageRank show the superiority of proposed method over the existing state-of-the-art methods.
  We used the released implementation of IPS (Liu et al., 2018) from https://github.com/zt95/ infinite-horizon-off-policy-estimation.

```
