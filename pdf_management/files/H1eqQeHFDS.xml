<article article-type="research-article"><front><article-meta><title-group><article-title>ADVECTIVENET: AN EULERIAN-LAGRANGIAN FLUIDIC RESERVOIR FOR POINT CLOUD PROCESSING</article-title></title-group><contrib-group content-type="author"><contrib contrib-type="person"><name><surname>He</surname><given-names>Xingzhe</given-names></name></contrib><contrib contrib-type="person"><name><surname>College</surname><given-names>Dartmouth</given-names></name></contrib><contrib contrib-type="person"><name><surname>University</surname><given-names>Rutgers</given-names></name></contrib><contrib contrib-type="person"><name><surname>Cao</surname><given-names>Helen Lu</given-names></name></contrib><contrib contrib-type="person"><name><surname>College</surname><given-names>Dartmouth</given-names></name></contrib><contrib contrib-type="person"><name><surname>Zhu</surname><given-names>Bo</given-names></name></contrib><contrib contrib-type="person"><name><surname>College</surname><given-names>Dartmouth</given-names></name></contrib></contrib-group><abstract><p>This paper presents a novel physics-inspired deep learning approach for point cloud processing motivated by the natural flow phenomena in fluid mechanics. Our learning architecture jointly defines data in an Eulerian world space, using a static background grid, and a Lagrangian material space, using moving particles. By introducing this Eulerian-Lagrangian representation, we are able to naturally evolve and accumulate particle features using flow velocities generated from a generalized, high-dimensional force field. We demonstrate the efficacy of this system by solving various point cloud classification and segmentation problems with state-of-the-art performance. The entire geometric reservoir and data flow mimics the pipeline of the classic PIC/FLIP scheme in modeling natural flow, bridging the disciplines of geometric machine learning and physical simulation.</p></abstract></article-meta></front><body><sec><p>The fundamental mechanism of deep learning is to uncover complex feature structures from large data sets using a hi- erarchical model composed of simple layers. These data structures, such as a uniform grid (<xref ref-type="bibr" rid="b5">Lecun et al., 1998</xref>), an unstructured graph (<xref ref-type="bibr" rid="b24">Kipf &amp; Welling, 2016</xref>), or a hierar- chical point set (<xref ref-type="bibr" rid="b0">Qi et al., 2016a</xref>; 2017), function as ge- ometric reservoirs to yield intricate underpinning patterns by evolving the massive input data in a high-dimensional parameter space. On another front, computational physics researchers have been mastering the art of inventing geo- metric data structures and simulation algorithms to model complex physical systems (<xref ref-type="bibr" rid="b15">Gibou et al., 2019</xref>). Lagrangian structures, which track the motion in a moving local frame such as a particle system (<xref ref-type="bibr" rid="b37">Monaghan, 1992</xref>), and Eulerian structures, which describe the evolution in a fixed world frame such as a Cartersian grid (<xref ref-type="bibr" rid="b12">Fedkiw et al., 2001</xref>), are the two mainstream approaches. Various differential oper- ators have been devised on top of these data structures to model complex fluid or solid systems. <xref ref-type="bibr" rid="b3">Pioneered by E (2017)</xref> and popularized by many others, e.g., (<xref ref-type="bibr" rid="b31">Long et al., 2018</xref>; <xref ref-type="bibr" rid="b7">Chen et al., 2018</xref>; <xref ref-type="bibr" rid="b0">Ruthotto &amp; Haber, 2018</xref>), treating the data flow as the evolution of a dynamic system is connecting machine learning and physics simulation. <xref ref-type="bibr" rid="b1">As E (2017)</xref> notes, there exists a mathematical equivalence between the forward data propagation on a neural network and the temporal evolution of a dynamic system. Accordingly, the training process of a neural network amounts to finding the optimal control forces exerted on a dynamic system to minimize a specific energy form.</p><p>Point cloud processing is of particular interest under this perspective. The two main challenges: to build effective convolution stencils and to evolve learned nonlinear features (<xref ref-type="bibr" rid="b0">Qi et al., 2016a</xref>; <xref ref-type="bibr" rid="b3">Atzmon et al., 2018</xref>; <xref ref-type="bibr" rid="b19">Wang et al., 2019</xref>), can map conceptually to the challenges of devising world- frame differential operators and tracking material-space continuum deformations when simulating a PDE-driven dynamic system in computational physics. We envision that the key to solving these challenges lies in the adaption of the most suited geometric data structures to synergistically handle the Eulerian and Lagrangian aspects of the problem. In particular, it is essential to devise data structures and computational paradigms that can accommodate global fast convolutions, and at the same time track the non-linear feature evolution.</p><p>The key motivation of this work originates from physical computing that tackles its various frame- dependent and temporally-evolved computational challenges by creating the most natural and effec- tive geometric toolsets under the two different viewpoints. We are specifically interested in uncov- ering the intrinsic connections between a point cloud learning problem and a computational fluid dynamic (CFD) problem. We observe that the two problems share an important common thread regarding their computational model, which both evolve Lagrangian particles in an Eulerian space guided by the first principle of energy minimization. Such observations shed new insight into the 3D point cloud processing and further opens the door for marrying the state-of-the-art CFD techniques to tackle the challenges emerging in point cloud learning.</p><p>To this end, this paper conducts a preliminary exploration to establish an Eulerian-Lagrangian flu- idic reservoir that accommodates the learning process of point clouds. The key idea of the proposed method is to solve the point cloud learning problem as a flow advection problem jointly defined in a Eulerian world space and a Lagrangian material space. The defining characteristic distinguishing our method from others is that the spatial interactions among the Lagrangian particles can evolve temporally via advection in a learned flow field, like their fluidic counterpart in a physical circum- stance. This inherently takes advantage of the fundamental flow phenomena in evolving and sepa- rating Lagrangian features non-linearly (see <xref ref-type="fig" rid="fig_0">Figure 1</xref>). In particular, we draw the idea of Lagrangian advection on an Eulerian reservoir from both the Particle-In-Cell (PIC) method (<xref ref-type="bibr" rid="b10">Evans &amp; Harlow, 1957</xref>) and the Fluid-Implicit-Particle (FLIP) method (<xref ref-type="bibr" rid="b4">Brackbill et al., 1987</xref>), which are wholly rec- ognized as 'PIC/FLIP' in modeling large-scale flow phenomena in both computational fluids, solids, and even visual effects. We demonstrate the result of this synergy by building a physics-inspired learning pipeline with straightforward implementation and matching the state-of-the-art with this framework.</p><p>The key contributions of our work include:</p><p>&#8226; An advective scheme to mimic the natural flow convection process for feature separation;</p><p>&#8226; A fluid-inspired learning paradigm with effective particle-grid transfer schemes;</p><p>&#8226; A fully Eulerian-Lagrangian approach to process point clouds, with the inherent advantages in creating Eulerian differential stencils and tracking Lagrangian evolution;</p><p>&#8226; A simple and efficient physical reservoir learning algorithm.</p></sec><sec><title>RELATED WORKS</title><p>This section briefly reviews the recent related work on point cloud processing. According to data structures used for building the convolution stencil, the methods can be categorized as Lagrangian (using particles only), Eulerian (using a background grid), and hybrid (using both). We also review the physical reservoir methods that embed network training into a physical simulation process. Eulerian Eulerian approaches leverage background discretizations to perform computation. The most successful Eulerian method is the CNN (<xref ref-type="bibr" rid="b5">Lecun et al., 1998</xref>), which builds the convolution op- erator on a 2D uniform grid. This Eulerian representation can be used to process 3D data by using multiple views (<xref ref-type="bibr" rid="b11">Su et al., 2015</xref>; <xref ref-type="bibr" rid="b0">Qi et al., 2016b</xref>; <xref ref-type="bibr" rid="b13">Feng et al., 2018</xref>) and extended to 3D volumetric grids (<xref ref-type="bibr" rid="b35">Maturana &amp; Scherer, 2015</xref>; <xref ref-type="bibr" rid="b0">Qi et al., 2016b</xref>; <xref ref-type="bibr" rid="b9">Z. Wu, 2015</xref>). Grid resolution is the main per- formance bottleneck for 3D CNN methods. Adaptive data structures such as Octree (<xref ref-type="bibr" rid="b0">Riegler et al., 2016</xref>; <xref ref-type="bibr" rid="b19">Wang et al., 2017</xref>; 2018), Kd-tree (<xref ref-type="bibr" rid="b25">Klokov &amp; Lempitsky, 2017</xref>), and multi-level 3D CNN (<xref ref-type="bibr" rid="b14">Ghadai et al., 2018</xref>) were invented to alleviate the problem. Another example of Eulerian structures is Spherical CNN (Cohen et al., 2018) that projects 3D shapes onto a spherical coordinate system to define equivalent rotation convolution. In addition to these voxel-based, diffusive representations, shapes can also be described as a sharp interface modeled as an implicit level set function (<xref ref-type="bibr" rid="b19">Hu et al., 2017</xref>; <xref ref-type="bibr" rid="b0">Park et al., 2019</xref>; <xref ref-type="bibr" rid="b36">Mescheder et al., 2019</xref>). For each point in the space, the level set function acts as a binary classifier checking whether the point is inside the shape or not.</p></sec><sec><title>Hybrid</title><p>There have been recent attempts to transfer data between Lagrangian and Eulerian rep- resentations for efficient convolution implementation. These data transfer methods can be one-way (<xref ref-type="bibr" rid="b19">Wang et al., 2017</xref>; <xref ref-type="bibr" rid="b25">Klokov &amp; Lempitsky, 2017</xref>; <xref ref-type="bibr" rid="b0">Tchapmi et al., 2017</xref>; <xref ref-type="bibr" rid="b26">Le &amp; Duan, 2018</xref>), in which case the data is mapped from points to grid cells permanently, or two-way (<xref ref-type="bibr" rid="b11">Su et al., 2018</xref>; <xref ref-type="bibr" rid="b3">Atzmon et al., 2018</xref>; <xref ref-type="bibr" rid="b19">Liu et al., 2019</xref>; <xref ref-type="bibr" rid="b16">Groueix et al., 2018</xref>), in which case data is pushed forward from parti- cle to grid for convolution and pushed backward from grid to particle for evolution. Auto-encoders on point clouds (<xref ref-type="bibr" rid="b11">Fan et al., 2016</xref>; <xref ref-type="bibr" rid="b0">Achlioptas et al., 2017</xref>; <xref ref-type="bibr" rid="b0">Yang et al., 2017</xref>; <xref ref-type="bibr" rid="b0">Yu et al., 2018</xref>; <xref ref-type="bibr" rid="b13">Zhao et al., 2019</xref>) can be also regarded as a hybrid approach, where encoded data is Eulerian and decoded data is Lagrangian. In addition, we want to mention the physical reservoir computing techniques that focus on the leverage of the temporal, physical evolution to solve learning problems, e.g., see (<xref ref-type="bibr" rid="b20">Jaeger, 2001</xref>) and (<xref ref-type="bibr" rid="b17">Maass et al., 2002</xref>). Physical reservoir computing is demonstrating successes in various applications (<xref ref-type="bibr" rid="b22">Jalalvand et al., 2015</xref>; <xref ref-type="bibr" rid="b20">Jaeger, 2002</xref>; <xref ref-type="bibr" rid="b17">Hauser et al., 2012</xref>; <xref ref-type="bibr" rid="b33">Luko&#353;evi&#269;ius &amp; Jaeger, 2009</xref>; <xref ref-type="bibr" rid="b0">Tanaka et al., 2019</xref>).</p></sec><sec><title>ALGORITHM</title><p>PIC/FLIP overview Before describing the details of our method, we begin with briefly surveying the background of the PIC/FLIP method. PIC/FLIP uses a hybrid grid-particle representation to describe fluid evolution. The particles are used for tracking materials, and the grid is used for dis- cretizing space. Properties such as mass, density, and velocity are carried on particles. Each simula- tion step consists of four substeps: particle-to-grid transfer I G P , grid force calculation (Projection), grid-to-particle transfer I P G , and moving particles (Advection). In the I G P step, the properties on each particle are interpolated onto a background grid. In the Projection step, calculations such as adding body forces and enforcing incompressibility are conducted on the background grid. After this, the velocities on grid nodes are interpolated back onto particles, i.e., I P G . Finally, particles move to their new positions for the next time step using the updated velocities (Advection). As summarized above, the key philosophy of PIC/FLIP is to carry all features on particles and to perform all differential calculations on the grid. The background grid functions as a computational paradigm that can be established extemporaneously when needed. Data will transfer from particle to grid and then back to particle to finish a simulation loop.</p><p>Our proposed approach follows the same design philosophy as PIC/FLIP by storing the learned features on particles and conducting differential calculations on the grid. The Lagrangian features will evolve with the particles moving in an Eulerian space and interact with local grid nodes. As shown in <xref ref-type="fig" rid="fig_1">Figure 2</xref>, the learning pipeline mimics the PIC/FLIP simulation loop in the sense that Lagrangian particles are advected passively in an Eulerian space guided by a learned velocity field.</p></sec><sec><title>Initialization</title><p>We initialize a particle system P and a background grid G as the Lagrangian and Eulerian representations respectively for processing point clouds. We use the subscript p to refer to particle indices and i to refer to the grid nodes. For the Lagrangian portion, the particle system has n particles, with each particle P p carrying its position x p &#8712; R 3 , velocity v p &#8712; R 3 , mass m p &#8712; R, and a feature vector f p &#8712; R k (k = 64 initially). The particle velocity is zero at the beginning. The particle mass m p = 1 will keep constant over the entire evolution. To initialize the feature vector f p , we first put the particles in a grid with size N 3 . For each cell, we calculate 1) the center of mass of all the particles in the cell, and 2) the normalized vector pointing from each particle to the this mass center. For each particle, we concatenate these two vectors to the initial feature vector. This process is repeated for N = 2, 4, 6, 8, 10, 12. The resulting feature vector with the length of 6 &#215; 6 are fed into a multi-layer perceptron (MLP) to generate the feature vector f p .</p><p>For the Eulerian part, we start with a 3D uniform grid G to represent the bounding box of the particles. The resolution of the grid is N 3 (N = 16 for most of our cases). At the beginning, the particle system and its bounding box are normalized to the space of [&#8722;1, 1] 3 . Each grid node G i of G stores data interpolated from the particles.</p></sec><sec><title>Particle-grid transfer</title><p>Both the interpolation from grid to particle and particle to grid are executed using tri-linear interpolation, which is a common scheme for property transfer in simulation and learning code.</p></sec><sec><title>Generalized grid forces</title><p>With the feature vectors transferred from particles to grid nodes, we devise a 3D CNN on the grid to calculate a generalized force field based on the Eulerian features. The network consists of three convolution layers, with each layer as a combination of 3D convolution, batch norm, and ReLU. The input of the network is a vector field F (kj)&#215;N &#215;N &#215;N composed of the feature vectors on all grid nodes, with k as the feature vector size (64 by default) and j as the iteration index in the evolution loop (see <xref ref-type="fig" rid="fig_1">Figure 2</xref>). The output is a convoluted vector field F (kj)&#215;N &#215;N &#215;N c with the same size as F.</p><p>We use F c for two purposes: 1) To interpolate F c from the grid back onto particles and append it to the current feature vector in order to enrich its feature description; 2) To feed F c into another single- layer network to generate the new Eulerian velocity field V for the particle advection. Specifically, this V is interpolated back onto particles in the same way as the feature interpolation to update the particle positions for the next iteration (see Advection for details).</p></sec><sec><title>Advection</title><p>The essence of an advection process is to solve the advection equation with the La- grangian form Dv/Dt = 0 or the Eulerian form &#8706;v/&#8706;t + v &#183; &#8711;v = 0. The advection equation de- scribes the passive evolution of particle properties within a flow field. With the learned grid velocity field in hand, we will update the particle velocity following the conventional scheme of PIC/FLIP. Specifically, the new velocity is first updated by interpolating the Eulerian velocity to particles (the PIC step):</p><p>Then, we interpolate the difference between the new and the old Eulerian velocity: v n+1 F LIP = v n p + I P G (v n+1 g &#8722; I G P (v n p )), (2) and then add them to the particle with a weight &#945; (=0.5 in default.):</p><p>With the updated velocity on each particle from the I P G interpolation, the particle's position for the next time step can be updated using a standard time integration scheme (explicit Euler in our implementation):</p></sec><sec><title>Boundary conditions</title><p>We apply a soft boundary constraints by adding an penalty term in the objective function to avoid particles moving outside of the grid: &#966; b = 1 n p max(0, x p 2 &#8722; 1) (5) where x p represents the pth particle in the whole batch and n is the number of particles in the whole batch. We penalize on all the particles that run outside the grid.</p><p>We also design the gather penalty and the diffusion objectives to enhance the particle diffusion and clustering effects during evolution (specifically for the segmentation application):</p><p>where c l and c m are the centers of particles of label l and m and x lp is the pth particle with label l.</p></sec><sec><title>NETWORK ARCHITECTURE</title><p>The global architecture of our network is shown in <xref ref-type="fig" rid="fig_2">Figure 3</xref>. Our model starts from a point cloud with the position of each point. After an initialization step ending with a two-layer MLP (64,64), each point carries a feature vector of length 64. These features are fed into the advection module to exchange information with neighbors. The generated features have two uses: to generate the velocity for each particle, and to be used along with the new advected particle position to collect information from neighbors. This process repeats for a few times to accumulate features in the feature space and to aggregate particles in the physical space.</p></sec><sec><title>Advection module</title><p>The data flow inside the advection module starts with particles, passes through layers of grids, then sinks back to particles. This module takes the position and the feature vector as input. The feature vectors are first fed into an MLP to reduce its dimensions to 32, which saves com- putational time and prevents over-fitting. Then, we apply three layers of convolution that are each a combination of 3D convolution, batch norm, and ReLU, with a hidden-layer size as (32,16,32) on the grid, to obtain a high-dimensional, generalized force field on the grid. Afterwards, a velocity field is generated from this force field by another two-layer network. The velocity field is then interpolated back to particles for Lagrangian advection. Additionally, to generate the output feature vector, the input and output features (with 32-dimension each) are concatenated together and appended to the original feature vector. The output of the advection module is a set of particles with new positions and new features that are ready to process for the next iteration as in <xref ref-type="fig" rid="fig_1">Figure 2</xref>.</p></sec><sec><title>EXPERIMENTS</title><p>We conducted three parts of experiments, including the ablation tests and the applications for classi- fication and segmentation. We implemented the system in PyTorch (see the submitted source code) and conducted all the tests on a single RTX 2080 Ti GPU. In the ablation tests, we evaluated the functions of the advection module, temporal resolution, grid resolution, and the functions of the PIC/FLIP scheme on ModelNet10 (<xref ref-type="bibr" rid="b9">Z. Wu, 2015</xref>) and ShapeNet (<xref ref-type="bibr" rid="b0">Yi et al., 2016</xref>). For classification, we tested our network on ModelNet40 and its subset ModelNet10. We used the class prediction accuracy as our metric. For segmentation, we tested our network on ShapeNet (<xref ref-type="bibr" rid="b0">Yi et al., 2016</xref>) and S3DIS data set (<xref ref-type="bibr" rid="b1">Armeni et al., 2016</xref>). We used mean Intersection over Union (mIoU) to evaluate our method and compare with other benchmarks.</p></sec><sec><title>ABLATION EXPERIMENTS</title></sec><sec><title>Advection</title><p>We turn off the advection module to verify the its effectiveness for the final perfor- mance. We conducted the comparison on the ShapeNet data set (<xref ref-type="bibr" rid="b0">Yi et al., 2016</xref>). The mIoU reached 86.2% with the advection module in comparison to 85.3% without it, necessitating the role of the advection step.</p></sec><sec><title>Temporal resolution (Physical Intuition)</title><p>The evolu- tion of a dynamic system can be discretized on the tem- poral axis by the numerical integration with a number of steps. Given a fixed total time, the number of timesteps is in an inverse ratio to the length of each step. For a typ- ical explicit scheme (e.g., explicit Euler), a small timestep leads to a numerically secure result at the expense of performing more time integrations; while a large timestep, although efficient, might explode out of the stable region. (Numerical Tests) Motivated by this numerical intuition, we in- vestigated the effects of temporal resolution on our learning prob- lem. Specifically, we tested the performance of the network re- garding both the learning accuracy and the evolved shape by sub- dividing the numerical integration into 0-8 steps (0 means no in- tegration). The test was performed on ModelNet10. As shown in <xref ref-type="table" rid="tab_0">Table 1</xref> and <xref ref-type="fig" rid="fig_3">Figure 4</xref>, the learning accuracy stabilizes around 95% as the number of integration increases, with 2 steps and 4 steps as the maximum (95.4%) and minimum (94.7%), indicating a minor effect from the temporal resolution on learning accuracy. For the shape convergence, we demonstrated that different temporal resolutions converge to very similar final equilibrium states, despite of the different time step sizes. As shown in <xref ref-type="fig" rid="fig_4">Figure 5</xref>, the point- cloud model of an airplane is advected with different velocity fields generated on different temporal resolutions. The final shapes with timestep 2, 3, and 6 all exhibit the same geometric feature separa- tions and topological relations. This result evidences our conjecture that all the temporal resolutions we used are within the stable region, motivating us to pick a larger time step size (total time/3 for most of our cases) for efficiency.</p></sec><sec><title>Spatial resolution (Physical Intuition)</title><p>For a typ- ical particle-grid simulation in CFD, the resolution of the grid and the number of particles are corre- lated. Making sure that each grid cell should contain enough number of particles (e.g., 1-2 particles per cell), ensures information exchange between these two discretizations is accurate. Empirically, an overly refined grid will lead to inaccurate Eulerian convolution due to the large bulk of empty cells, while an overly coarse grid will dampen the motion of particles due to artificial viscosity (e.g., see <xref ref-type="bibr" rid="b10">Evans &amp; Harlow (1957)</xref>; <xref ref-type="bibr" rid="b4">Brackbill et al. (1987)</xref>), which makes the number of particles per cell ppc a key hyperparameter.</p><p>(Numerical Validation) We validate this grid-particle design art from scientific computing by testing our network with different grid resolutions. As shown in Table 5.1, we tested the grid resolution of 8 3 , 16 3 , and 32 3 on two datasets with 1024 and 2048 particles separately. We observed that a 16 3 grid fits the 1024 dataset best and a 32 3 grid fits the 2048 dataset best. By calculating the average ppc for each case, we made a preliminary conclusion that the optimal ppc is around 1.5-1.8. This also implies an optimal grid resolution for a point-set with N particles to be (ppc * N ) 1/3 .</p></sec><sec><title>Classification</title><p>We tested our network on ModelNet40 (<xref ref-type="bibr" rid="b9">Z. Wu, 2015</xref>) and ModelNet10 for classification. We use a grid resolution 16 3 to train both the networks. As shown in <xref ref-type="table" rid="tab_2">Table 3</xref>, our result outperforms the state-of- art on ModelNet10, noticeably surpassing those using grids. On ModelNet40, our result rivals DGCNN (92.8% v.s. 92.9%). But our parameter number is significantly smaller than DGCNN (1M v.s. 21M .)</p></sec><sec><title>Segmentation</title><p>We tested our algorithm for object part segmentation on ShapeNet (<xref ref-type="bibr" rid="b0">Yi et al., 2016</xref>). We used a grid resolution and 32 3 for training and testing. We showed the state-of-art performance of our approach in <xref ref-type="table" rid="tab_3">Table 4</xref>. Since the category of each input object is known beforehand, we trained separate models for each category. Note that we only com- pared with point-based methods that had similar input (points or/and normals) as ours. It can be seen that we outperform all the state-of-art with less parameters (1.1M , 2 time steps) Some exam- ples animating the segmentation process can be seen in <xref ref-type="fig" rid="fig_5">Figure 6</xref>.</p></sec><sec><title>DISCUSSION AND CONCLUSION</title><p>This paper presents a new perspective in treating the point cloud learning problem as a dynamic advection problem using a learned background velocity field. The key technical contribution of the proposed approach is to jointly define the point cloud learning problem as a flow advection problem in a world space using a static background grid and the local space using moving particles. Compared with the previous hybrid grid-point learning methods, e.g. two-way coupled particle-grid schemes (<xref ref-type="bibr" rid="b11">Su et al., 2018</xref>; <xref ref-type="bibr" rid="b3">Atzmon et al., 2018</xref>; <xref ref-type="bibr" rid="b19">Liu et al., 2019</xref>), our approach solves the learning problem from a dynamic system perspective which accumulates features in a flow field learned temporally. The coupled Eulerian-Lagrangian data structure in conjunction with its accommodated interpolation schemes provide an effective solution to tackle the challenges regarding both stencil construction and feature evolution by leveraging a numerical infrastructure that is matured in the scientific computing community. On another hand, our approach can be thought of as an exploration in creating a new physical reservoir motivated by continuum mechanics in order to find alternative solutions for the conventional point cloud processing networks. Thanks to the low-dimensional physical space and the large time step our network allows, our learning accuracy rivals the state-of- the-art deep networks such as PointCNN (<xref ref-type="bibr" rid="b28">Li et al., 2018b</xref>) and DGCNN (<xref ref-type="bibr" rid="b19">Wang et al., 2019</xref>) while using significantly fewer network parameters (4% to 25% in our comparisons). Our future plan is to scale the algorithm to larger data sets and handle more complex point clouds with sparse and adaptive grid structures.</p></sec><sec><title>ACKNOWLEDGEMENT</title></sec><sec id="figures"><title>Figures</title><fig id="fig_0"><object-id>fig_0</object-id><label>Figure 1:</label><caption><title>Figure 1:</title><p>We build an advective net- work to create a fluidic reservoir with hybrid Eulerian-Lagrangian represen- tations for point cloud processing.</p></caption><graphic /></fig><fig id="fig_1"><object-id>fig_1</object-id><label>Figure 2:</label><caption><title>Figure 2:</title><p>Workflow overview: a) The feature vector for each particle is initialized by a 1 &#215; 1 con- volution; b) Particles are embedded in an Eulerian grid; c) Features are interpolated from particles to the grid, denoted as I G P ; d) 3D convolution is applied on the grid to calculate the generalized forces and grid features; e) A velocity field is generated on the background grid; f) Particles advect in the Eulerian space using the interpolated velocities; grid features are interpolated to particles, denoted as I P G , and appended to its feature vector; g) Particles aggregate. The workflow consists of one loop to update the particle positions and features iteratively with temporal evolution. Finally, the Lagrangian features are fed into a fully-connected network for classification and segmentation.</p></caption><graphic /></fig><fig id="fig_2"><object-id>fig_2</object-id><label>Figure 3:</label><caption><title>Figure 3:</title><p>Network architectures: The top diagram demonstrates the global architecture of our net- work with detailed information for tensor dimensionality and modular connectivity. The blue box is for particle states and the orange box indicates grid states. The dotted green box is the module gen- erating the initial Lagrangian features. The dotted red box is for the functional module of advection (see the bottom diagram). The states are connected with multi-layer perceptrons (black arrows in the diagram). Each MLP has a number of hidden layers with a different number of neurons (spec- ified by the numbers within the parentheses). The bottom figure shows the details of the advection module updating the particle features by transferring data on the grid and concatenating particles. Meanwhile, it updates the particle positions with the generalized Eulerian forces calculated on the grid.</p></caption><graphic /></fig><table-wrap id="tab_0"><label>Table 1:</label><caption><title>Table 1:</title><p>Temporal resolution</p></caption><table><tbody><tr><td /></tr></tbody></table></table-wrap><fig id="fig_3"><object-id>fig_3</object-id><label>Figure 4:</label><caption><title>Figure 4:</title><p>Temporal accuracy</p></caption><graphic /><graphic /></fig><fig id="fig_4"><object-id>fig_4</object-id><label>Figure 5:</label><caption><title>Figure 5:</title><p>Visualization of the advection of an airplane is shown with time steps of 2, 3 and 6. Note that we rotate the point cloud and normalize the velocity field for visualization purposes.</p></caption><graphic /><graphic /><graphic /><graphic /><graphic /><graphic /><graphic /><graphic /><graphic /><graphic /><graphic /><graphic /><graphic /><graphic /><graphic /><graphic /><graphic /><graphic /><graphic /><graphic /><graphic /><graphic /><graphic /><graphic /><graphic /><graphic /><graphic /><graphic /><graphic /><graphic /><graphic /><graphic /><graphic /><graphic /><graphic /><graphic /><graphic /><graphic /><graphic /><graphic /><graphic /><graphic /><graphic /><graphic /><graphic /><graphic /><graphic /><graphic /><graphic /><graphic /><graphic /><graphic /><graphic /><graphic /><graphic /><graphic /></fig><fig id="fig_5"><object-id>fig_5</object-id><label>Figure 6:</label><caption><title>Figure 6:</title><p>Visualization of segmentation. Examples of different categories are depicted, consisting of initial shape, intermediary grouping, and final part prediction.</p></caption><graphic /><graphic /><graphic /><graphic /><graphic /><graphic /><graphic /><graphic /><graphic /><graphic /><graphic /><graphic /><graphic /><graphic /><graphic /><graphic /><graphic /><graphic /><graphic /><graphic /><graphic /><graphic /><graphic /><graphic /><graphic /><graphic /></fig><table-wrap id="tab_1"><label>Table 2:</label><caption><title>Table 2:</title><p>Spatial resolution</p></caption><table><tbody><tr><td /></tr></tbody></table></table-wrap><fig id="fig_6"><object-id>fig_6</object-id><label>Figure 7:</label><caption><title>Figure 7:</title><p>PIC/FLIP VS PIC PIC/FLIP (Physical Intuition) Temporal smoothness is key for developing a dynamic system to achieve its equi- librium state. PIC/FLIP obtains such smoothness by av- eraging weighted velocities between two adjacent time steps. (Numerical Validation) To highlight the role of this averaging, we compared the accuracy between PIC/FLIP and PIC only (no temporal averaging) on ModelNet10. We can see from Figure 7 that the model with PIC/FLIP quickly stabilizes to a high accuracy, outperforming the model with PIC only.</p></caption><graphic /><graphic /></fig><table-wrap id="tab_2"><label>Table 3:</label><caption><title>Table 3:</title><p>Classification on ModelNet.</p></caption><table><tbody><tr><td /></tr></tbody></table></table-wrap><table-wrap id="tab_3"><label>Table 4:</label><caption><title>Table 4:</title><p>Segmentation results on ShapeNet.</p></caption><table><tbody><tr><td /></tr></tbody></table></table-wrap></sec></body><back><ack /><ref-list id="ref-list-1"><ref id="b0"><element-citation publication-type="journal"><article-title>Representation learn- ing and adversarial generation of 3d point clouds</article-title><source>CoRR</source><year>2017</year><person-group person-group-type="author"><name><surname>References Panos Achlioptas</surname><given-names>Olga</given-names></name><name><surname>Diamanti</surname><given-names>Ioannis</given-names></name><name><surname>Mitliagkas</surname><given-names>Leonidas J</given-names></name><name><surname>Guibas</surname><given-names /></name></person-group></element-citation></ref><ref id="b1"><element-citation publication-type="journal"><article-title>Joint 2D-3D-Semantic Data for Indoor Scene Understanding</article-title><source>ArXiv e-prints</source><year>2017</year><person-group person-group-type="author"><name><surname>Armeni</surname><given-names>I</given-names></name><name><surname>Sax</surname><given-names>A</given-names></name><name><surname>Zamir</surname><given-names>A R</given-names></name><name><surname>Savarese</surname><given-names>S</given-names></name></person-group></element-citation></ref><ref id="b2"><element-citation publication-type="journal"><article-title>3d semantic parsing of large-scale indoor spaces</article-title><source>The IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</source><year>2016</year><person-group person-group-type="author"><name><surname>Armeni</surname><given-names>Iro</given-names></name><name><surname>Sener</surname><given-names>Ozan</given-names></name><name><surname>Amir</surname><given-names>R</given-names></name><name><surname>Zamir</surname><given-names>Helen</given-names></name><name><surname>Jiang</surname><given-names>Ioannis</given-names></name><name><surname>Brilakis</surname><given-names>Martin</given-names></name><name><surname>Fischer</surname><given-names>Silvio</given-names></name><name><surname>Savarese</surname><given-names /></name></person-group></element-citation></ref><ref id="b3"><element-citation publication-type="journal"><article-title>Point convolutional neural networks by exten- sion operators</article-title><source>CoRR</source><year>2018</year><person-group person-group-type="author"><name><surname>Atzmon</surname><given-names>Matan</given-names></name><name><surname>Maron</surname><given-names>Haggai</given-names></name><name><surname>Lipman</surname><given-names>Yaron</given-names></name></person-group></element-citation></ref><ref id="b4"><element-citation publication-type="journal"><article-title>FLIP (Fluid-Implicit-Particle): A low- dissipation, particle-in-cell method for fluid flow</article-title><year>1987</year><person-group person-group-type="author"><name><surname>Brackbill</surname><given-names>J U</given-names></name><name><surname>Kothe</surname><given-names>D B</given-names></name><name><surname>Ruppel</surname><given-names>H M</given-names></name></person-group></element-citation></ref><ref id="b5"><element-citation publication-type="journal"><article-title>Geo- metric deep learning: going beyond euclidean data</article-title><source>CoRR</source><year>2016</year><person-group person-group-type="author"><name><surname>Michael</surname><given-names>M</given-names></name><name><surname>Bronstein</surname><given-names>Joan</given-names></name><name><surname>Bruna</surname><given-names>Yann</given-names></name><name><surname>Lecun</surname><given-names>Arthur</given-names></name><name><surname>Szlam</surname><given-names>Pierre</given-names></name><name><surname>Vandergheynst</surname><given-names /></name></person-group></element-citation></ref><ref id="b6"><element-citation publication-type="journal"><article-title>Spectral networks and locally connected networks on graphs</article-title><source>CoRR</source><year>2013</year><volume>1312.6203</volume><person-group person-group-type="author"><name><surname>Bruna</surname><given-names>Joan</given-names></name><name><surname>Zaremba</surname><given-names>Wojciech</given-names></name><name><surname>Szlam</surname><given-names>Arthur</given-names></name><name><surname>Lecun</surname><given-names>Yann</given-names></name></person-group></element-citation></ref><ref id="b7"><element-citation publication-type="journal"><article-title>Neural ordinary dif- ferential equations</article-title><source>Advances in Neural Information Processing Systems</source><year>2018</year><person-group person-group-type="author"><name><surname>Ricky</surname><given-names>T Q</given-names></name><name><surname>Chen</surname><given-names>Yulia</given-names></name><name><surname>Rubanova</surname><given-names>Jesse</given-names></name><name><surname>Bettencourt</surname><given-names>David</given-names></name><name><surname>Duvenaud</surname><given-names /></name></person-group></element-citation></ref><ref id="b8"><element-citation publication-type="journal"><article-title>Spherical cnns</article-title><source>CoRR, abs/1801.10130</source><year>2018</year><person-group person-group-type="author"><name><surname>Taco</surname><given-names>S</given-names></name><name><surname>Cohen</surname><given-names>Mario</given-names></name><name><surname>Geiger</surname><given-names>Jonas</given-names></name><name><surname>K&#246;hler</surname><given-names>Max</given-names></name><name><surname>Welling</surname><given-names /></name></person-group></element-citation></ref><ref id="b9"><element-citation publication-type="journal"><article-title>A proposal on machine learning via dynamical systems</article-title><source>Communications in Mathematics and Statistics</source><year>2017</year><volume>5</volume><issue>1</issue><fpage>1</fpage><lpage>11</lpage><person-group person-group-type="author"><name><surname>Weinan</surname><given-names>E</given-names></name></person-group></element-citation></ref><ref id="b10"><element-citation publication-type="journal"><source>The particle-in-cell method for hydrodynamic calculations</source><year>1957</year><person-group person-group-type="author"><name><surname>Evans</surname><given-names>M W</given-names></name><name><surname>Harlow</surname><given-names>F H</given-names></name></person-group></element-citation></ref><ref id="b11"><element-citation publication-type="journal"><article-title>A point set generation network for 3d object reconstruction from a single image</article-title><source>CoRR</source><year>2016</year><person-group person-group-type="author"><name><surname>Fan</surname><given-names>Haoqiang</given-names></name><name><surname>Su</surname><given-names>Hao</given-names></name><name><surname>Guibas</surname><given-names>Leonidas J</given-names></name></person-group></element-citation></ref><ref id="b12"><element-citation publication-type="journal"><source>Proceedings of the 28th Annual Conference on Computer Graphics and Interactive Techniques, SIGGRAPH '01</source><year>2001</year><fpage>15</fpage><lpage>22</lpage><person-group person-group-type="author"><name><surname>Fedkiw</surname><given-names>Ronald</given-names></name><name><surname>Stam</surname><given-names>Jos</given-names></name><name><surname>Jensen</surname><given-names>Henrik Wann</given-names></name></person-group></element-citation></ref><ref id="b13"><element-citation publication-type="journal"><article-title>Gvcnn: Group-view convolu- tional neural networks for 3d shape recognition</article-title><source>The IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</source><year>2018</year><person-group person-group-type="author"><name><surname>Feng</surname><given-names>Yifan</given-names></name><name><surname>Zhang</surname><given-names>Zizhao</given-names></name><name><surname>Zhao</surname><given-names>Xibin</given-names></name><name><surname>Ji</surname><given-names>Rongrong</given-names></name><name><surname>Gao</surname><given-names>Yue</given-names></name></person-group></element-citation></ref><ref id="b14"><element-citation publication-type="journal"><article-title>Multi- resolution 3d convolutional neural networks for object recognition</article-title><source>CoRR, abs/1805.12254</source><year>2018</year><person-group person-group-type="author"><name><surname>Ghadai</surname><given-names>Sambit</given-names></name><name><surname>Xian</surname><given-names>Yeow</given-names></name><name><surname>Lee</surname><given-names>Aditya</given-names></name><name><surname>Balu</surname><given-names>Soumik</given-names></name><name><surname>Sarkar</surname><given-names>Adarsh</given-names></name><name><surname>Krishnamurthy</surname><given-names /></name></person-group></element-citation></ref><ref id="b15"><element-citation publication-type="journal"><article-title>Sharp interface approaches and deep learning tech- niques for multiphase flows</article-title><source>Journal of Computational Physics</source><year>2019</year><volume>380</volume><fpage>442</fpage><lpage>463</lpage><person-group person-group-type="author"><name><surname>Gibou</surname><given-names>Frederic</given-names></name><name><surname>Hyde</surname><given-names>David</given-names></name><name><surname>Fedkiw</surname><given-names>Ron</given-names></name></person-group></element-citation></ref><ref id="b16"><element-citation publication-type="journal"><article-title>AtlasNet: A Papier-M&#226;ch&#233; Approach to Learning 3D Surface Generation</article-title><source>Proceedings IEEE Conf. on Computer Vision and Pattern Recognition (CVPR)</source><year>2018</year><person-group person-group-type="author"><name><surname>Groueix</surname><given-names>Thibault</given-names></name><name><surname>Fisher</surname><given-names>Matthew</given-names></name><name><surname>Kim</surname><given-names>Vladimir G</given-names></name><name><surname>Russell</surname><given-names>Bryan</given-names></name><name><surname>Aubry</surname><given-names>Mathieu</given-names></name></person-group></element-citation></ref><ref id="b17"><element-citation publication-type="journal"><article-title>The role of feedback in morphological computation with compliant bodies</article-title><source>Biological Cybernetics</source><year>2012</year><volume>106</volume><issue>10</issue><fpage>595</fpage><lpage>613</lpage><person-group person-group-type="author"><name><surname>Hauser</surname><given-names>Helmut</given-names></name><name><surname>Ijspeert</surname><given-names>Auke J</given-names></name><name><surname>F&#252;chslin</surname><given-names>Rudolf M</given-names></name><name><surname>Pfeifer</surname><given-names>Rolf</given-names></name><name><surname>Maass</surname><given-names>Wolfgang</given-names></name></person-group></element-citation></ref><ref id="b18"><element-citation publication-type="journal"><article-title>Deep residual learning for image recog- nition</article-title><source>arXiv preprint arXiv:1512.03385</source><year>2015</year><person-group person-group-type="author"><name><surname>He</surname><given-names>Kaiming</given-names></name><name><surname>Zhang</surname><given-names>Xiangyu</given-names></name><name><surname>Ren</surname><given-names>Shaoqing</given-names></name><name><surname>Sun</surname><given-names>Jian</given-names></name></person-group></element-citation></ref><ref id="b19"><element-citation publication-type="journal"><article-title>Deep level sets for salient object detection</article-title><source>IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</source><year>2017</year><fpage>540</fpage><lpage>549</lpage><person-group person-group-type="author"><name><surname>Hu</surname><given-names>Ping</given-names></name><name><surname>Shuai</surname><given-names>Bing</given-names></name><name><surname>Liu</surname><given-names>Jun</given-names></name><name><surname>Wang</surname><given-names>Gang</given-names></name></person-group></element-citation></ref><ref id="b20"><element-citation publication-type="journal"><article-title>The" echo state" approach to analysing and training recurrent neural networks-with an erratum note</article-title><source>Bonn, Germany: German National Research Center for Information Technology GMD Technical Report</source><year>2001</year><volume>148</volume><person-group person-group-type="author"><name><surname>Jaeger</surname><given-names>Herbert</given-names></name></person-group></element-citation></ref><ref id="b21"><element-citation publication-type="journal"><article-title>Adaptive nonlinear system identification with echo state networks</article-title><source>NIPS</source><year>2002</year><person-group person-group-type="author"><name><surname>Jaeger</surname><given-names>Herbert</given-names></name></person-group></element-citation></ref><ref id="b22"><element-citation publication-type="journal"><article-title>Real-time reservoir computing network-based systems for detection tasks on visual contents</article-title><year>2015</year><fpage>146</fpage><lpage>151</lpage><person-group person-group-type="author"><name><surname>Jalalvand</surname><given-names>Azarakhsh</given-names></name><name><surname>Wallendael</surname><given-names>Glenn</given-names></name><name><surname>Van De Walle</surname><given-names>Rik</given-names></name></person-group></element-citation></ref><ref id="b23"><element-citation publication-type="journal"><article-title>Pointsift: A sift-like network module for 3d point cloud semantic segmentation</article-title><source>CoRR</source><year>2018</year><person-group person-group-type="author"><name><surname>Jiang</surname><given-names>Mingyang</given-names></name><name><surname>Wu</surname><given-names>Yiran</given-names></name><name><surname>Lu</surname><given-names>Cewu</given-names></name></person-group></element-citation></ref><ref id="b24"><element-citation publication-type="journal"><article-title>Semi-supervised classification with graph convolutional net- works</article-title><source>CoRR</source><year>2016</year><person-group person-group-type="author"><name><surname>Thomas</surname><given-names>N</given-names></name><name><surname>Kipf</surname><given-names>Max</given-names></name><name><surname>Welling</surname><given-names /></name></person-group></element-citation></ref><ref id="b25"><element-citation publication-type="journal"><article-title>Escape from cells: Deep kd-networks for the recognition of 3d point cloud models</article-title><source>CoRR</source><year>2017</year><person-group person-group-type="author"><name><surname>Klokov</surname><given-names>Roman</given-names></name><name><surname>Victor</surname><given-names>S</given-names></name><name><surname>Lempitsky</surname><given-names /></name></person-group></element-citation></ref><ref id="b26"><element-citation publication-type="journal"><article-title>Pointgrid: A deep network for 3d shape understanding</article-title><source>The IEEE Confer- ence on Computer Vision and Pattern Recognition (CVPR)</source><year>2018</year><person-group person-group-type="author"><name><surname>Le</surname><given-names>Truc</given-names></name><name><surname>Duan</surname><given-names>Ye</given-names></name></person-group></element-citation></ref><ref id="b27"><element-citation publication-type="journal"><article-title>Gradient-based learning applied to doc- ument recognition</article-title><source>Proceedings of the IEEE</source><year>1998</year><volume>86</volume><fpage>2278</fpage><lpage>2324</lpage><person-group person-group-type="author"><name><surname>Lecun</surname><given-names>Yann</given-names></name><name><surname>Bottou</surname><given-names>Leon</given-names></name><name><surname>Bengio</surname><given-names>Y</given-names></name><name><surname>Haffner</surname><given-names>Patrick</given-names></name></person-group></element-citation></ref><ref id="b28"><element-citation publication-type="journal"><article-title>So-net: Self-organizing network for point cloud analysis</article-title><source>CoRR</source><year>2018</year><person-group person-group-type="author"><name><surname>Li</surname><given-names>Jiaxin</given-names></name><name><surname>Chen</surname><given-names>Ben M</given-names></name><name><surname>Lee</surname><given-names>Gim Hee</given-names></name></person-group></element-citation></ref><ref id="b29"><element-citation publication-type="journal"><source>Advances in Neural Information Processing Systems</source><year>2018</year><volume>31</volume><fpage>820</fpage><lpage>830</lpage><person-group person-group-type="author"><name><surname>Li</surname><given-names>Yangyan</given-names></name><name><surname>Bu</surname><given-names>Rui</given-names></name><name><surname>Sun</surname><given-names>Mingchao</given-names></name><name><surname>Wu</surname><given-names>Wei</given-names></name><name><surname>Di</surname><given-names>Xinhan</given-names></name><name><surname>Chen</surname><given-names>Baoquan</given-names></name></person-group></element-citation></ref><ref id="b30"><element-citation publication-type="journal"><article-title>Point-voxel CNN for efficient 3d deep learn- ing</article-title><source>CoRR</source><year>2019</year><person-group person-group-type="author"><name><surname>Liu</surname><given-names>Zhijian</given-names></name><name><surname>Tang</surname><given-names>Haotian</given-names></name><name><surname>Lin</surname><given-names>Yujun</given-names></name><name><surname>Han</surname><given-names>Song</given-names></name></person-group></element-citation></ref><ref id="b31"><element-citation publication-type="journal"><year>2018</year><fpage>3208</fpage><lpage>3216</lpage><person-group person-group-type="author"><name><surname>Long</surname><given-names>Zichao</given-names></name><name><surname>Lu</surname><given-names>Yiping</given-names></name><name><surname>Ma</surname><given-names>Xianzhong</given-names></name><name><surname>Dong</surname><given-names>Bin</given-names></name><name><surname>Stockholmsm&#228;ssan</surname><given-names>Stockholm</given-names></name><name><surname>Sweden</surname><given-names /></name></person-group></element-citation></ref><ref id="b32"><element-citation publication-type="journal"><article-title>Fixing weight decay regularization in adam</article-title><source>CoRR</source><year>2017</year><person-group person-group-type="author"><name><surname>Loshchilov</surname><given-names>Ilya</given-names></name><name><surname>Hutter</surname><given-names>Frank</given-names></name></person-group></element-citation></ref><ref id="b33"><element-citation publication-type="journal"><article-title>Reservoir computing approaches to recurrent neural network training</article-title><source>Computer Science Review</source><year>2009</year><volume>3</volume><issue>3</issue><fpage>127</fpage><lpage>149</lpage><person-group person-group-type="author"><name><surname>Luko&#353;evi&#269;ius</surname><given-names>Mantas</given-names></name><name><surname>Jaeger</surname><given-names>Herbert</given-names></name></person-group></element-citation></ref><ref id="b34"><element-citation publication-type="journal"><article-title>Real-time computing without stable states: A new framework for neural computation based on perturbations</article-title><source>Neural Comput.</source><year>2002</year><volume>14</volume><issue>11</issue><fpage>2531</fpage><lpage>2560</lpage><person-group person-group-type="author"><name><surname>Maass</surname><given-names>Wolfgang</given-names></name><name><surname>Natschl&#228;ger</surname><given-names>Thomas</given-names></name><name><surname>Markram</surname><given-names>Henry</given-names></name></person-group></element-citation></ref><ref id="b35"><element-citation publication-type="journal"><article-title>VoxNet: A 3D Convolutional Neural Network for Real-Time Object Recognition</article-title><source>IROS</source><year>2015</year><person-group person-group-type="author"><name><surname>Maturana</surname><given-names>D</given-names></name><name><surname>Scherer</surname><given-names>S</given-names></name></person-group></element-citation></ref><ref id="b36"><element-citation publication-type="journal"><article-title>Occupancy networks: Learning 3d reconstruction in function space</article-title><source>Proceedings IEEE Conf. on Computer Vision and Pattern Recognition (CVPR)</source><year>2019</year><person-group person-group-type="author"><name><surname>Mescheder</surname><given-names>Lars</given-names></name><name><surname>Oechsle</surname><given-names>Michael</given-names></name><name><surname>Niemeyer</surname><given-names>Michael</given-names></name><name><surname>Nowozin</surname><given-names>Sebastian</given-names></name><name><surname>Geiger</surname><given-names>Andreas</given-names></name></person-group></element-citation></ref><ref id="b37"><element-citation publication-type="journal"><article-title>Smoothed particle hydrodynamics</article-title><source>Annual review of astronomy and astrophysics</source><year>1992</year><volume>30</volume><issue>1</issue><fpage>543</fpage><lpage>574</lpage><person-group person-group-type="author"><name><surname>Joe J Monaghan</surname><given-names /></name></person-group></element-citation></ref></ref-list></back></article>