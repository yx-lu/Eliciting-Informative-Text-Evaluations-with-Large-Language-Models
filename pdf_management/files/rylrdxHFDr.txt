Title:
```
STATE ALIGNMENT-BASED IMITATION LEARNING
```
Abstract:
```
Consider an imitation learning problem that the imitator and the expert have differ- ent dynamics models. Most of the current imitation learning methods fail because they focus on imitating actions. We propose a novel state alignment based imita- tion learning method to train the imitator to follow the state sequences in expert demonstrations as much as possible. The state alignment comes from both lo- cal and global perspectives and we combine them into a reinforcement learning framework by a regularized policy update objective. We show the superiority of our method on standard imitation learning settings and imitation learning settings where the expert and imitator have different dynamics models.
```

Figures/Tables Captions:
```
Figure 2: Visualization of state alignment distribution q φ (z|x). β-VAE is a variant VAE that introduces an adjustable hyperparameter β to the original objective:
Figure 1: Using VAE as a state predic- tive model will be more self-correctable because of the stochastic sampling mech- anism. But this won't happen when we use VAE to predict actions.
Figure 3: Comparison with BC, GAIL and AIRL when dynamics are different from experts.
Figure 4: Imitation Learning of Actors with Heterogeneous Action Dynamics.
Figure 5: (a), (b) show the effects of Wasserstein distance and KL regularization on HalfCheetah-v2 and Humanoid-v2 given 20 demonstration trajectories. And (c) presents the result on Antmaze.
Table 1: Performance on Hopper-v2 and HalfCheetah-v2
Table 2: Analyze the role of VAE coefficient. The "None" item means replacing VAE with an ordinary network with linear layers.
Table 3: Compare behavior cloning to variational behavior cloning
```

Main Content:
```

Section Title: INTRODUCTION
  INTRODUCTION Learning from demonstrations (imitation learning, abbr. as IL) is a basic strategy to train agents for solving complicated tasks. Imitation learning methods can be generally divided into two categories: behavior cloning (BC) and inverse reinforcement learning (IRL). Behavior cloning ( Ross et al., 2011b ) formulates a supervised learning problem to learn a policy that maps states to actions using demonstration trajectories. Inverse reinforcement learning (Russell, 1998;  Ng et al., 2000 ) tries to find a proper reward function that can induce the given demonstration trajectories. GAIL ( Ho & Ermon, 2016 ) and its variants ( Fu et al.; Qureshi et al., 2018 ;  Xiao et al., 2019 ) are the recently proposed IRL-based methods, which uses a GAN-based reward to align the distribution of state- action pairs between the expert and the imitator. Although state-of-the-art BC and IRL methods have demonstrated compelling performance in stan- dard imitation learning settings, e.g. control tasks ( Ho & Ermon, 2016 ;  Fu et al.; Qureshi et al., 2018 ;  Xiao et al., 2019 ) and video games ( Aytar et al., 2018b ), these approaches are developed based on a strong assumption: the expert and the imitator share the same dynamics model; specifically, they have the same action space, and any feasible state-action pair leads to the same next state in proba- bility for both agents. The assumption brings severe limitation in practical scenarios: Imagine that a robot with a low speed limit navigates through a maze by imitating another robot which moves fast, then, it is impossible for the slow robot to execute the exact actions as the fast robot. However, the demonstration from the fast robot should still be useful because it shows the path to go through the maze. We are interested in the imitation learning problem under a relaxed assumption: Given an imita- tor that shares the same state space with the expert but their dynamics may be different, we train the imitator to follow the state sequence in expert demonstrations as much as possible. This is a more general formulation since it poses fewer requirements on the experts and makes demonstra- tion collection easier. Due to the dynamics mismatch, the imitator becomes more likely to deviate from the demonstrations compared with the traditional imitation learning setting. Therefore, it is very important that the imitator should be able to resume to the demonstration trajectory by itself. Note that neither BC-based methods nor GAIL-based IRL methods have learned to handle dynamics misalignment and deviation correction. To address the issues, we propose a novel approach with four main features: 1) State-based. Com- pared to the majority of literature in imitation learning, our approach is state-based rather than action-based. Not like BC and IRL that essentially match state-action pairs between the expert and the imitator, we only match states. An inverse model of the imitator dynamics is learned to recover the action; 2) Deviation Correction. A state-based β-VAE ( Higgins et al., 2017 ) is learned as the prior for the next state to visit. Compared with ordinary behavior cloning, this VAE-based next state predictor can advise the imitator to return to the demonstration trajectory when it deviates. The Published as a conference paper at ICLR 2020 robustness benefits from VAE's latent stochastic sampling; 3) Global State Alignment. While the VAE can help the agent to correct its trajectory to some extent, the agent may still occasionally enter states that are far away from demonstrations, where the VAE has no clue how to correct it. So we have to add a global constraint to align the states in demonstration and imitation. Inspired by GAIL that uses reward to align the distribution of state-action pairs, we also formulate an IRL problem whose maximal cumulative reward is the Wasserstein Distance between states of demonstration and imitation. Note that we choose not to involve state-action pairs as in GAIL( Ho & Ermon, 2016 ), or state-state pairs as in an observation-based GAIL ( Torabi et al., 2018a ), because our state-only formulation imposes weaker constraints than the two above options, thus providing more flexibility to handle different agent dynamics; 4) Regularized Policy Update. We combine the prior for next state learned from VAE and the Wasserstein distance-based global constraint from IRL in a unified framework, by imposing a Kullback-Leibler divergence based regularizer to the policy update in the Proximal Policy Optimization algorithm. To empirically justify our ideas, we conduct experiments in two different settings. We first show that our approach can achieve similar or better results on the standard imitation learning setting, which assumes the same dynamics between the expert and the imitator. We then evaluate our approach in the more challenging setting that the dynamics of the expert and the imitator are different. In a number of control tasks, we either change the physics properties of the imitators or cripple them by changing their geometries. Existing approaches either fail or can only achieve very low rewards, but our approach can still exhibit decent performance. Finally, we show that even for imitation across agents of completely different actuators, it is still possible for the state-alignment based method to work. Surprisingly, a point mass and an ant in MuJoCo ( Todorov et al., 2012 ) can imitate each other to navigate in a maze environment. Our contributions can be summarized as follows: • Propose to use a state alignment based method in the imitation learning problems where the expert's and the imitator's dynamics are different. • Propose a local state alignment method based on β-VAE and a global state alignment method based on Wasserstein distance. • Combine the local alignment and global alignment components into a reinforcement learn- ing framework by a regularized policy update objective.

Section Title: RELATED WORK
  RELATED WORK Imitation learning is widely used in solving complicated tasks where pure reinforcement learning might suffer from high sample complexity, like robotics control ( Le et al., 2017 ;  Ye & Alterovitz, 2017 ;  Pathak et al., 2018 ), autonomous vehicle ( Fu et al.; Pomerleau, 1989 ), and playing video game ( Hester et al., 2018 ;  Pohlen et al., 2018 ;  Aytar et al., 2018a ). Behavioral cloning ( Bain & Sommut, 1999 ) is a straight-forward method to learn a policy in a supervised way. However, behavioral cloning suffers from the problem of compounding errors as shown by (Ross & Bagnell, 2010), and this can be somewhat alleviated by interactive learning, such as DAGGER ( Ross et al., 2011b ). Another important line in imitation learning is inverse reinforcement learning (Russell, 1998;  Ng et al., 2000 ;  Abbeel & Ng, 2004 ;  Ziebart et al., 2008 ;  Fu et al. ), which finds a cost function under which the expert is uniquely optimal. Since IRL can be connected to min-max formulations, works like GAIL, SAM ( Ho & Ermon, 2016 ;  Blondé & Kalousis, 2018 ) utilize this to directly recover policies. Its connections with GANs (Good- fellow et al., 2014) also lead to f -divergence minimization ( Ke et al., 2019 ;  Nowozin et al., 2016 ) and Wasserstein distance minimization ( Xiao et al., 2019 ). One can also extend the framework from matching state-action pairs to state distribution matching, such as  Torabi et al. (2018a) ;  Sun et al. (2019) ;  Schroecker & Isbell (2017) . Other works ( Aytar et al., 2018b ;  Liu et al., 2018 ;  Peng et al., 2018 ) also learn from observation alone, by defining reward on state and using IRL to solve the tasks. Works like ( Lee et al., 2019 ;  Lee et al. ) also use state-based reward for exploration.  Torabi et al. (2018b) ;  Edwards et al. (2018)  will recover actions from observations by learning an inverse model or latent actions. However, our work aims to combine the advantage of global state distribu- tion matching and local state transition alignment, which combines the advantage of BC and IRL through a novel framework.

Section Title: BACKGROUNDS
  BACKGROUNDS Variational Autoencoders Kingma & Welling (2013);  Rezende et al. (2014)  provides a frame- work to learn both a probabilistic generative model p θ (x|z) as well as an approximated posterior Published as a conference paper at ICLR 2020 Larger β will penalize the total correlation ( Chen et al., 2018 ) to encourage more disentangled latent representations, while smaller β often results in sharper and more precise reconstructions.

Section Title: Wasserstein distance
  Wasserstein distance The Wasserstein distance between two density functions p(x) and q(x) with support on a compact metric space (M, d) has an alternative form due to Kantorovich-Rubenstein duality (Villani, 2008): Here, L 1 is the set of all 1-Lipschitz functions from M to R. Compared with the prevalent KL- divergence and its extension, the f-divergence family, Wasserstein distance has a number of advan- tages theoretically and numerically. Please refer to  Arjovsky et al. (2017)  and Solomon (2018) for a detailed discussion. Our imitation learning method is based on state align- ment from both local and global perspectives. For lo- cal alignment, the goal is to follow the transition of the demonstration as much as possible, and allow the re- turn to the demonstration trajectory whenever the im- itation deviates. To achieve both goals, we use a β- VAE ( Higgins et al., 2017 ) to generate the next state ( Figure 2  Left). For global alignment, we set up an ob- jective to minimize the Wasserstein distance between the states in the current trajectory and the demonstra- tions ( Figure 2  Right). There has to be a framework to naturally combine the local alignment and global alignment components. We resort to the reinforcement learning framework by encoding the local alignment as policy prior and encoding the global alignment as reward over states. Using Proximal Policy Optimiza- tion (PPO) by  Schulman et al. (2017)  as the backbone RL solver, we derive a regularized policy update. To maximally exploit the knowledge from demon- strations and reduce interactions with the environment, we adopt a pre-training stage to produce a good initialization based on the same policy prior induced by the local alignment. Our method is summarized in Algorithm 1. In the rest parts of this section, we will introduce all the components of our method in details.

Section Title: LOCAL ALIGNMENT BY STATE PREDICTIVE VAE
  LOCAL ALIGNMENT BY STATE PREDICTIVE VAE To align the transition of states locally, we need a predictive model to generate the next state which the agent should target at. And then we can train an inverse dynamics model to recover the cor- Published as a conference paper at ICLR 2020 Update policy using (5) 17: end while responding action, so as to provide a direct supervision for policy. It is worth-noting that, while training an inverse dynamics model is generally challenging, it is not so hard if we only focus on the agent dynamics, especially when the low-dimensional control states are accessible as in many practical scenarios. The problem of how to learn high-quality inverse/forward dynamics models is an active research topic. Instead of using an ordinary network to memorize the subsequent states, which will suffer from the same issue of compounding errors as behavioral cloning (Ross & Bagnell, 2010;  Ross et al., 2011a ), we propose to use VAE to generate the next state based on the following two reasons. First, as shown in ( Dai et al., 2018 ), VAE is more robust to outliers and regularize itself to find the support set of a data manifold, so it will generalize better for unseen data. Second, because of the latent stochastic sampling, the local neighborhood of a data point will have almost the same prediction, which is self-correctable when combined with a precise inverse dynamics model as illustrated in  Figure 1 . We can also use a VAE to generate action based on the current state. But if the agent deviated from the demonstration trajectory a little bit, this predicted action is not necessarily guide the agent back to the trajectory, as shown in  Figure 1 . And in Sec 5.3.2, we conduct experiments to compare the state predictive VAE and the action predictive VAE. Instead of the vanilla VAE, we use β-VAE to balance the KL penalty and prediction error, with formulation shown in (1). In Sec 5, we discuss the effects of the hyper-parameter β in different experiment settings as one of the ablation studies.

Section Title: GLOBAL ALIGNMENT BY WASSERSTEIN DISTANCE
  GLOBAL ALIGNMENT BY WASSERSTEIN DISTANCE Due to the difference of dynamics between the expert and the imitator, the VAE-based local align- ment cannot fully prevent the imitator from deviating from demonstrations. In such circumstances, we still need to assess whether the imitator is making progress in learning from the demonstra- tions. We, therefore, seek to control the difference between the state visitation distribution of the demonstration and imitator trajectories, which is a global constraint. Note that using this global constraint alone will not induce policies that follow from the demon- stration. Consider the simple case of learning an imitator from experts of the same dynamics. The expert takes cyclic actions. If the expert runs for 100 cycles with a high velocity and the imitator runs for only 10 cycles with a low velocity within the same time span, their state distribution would still roughly align. That is why existing work such as GAIL aligns state-action occupancy measure. However, as shown later, our state-based distribution matching will be combined with the local alignment component, which will naturally resolve this issue. The advantage of this state-based distribution matching over state-action pair matching as in GAIL or state-next-state pair matching in ( Torabi et al., 2018a ) is that the constraint becomes loosened.

Section Title: Published as a conference paper at ICLR 2020
  Published as a conference paper at ICLR 2020 We use IRL approach to achieve the state distribution matching by introducing a reinforcement learn- ing problem. Our task is to design the reward to train an imitator that matches the state distribution of the expert. Before introducing the reward design, we first explain the computation of the Wasserstein distance between the expert trajectories {τ e } and imitator trajectory {τ } using the Kantorovich duality: W(τ e , τ ) = sup φ∈L1 E s∼τe [φ(s)] − E s∼τ [φ(s)] (3) where φ is the Kantorovich's potential, and serves as the discriminator in WGAN ( Arjovsky et al., 2017 ). φ is trained with a gradient penalty term as WGAN-GP introduced in ( Gulrajani et al., 2017 ) After the rollout of imitator policy is obtained, the potential φ will be updated by (3). Assume a transition among an imitation policy rollout of length T is (s i , s i+1 ). To provide a dense signal every timestep, we assign the reward as: We now explain the intuition of the above reward. By solving (3), those states of higher probability in demonstration will have a larger φ value. The reward in (4) will thus encourage the imitator to visit such states. Maximizing the curriculum reward will be equivalent to In other words, the optimal policy of this MDP best matches the state visitation distributions w.r.t Wasserstein distance. Compared with AIRL ( Fu et al. ) that also defines rewards on states only, our approach indeed enjoys certain advantages in certain cases. We provide a theoretical justification in the Appendix D.

Section Title: REGULARIZED PPO POLICY UPDATE OBJECTIVE
  REGULARIZED PPO POLICY UPDATE OBJECTIVE As mentioned in the second paragraph of Sec 4.3, the global alignment has to be combined with local alignment. This is achieved by adding a prior to the original clipped PPO objective. We maximize the following unified objective function: We will explain the two terms in detail. L CLIP (θ) denotes the clipped surrogate objective used in the original PPO algorithm: L CLIP (θ) =Ê t min π θ (a|s) π θ old (a|s)Â t , clip π θ (a|s) π θ old (a|s) , 1 − , 1 + Â t , (6) whereÂ t is an estimator of the advantage function at timestep t. The advantage function is calcu- lated based on a reward function described in Sec 4.3. The D KL term in (5) serves as a regularizer to keep the policy close to a learned policy prior p a . This policy prior p a is derived from the state predictive VAE and an inverse dynamics model. Assume the β-VAE is f (s t ) = s t+1 and the inverse dynamics model is g inv (s t , s t+1 ) = a. To solve the case when the agents have different dynamics, we learn a state prediction network and use a learned inverse dynamics to decode the action. We define the action prior as p a (a t |s t ) ∝ exp − g inv (s t , f (s t )) − a t σ 2 (7) where the RHS is a pre-defined policy prior, a Gaussian distribution centered at g inv (s t , f (s t )). σ controls how strong the action prior is when regularizing the policy update, which is a hyper- parameter. Note that the inverse model can be further adjusted during interactions. Published as a conference paper at ICLR 2020 L CLIP is computed through the advantageÂ t and reflects the global alignment. The policy prior is obtained from the inverse model and local β-VAE, which makes the D KL serve as a local alignment constraint. Furthermore, our method can be regard as a combination of BC and IRL because our KL- divergence based action prior encodes the BC policy and we update the policy leveraging reward. We would note that our state-alignment method augments state distribution matching by taking relationships of two consecutive states into account with robustness concern.

Section Title: PRE-TRAINING
  PRE-TRAINING We pretrain the state predictive VAE and the inverse dynamics model, and then obtain the policy prior in (7), which is a Gaussian distribution. For pre-training, we want to initialize PPO's Gaussian policy π by this prior p a , by minimizing the KL-divergence between them. Practically, we use direct supervision from g inv (s t , f (s t )) and σ in (7) to directly train both the mean and variance of the policy network, which is more efficient during the pre-training stage. During the online interaction, the update rule of PPO's policy is by optimizing (5), and the variance will be further adjusted for all the dimensions of the action space.

Section Title: EXPERIMENTS
  EXPERIMENTS We conduct two different kinds of experiments to show the superiority of our method. In Sec 5.1, we compare our method with behavior cloning ( Bain & Sommut, 1999 ), GAIL ( Ho & Ermon, 2016 ), and AIRL (Fu et al.) in control setting where the expert and the imitator have different dynamics model, e.g., both of them are ant robots but the imitator has shorter legs. In Sec 5.1, we further evaluate in the traditional imitation learning setting. Finally, in Sec 5.3, we conduct ablation study to show the contribution of the components.

Section Title: IMITATION LEARNING ACROSS AGENTS OF DIFFERENT ACTION DYNAMICS
  IMITATION LEARNING ACROSS AGENTS OF DIFFERENT ACTION DYNAMICS

Section Title: ACTORS OF MODIFIED PHYSICS AND GEOMETRY PROPERTIES
  ACTORS OF MODIFIED PHYSICS AND GEOMETRY PROPERTIES We create environments using MuJoCo ( Todorov et al., 2012 ) by changing some properties of ex- perts, such as density and geometry of the body. We choose 2 environments, Ant and Swimmer, and augment them to 6 different environments: Heavy/Light/Disabled Ant/Swimmer. The Heavy/Light agents have modified density, and the disabled agents have modified head/tail/leg lengths. The demonstrations are collected from the standard Ant-v2 and Swimmer-v2. More descriptions of the environments and the demonstration collection process can be founded in the Appendix. We then evaluate our method on them.

Section Title: Published as a conference paper at ICLR 2020
  Published as a conference paper at ICLR 2020   Figure 3  demonstrates the superiority of our methods over all the baselines. Our approach is the most stable in all the 6 environments and shows the leading performance in each of them. GAIL seems to be the most sensitive to dynamics difference. AIRL, which is designed to solve imitation learning for actors of different dynamics, can perform on par with our method in two swimmer- based environments (DisabledSwimmer and HeavySwimmer) that have relatively lower dimensional action space (2D for swimmer versus 8D for ants). Interestingly, the stability and performance of vanilla behavior cloning are quite reasonable in 4 of the environments, although it failed to move about in the DisabledAnt and HeavyAnt environments. For these two tasks, the agent will reach dangerous states by cloning actions, yet our method will not approach these states by using state-based imitation. In the other four games, BC agents do not die but just move less efficiently, so they have a sub-optimal yet still reasonable score.

Section Title: ACTORS OF HETEROGENEOUS ACTION DYNAMICS
  ACTORS OF HETEROGENEOUS ACTION DYNAMICS We consider an extremely challenging setting that the imitator and demonstrator are functionally different. One typical example of expert/imitator pair in practice would be a human and a humanoid robot. We consider a much simplified version but with similar nature - a Point and an Ant in MuJoCo. In this task, even if the state space cannot be exactly matched, there are still some shared dimensions across the state space of the imitator and the actor, e.g., the location of the center of mass, and the demonstration should still teach the imitator in these dimensions. We use the same setting as many hierarchical RL papers, such as HIRO and Near-Optimal RL ( Nachum et al., 2018a ; b ). The agent need to reach a goal position in a maze, which is represented by (x,y) coordinates. We also know that the first two dimensions of states are the position of the agent. The prior knowledge includes: (1) the goal space (or the common space that need to be matched) (2) the projection from the state space to the goal space (select the first two dimensions of the states). The first task is that the Ant should reach the other side of the maze from several successful demon- strations of a Point robot. As shown in Figure 4(c) and Figure 4(d), the maze structure for the ant and point mass is exactly the same. To solve this problem, we first pre-train an VAE on the demonstrations, and use this VAE to propose the next "subgoal" for the Ant. This VAE is trained on the goal space (i.e. the first two dimensions) of the Point robot's trajectory. Then we train an inverse model for Ant, which will generate an action based on the Ant's current state (high dimensional) and goal predicted by VAE (2 dimensional). Our performance is shown in Figure 5(c). After 1M training steps, the agent has success rate of 0.8 to reach the other side of the maze.

Section Title: ACTORS OF THE SAME DYNAMICS (STANDARD IMITATION LEARNING)
  ACTORS OF THE SAME DYNAMICS (STANDARD IMITATION LEARNING) We also evaluate our algorithm on 6 non-trivial control tasks in MuJoCo: Swimmer, Hopper, Walker, Ant, HalfCheetach, and Humanoid. We first collect demonstration trajectories with Soft Actor- Critic, which can learn policies that achieve high scores in most of these environments 2 . For com- 1 For LightSwimmer 3(e), AIRL meets MuJoCo numerical exception for several trials. We collect near-optimal demonstration on Swimmer using TRPO due to the limited performance of SAC. Published as a conference paper at ICLR 2020 parison, we evaluate our method against 3 baselines: behavior cloning, GAIL, and AIRL 3 . Also, to create even stronger baselines for the cumulative reward and imitator run-time sample complex- ity, we initialize GAIL with behavior cloning, which would obtain higher scores in Swimmer and Walker. Lastly, to evaluate how much each algorithm depends on the amount of demonstrations, we sampled demonstration trajectories of ten and fifty episodes.  Table 1  depicts representative results in Hopper and HalfCheetah 4 . The advantage of our meth- ods over BC should be attributed to the inherent data augmentation by VAE. On Hopper-v2, we are significantly better with 10 demos but are just on par if the demos are increased to 50. On HalfCheetah-v2, the demo cheetah runs almost perfectly ( 12294 scores); in other words, the demo provides limited instruction when the imitator is even slightly off the demo states, thus the robustness from VAE becomes critical. To empirically show the role of beta and check the sensitivity of our algorithm with respect to beta, we evaluate VAE in settings of both the imitator has the same dynamics and has different dynam- ics. We select HalfCheetah-v2 and HeavyAnt as an example. For HalfCheetah-v2, we pretrain the inverse dynamics and VAE using given demonstrations so that the initial performance will tell the quality of the VAE's prediction. For DisabledAnt, we pretrain the dynamics with random trials, which results in forward/inverse dynamics estimation of less accuracy. In this case, we examine both its initialized performance and final performance. The results are shown in  Table 2 . We find out that for β in [0.01, 0.1], the performance is better. Specifically, when the imitator is different from the expert, a smaller β will result in poor performance as it overfits the demonstration data. We also compare our method with an ordinary MLP trained by MSE loss. We find out that VAE outperforms MLP in all settings. Note that the MLP-based approach is very similar to the state-based behavior cloning work of ( Torabi et al., 2018b ).

Section Title: ACTION PREDICTIVE β-VAE
  ACTION PREDICTIVE β-VAE In  Figure 1 , we mentioned that a VAE to predict the next action is less favorable. To justify the claim, we compare a VAE-based BC with a vanilla BC that both predict actions, as shown in  Table 3 . Experiments show that VAE-BC is even outperformed by a vanilla BC, especially when β is larger than 0.001. Compared with the last line in  Table 2 , we can conclude that VAE is more useful when predicting state, which consolidates that the advantage really comes from our state-based approach but not only the robustness of VAE.

Section Title: EFFECT OF WASSERSTEIN DISTANCE AND KL REGULARIZATION
  EFFECT OF WASSERSTEIN DISTANCE AND KL REGULARIZATION In our policy update process, we use Wasserstein distance with KL regularization to update the policy. To analyze their effects on the performance, we use HalfCheetah-v2 and Humanoid-v2 with Published as a conference paper at ICLR 2020 20 expert trajectories. For each environment, they use the same pretrained inverse model and VAE, thus they have the same behavior after pretraining. As shown in Figure 5(a)(b), Wasserstein distance combined with KL regularization performs the best. Wasserstein objective is used in our inverse RL based mechanism that would significantly penalize the exploration when the agent deviates from the demonstration far away. However, using this objective alone lacks constraints over consecutive states, thus performing the worst. The KL objective adds constraints over consecutive states using a VAE prior; however, VAE is unable to extrapolate to states when the imitator deviates far from the demo (green line gradually fails as in  Fig 5 (b) ), but this is the scenario when the Wasserstein distance would not favor, thus the reward from the Wasserstein distance will push the imitator back to the demonstration states.

Section Title: CONCLUSION
  CONCLUSION We proposed SAIL, a flexible and practical imitation learning algorithms that use state alignment from both local and global perspective. We demonstrate the superiority of our method using MuJoCo environments, especially when the action dynamics are different from the demonstrations.

```
