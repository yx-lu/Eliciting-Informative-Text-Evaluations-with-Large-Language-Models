Title:
```
Under review as a conference paper at ICLR 2020 Learning Representations in Reinforcement Learning: An Information Bottleneck Approach
```
Abstract:
```
The information bottleneck principle in (Tishby et al., 2000) is an elegant and useful approach to representation learning. In this paper, we investi- gate the problem of representation learning in the context of reinforcement learning using the information bottleneck framework, aiming at improving the sample efficiency of the learning algorithms. We analytically derive the optimal conditional distribution of the representation, and provide a vari- ational lower bound. Then, we maximize this lower bound with the Stein variational (SV) gradient method (originally developed in (Liu & Wang, 2016; Liu et al., 2017)). We incorporate this framework in the advanta- geous actor critic algorithm (A2C)(Mnih et al., 2016) and the proximal policy optimization algorithm (PPO) (Schulman et al., 2017). Our experi- mental results show that our framework can improve the sample efficiency of vanilla A2C and PPO significantly. Finally, we study the information bottleneck (IB) perspective in deep RL with the algorithm called mutual information neural estimation(MINE) (Belghazi et al., 2018). We exper- imentally verify that the information extraction-compression process also exists in deep RL and our framework is capable of accelerating this process. We also analyze the relationship between MINE and our method, through this relationship, we theoretically derive an algorithm to optimize our IB framework without constructing the lower bound.
```

Figures/Tables Captions:
```
Figure 1: Mutual information visualization in Pong After completing the visualization of MI with MINE, we analyze the relationship between our framework and MINE. According to (Belghazi et al., 2018), the optimal function T * in (20) goes as follows:
Figure 2: (a)-(e) show the performance of four A2C-based algorithms, x-axis is time steps(2000 update steps for each time step) and y-axis is the average reward over 10 episodes, (f)-(h) show the performance of four PPO-based algorithms, x-axis is time steps(300 update steps for each time step). We make exponential moving average of each game to smooth the curve(In PPO-Pong, we add 21 to all four curves in order to make exponential moving average). We can see that our framework improves sample efficiency of basic A2C and PPO.
```

Main Content:
```

Section Title: Introduction
  Introduction In training a reinforcement learning algorithm, an agent interacts with the environment, ex- plores the (possibly unknown) state space, and learns a policy from the exploration sample data. In many cases, such samples are quite expensive to obtain (e.g., requires interactions with the physical environment). Hence, improving the sample efficiency of the learning al- gorithm is a key problem in RL and has been studied extensively in the literature. Popular techniques include experience reuse/replay, which leads to powerful off-policy algorithms (e.g., ( Mnih et al., 2013 ;  Silver et al., 2014 ;  Van Hasselt et al., 2015 ;  Nachum et al., 2018a ;  Espeholt et al., 2018 )), and model-based algorithms (e.g., (Hafner et al., 2018;  Kaiser et al., 2019 )). Moreover, it is known that effective representations can greatly reduce the sample complexity in RL. This can be seen from the following motivating example: In the envi- ronment of a classical Atari game: Seaquest, it may take dozens of millions samples to converge to an optimal policy when the input states are raw images (more than 28,000 di- mensions), while it takes less samples when the inputs are 128-dimension pre-defined RAM data( Sygnowski & Michalewski, 2016 ). Clearly, the RAM data contain much less redundant information irrelevant to the learning process than the raw images. Thus, we argue that an efficient representation is extremely crucial to the sample efficiency. In this paper, we try to improve the sample efficiency in RL from the perspective of representation learning using the celebrated information bottleneck framework ( Tishby et al., 2000 ). In standard deep learning, the experiments in ( Shwartz-Ziv & Tishby, 2017 ) show that during the training process, the neural network first "remembers" the inputs by increasing the mutual information between the inputs and the repre- sentation variables, then compresses the inputs to efficient representation related to Under review as a conference paper at ICLR 2020 the learning task by discarding redundant information from inputs (decreasing the mu- tual information between inputs and representation variables). We call this phenomena "information extraction-compression process" "information extraction-compression process" "information extraction-compression process"(information E-C process). Our experiments shows that, similar to the results shown in ( Shwartz-Ziv & Tishby, 2017 ), we first (to the best of our knowledge) observe the information extraction-compression phenomena in the context of deep RL (we need to use MINE(Belghazi et al., 2018) for estimating the mu- tual information). This observation motivates us to adopt the information bottleneck (IB) framework in reinforcement learning, in order to accelerate the extraction-compression pro- cess. The IB framework is intended to explicitly enforce RL agents to learn an efficient representation, hence improving the sample efficiency, by discarding irrelevant information from raw input data. Our technical contributions can be summarized as follows: 1. We observe that the "information extraction-compression process" also exists in the context of deep RL (using MINE(Belghazi et al., 2018) to estimate the mutual information). 2. We derive the optimization problem of our information bottleneck framework in RL. In order to solve the optimization problem, we construct a lower bound and use the Stein variational gradient method developed in ( Liu et al., 2017 ) to optimize the lower bound. 3. We show that our framework can accelerate the information extraction-compression process. Our experimental results also show that combining actor-critic algorithms (such as A2C, PPO) with our framework is more sample-efficient than their original versions. 4. We analyze the relationship between our framework and MINE, through this rela- tionship, we theoretically derive an algorithm to optimize our IB framework without constructing the lower bound. Finally, we note that our IB method is orthogonal to other methods for improving the sample efficiency, and it is an interesting future work to incorporate it in other off-policy and model-based algorithms.

Section Title: Related Work
  Related Work Information bottleneck framework was first introduced in ( Tishby et al., 2000 ). They solve the framework by iterative Blahut Arimoto algorithm, which is infeasible to apply to deep neural networks. ( Shwartz-Ziv & Tishby, 2017 ) tries to open the black box of deep learning from the perspective of information bottleneck, though the method they use to compute the mutual information is not precise. ( Alemi et al., 2016 ) derives a variational information bottleneck framework, yet apart from adding prior target distribution of the representation distribution P (Z|X), they also assume that P (Z|X) itself must be a Gaussian distribution, which limits the capabilities of the representation function. ( Peng et al., 2018 ) extends this framework to variational discriminator bottleneck to improve GANs( Goodfellow et al., 2014 ), imitation learning and inverse RL. As for improving sample-efficiency, ( Mnih et al., 2013 ;  Van Hasselt et al., 2015 ;  Nachum et al., 2018a ) mainly utilize the experience-reuse. Besides experience-reuse, ( Silver et al., 2014 ;  Fujimoto et al., 2018 ) tries to learn a deterministic policy, ( Espeholt et al., 2018 ) seeks to mitigate the delay of off-policy. (Hafner et al., 2018;  Kaiser et al., 2019 ) learn the environment model. Some other powerful techniques can be found in ( Botvinick et al., 2019 ). State representation learning has been studied extensively, readers can find some classic works in the overview ( Lesort et al., 2018 ). Apart from this overview, ( Nachum et al., 2018b ) shows a theoretical foundation of maintaining the optimality of representation space. ( Bellemare et al., 2019 ) proposes a new perspective on representation learning in RL based on geometric properties of the space of value function. ( Abel et al., 2019 ) learns representation via information bottleneck(IB) in imitation/apprenticeship learning. To the best of our knowledge, there is no work that intends to directly use IB in basic RL algorithms.

Section Title: Preliminaries
  Preliminaries A Markov decision process(MDP) is a tuple, (X , A, R, P, µ), where X is the set of states, A is the set of actions, R : X × A × X → R is the reward function, P : X × A × X →[0, 1] is the transition probability function(where P (X ′ |X, a) is the probability of transitioning to state X ′ given that the previous state is X and the agent took action a in X), and µ : X →[0, 1] is the starting state distribution. A policy π : X → P(A) is a map from states to probability distributions over actions, with π(a|X) denoting the probability of choosing action a in state X. In reinforcement learning, we aim to select a policy π which maximizes K(π) = E τ ∼π [ ∑ ∞ t=0 γ t R(X t , a t , X t+1 )], with a slight abuse of notation we denote R(X t , a t , X t+1 ) = r t . Here γ ∈ [0, 1) is a discount factor, τ denotes a trajectory (X 0 , a 0 , X 1 , a 1 , ...). Define the state value function as V π (X) = E τ ∼π [ ∑ ∞ t=0 γ t r t |X 0 = X], which is the expected return by policy π in state X. And the state-action value function Q π (X, a) = E τ ∼π [ ∑ ∞ t=0 γ t r t |X 0 = X, a 0 = a] is the expected return by policy π after taking action a in state X. Actor-critic algorithms take the advantage of both policy gradient methods and value- function-based methods such as the well-known A2C( Mnih et al., 2016 ). Specifically, in the case that policy π(a|X; θ) is parameterized by θ, A2C uses the following equation to approximate the real policy gradient ∇ θ K(π) = ∇ θĴ (θ): where R t = ∑ ∞ i=0 γ i r t+i is the accumulated return from time step t, H(p) is the entropy of distribution p and b(X t ) is a baseline function, which is commonly replaced by V π (X t ). A2C also includes the minimization of the mean square error between R t and value function V π (X t ). Thus in practice, the total objective function in A2C can be written as: where α 1 , α 2 are two coefficients. In the context of representation learning in RL, J(X t ; θ)(including V π (X t ) and Q π (X t , a t )) can be replaced by J(Z t ; θ) where Z t is a learnable low-dimensional representation of state X t . For example, given a representation function Z ∼ P ϕ (·|X) with parameter ϕ, define V π (Z t ; X t , ϕ) | Zt∼P ϕ (·|Xt) = V π (X t ). For simplicity, we write V π (Z

Section Title: Framework
  Framework

Section Title: Information Bottleneck in Reinforcement Learning
  Information Bottleneck in Reinforcement Learning The information bottleneck framework is an information theoretical framework for extract- ing relevant information, or yielding a representation, that an input X ∈ X contains about an output Y ∈ Y. An optimal representation of X would capture the relevant factors and compress X by diminishing the irrelevant parts which do not contribute to the prediction of Y . In a Markovian structure X → Z → Y where X is the input, Z is representation of X and Y is the label of X, IB seeks an embedding distribution P ⋆ (Z|X) such that: Under review as a conference paper at ICLR 2020 for every X ∈ X , which appears as the standard cross-entropy loss 1 in supervised learning with a MI-regularizer, β is a coefficient that controls the magnitude of the regularizer. Next we derive an information bottleneck framework in reinforcement learning. Just like the label Y in the context of supervised learning as showed in (3), we assume the supervising signal Y in RL to be the accurate value R t of a specific state X t for a fixed policy π, which can be approximated by an n-step bootstrapping function Y t = R t = ∑ n−2 i=0 γ i r t+i + γ n−1 V π (Z t+n−1 ) in practice. Let P (Y |Z) be the following distribution: P (Y t |Z t ) ∝ exp(−α(R t − V π (Z t )) 2 ) (4) .This assumption is heuristic but reasonable: If we have an input X t and its relative label Y t = R t , we now have X t 's representation Z t , naturally we want to train our decision function V π (Z t ) to approximate the true label Y t . If we set our target distribution to be C · exp(−α(R t − V π (Z t )) 2 ), the probability decreases as V π (Z t ) gets far from Y t while increases as V π (Z t ) gets close to Y t . For simplicity, we just write P (R|Z) instead of P (Y t |Z t ) in the following context. With this assumption, equation (3) can be written as: The first term looks familiar with classic mean squared error in supervisd learning. In a network with representation parameter ϕ and policy-value parameter θ, policy lossĴ(Z; θ) in equation(1) and IB loss in (5) can be jointly written as: where I(X, Z; ϕ) denotes the MI between X and Z ∼ P ϕ (·|X). Notice that J(Z; θ) itself is a standard loss function in RL as showed in (2). Finally we get the ultimate formalization of IB framework in reinforcement learning: The following theorem shows that if the mutual information I(X, Z) of our framework and common RL framework are close, then our framework is near-optimality. Theorem Theorem Theorem 1 (Near-optimality theorem). Policy π r = π θ r , parameter ϕ r , optimal policy π ⋆ = π θ ⋆ and its relevant representation parameter ϕ ⋆ are defined as following: Define J π r as E P ϕ r (X,Z) [J(Z; θ r )] and J π ⋆ as E P ϕ ⋆ (X,Z) [J(Z; θ ⋆ )]. Assume that for any

Section Title: Target Distribution Derivation and Variational Lower Bound Construction
  Target Distribution Derivation and Variational Lower Bound Construction In this section we first derive the target distribution in (7) and then seek to optimize it by constructing a variational lower bound.

Section Title: Under review as a conference paper at ICLR 2020
  Under review as a conference paper at ICLR 2020 We would like to solve the optimization problem in (7): Combining the derivative of L 1 and L 2 and setting their summation to 0, we can get that We provide a rigorous derivation of (11) in the appendix(A.2). We note that though our derivation is over the representation space instead of the whole network parameter space, the optimization problem (10) and the resulting distribution (11) are quite similar to the one studied in ( Liu et al., 2017 ) in the context of Bayesian inference. However, we stress that our formulation follows from the information bottleneck framework, and is mathematically different from that in ( Liu et al., 2017 ). In particular, the difference lies in the term L 2 , which depends on the the distribution P ϕ (Z | X) we want to optimize (while in ( Liu et al., 2017 ), the corresponding term is a fixed prior). The following theorem shows that the distribution in (11) is an optimal target distribution (with respect to the IB objective L). The proof can be found in the appendix(A.3). Theorem Theorem Theorem 2. (Representation Improvement Theorem) Consider the objective function L(θ, ϕ) = E X∼P (X),Z∼P ϕ (Z|X) [J(Z; θ)] − βI(X, Z; ϕ), given a fixed policy-value parame- ter θ, representation distribution P ϕ (Z|X) and state distribution P (X). Define a new representation distribution: Pφ(Z|X) ∝ P ϕ (Z) exp( 1 β J(Z; θ)). We have L(θ,φ) ≥ L(θ, ϕ). Though we have derived the optimal target distribution, it is still difficult to compute P ϕ (Z). In order to resolve this problem, we construct a variational lower bound with a distribution U (Z) which is independent of ϕ. Notice that ∫ dZP ϕ (Z) log P ϕ (Z) ≥ ∫ dZP ϕ (Z) log U (Z). Now, we can derive a lower bound of L(θ, ϕ) in (6) as follows: Naturally the target distribution of maximizing the lower bound is:

Section Title: Optimization by Stein Variational Gradient Descent
  Optimization by Stein Variational Gradient Descent Next we utilize the method in ( Liu & Wang, 2016 )( Liu et al., 2017 )( Haarnoja et al., 2017 ) to optimize the lower bound. Stein variational gradient descent(SVGD) is a non-parametric variational inference algo- rithm that leverages efficient deterministic dynamics to transport a set of particles {Z i } n i=1 to approximate given target distributions Q(Z). We choose SVGD to optimize the lower bound because of its ability to handle unnormalized target distributions such as (13). Briefly, SVGD iteratively updates the "particles" {Z i } n i=1 via a direction function Φ ⋆ (·) in the unit ball of a reproducing kernel Hilbert space (RKHS) H: Z i ← Z i + ϵΦ ⋆ (Z i ) (14) where Φ * (·) is chosen as a direction to maximally decrease 2 the KL divergence between the particles' distribution P (Z) and the target distribution Q(Z) =Q (Z) C (Q is unnormalized 2 In fact, Φ * is chosen to maximize the directional derivative of F (P ) = −DKL(P ||Q), which appears to be the "gradient" of F Under review as a conference paper at ICLR 2020 distribution, C is normalized coefficient) in the sense that Φ ⋆ ← arg max ϕ∈H {− d dϵ D KL (P [ϵϕ] ||Q) s.t. ∥Φ∥ H ≤ 1} (15) where P [ϵΦ] is the distribution of Z + ϵΦ(Z) and P is the distribution of Z. ( Liu & Wang, 2016 ) showed a closed form of this direction: Φ(Z i ) = E Zj ∼P [K(Z j , Z i )∇Ẑ logQ(Ẑ) |Ẑ =Zj +∇ẐK(Ẑ, Z i ) |Ẑ =Zj ] (16) where K is a kernel function(typically an RBF kernel function). Notice that C has been omitted. In our case, we seek to minimize D KL (P ϕ (·|X)|| U (·) exp( 1 β J(·;θ)) C ) | C= ∫ dZU (Z) exp( 1 β J(Z;θ)) , which is equivalent to maximizeL(θ, ϕ), the greedy direction yields: In practice we replace log U (Ẑ) with ζ log U (Ẑ) where ζ is a coefficient that controls the magnitude of ∇Ẑ log U (Ẑ). Notice that Φ(Z i ) is the greedy direction that Z i moves towardŝ L(θ, ϕ)'s target distribution as showed in (13)(distribution that maximizesL(θ, ϕ)). This means Φ(Z i ) is the gradient ofL(Z i , θ, ϕ): ∂L(Zi,θ,ϕ) ∂Zi ∝ Φ(Z i ). Since our ultimate purpose is to update ϕ, by the chain rule, ∂L(Zi,θ,ϕ) Φ(Z i ) is given in equation(17). In practice we update the policy-value parameter θ by common policy gradient algorithm since: ∂L(θ, ϕ) ∂θ = E P ϕ (X,Z) [ ∂J(Z; θ) ∂θ ] (19) and update representation parameter ϕ by (18).

Section Title: Verify the information E-C process with MINE
  Verify the information E-C process with MINE This section we verify that the information E-C process exists in deep RL with MINE and our framework accelerates this process. Mutual information neural estimation(MINE) is an algorithm that can compute mutual information(MI) between two high dimensional random variables more accurately and effi- ciently. Specifically, for random variables X and Z, assume T to be a function of X and Z, the calculation of I(X, Z) can be transformed to the following optimization problem: The optimal function T ⋆ (X, Z) can be approximated by updating a neural network T (X, Z; η). With the aid of this powerful tool, we would like to visualize the mutual information between input state X and its relative representation Z: Every a few update steps, we sample a batch of inputs and their relevant representations {X i , Z i } n i=1 and compute their MI with MINE, every time we train MINE(update η) we just shuffle {Z i } n i=1 and roughly assume the shuffled representations {Z shuffled i } n i=1 to be independent with {X i } n i=1 : Under review as a conference paper at ICLR 2020  Figure(1)  is the tensorboard graph of mutual information estimation between X and Z in Atari game Pong, x-axis is update steps and y-axis is MI estimation. More details and results can be found in appendix(A.6) and (A.7). As we can see, in both A2C with our framework and common A2C, the MI first increases to encode more information from inputs("remember" the inputs), then decreases to drop irrelevant information from in- puts("forget" the useless information). And clearly, our framework extracts faster and compresses faster than common A2C as showed in figure(1)(b). Combining the result with Theorem(2), we get: Through this relationship, we theoretically derive an algorithm that can directly optimize our framework without constructing the lower bound, we put this derivation in the ap- pendix(A.5).

Section Title: Experiments
  Experiments In the experiments we show that our framework can improve the sample efficiency of basic RL algorithms(typically A2C and PPO). Our anonymous code can be found in https://github. com/AnonymousSubmittedCode/SVIB. Other results can be found in last two appendices. In A2C with our framework, we sample Z by a network ϕ(X, ϵ) where ϵ ∼ N (·; 0, 0.1) and the number of samples from each state X is 32, readers are encouraged to take more samples if the computation resources are sufficient. We set the IB coefficient as β = 0.001. We choose two prior distributions U (Z) of our framework, the first one is uniform distribution, apparently when U (Z) is the uniform distribution, ∇Ẑ log U (Ẑ) |Ẑ =Z can be omitted. The second one is a Gaussian distribution, which is defined as follows: for a given state X i , sample a batch of {Z i j } n=32 j=1 , then: We also set ζ as 0.005∥∇Ẑ 1 β J(Ẑ; θ)/∇Ẑ log U (Ẑ)∥ |Ẑ =Z to control the magnitude of ∇Ẑ log U (Ẑ) |Ẑ =Z . Following ( Liu et al., 2017 ), the kernel function in (17) we used is the Gaussian RBF kernel K(Z i , Z j ) = exp(−∥Z i − Z j ∥ 2 /h) where h = med 2 /2 log(n + 1), med denotes the median of pairwise distances between the particles {Z i j } n i=1 . As for the hyper-parameters in RL, we simply choose the default parameters in A2C of Openai- baselines(https://github.com/openai/baselines/tree/master/baselines/a2c). In summary, we implement the following four algorithms: Under review as a conference paper at ICLR 2020 A2C with uniform SVIB A2C with uniform SVIB A2C with uniform SVIB: Use ϕ(X, ϵ) as the embedding function, optimize by our frame- work(algorithm(A.4)) with U (Z) being uniform distribution. A2C with Gaussian SVIB A2C with Gaussian SVIB A2C with Gaussian SVIB: Use ϕ(X, ϵ) as the embedding function, optimize by our frame- work(algorithm(A.4)) with U (Z) being Gaussian distribution. A2C A2C A2C:Regular A2C in Openai-baselines with ϕ(X) as the embedding function. A2C with noise A2C with noise A2C with noise(For fairness):A2C with the same embedding function ϕ(X, ϵ) as A2C with our framework. Figure(2)(a)-(e) show the performance of four A2C-based algorithms in 5 gym Atari games. We can see that A2C with our framework is more sample-efficient than both A2C and A2C with noise in nearly all 5 games. Notice that in SpaceInvaders, A2C with Gaussian SVIB is worse. We suspect that this is because the agent excessively drops information from inputs that it misses some information related to the learning process. There is a more detailed experimental discussion about this phenomena in appendix(A.7) . We also implement four PPO-based algorithms whose experimental settings are same as A2C except that we set the number of samples as 26 for the sake of computation efficiency. Results can be found in the in figure(2)(f)-(h).

Section Title: Conclusion
  Conclusion We study the information bottleneck principle in RL: We propose an optimization problem for learning the representation in RL based on the information-bottleneck framework and derive the optimal form of the target distribution. We construct a lower bound and utilize Stein Variational gradient method to optimize it. Finally, we verify that the information extraction and compression process also exists in deep RL, and our framework can accelerate this process. We also theoretically derive an algorithm based on MINE that can directly optimize our framework and we plan to study it experimentally in the future work.

Section Title: A.1 Proof of Theorem 1
  A.1 Proof of Theorem 1 Theorem Theorem Theorem. (Theorem 1 restated)Policy π r = π θ r , parameter ϕ r , optimal policy π ⋆ = π θ ⋆ and its relevant representation parameter ϕ ⋆ are defined as following: Specifically, in value-based algorithm, this theorem also holds between expectation of two value functions. Proof Proof Proof. From equation(24) we can get: From equation(25) we can get: These two equations give us the following inequality:

```
