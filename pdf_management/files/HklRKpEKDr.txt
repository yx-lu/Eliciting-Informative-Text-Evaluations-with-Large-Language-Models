Title:
```
Under review as a conference paper at ICLR 2020 DEEP COORDINATION GRAPHS
```
Abstract:
```
This paper introduces the deep coordination graph (DCG) for collaborative multi- agent reinforcement learning. DCG strikes a flexible trade-off between represen- tational capacity and generalization by factorizing the joint value function of all agents according to a coordination graph into payoffs between pairs of agents. The value can be maximized by local message passing along the graph, which allows training of the value function end-to-end with Q-learning. Payoff functions are approximated with deep neural networks and parameter sharing improves gener- alization over the state-action space. We show that DCG can solve challenging predator-prey tasks that are vulnerable to the relative overgeneralization pathol- ogy and in which all other known value factorization approaches fail. 1 Not to be confused with generalization in the context of function approximation.
```

Figures/Tables Captions:
```
Figure 1: Examples of value factorization for 3 agents: (a) sum of independent utilities (as in VDN, Sunehag et al., 2018) corresponds to an unconnected CG. QMIX uses a monotonic mixture of util- ities instead of a sum (Rashid et al., 2018); (b) sum of pairwise payoffs (Castellini et al., 2019), which correspond to pairwise edges; (c) no factorization (as in QTRAN, Son et al., 2019) corre- sponds to one hyper-edge connecting all agents. Factorization allows parameter sharing between factors, shown next to the CG, which can dramatically improve the algorithm's sample complexity.
Figure 2: Influence of punishment p for attempts to catch prey alone on greedy test episode return (mean and shaded standard error, [number of seeds]) in a coordination task where 8 agents hunt 8 prey (dotted line denotes best possible return). Note that fully connected DCG (DCG, solid) are able to represent the value of joint actions and coordinate maximization, which leads to a better performance for larger p, where DCG without edges (VDN, dashed) has to fail eventually (p < −1).
Figure 3: Greedy test episode return for the coordination task of Figure 2 with punishment p = −2: (a) comparison to baseline algorithms; (b) comparison between DCG topologies. Note that QMIX, IQL, VDN and CG (dashed) do not solve the task (return 0) due to relative overgeneralization and that QTRAN learns very slowly due to the large action space. The reliability of DCG depends on the CG- topology: all seeds with fully connected DCG solved the task, but the high standard error for CYCLE, LINE and STAR topologies is caused by some seeds succeeding while others fail completely.
Figure 4: Greedy test episode return (mean and shaded standard error, [number of seeds]) in a non-decentralizable task where 8 agents hunt 8 prey: (a) comparison to baseline algorithms; (b) comparison between DCG topologies. The prey turns randomly into punishing ghosts, which are indistinguishable from normal prey. The prey status is only visible at an indicator that is placed ran- domly at each episode in one of the grid's corners. QTRAN, QMIX, IQL and VDN learn decentralized policies, which are at best suboptimal in this task (around lower dotted line). Fully connected DCG and CG can learn a near-optimal policy (upper dotted line denotes best possible return), but a lack of parameter sharing slows down CG and yields sub-optimal performance in comparison to DCG.
Figure 5: Low-rank payoff approximation for the tasks in (a) Figure 3 and (b) Figure 4. The inlay in (a) magnifies the area of the gray box. Note that in (a) all approximation ranks learn much faster than full DCG, and rank 2-4 also have better performance.
Figure 6: Cumulative reward for test episodes on SMAC maps (mean and shaded standard error, [number of seeds]) for QMIX, VDN and fully connected DCG with rank K = 1 payoff approximation (DCG (rank 1)) and additional state-dependent bias function (DCG-V (rank 1)).
Table 1: Tested graph topologies for DCG.
```

Main Content:
```

Section Title: INTRODUCTION
  INTRODUCTION One of the central challenges in cooperative multi-agent reinforcement learning (MARL, Oliehoek & Amato, 2016) is coping with the size of the joint action space, which grows exponentially in the number of agents. For example, this paper evaluates tasks where eight agents each have six actions to choose from, yielding a joint action space with more than a million actions. Efficient MARL methods must thus be able to generalize over large joint action spaces, in the same way that convolutional neural networks allow deep RL to generalize over large visual state spaces. Even though few benchmark tasks actually require agent policies to be independently executable, one common approach to coping with large action spaces is to decentralize the decision policy and/or value function. For example, Figure 1a shows how the joint value function can be factorized into utility functions that each depend only on the actions of one agent (Sunehag et al., 2018; Rashid et al., 2018). Consequently, the joint value function can be efficiently maximized if each agent simply selects the action that maximizes its corresponding utility function. This factorization can represent any deterministic policy and thus can represent at least one optimal policy. However, that policy may not be learnable due to a game-theoretic pathology called relative overgeneralization 1 (Panait et al., 2006): during exploration other agents act randomly and punishment caused by uncooperative agents may outweigh rewards that would be achievable with coordinated actions. If the employed value function does not have the representational capacity to distinguish the values of coordinated and uncoordinated actions, an optimal policy cannot be learned. However, Castellini et al. (2019) show that higher-order factorization of the value function works surprisingly well in one-shot games that are vulnerable to relative overgeneralization, even if each factor depends on the actions of only a small subset of agents. Such a higher-order factorization can be expressed as an undirected coordination graph (CG, Guestrin et al., 2002a), where each vertex represents one agent and each (hyper-)edge one payoff function over the joint action space of the connected agents. Figure 1b shows a CG with pairwise edges and the corresponding value factorization. Depending on the CG topology, the value can thus depend nontrivially on the actions of all agents, yielding a richer representation. Although the value can no longer be maximized by each agent individually, the greedy action can be found using message passing along the edges (also known as belief propagation, Pearl, 1988). Sparse cooperative Q-learning (Kok & Vlassis, 2006) applies CGs to MARL but does not scale to modern benchmarks, as each payoff function (f 12 and f 23 in Figure 1b) is represented as a table over the state and joint action space of the connected agents. Castellini et al. (2019) use neural networks to approximate payoff functions, but only in one-shot games, and still require a unique function for each edge in the CG. Consequently, each agent group, represented by an edge, must still experience all corresponding action combinations, which can require executing a significant subset of the joint action space. To address these issues, this paper proposes the deep coordination graph (DCG), a deep RL al- gorithm that scales to modern benchmark tasks. DCG represents the value function as a CG with pairwise payoffs 2 (Figure 1b) and individual utilities (Figure 1a). This improves the representational capacity beyond state-of-the-art value factorization approaches like VDN (Sunehag et al., 2018) and QMIX (Rashid et al., 2018). To achieve scalability, DCG employs parameter sharing between pay- offs and utilities. Parameter sharing has long been a staple of factorized MARL. Methods like VDN and QMIX condition an agent's utility on its history, that is, its past observations and actions, and share the parameters of all utility functions. Experiences of one agent are thus used to train all. This can dramatically improve the sample efficiency compared to unfactored methods (Foerster et al., 2016; 2018; Lowe et al., 2017; Schröder de Witt et al., 2019; Son et al., 2019), which correspond to a CG with one hyper-edge connecting all agents (Figure 1c). DCG takes parameter sharing one step further by approximating all payoff functions with the same neural network. To allow unique out- puts for each payoff, the network is conditioned on a learned embedding of the participating agents' histories. This requires only one linear layer more than VDN and has thus less parameters as QMIX. DCG is trained end-to-end with deep Q-learning (DQN, Mnih et al., 2015), but uses message passing to coordinate greedy action selection between all agents in the graph. For k message passes over n agents with m actions each, the time complexity of maximization is only O(km(n + m)|E|), where |E| ≤ n 2 −n 2 is the number of (pairwise) edges, compared to O(m n ) for DQN without factorization. We compare DCG's performance with that of other MARL Q-learning algorithms in a challenging family of predator-prey tasks that require coordinated actions. Here DCG is the only algorithm that solves the harder tasks. We also investigate the influence of graph topologies on the performance.

Section Title: RELATED WORK
  RELATED WORK A general overview over cooperative deep MARL can be found in OroojlooyJadid & Hajinezhad (2019). Independent Q-learning (IQL Tan, 1993) decentralizes the agents' policy by modeling each agent as an independent Q-learner. However, the task from the perspective of a single agent becomes nonstationary as other agents change their policies. To address this, Foerster et al. (2017) show how to stabilize IQL when using experience replay buffers. Another approach to decentralized agents is centralized training and decentralized execution (Foerster et al., 2016) with a factorized value function. Value decomposition networks (VDN, Sunehag et al., 2018) performs central Q-learning with a value function that is the sum of independent utility functions for each agent (Figure 1a). The greedy policy can be executed by maximizing each utility independently. QMIX (Rashid et al., 2018) improves upon this approach by combining the agents' utilities with a mixing network, which is monotonic in the utilities and depends on the global state. This allows different mixtures in different states and the central value can be maximized independently due to monotonicity. All of these approaches are derived in Appendix A.1 and can use parameter sharing between the value/utility functions. However, they represent the joint value with independent values/utilities and are therefore susceptible to the relative overgeneralization pathology. We demonstrate this by comparing DCG with all the above algorithms.

Section Title: Under review as a conference paper at ICLR 2020
  Under review as a conference paper at ICLR 2020 Another straightforward way to decentralize in MARL is to define the joint policy as a product of in- dependent agent policies. This lends itself to the actor-critic framework, where the critic is discarded during execution and can therefore condition on the global state and all agents' actions during train- ing. Examples are MADDPG (Lowe et al., 2017) for continuous actions and COMA (Foerster et al., 2018) for discrete actions. Wei et al. (2018) specifically investigate the relative overgeneralization pathology in continuous multi-agent tasks and show improvement over MADDPG by introducing policy entropy regularization. MACKRL (Schröder de Witt et al., 2019) follows the approach in Foerster et al. (2018), but uses common knowledge to coordinate agents during centralized training. Son et al. (2019) define QTRAN, which also has a centralized critic but uses a greedy actor w.r.t. a VDN factorized function. The corresponding utility functions are distilled from the critic under constraints that ensure proper decentralization. Böhmer et al. (2019) present another approach to decentralize a centralized value function, which is locally maximized by coordinate ascent and de- centralized by training IQL agents from the same replay buffer. Centralized joint Q-value functions do not allow to share parameters to the same extent as value factorization, and we compare DCG to QTRAN to demonstrate the advantage in sample efficiency. That being said, DCG value factor- ization can in principle be applied to any of the above centralized critics to equally improve sample efficiency at the same cost of representational capacity. We leave this to future work. Other work deals with gigantic numbers of agents, which requires additional assumptions to reduce the sample complexity. For example, Yang et al. (2018) introduce mean-field multi-agent learn- ing (MF-MARL), which factorizes a tabular value function for hundreds of agents into pairwise payoff functions between neighbors in a uniform grid of agents. These payoffs share parameters similar to DCG. Chen et al. (2018) introduce a value factorization for a similar setup based on a low-rank approximation of the joint value. This approach is restricted by uniformity assumptions between agents, but uses otherwise parameter sharing similar to DCG. The value function cannot be maximized globally and must be locally maximized with coordinate ascent. These techniques are designed for much larger sets of agents and do not perform well in the usual MARL settings con- sidered in this paper. While they use similar parameter sharing techniques as DCG, we do therefore not compare against them.

Section Title: Coordination graphs (CG)
  Coordination graphs (CG) have been extensively studied in multi-agent robotics with given payoffs (e.g. Rogers et al., 2011; Yedidsion et al., 2018). Sparse cooperative Q-learning (SCQL, Kok & Vlassis, 2006) uses CG in discrete state and action spaces by representing all utility and payoff functions as tables. However, the tabular approach restricts practical application of SCQL to tasks with few agents and small state and action spaces. Castellini et al. (2019) use neural networks to approximate payoff functions, but only in one-shot games, and still require a unique function for each edge in the CG. DCG expands greatly upon these works by introducing parameter sharing between all payoffs (as in VDN/QMIX), conditioning on local information (as in MF-MARL) and evaluating in more complex tasks that are vulnerable to relative overgeneralization.

Section Title: BACKGROUND
  BACKGROUND In this paper we assume a Dec-POMDP for n agents S, {A i } n i=1 , P, r, {O i } n i=1 , {σ i } n i=1 , n, γ (Oliehoek & Amato, 2016). S denotes a finite or continuous set of environmental states and A i the discrete set of actions available to agent i. At discrete time t, the next state s t+1 ∈ S is drawn from transition kernel s t+1 ∼ P (·|s t , a t ), conditioned on the current state s t ∈ S and joint action a t ∈ A := A 1 × . . . × A n of all agents. A transition yields collaborative reward r t := r(s t , a t ), and γ ∈ [0, 1) denotes the discount factor. Each agent i observes the state only partially by drawing observations o i t ∈ O i from its observation kernel o i t ∼ σ i (·|s t ). The history of agent i's observations o i t ∈ O i and actions a i t ∈ A i is in the following denoted as τ i t := (o i 0 , a i 0 , o i 1 , . . . , o i t−1 , a i t−1 , o i t ) ∈ (O i × A i ) t × O i . Without loss of generality, this paper restricts itself to episodic tasks, which yield episodes (s 0 , {o i 0 } n i=1 , a 0 , r 0 , . . . , s T , {o i T } n i=1 ) of varying (but finite) length T .

Section Title: DEEP Q-LEARNING
  DEEP Q-LEARNING The goal of collaborative multi-agent reinforcement learning (MARL) is to find an optimal policy π * : S × A → [0, 1], that chooses joint actions a t ∈ A such that the expected discounted sum of future reward is maximized. This can be achieved by estimating the optimal Q-value function 3 : The optimal policy π * (·|s t ) chooses greedily the action a ∈ A that maximizes the corresponding optimal Q-value q * (a|s t ). In fully observable discrete state and action spaces, q * can be learned in the limit from interactions with the environment (Watkins & Dayan, 1992). For large or continuous state spaces, q * can only be approximated, e.g., with a deep neural network q θ (DQN, Mnih et al., 2015), parameterized by θ, by minimizing the mean-squared Bellman loss with gradient descent: The expectation is estimated with samples from an experience replay buffer D holding previ- ously observed episodes (Lin, 1992), andθ denotes the parameter of a separate target network, which is periodically replaced with a copy of θ to improve stability. Double Q-learning fur- ther stabilizes training by choosing the next action greedily w.r.t. the current network q θ , i.e., qθ(arg max q θ (·|s t+1 )|s t+1 ) instead of the target network max qθ(·|s t+1 ) (van Hasselt et al., 2016). In partially observable environments, the learned policy cannot condition on the state s t . Instead, Hausknecht & Stone (2015) approximate a Q-function that conditions on the agent's history τ t := {τ i t } n i=1 , i.e., q θ (a|τ t ), by conditioning a recurrent neural network (e.g., a GRU, Chung et al., 2014) on the agents' observations o t := (o 1 t , . . . , o n t ) and last actions a t−1 , that is, q θ (a|h t ) conditions on the recurrent network's hidden state h ψ (h t |h t−1 , o t , a t−1 ), where h 0 is initialized with zeros. Applying DQN to multi-agent tasks quickly becomes infeasible, due to the combinatorial explosion of state and action spaces. Moreover, DQN value functions cannot be maximized without evaluating all possible actions. To allow MARL Q-learning with efficient maximization, various algorithms based on value factorization have been developed. We derive IQL (Tan, 1993), VDN (Sunehag et al., 2018), QMIX (Rashid et al., 2018) and QTRAN (Son et al., 2019) in Appendix A.1.

Section Title: COORDINATION GRAPHS
  COORDINATION GRAPHS An undirected coordination graph (CG, Guestrin et al., 2002a) G = V, E contains a vertex v i ∈ V for each agent 1 ≤ i ≤ n and a set of undirected edges {i, j} ∈ E between vertices v i and v j . The graph is usually specified before training, but Guestrin et al. (2002b) suggest that the graph could also depend on the state, that is, each state can have its own unique CG. A CG induces a factorization 4 of the Q-function into utility functions f i and payoff functions f ij (Fig. 1a and 1b): The special case E = ∅ yields VDN, but each additional edge enables the representation of the value of the actions of a pair of agents and can thus help to avoid relative overgeneralization. Prior work also considered higher order coordination where the payoff functions depend on arbitrary sets of actions (Guestrin et al., 2002a; Kok & Vlassis, 2006; Castellini et al., 2019), corresponding to graphs with hyper-edges (Figure 1c). For the sake of simplicity we restrict ourselves here to pairwise edges, which yield at most |E| ≤ 1 2 (n 2 − n) edges, in comparison to up to n! d! (n−d)! hyper-edges of degree d. The induced Q-function q CG can be maximized locally using max-plus, also known as belief propagation (Pearl, 1988). At time t each node sends messages µ ij t (a j ) ∈ IR over all adjacent edges {i, j} ∈ E, which can be computed locally: This process repeats for a number of iterations, after which each agent i can locally find the action a i * that maximizes the estimated Q-value: Convergence of messages is guaranteed for acyclic CGs (Pearl, 1988; Wainwright et al., 2004), but messages can diverge in cyclic graphs, for example fully connected CGs. Subtracting a normal- ization constant c ij := a µ ij t (a) / |A i | from each message µ ij before it is sent often leads to convergence in practice (Murphy et al., 1999; Crick & Pfeffer, 2002; Yedidia et al., 2003). See Algorithm 3 in

Section Title: METHOD
  METHOD We now introduce the deep coordination graph (DCG), which learns the utility and payoff functions of a coordination graph V, E with deep neural networks. A direct implementation as in Castellini et al. (2019) would learn a separate network for each function f i and f ij . However, properly ap- proximating these Q-values requires observing the joint actions of each agent pair in the edge set E, which for dense graphs can be a significant subset of the joint action space of all agents A. We address this issue by focusing on an architecture that shares parameters across functions and restricts them to locally available information, i.e., to the histories of the participating agents. Sunehag et al. (2018) introduces parameter sharing between the agents' utility functions f i (u i |s t ) ≈ f v θ (u i |τ i t ) to dramatically improve the sample efficiency of VDN. Agents can have different action spaces A i but the choice of unavailable actions during maximization can be prevented by setting the utilities of unavailable actions to −∞. Specialized roles for individual agents can be achieved by conditioning f v θ on the agent's role, or more generally on the agent's ID (Foerster et al., 2018; Rashid et al., 2018). The DCG uses the same utility functions and adds payoff functions specified by pairwise edges in a given CG (Guestrin et al., 2002a). We take inspiration from highly scalable methods (Yang et al., 2018; Chen et al., 2018) and improve over SCQL (Kok & Vlassis, 2006) and the approach of Castellini et al. (2019) by incorporating the following design principles: i. restricting the payoffs f ij (a i , a j |τ i t , τ j t ) to local information of agents i and j only; ii. sharing parameters between all payoff and utility functions through a common RNN; iii. low-rank approximation of payoff matrices f ij (·, ·|τ i t , τ j t ) for large action spaces; iv. allowing transfer/generalization to different CG (as suggested in Guestrin et al., 2002b); v. allowing the use of privileged information like the global state during training. Restricting the payoff's input (i) and sharing parameters (ii) improves sample efficiency signif- icantly. As in Sunehag et al. (2018), all utilities are computed with the same neural network f i θ (u i |τ i t ) ≈ f v θ (u i |h i t ), but unlike Castellini et al. (2019), all payoffs are computed with the same neural network f ij (a i , a j |τ i t , τ j t ) ≈ f e φ (a i , a j |h i t , h j t ), too. Both share parameters though a common RNN h i t := h ψ (·|h i t−1 , o i t , a i t−1 ), which is initialized with h i 0 := h ψ (·|0, o i 0 , 0). Modeling the payoff function f e φ similar to DQN (Mnih et al., 2015) yields |A i × A j | separate outputs for edge {i, j}. For example, each agent in a StarCraft 2 map with 8 enemies has 13 actions (SMAC, Samvelyan et al., 2019), which yields 169 outputs of f e φ . As only executed actions-pairs are updated during Q-learning, the parameters of many outputs remain unchanged for long stretches of time, while the underlying RNN h ψ keeps evolving. This can slow down training and affect message passing. To reduce the number of parameters and improve the frequency in which they are updated, we propose a low-rank approximation of the payoff (iv) with rank K, similar to Chen et al. (2018): The approximation can be computed in one forward-pass with K(|A i | + |A j |) outputs and parame- ters φ := {φ,φ}. Note that a rank K = min{|A i |, |A j |} approximation does not restrict the output's expressiveness, while lower ranks share parameters and updates to speed up learning. Generalization (or zero-shot transfer) of the learned functions onto new CGs in (iv) poses some practical design challenges. To be applicable to different graphs/topologies, DCG must be invariant to reshuffling of agent indices. This requires the payoff matrix f ij , of dimensionality |A i | × |A j |, to be the same as (f ji ) with swapped inputs. We enforce invariance by computing the function f e φ for both combinations and use the average between the two. Note that this retains the ability to learn asymmetric payoff matrices f ij = (f ij ) . However, this paper does not evaluate (iv) and we leave the transfer of a learned DCG onto different graphs to future work. The DCG Q-value function is: Under review as a conference paper at ICLR 2020 Moreover, some tasks allow access to privileged information like the global state s t ∈ S during training (but not execution). We therefore propose in (v) to use this information in a privileged bias function v ϕ : S → IR with parameters ϕ, that is, q DCG-V θφψϕ (s t , τ t , a) := q DCG θφψ (τ t , a) + v ϕ (s t ). We train DCG end-to-end with the DQN loss in (2) and Double Q-learning (van Hasselt et al., 2016). Given the tensors f V ∈ IR |V|×A and f E ∈ IR |E|×A×A , A := | ∪ i A i |, where all unavailable actions are set to −∞, the Q-value can be maximized by message passing as defined in (4) and (5). The detailed procedure of computing the tensors (Algorithm 1), the Q-value (Algorithm 2) and greedy action selection (Algorithm 3) is given in the appendix. Note that no gradients flow through the message passing loop, as DQN maximizes only the bootstrapped future value. The key benefit of DCG lies in its ability to prevent relative overgeneralization during the explo- ration of agents: take the example of two hunters who have cornered their prey. The prey is danger- ous and attempting to catch it alone can lead to serious injuries. From the perspective of each hunter, the expected reward for an attack depends on the actions of the other agent, who will initially behave randomly. If the punishment for attacking alone outweighs the reward for catching the prey, agents that cannot represent the value for joint actions (QMIX, VDN, IQL) cannot learn the optimal policy. However, estimating a value function over the joint action space (as in QTRAN) can be equally pro- hibitive, as it requires many more samples for the same prediction quality. DCG provides a flexible function class between these extremes that can be tailored to the task at hand. In this section we compare the performance of DCG with various topologies (see  Table 1 ) to the state-of-the-art algorithms QTRAN (Son et al., 2019), QMIX (Rashid et al., 2018), VDN (Sunehag et al., 2018) and IQL (Tan, 1993). We also evaluate a CG baseline similar to Castellini et al. (2019), which conditions on a shared RNN which summarizes all agents' histories. All algo- rithms are implemented in the multi-agent framework PYMARL (Samvelyan et al., 2019). We evaluate these methods in two complex grid-world tasks: the first formulates the relative over- generalization problem as a family of predator-prey tasks and the second investigates how artificial decentralization can hurt tasks that demand non-local coordination between agents. In the latter case, decentralized value functions (QMIX, VDN, IQL) cannot learn coordinated action selection between agents that cannot see each other directly and thus converge to a sub-optimal policy. We also evaluate DCG and DCG-V in StarCraft 2 micromanagement tasks from the StarCraft Multi- Agent Challenge (SMAC, Samvelyan et al., 2019).

Section Title: RELATIVE OVERGENERALIZATION
  RELATIVE OVERGENERALIZATION To model the challenge of relative overgeneralization, we consider a partially observable grid-world predator-prey task: 8 agents have to hunt 8 prey in a 10 × 10 grid. Each agent can either move in one of the 4 compass directions, remain still, or try to catch any adjacent prey. Impossible actions, that is, moves into an occupied target position or catching when there is no adjacent prey, are treated as unavailable. The prey moves by randomly selecting one available movement or remains motionless if all surrounding positions are occupied. If two adjacent agents execute the catch action, a prey is caught and both the prey and the catching agents are removed from the grid. An agent's observation is a 5 × 5 sub-grid centered around it, with one channel showing agents and another indicating prey. Removed agents and prey are no longer visible and removed agents receive a special observation of all zeros. An episode ends if all agents have been removed or after 200 time steps. Capturing a prey is rewarded r = 10, but unsuccessful attempts by single agents are punished by a negative reward p. The task is similar to one proposed by Son et al. (2019), but significantly more complex, both in terms of the optimal policy and in the number of agents. To demonstrate the effect of relative overgeneralization,  Figure 2  shows the average return of greedy test episodes for varying punishment p as mean and standard error over 8 independent runs. Without punishment (p = 0 in Figure 2a), fully connected DCG (DCG, solid) performs as well as DCG without edges (VDN, dashed). However, for stronger punishment VDN becomes more and more unreliable, which is visible in the large standard errors in Figures 2b and 2c, until it fails completely for p ≤ −1.5 in Figure 2d. This is due to relative overgeneralization, as VDN cannot represent the values of joint actions during exploration. DCG, on the other hand, learns only slightly slower with punishment and converges otherwise reliably to the optimal solution (dotted line). Figure 3a shows how well DCG performs in comparison with the baseline algorithms in Appendix A.1 for a strong punishment of p = −2. Note that QMIX, IQL and VDN completely fail to learn the task (return 0) due to their restrictive value factorization. While CG could in principle learn the same policy as DCG, the lack of parameter sharing appears to slow down learning dramatically here, which yields no reward in the first million transitions. QTRAN estimates the values with a centralized function, which conditions on all agents' actions, and can therefore learn the task. However, QTRAN requires much more samples than DCG before a useful policy can be learned, due to the size of the joint action space. This is in line with the findings of Son et al. (2019), which required much more samples to learn a task with four agents than with two and also show the characteristic dip in performance with more agents. In comparison with both QTRAN and CG, fully connected DCG (DCG) learn near-optimal policies remarkably fast and reliable. We also investigated the performance of various DCG topologies defined in  Table 1 . Figure 3b shows that in particular the reliability of the achieved test episode return depends strongly on the graph topology. While all seeds of fully connected DCG succeed (DCG), DCG with CYCLE, LINE and STAR topologies have varying means while exhibiting large standard errors. The high devia- tions are caused by some runs finding near-optimal policies, while others fail completely (return 0). One possible explanation is that for the failed seeds the rewarded experiences, observed in the initial exploration, are only amongst agents that do not share a payoff function. Due to the relative over- generalization pathology, the learned greedy policy no longer explores 'catch' actions and existing payoff functions cannot experience the reward for coordinated actions anymore. It is therefore not surprising that fully connected graphs perform best, as they represent the largest function class and require the fewest assumptions. The topology had also little influence on the runtime of DCG, due to efficient batching on the GPU. The tested fully connected DCG only considers pairwise edges. Hyper-edges between more than two agents (Figure 1c) would yield even richer value representa- tions, but would also require more samples to sufficiently approximate the payoff functions. This effect can be seen in the much slower learning QTRAN results in Figure 3a.

Section Title: ARTIFICIAL DECENTRALIZATION
  ARTIFICIAL DECENTRALIZATION The choice of decentralized value functions is in most cased motivated by the huge joint action spaces and not because the task actually requires decentralized execution: it is an artificial decen- tralization. While this often works surprisingly well, we want to investigate how existing algorithms deal with tasks that cannot be fully decentralized. One obvious case in which decentralization must fail is when the optimal policy cannot be represented by utility functions alone. For example, decen- tralized policies behave suboptimally in tasks where the optimal policy must condition on multiple agents' observations in order to achieve the best return. Payoff functions in DCG, on the other hand, condition on pairs of agents and can thus represent a richer class of policies. Note that dependencies on more agents can be modeled as hyper-edges in the DCG (Figure 1c), but this hurts the sample efficiency as discussed above. We evaluate the advantage of a richer policy class with a variation of the above predator-prey task. Inspired by the video game PACMAN, at each turn a fair coin flip decides randomly whether all prey are turned into dangerous ghosts. To disentangle the effects of relative overgeneralization, prey can be caught by only one agent (without punishment), yielding a reward of r = 1. However, if the agent captures a ghost, the team is punished with r = −1. Ghosts are indistinguishable from normal prey, except for a special indicator that is placed in a random corner at the beginning of each episode. The indicator signals on an additional channel of the agents' observations whether the prey are currently Under review as a conference paper at ICLR 2020 ghosts. Due to the short visibility range of the agents, the indicator is only visible in one of the 9 positions closest to its corner. Figure 4a shows the performance of QTRAN, QMIX, IQL and VDN, all of which have decentralized policies, in comparison to fully connected DCG and CG. The baseline algorithms have to learn a policy that first identifies the location of the indicator and then herds prey into that corner, where the agent is finally able to catch it without risk. By contrast, DCG and CG can learn a policy where one agent finds the indicator, allowing all agents that share an edge to condition their payoffs on that agent's current observation. As a result, this policy can catch prey much more reliably, as seen in the high performance of DCG compared to all baseline algorithms. Interestingly, as CG conditions on all agents' histories, the baseline shows an advantage in the beginning but then learns much slower and reaches a significantly lower performance. We also investigate the influence of the DCG topologies of  Table 1 , shown in Figure 4b. Note that while other topologies do not reach the same performance as fully connected DCG, they still reach a policy that significantly outperforms all baseline algorithms, around the same performance as fully connected CG.

Section Title: LOW-RANK APPROXIMATION
  LOW-RANK APPROXIMATION While the above experiments already show a significant advantage of DCG with independent payoff outputs for each action pair, we observed some serious performance issues on StarCraft 2 maps with this architecture. The most likely cause is the difference in the number of actions per agent: predator-prey agents choose between |A i | = 6 actions, whereas SMAC agents on comparable maps with 8 enemies have |A i | = 13 actions. While payoff matrices with 36 outputs in predator-prey appear reasonable to learn, 169 outputs in StarCraft 2 would require significantly more samples to estimate the payoff of each joint-action properly.  Figure 5  shows the influence of low-rank payoff approximation (Equation 6 with K ∈ {1, . . . , 4}) on both predator-prey tasks from previous subsections. One can see in Figure 5a that any low-rank approximation (DCG (rank K)) significantly improves the sample efficiency over the default architecture with independent payoff for each action pair (DCG (full)). Only rank K = 1 leads to slightly lower performance, which can be seen in the inlay plot. We conclude that rank K ≥ 2 is needed to represent the true values, but rank K = 1 already suffices to overcome the relative overgeneralization pathology. The improvement in Figure 5b is less impressive, but shows that even rank K = 1 approximation (DCG (rank 1)) yields slight performance gains over DCG (full).

Section Title: STARCRAFT 2
  STARCRAFT 2 The default architecture of DCG with independent payoff for each action pair performed poorly in StarCraft 2. We therefore tested K = 1 low-rank payoff approximation DCG with (DCG-V) and without (DCG) privileged information bias function v ϕ , as described in Section 4, on six StarCraft 2 maps (from SMAC, Samvelyan et al., 2019). The learning curves for all StarCraft 2 maps are Under review as a conference paper at ICLR 2020 given in  Figure 6 . We expected DCG to only yield an advantage on maps which struggle with the relative overgeneralization pathology or some related issue. In this light it is somewhat surprising that both DCG and DCG-V outperform VDN on almost all maps. DCG-V performs at least as good as DCG and clearly outperforms it on some maps, which demonstrates that privileged information is clearly useful. As expected, a direct comparison with the state-of-the-art method QMIX depends strongly on the StarCraft 2 map. On the one hand, DCG-V clearly outperforms QMIX on MMM2 (Figure 6a), which is classified as super hard by SMAC. We also learn much faster on the easy map so many baneling (Figure 6b). On the other hand, QMIX performs better on the hard map 3s vs 5z (Figure 6d), which might be due to the low number of 3 agents. For this few agents, the added representational capacity of DCG may not improve the task as much as the non-linear state-dependent mixing of QMIX. However, it is hard to pin-point why state dependent mixing is an advantage here. We conclude from these results that that some maps (like MMM2) clearly benefit from the improved coordination and value representation of DCG, while in most others DCG-V is on par with QMIX.

Section Title: CONCLUSIONS & FUTURE WORK
  CONCLUSIONS & FUTURE WORK This paper introduced the deep coordination graph (DCG), an architecture for value factorization that is specified by a coordination graph (CG) and can be maximized by message passing. We evaluated deep Q-learning with DCG and show that the architecture enables learning of tasks where relative overgeneralization causes all decentralized baselines to fail, whereas centralized critics are much less sample efficient than DCG. We also demonstrated that artificial decentralization can lead to suboptimal behavior in all compared methods except DCG. Our method significantly improves over existing CG methods, which we demonstrate experimentally as well. Fully connected DCG performed best in all experiments and should be preferred in the absence of prior knowledge about the task. Additionally, we introduced a low-rank payoff approximation for large action spaces and a privileged bias function (DCG-V). Evaluated on StarCraft 2 micromanagement tasks, DCG-V per- forms competitive with the state-of-the-art QMIX. Although not evaluated in this paper, DCG should be able to transfer/generalize to different graphs/topologies and can also be defined for higher-order dependencies. This would in principle allow the training of DCG on dynamically generated graphs, including hyper-edges with varying degrees. We plan to investigate this in future work.
  The method can be generalized to CG with hyper-edges, that is, to payoff functions for more than 2 agents.

```
