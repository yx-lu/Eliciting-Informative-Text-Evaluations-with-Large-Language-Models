Title:
```
Under review as a conference paper at ICLR 2020 GROWING ACTION SPACES
```
Abstract:
```
In complex tasks, such as those with large combinatorial action spaces, random exploration may be too inefficient to achieve meaningful learning progress. In this work, we use a curriculum of progressively growing action spaces to acceler- ate learning. We assume the environment is out of our control, but that the agent may set an internal curriculum by initially restricting its action space. Our ap- proach uses off-policy reinforcement learning to estimate optimal value functions for multiple action spaces simultaneously and efficiently transfers data, value es- timates, and state representations from restricted action spaces to the full task. We show the efficacy of our approach in proof-of-concept control tasks and on challenging large-scale StarCraft micromanagement tasks with large, multi-agent action spaces.
```

Figures/Tables Captions:
```
Figure 1: Discretised continuous control with growing action spaces. We report the mean and standard error (over 10 random seeds) of the returns during training, with a moving average over the past 20 episodes. A 2 (slow ) is an ablation of A 2 that decays at a quarter the rate.
Figure 2: Architecture for GAS with hierarchical clustering. For clarity, only two levels of hierarchy are shown. The dark shaded regions identify the locations that are pooled over before state-value or group-action scores are computed.
Figure 3: StarCraft micromanagement with growing action spaces. We report the mean and standard error (over 5 random seeds) of the evaluation winrate during training, with a moving average over the past 500 episodes.
```

Main Content:
```

Section Title: INTRODUCTION
  INTRODUCTION The value of curricula has been well established in machine learning, reinforcement learning, and in biological systems. When a desired behaviour is sufficiently complex, or the environment too unforgiving, it can be intractable to learn the behaviour from scratch through random exploration. Instead, by "starting small" ( Elman, 1993 ), an agent can build skills, representations, and a dataset of meaningful experiences that allow it to accelerate its learning. Such curricula can drastically improve sample efficiency ( Bengio et al., 2009 ). Typically, curriculum learning uses a progression of tasks or environments. Simple tasks that provide meaningful feedback to random agents are used first, and some schedule is used to introduce more challenging tasks later during training ( Graves et al., 2017 ). However, in many contexts neither the agent nor experimenter has such unimpeded control over the environment. In this work, we instead make use of curricula that are internal to the agent, simplifying the exploration problem without changing the environment. In particular, we grow the size of the action space of reinforcement learning agents over the course of training. At the beginning of training, our agents use a severely restricted action space. This helps explo- ration by guiding the agent towards rewards and meaningful experiences, and provides low variance updates during learning. The action space is then grown progressively. Eventually, using the most unrestricted action space, the agents are able to find superior policies. Each action space is a strict superset of the more restricted ones. This paradigm requires some domain knowledge to identify a suitable hierarchy of action spaces. However, such a hierarchy is often easy to find. Continuous action spaces can be discretised with increasing resolution. Similarly, curricula for coping with the large combinatorial action spaces induced by many agents can be obtained from the prior that nearby agents are more likely to need to coordinate. For example, in routing or traffic flow problems nearby agents or nodes may wish to adopt similar local policies to alleviate global congestion. Our method will be valuable when it is possible to identify a restricted action space in which random exploration leads to significantly more meaningful experiences than random exploration in the full action space. We propose an approach that uses off-policy reinforcement learning to improve sample efficiency in this type of curriculum learning. Since data from exploration using a restricted action space is still valid in the Markov Decision Processes (MDPs) corresponding to the less restricted action spaces, we can learn value functions in the less restricted action space with 'off-action-space' data collected by exploring in the restricted action space. In our approach, we learn value functions corresponding to each level of restriction simultaneously. We can use the relationships of these value functions to Under review as a conference paper at ICLR 2020 each other to accelerate learning further, by using value estimates themselves as initialisations or as bootstrap targets for the less restricted action spaces, as well as sharing learned state representations. Empirically, we first demonstrate the efficacy of our approach in two simple control tasks, in which the resolution of discretised actions is progressively increased. We then tackle a more challenging set of problems with combinatorial action spaces, in the context of StarCraft micromanagement with large numbers of agents (50-100). Given the heuristic prior that nearby agents in a multiagent setting are likely to need to coordinate, we use hierarchical clustering to impose a restricted action space on the agents. Agents in a cluster are restricted to take the same action, but we progressively increase the number of groups that can act independently of one another over the course of training. Our method substantially improves sample efficiency on a number of tasks, outperforming learning any particular action space from scratch, a number of ablations, and an actor-critic baseline that learns a single value function for the behaviour policy, as in the work of  Czarnecki et al. (2018) . Code is available, but redacted here for anonymity.

Section Title: RELATED WORK
  RELATED WORK Curriculum learning has a long history, appearing at least as early as the work of  Selfridge et al. (1985)  in reinforcement learning, and for the training of neural networks since  Elman (1993) . In supervised learning, one typically has control of the order in which data is presented to the learning algorithm. For learning with deep neural networks,  Bengio et al. (2009)  explored the use of curricula in computer vision and natural language processing. Many approaches use handcrafted schedules for task curricula, but others ( Zaremba & Sutskever, 2014 ;  Pentina et al., 2015 ;  Graves et al., 2017 ) study diagnostics that can be used to automate the choice of task mixtures throughout training. In a self-supervised control setting,  Murali et al. (2018)  use sensitivity analysis to automatically define a curriculum over action dimensions and prioritise their search space. In some reinforcement learning settings, it may also be possible to control the environment so as to induce a curriculum. With a resettable simulator, it is possible to use a sequence of progressively more challenging initial states ( Asada et al., 1996 ;  Florensa et al., 2017 ). With a procedurally gen- erated task, it is often possible to automatically tune the difficulty of the environments ( Tamar et al., 2016 ). Similar curricula also appear often in hierarchical reinforcement learning, where skills can be learned in comparatively easy settings and then composed in more complex ways later ( Singh, 1992 ).  Taylor et al. (2007)  use more general inter-task mappings to transfer Q-values between tasks that do not share state and action spaces. In adversarial settings, one may also induce a curricu- lum through self-play ( Tesauro, 1995 ;  Sukhbaatar et al., 2017 ;  Silver et al., 2017 ). In this case, the learning agents themselves define the changing part of the environment. A less invasive manipulation of the environment involves altering the reward function. Such reward shaping allows learning policies in an easier MDP, which can then be transferred to the more difficult sparse-reward task ( Colombetti & Dorigo, 1992 ;  Ng et al., 1999 ). It is also possible to learn reward shaping on simple tasks and transfer it to harder tasks in a curriculum ( Konidaris & Barto, 2006 ). In contrast, learning with increasingly complex function approximators does not require any control of the environment. In reinforcement learning, this has often taken the form of adaptively grow- ing the resolution of the state space considered by a piecewise constant discretised approximation ( Moore, 1994 ;  Munos & Moore, 2002 ;  Whiteson et al., 2007 ).  Stanley & Miikkulainen (2004)  study continual complexification in the context of coevolution, growing the complexity of neural network architectures through the course of training. These works progressively increase the capabilities of the agent, but not with respect to its available actions. In the context of planning on-line with a model, there are a number of approaches that use progres- sive widening to consider increasing large action spaces over the course of search ( Chaslot et al., 2008 ), including in planning for continuous action spaces ( Couëtoux et al., 2011 ). However, these methods cannot directly be applied to grow the action space in the model-free setting. A recent related work tackling our domain is that of  Czarnecki et al. (2018) , who train mixtures of two policies with an actor-critic approach, learning a single value function for the current mixture of policies. The mixture contains a policy that may be harder to learn but has a higher performance ceiling, such as a policy with a larger action space as we consider in this work. The mixing coef- ficient is initialised to only support the simpler policy, and adapted via population based training Under review as a conference paper at ICLR 2020 ( Jaderberg et al., 2017 ). In contrast, we simultaneously learn a different value function for each policy, and exploit the properties of the optimal value functions to induce additional structure on our models. We further use these properties to construct a scheme for off-action-space learning which means our approach may be used in an off-policy setting. Empirically, in our settings, we find our approach to perform better and more consistently than an actor-critic algorithm modeled after  Czar- necki et al. (2018) , although we do not take on the significant additional computational requirements of population based training in any of our experiments. A number of other works address the problem of generalisation and representation for value func- tions with large discrete action spaces, without explicitly addressing the resulting exploration prob- lem ( Dulac-Arnold et al., 2015 ;  Pan et al., 2018 ). These approaches typically rely on action rep- resentations from prior knowledge. Such representations could be used in combination with our method to construct a hierarchy of action spaces with which to shape exploration.

Section Title: BACKGROUND
  BACKGROUND We formalise our problem as a MDP, specified by a tuple < S, A, P, r, γ >. The set of possible states and actions are given by S and A, P is the transition function that specifies the environment dynamics, and γ is a discount factor used to specify the discounted return R = T t=0 γ t r t for an episode of length T . We wish our agent to maximise this return in expectation by learning a policy π that maps states to actions. The state-action value function (Q-function) is given by Q π = E π [R|s, a]. The optimal Q-function Q * satisfies the Bellman optimality equation: Q-learning ( Watkins & Dayan, 1992 ) uses a sample-based approximation of the Bellman optimality operator T to iteratively improve an estimate of Q * . Q-learning is an off-policy method, meaning that samples from any policy may be used to improve the value function estimate. We use this property to engage Q-learning for off-action-space learning, as described in the next section. We also introduce some notation for restricted action spaces. In particular, for an MDP with unre- stricted action space A we define a set of N action spaces A , ∈ {0, . . . , N − 1}. Each action space is a subset of the next: A 0 ⊂ A 1 ⊂ . . . ⊂ A N −1 ⊆ A. A policy restricted to actions A is denoted π (a|s). The optimal policy in this restricted policy class is π * (a|s), and its corresponding action-value and value functions are Q * (s, a) and V * (s) = max a Q * (s, a). Additionally, we define a hierarchy of actions by identifying for every action a ∈ A , > 0 a parent action parent (a) in the space of A −1 . Since action spaces are subsets of larger action spaces, for all a ∈ A −1 , parent (a) = a, i.e., one child of each action is itself. Simple pieces of domain knowledge are often sufficient to define these hierarchies. For example, a discretised continuous action can identify its nearest neighbour in A −1 as a parent. In Section 5 we describe a possible hierarchy for multi-agent action spaces. One could also imagine using action-embeddings ( Tennenholtz & Mannor, 2019 ) to learn such a hierarchy from data.

Section Title: CURRICULUM LEARNING WITH GROWING ACTION SPACES
  CURRICULUM LEARNING WITH GROWING ACTION SPACES We build our approach to growing action spaces (GAS) on off-policy value-based reinforcement learning. Q-learning and its deep-learning adaptations have shown strong performance ( Hessel et al., 2018 ), and admit a simple framework for off-policy learning.

Section Title: OFF-ACTION-SPACE LEARNING
  OFF-ACTION-SPACE LEARNING A value function for an action space A may be updated with transitions using actions drawn from its own action space, or any more restricted action spaces, if we use an off-policy learning algorithm. The restricted transitions simply form a subset of the data required to learn the value functions of the less restricted action spaces. To exploit this, we simultaneously learn an estimated optimal value functionQ * (s, a) for each action space A , and use samples drawn from a behaviour policy based on a value function for low to directly train the higher value functions.

Section Title: Under review as a conference paper at ICLR 2020
  Under review as a conference paper at ICLR 2020 At the beginning of each episode, we sample according to some distribution. The experiences generated in that episode are used to update all of theQ * ≥ (s, a). This off-action-space learning is a type of off-policy learning that enables efficient exploration by restricting it to the low- regime. We sample at the beginning of the episode rather than at each timestep because, if the agent uses a high- action, it may enter a state that is inaccessible for a lower- policy, and we do not wish to force a low- value function to generalise to states that are only accessible at higher . Since data from a restricted action space only supports a subset of the state-action space relevant for the value functions of less restricted action spaces, we hope that a suitable function approximator still allows some generalisation to the unexplored parts of the less restricted state-action space. This is because each action space is a strict subset of the larger ones, so the agent can always in the worst case fall back to a policy using a more restricted action space. This monotonicity intuitively recommends an iterative decomposition of the value estimates, in whichQ * +1 (s, a) is estimated as a sum ofQ * (s, a) and some positive ∆ (s, a). This is not im- mediately possible due to the mismatch in the support of each function. However, we can leverage a hierarchical structure in the action spaces when present, as described in Section 3. In this case we can use:Q This is a task-specific upsampling of the lower- value function to intialise the next value function. BothQ * (s, a) and ∆ (s, a) are learned components. We could further regularise or restrict the functional form of ∆ to ensure its positivity when parent (a) = a. However, we did not find this to be valuable in our experiments, and simply initialised ∆ to be small. The property (2) also implies a modified Bellman optimality equation: The max i< are redundant in their role as conditions on the optimal value function Q * . However, the Bellman optimality equation also gives us the form of a Q-learning update, where the term in the expectation on the RHS is used as an operator that iteratively improves an estimate of Q * . When these estimates are inaccurate, the modified form of the Bellman equation may lead to different updates, allowing the solutions to higher to be bootstrapped from those at lower . We expect that policies with low are easier to learn, and that therefore the correspondingQ * is higher value and more accurate earlier in training than those at high . These high values could be picked up by the extra maximisation in the modified bootstrap, and thereby rapidly learned by the higher- value functions. Empirically however, we find that using this form for the target in our loss function performs no better than just maximising overQ * (s , a ). We discuss the choice of target and these results in more detail in Section 6.2.

Section Title: REPRESENTATION
  REPRESENTATION By sharing parameters between the function approximators of each Q , we can learn a joint state representation, which can then be iteratively decoded into estimates of Q * for each . This shared embedding can be iteratively refined by, e.g., additional network layers for eachQ * to maintain flexibility along with transfer of useful representations. This simple approach has had great success in improving the efficiency of many multi-task solutions using deep learning ( Ruder, 2017 ).

Section Title: CURRICULUM SCHEDULING
  CURRICULUM SCHEDULING We need to choose a schedule with which to increase the used by the behaviour policy over the course of training.  Czarnecki et al. (2018)  use population based training ( Jaderberg et al., 2017 ) to choose a mixing parameter on the fly. However, this comes at significant computational cost, and Under review as a conference paper at ICLR 2020 optimises greedily for immediate performance gains. We use a simple linear schedule on a mixing parameter α ∈ [0, N ]. Initially α = 0 and we always choose = 0. Later, we pick = α with probability α − α and = α with probability α − α (e.g. if α = 1.1, we choose = 1 with 90% chance and = 2 with 10% chance). This worked well empirically with little effort to tune. Many other strategies exist for tuning a curriculum automatically (such as those explored by  Graves et al. (2017) ), and could be beneficial, at the cost of additional overhead and algorithmic complexity.

Section Title: GROWING ACTION SPACES FOR MULTI-AGENT CONTROL
  GROWING ACTION SPACES FOR MULTI-AGENT CONTROL In cooperative multi-agent control, the full action space allows each of N agents to take actions from a set A agent , resulting in an exponentially large action space of size |A agent | N . Random exploration in this action space is highly unlikely to produce sensical behaviours, so growing the action space as we propose is particularly valuable in this setting. One approach would be to limit the actions available to each agent, as done in our discretised continuous control experiments (Section 6.1) and those of  Czarnecki et al. (2018) . However, the joint action space would still be exponential in N . We propose instead to use hierarchical clustering, and to assign the same action to nearby agents. At the first level of the hierarchy, we treat the whole team as a single group, and all agents are constrained to take the same action. At the next level of the hierarchy, we split the agents into k groups using an unsupervised clustering algorithm, allowing each group to act independently. At each further level, every group is split once again into k smaller groups. In practice, we simply use k-means clustering based on the agent's spatial position, but this can be easily extended to more complex hierarchies using other clustering approaches. To estimate the value function, we compute a state-value scoreV (s), and a group-action delta ∆ (s, a g , g) for each group g at each level . Then, we compute an estimated group-action value for each group, at each level, using a per-group form of (3):Q * +1 (s, a g ) =Q * (s, parent k (a g )) + ∆ (s, a g , g). We useQ * −1 (s, ·) =V (s) to initialise the iterative computation, similarly to the du- eling architecture of  Wang et al. (2015) . The estimated value of the parent action is the estimated value of the entire parent group all taking the same action as the child group. At each level we now have a set of group-action values. In effect, a multi-agent value-learning problem still remains at each level , but with a greatly re- duced number of agents at low . We could simply use independent Q-learning ( Tan, 1993 ), but instead choose to estimate the joint-action value at each level as the mean of the group-action values for the groups at that , as in the work of  Sunehag et al. (2017) . A less restrictive representation, such as that proposed by  Rashid et al. (2018) , could help, but we leave this direction to future work. A potential problem is that the clustering changes for every state, which may interfere with gener- alisation as group-actions will not have consistent semantics. We address this in two ways. First, we include the clustering as part of the state, and the cluster centroids are re-initialised from the previous timestep for t > 0 to keep the cluster semantics approximately consistent. Second, we use a functional representation that produces group-action values that are broadly agnostic to the identifier of the group. In particular, we compute a spatially resolved embedding, and pool over the locations occupied by each group. See  Figure 2  and Section 6.2 for more details.

Section Title: EXPERIMENTS
  EXPERIMENTS We investigate two classes of problems that have a natural hierarchy in the action space. First, simple control problems where a coarse action discretisation can help accelerate exploration, and fine action discretisation allows for a more optimal policy. Second, the cooperative multi-agent setting, discussed in Section 5, using large-scale StarCraft micromanagement scenarios.

Section Title: DISCRETISED CONTINUOUS CONTROL
  DISCRETISED CONTINUOUS CONTROL As a proof-of-concept, we look at two simple examples: versions of the classic Acrobot and Moun- tain Car environments with discretised action spaces. Both tasks have a sparse reward of +1 when the goal is reached, and we make the exploration problem more challenging by terminating episodes Under review as a conference paper at ICLR 2020 (a) Acrobot (b) Mountain Car with a penalty of -1 if the goal is not reached within 500 timesteps. The normalised remaining time is concatenated to the state so it remains Markovian despite the time limit. There is a further actu- ation cost of 0.05 a 2 . At A 0 , the actions apply a force of +1 and −1. At each subsequent A >0 , each action is split into two children, one that is the same as the parent action, and the other applying half the force. Thus, there are 2 actions in A . The results of our experiments are shown in  Figure 1 . Training with the lower resolutions A 0 and A 1 from scratch converges to finding the goal, but incurs significant actuation costs. Training with A 2 from scratch almost never finds the goal with -greedy exploration. We also tried decaying the at a quarter of the rate (A 2 slow ) without success. In these cases, the policy converges to the one that minimises actuation costs, never finding the goal. Training with a growing action space explores to find the goal early, and then uses this experience to transition smoothly into a solution that finds the goal but takes a slower route that minimises actuation costs while achieving the objective.

Section Title: COMBINATORIAL ACTION SPACES: STARCRAFT BATTLES
  COMBINATORIAL ACTION SPACES: STARCRAFT BATTLES

Section Title: LARGE-SCALE STARCRAFT MICROMANAGEMENT
  LARGE-SCALE STARCRAFT MICROMANAGEMENT The real-time strategy game StarCraft and its sequel StarCraft II have emerged as popular platforms for benchmarking reinforcement learning algorithms ( Synnaeve et al., 2016 ;  Vinyals et al., 2017 ). Full game-play has been tackled by e.g. ( Lee et al., 2018 ;  Vinyals et al., 2019 ), while other works focus on sub-problems such as micromanagement, the low-level control of units engaged in a battle between two armies (e.g. ( Usunier et al., 2016 )). Efforts to approach the former problem have required some subset of human demonstrations, hierarchical methods, and massive compute scale, and so we focus on the latter as a more tractable benchmark to evaluate our methods. Most previous work on RL benchmarking with StarCraft micromanagement is restricted to maxi- mally 20-30 units ( Samvelyan et al., 2019 ;  Usunier et al., 2016 ). In our experiments we focus on much larger-scale micromanagement scenarios with 50-100 units on each side of the battle. To fur- ther increase the difficulty of these micromanagement scenarios, in our setting the starting locations of the armies are randomised, and the opponent is controlled by scripted logic that holds its position until any agent-controlled unit is in range, and then focus-fires on the closest enemy. This increases the exploration challenge, as our agents need to learn to find the enemy first, while they hold a strong defensive position. The action space for each unit permits an attack-move or move action in eight cardinal directions, as well as a stop action that causes the unit to passively hold its position. In our experiments, we use k = 2 for k-means clustering and split down to at most four or eight groups. The maximum number of groups in an experiment with A is 2 . Although our approach is designed for off-policy learning, we follow the common practice of using n-step Q-learning to accelerate the propagation of values ( Hessel et al., 2018 ). Our base algorithm uses the objective of n-step Q-learning from the work of  Mnih et al. (2016) , and collects data from multiple workers into a short queue similarly to  Espeholt et al. (2018) . Full details can be found in the Appendix.

Section Title: MODEL ARCHITECTURE
  MODEL ARCHITECTURE We propose an architecture to efficiently represent the value functions of the action-space hierarchy. The overall structure is shown in  Figure 2 . We start with the state of the scenario (1). Ally units are blue and split into two groups. From the state, features are extracted from the units and map (see Appendix for full details). These features are concatenated with a one-hot representation of the unit's group (for allied agents), and are embedded with a small MLP. A 2-D grid of embeddings is constructed by adding up the unit embeddings for all units in each cell of the grid (2). The embeddings are passed through a residual CNN to produce a final embedding (3), which is copied several times and decoded as follows. First, a state-value branch computes a scalar value by taking a global mean pooling (4) and passing the result through a 2-layer MLP (6). Then, for each , a masked mean-pooling is used to produce an embedding for each group at that A by masking out the positions in the spatial embedding where there are no units of that group (5a, 5b, 5c). A single evaluation MLP for each is used to decode this embedding into a group action-score (7a, 7b, 7c). This architecture allows a shared state representation to be efficiently decoded into value-function contributions for groups of any size, at any level of restriction in the action space. We consider two approaches for combining these outputs. In our default approach, described in Section 5, each group's action-value is given by the sum of the state-value and group-action-scores for the group and its parents (8a, 8b). In 'SEP-Q', each group's action-value is simply given by the state-value added to the group-action score, i.e.,Q * (s, a g ) =V (s) + ∆ (s, a g , g). This is an ablation in which the action-value estimates for restricted action spaces do not initialise the action- value estimates of their child actions.

Section Title: RESULTS AND DISCUSSION
  RESULTS AND DISCUSSION   Figure 3  presents the results of our method, as well as a number of baselines and ablations, on a variety of micromanagement tasks. Our method is labeled Growing Action Spaces GAS( ), such that GAS(2) will grow from A 0 to A 2 . Our primary baselines are policies trained with action spaces A 0 or A 2 from scratch. GAS(2) consistently outperforms both of these variants. Policies trained from scratch on A 2 struggle with exploration, in particular in the harder scenarios where the opponent has a numbers advantage. Policies trained from scratch on A 0 learn quickly, but plateau comparatively low, due to the limited ability of a single group to position effectively. GAS(2) benefits from the efficient exploration enabled by an intialisation at A 0 , and uses the data gathered under this policy to efficiently transfer to A 2 ; enabling a higher asymptotic performance. We also compare against a Mix&Match (MM) baseline using the actor-critic approach of  Czarnecki et al. (2018) , but adapted for our new multi-agent setting and supporting a third level in the mixture Under review as a conference paper at ICLR 2020 of policies (A 0 , A 1 , A 2 ). We tuned hyperparameters for all algorithms on the easiest, fastest- training scenario (80 marines vs. 80 marines). On this scenario, MM learns faster but plateaus at the same level as GAS(2). MM underperforms on all other scenarios to varying degrees. Learning separate value functions for each A , as in our approach, appears to accelerate the transfer learning in the majority of settings. Another possible explanation is that MM may be more sensitive to hyperparameters. We do not use population based training to tune hyperparameters on the fly, which could otherwise help MM adapt to each scenario. However, GAS would presumably also benefit from population based training, at the cost of further computation and sample efficiency. The policies learned by GAS exhibit good tactics. Control of separate groups is used to position our army so as to maximise the number of attacking units by forming a wall or a concave that surrounds the enemy, and by coordinating a simultaneous assault. Figure 5 in the Appendix shows some example learned policies. In scenarios where MM fails to learn well, it typically falls into a local minimum of attacking head-on. In each scenario, we test an ablation GAS (2): ON-AC that does not use our off-action-space up- date, instead training each level of the Q-function only with data sampled at that level. This ablation performs somewhat worse on average, although the size of the impact varies in different scenar- ios. In some tasks, it is beneficial to accelerate learning for finer action spaces using data drawn from the off-action-space policy. In Appendix A.1.1, the same ablation shows significantly worse performance on the Mountain Car task and comparable performance on Acrobot. We present a number of further ablations on two scenarios. The most striking failure is of the 'SEP- Q' variant which does not compose the value function as a sum of scores in the hierarchy. It is critical to ensure that values are well-initialised as we move to less restricted action spaces. In the discretised continuous control tasks, 'SEP-Q' also underperforms, although less dramatically. The choice of target is less important: performing a max over coarser action spaces to construct the target as described in Section 4.2 does not improve learning speed as intended. One potential reason is that maximising over more potential targets increases the maximisation bias already present in Under review as a conference paper at ICLR 2020 Q-learning ( Hasselt, 2010 ). Additionally, we use an n-step objective which combines a partial on- policy return with the bootstrap target, which could reduce the relative impact of the choice of target. Finally, we experiment with a higher . Unfortunately, asymptotic performance is degraded slightly once we use A 3 or higher. One potential reason is that it decreases the average group size, pushing against the limits of the spatial resolution that may be captured by our CNN architecture. Higher increases the amount of time that there are fewer units than groups, leaving certain groups empty and rendering our masked pooling operation degenerate. We do not see a fundamental limitation that should restrict the further growth of the action space, although we note that most hierarchical approaches in the literature avoid too many levels of depth. For example,  Czarnecki et al. (2018)  only mix between two sizes of action spaces rather than the three we progress through in the majority of our GAS experiments.

Section Title: CONCLUSION
  CONCLUSION In this work, we presented an algorithm for growing action spaces with off-policy reinforcement learning to efficiently shape exploration. We learn value functions for all levels of a hierarchy of restricted action spaces simultaneously, and transfer data, value estimates, and representations from more restricted to less restricted action spaces. We also present a strategy for using this approach in cooperative multi-agent control. In discretised continuous control tasks and challenging multi- agent StarCraft micromanagement scenarios, we demonstrate empirically the effectiveness of our approach and the value of off-action-space learning. An interesting avenue for future work is to automatically identify how to restrict action spaces for efficient exploration, potentially through meta-optimisation. We also look to explore more complex and deeper hierarchies of action spaces.

```
