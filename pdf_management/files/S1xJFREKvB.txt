Title:
```
None
```
Abstract:
```
Stochastic Gradient Descent (SGD) with Nesterov's momentum is a widely used optimizer in deep learning, which is observed to have excellent generalization per- formance. In this work, we propose Amortized Nesterov's Momentum, which is a special variant of Nesterov's momentum. Compared with Nesterov's momentum, our new momentum has more robust iterates and higher efficiency. Our empir- ical results show that it achieves faster early convergence and comparable final generalization performance with little-to-no tuning. Just like Nesterov's method, the new schemes are also proved optimal in general convex setting. Our analysis sheds light on the understanding of the new variant. 1 In this work, robustness refers to the probability of an optimizer significantly deviating from its expected performance, which can be reflected by the deviations of accuracy or loss in the training process over multiple runs that start with the same initial guess.
```

Figures/Tables Captions:
```
Figure 1: ResNet34 on CIFAR-10. Initial learning rate η 0 = 0.1, momentum β = 0.9, run 5 seeds (same x 0 ). In (a) (c), we plot mean curves with shaded bands indicating ±1 standard deviation. (b) shows the standard deviation of test accuracy and its average over 90 epochs. Best viewed in color.
Figure 2: ResNet34 on CIFAR-10. For all methods, η 0 = 0.1, β = 0.9. Labels of AM1-SGD are 'AM1-SGD-{Option}'. Shaded bands (or bars) indicate ±1 standard deviation. Best viewed in color.
Figure 3 & Table 1: Detailed data of the curves in Figure 2b. Best viewed in color.
Figure 5: Convergence of LSTM and ResNet. We plot the curve of validation perplexity and test accuracy, respectively. Shaded bands indicate ±1 standard deviation. Best viewed in color.
Figure 4 & Table 2: ResNet18 with pre-activation on CIFAR-10. For all methods, η 0 = 0.1, β = 0.9, run 20 seeds. For AM1-SGD, m = 5 and its labels are formatted as 'AM1-SGD-{Option}'. Shaded bands indicate ±1 standard deviation. Best viewed in color.
Table 3: Detailed perplexity and accuracy results for Figure 5.
```

Main Content:
```

Section Title: INTRODUCTION
  INTRODUCTION In recent years, Gradient Descent (GD) ( Cauchy, 1847 ) and its variants have been widely used to solve large scale machine learning problems. Among them, Stochastic Gradient Descent (SGD) ( Robbins & Monro, 1951 ), which replaces gradient with an unbiased stochastic gradient estimator, is a popular choice of optimizer especially for neural network training which requires lower precision.  Sutskever et al. (2013)  found that using SGD with Nesterov's momentum ( Nesterov, 1983 ; 2013b), which was originally designed to accelerate deterministic convex optimization, achieves substantial speedups for training neural networks. This finding essentially turns SGD with Nesterov's momen- tum into the benchmarking method of neural network design, especially for classification tasks ( He et al., 2016b ;a;  Zagoruyko & Komodakis, 2016 ;  Huang et al., 2017 ). It is observed that in these tasks, the momentum technique plays a key role in achieving good generalization performance. Adaptive methods ( Duchi et al., 2011 ;  Kingma & Ba, 2015 ;  Tieleman & Hinton, 2012 ;  Reddi et al., 2018 ), which are also becoming increasingly popular in the deep learning community, diagonally scale the gradient to speed up training. However,  Wilson et al. (2017)  show that these methods always generalize poorly compared with SGD with momentum (both classical momentum ( Polyak, 1964 ) and Nesterov's momentum). In this work, we introduce Amortized Nesterov's Momentum, which is a special variant of Nes- terov's momentum. From users' perspective, the new momentum has only one additional integer hyper-parameter m to choose, which we call the amortization length. Learning rate and momentum parameter of this variant are strictly aligned with Nesterov's momentum and by choosing m = 1, it recovers Nesterov's momentum. This paper conducts an extensive study based on both empirical evaluation and convex analysis to identify the benefits of the new variant (or from users' angle, to set m apart from 1). We list the advantages of Amortized Nesterov's Momentum as follows: • Increasing m improves robustness 1 . This is an interesting property since the new momentum not only provides acceleration, but also enhances the robustness. We provide an understanding of this property by analyzing the relation between convergence rate and m in the convex setting. • Increasing m reduces (amortized) iteration complexity. • A suitably chosen m boosts the convergence rate in the early stage of training and produces comparable final generalization performance. Under review as a conference paper at ICLR 2020 • It is easy to tune m. The performances of the methods are stable for a wide range of m and we prove that the methods converge for any valid choice of m in the convex setting. • If m is not too large, the methods obtain the optimal convergence rate in general convex setting, just like Nesterov's method. The new variant does have some minor drawbacks: it requires one more memory buffer, which is acceptable in most cases, and it shows some undesired behaviors when working with learning rate schedulers, which can be addressed by a small modification. Considering these pros and cons, we believe that the proposed variant can benefit many large-scale deep learning tasks. Our high level idea is simple: the stochastic Nesterov's momentum can be unreliable since it is pro- vided only by the previous stochastic iterate. The iterate potentially has large variance, which may lead to a false momentum that perturbs the training process. We thus propose to use the stochastic Nesterov's momentum based on several past iterates, which provides robust acceleration. In other words, instead of immediately using an iterate to provide momentum, we put the iterate into an "amortization plan" and use it later.

Section Title: PRELIMINARIES: SGD AND NESTEROV'S MOMENTUM
  PRELIMINARIES: SGD AND NESTEROV'S MOMENTUM We start with a review of SGD and Nesterov's momentum. We discuss some subtleties in the imple- mentation and evaluation, which contributes to the interpretation of our methods.

Section Title: Notations
  Notations In this paper, we use x ∈ R d to denote the vector of model parameters. · and ·, · denote the standard Euclidean norm and inner product, respectively. Scalar multiplication for v ∈ R d and β ∈ R is denoted as β · v. f : R d → R denotes the loss function to be minimized and ∇f (x) represents the gradient of f evaluated at x. We denote the unbiased stochastic gradient estimator of ∇f (x) as ∇f i (x) with the random variable i independent of x (e.g., using mini-batch). We use x 0 ∈ R d to denote the initial guess. SGD SGD has the following simple iterative scheme, where γ ∈ R denotes the learning rate:

Section Title: Nesterov's momentum
  Nesterov's momentum The original Nesterov's accelerated gradient (with constant step) ( Nes- terov, 1983 ; 2013b) has the following scheme 2 (y ∈ R d , η, β ∈ R and y 0 = x 0 ): where we call β · (y k+1 − y k ) the momentum. By simply replacing ∇f (x k ) with ∇f i k (x k ), we obtain the SGD with Nesterov's momentum, which is widely used in deep learning. To make this point clear, recall that the reformulation in  Sutskever et al. (2013)  (scheme (2), also the Tensorflow ( Abadi et al., 2016 ) version) and the PyTorch ( Paszke et al., 2017 ) version (scheme (3)) have the following schemes Here the notations are modified based on their equivalence to scheme (1). It can be verified that schemes (2) and (3) are equivalent to (1) through v k = β −1 ·(x k −y k ) and v pt k = η −1 β −1 ·(y k −x k ), respectively (see  Defazio (2018)  for other equivalent forms of scheme (1)). Interestingly, both PyTorch and Tensorflow 3 track the values {x k }, which we refer to as M-SGD. This choice allows a consistent implementation when wrapped in a generic optimization layer ( De- fazio, 2018 ). However, the accelerated convergence rate (in the convex case) is built upon {y k } ( Nesterov, 2013b ) and {x k } may not possess such a theoretical improvement. We use OM-SGD to refer to the Original M-SGD that outputs {y k }.

Section Title: SGD and M-SGD
  SGD and M-SGD In order to study the features of momentum, in this work, we regard momentum as an add-on to plain SGD, which corresponds to fixing the learning rates 4 γ = η. From the interpretation in  Allen-Zhu & Orecchia (2017) , η represents the learning rate for the gradient descent "inside" Nesterov's method. To introduce the evaluation metrics of this paper, we report the results of training ResNet34 ( He et al., 2016b ) on CIFAR-10 ( Krizhevsky et al., 2009 ) (our basic case study) using SGD and M-SGD in  Figure 1 . In this paper, all the multiple runs start with the same initial guess x 0 . Figure 1a shows that Nesterov's momentum hurts the convergence in the first 60 epochs but accelerates the final convergence, which verifies the importance of momentum for achieving high accuracy. Figure 1b depicts the robustness of M-SGD and SGD, which suggests that adding Nesterov's momentum slightly increases the uncertainty in the training process of SGD.

Section Title: Train-batch loss vs. Full-batch loss
  Train-batch loss vs. Full-batch loss In Figure 1c, train-batch loss stands for the average of batch losses forwarded in an epoch, which is commonly used to indicate the training process in deep learning. Full-batch loss is the average loss over the entire training dataset evaluated at the end of each epoch. In terms of optimizer evaluation, full-batch loss is much more informative than train- batch loss as it reveals the robustness of an optimizer. However, full-batch loss is too expensive to evaluate and thus we only measure it on small datasets. On the other hand, test accuracy couples optimization and generalization, but since it is also evaluated at the end of the epoch, its convergence is similar to full-batch loss. Considering the basic usage of momentum in deep learning, we mainly use test accuracy to evaluate optimizers. We provide more discussion on this issue in Appendix C.2.

Section Title: M-SGD vs. OM-SGD
  M-SGD vs. OM-SGD We also include OM-SGD in Figure 1a. In comparison, the final accuracies of M-SGD and OM-SGD are 94.606% ± 0.152% and 94.728% ± 0.111% with average deviations at 1.040% and 0.634%, respectively. This difference can be explained following the interpretation in  Hinton (2012)  that {x k } are the points after "jump" and {y k } are the points after "correction".

Section Title: AMORTIZED NESTEROV'S MOMENTUM
  AMORTIZED NESTEROV'S MOMENTUM In this section, we formally introduce SGD with Amortized Nesterov's Momentum (AM1-SGD) in Algorithm 1 with the following remarks:

Section Title: Options
  Options

Section Title: Efficiency
  Efficiency We can improve the efficiency of Algorithm 1 by maintaining a running scaled mo- mentumṽ + m · (x + −x) instead of the running averagex + , by replacing the following steps in Algorithm 1: Then, in one m-iterations loop, for each of the first m − 1 iterations, AM1-SGD requires 1 vector addition and 1 scaled vector addition. At the m-th iteration, it requires 1 vector addition, 1 scalar- vector multiplication and 3 scaled vector additions. In comparison, M-SGD (standard PyTorch) requires 1 vector addition, 1 (in-place) scalar-vector multiplication and 2 scaled vector additions per iteration. Thus, as long as m > 2, AM1-SGD has lower amortized cost than M-SGD. For memory complexity, AM1-SGD requires one more auxiliary buffer than M-SGD. Tuning m We did a parameter sweep for m in our basic case study. We plot the final and the average deviation of test accuracies over 5 runs against m in Figure 2a. Note that m = 1 corresponds to the results of M-SGD and OM-SGD, which are already given in  Figure 1 . From this empirical result, m introduces a trade-off between final accuracy and robustness (the convergence behaviors can be found in Appendix A.1). Figure 2a suggests that m = 5 is a good choice for this task. For simplicity, and also as a recommended setting, we fix m = 5 for the rest of experiments in this paper.

Section Title: A momentum that increases robustness
  A momentum that increases robustness To provide a stronger justification, we ran 20 seeds with m = 5 in Figure 2b and the detailed data are given in  Figure 3  & Table 1. The results show that the amortized momentum significantly increases the robustness. Intuitively, the gap between Option I and Option II can be understood as the effect of tail averaging. However, the large gap between Option I and SGD is somewhat mysterious: what Option I does is to inject a very large momentum 6 into SGD every m iterations. It turns out that this momentum not only provides acceleration, but also helps the algorithm become more robust than SGD. This observation basically differentiates AM1-SGD from a simple interpolation in-between M-SGD and SGD.

Section Title: Learning rate scheduler issue
  Learning rate scheduler issue We observed that when we use schedulers with a large decay factor and the momentum β is too large for the task (e.g., 0.995 for the task of this section), there would be a performance drop after the learning rate reduction. We believe that it is caused by the different cardinalities of iterates being averaged inx + , which leads to a false momentum. This issue is resolved by restarting the algorithm after each learning rate reduction inspired by ( O'donoghue & Candes, 2015 ). We include more discussion and evidence in Appendix A.4.

Section Title: AM2-SGD: A VARIANT WITH IDENTICAL ITERATIONS
  AM2-SGD: A VARIANT WITH IDENTICAL ITERATIONS While enjoying an improved efficiency, AM1-SGD does not have identical iterations 7 , which to some extent limits its extensibility to other settings (e.g., asynchronous setting). In this section, we propose a variant of Amortized Nesterov's Momentum (AM2-SGD, Algorithm 2) to address this problem. To show the characteristics of AM2-SGD, we make the following remarks:

Section Title: Trading memory for extensibility
  Trading memory for extensibility In expectation, the point table φ stores the most recent m iterations and thus the outputφ K is an m-iterations tail average, which connects to AM1-SGD. The relation between AM1-SGD and AM2-SGD resembles that of SVRG ( Johnson & Zhang, 2013 ) and SAGA ( Defazio et al., 2014 ), the most popular methods in finite-sum convex optimization: to reuse the information from several past iterates, we can either maintain a "snapshot" that aggregates the information or keep the iterates in a table. A side-by-side comparison is given in Section 4.

Section Title: Options and convergence
  Options and convergence As in the case of AM1-SGD, if m = 1, AM2-SGD with Option I corresponds to M-SGD and Option II is OM-SGD. In our preliminary experiments, the convergence of AM2-SGD is similar to AM1-SGD and it also has the learning rate scheduler issue. In our preliminary experiments (can be found in Appendix A), we observed that Option I is consistently worse than Option II and it does not seem to benefit from increasing m. Thus, we do not recommend using Option I. We also set m = 5 for AM2-SGD for its evaluation due to the similarity. Additional randomness {j k } In our implementation, at each iteration, we sample an index in [m] as j k+1 and obtain the stored index j k . We observed that with Option I, AM2-SGD has much larger deviations than AM1-SGD, which we believe is caused by the additional random indexes {j k }.

Section Title: CONVERGENCE RESULTS
  CONVERGENCE RESULTS The original Nesterov's accelerated gradient is famous for its optimal convergence rates for solving convex problems. In this section, we analyze the convergence rates for AM1-SGD and AM2-SGD in the convex case, which explicitly model the effect of amortization (i.e., m). While these rates do not hold for deep learning problems in general, they help us understand the observed convergence behaviors of the proposed methods, especially on how they differ from M-SGD (m = 1). Moreover, the analysis also provides intuition on tuning m. Since the original Nesterov's method is determin- istic ( Nesterov, 1983 ; 2013b), we follow the setting of its stochastic variants ( Lan, 2012 ;  Ghadimi & Lan, 2012 ), in which Nesterov's acceleration also achieves the optimal rates. We consider the following convex composite problem ( Beck & Teboulle, 2009 ;  Nesterov, 2013a ): min x∈X F (x) f (x) + h(x) , (4) where X ⊆ R d is a non-empty closed convex set and h is a proper convex function with its proximal operator prox αh (·) 8 available. We impose the following assumptions on the regularity of f and the stochastic oracle ∇f i (identical to the ones in  Ghadimi & Lan (2012)  with µ = 0):

Section Title: Assumptions
  Assumptions The notation E i k [ · ] is E [ · | (i 0 , . . . , i k−1 )] for a random process i 0 , i 1 , . . .. These assumptions cover several important classes of convex problems. For example, (a) covers the cases of f being L-smooth (M = 0) or L 0 -Lipschitz continuous (M = 2L 0 , L = 0) convex functions and if σ = 0 in (c), the assumptions cover several classes of deterministic convex programming problems. We denote x ∈ X as a solution to problem (4) and x 0 ∈ X as the initial guess. Unlike its usage in deep learning, the momentum parameter β is always a variable in general convex analysis. For the simplicity of analysis, we reformulate AM1-SGD (Algorithm 1) and AM2-SGD (Algorithm 2) into the following schemes 10 (z ∈ X, α ∈ R): α k = η(1 − β k ) −1 . These reformulations are basically how Nesterov's momentum was migrated into deep learning ( Sutskever et al., 2013 ). Then we establish the convergence rates for AM1-SGD and AM2-SGD as follows. All the proofs in this paper are given in Appendix B.2. Theorem 1. For the reformulated AM1-SGD, suppose we choose (b) If the variance has a "light tail", i.e., E i exp ∇f i (x)−∇f (x) 2 /σ 2 ≤ exp{1}, ∀x ∈ X, and X is compact, denoting D X max x∈X x − x , for any Λ ≥ 0, we have Remarks: (a) Regarding K 0 (m), its minimum is obtained at either m = 1 or m = K. Note that for AM1-SGD, m is strictly constrained in {1, . . . , K}. It can be verified that when m = K, AM1-SGD becomes the modified mirror descent SA ( Lan, 2012 ), or under the Euclidean setting, the SGD that outputs the average of the whole history, which is rarely used in practice. In this case, the conver- gence rate in Theorem 1a becomes the corresponding O(L/K + (σ + M )/ √ K) (cf. Theorem 1 in  Lan (2012) ). Thus, we can regard AM1-SGD as a smooth transition between AC-SA and the mod- ified mirror descent SA. (b) The additional compactness and "light tail" assumptions are similarly required in  Nemirovski et al. (2009) ;  Lan (2012) ;  Ghadimi & Lan (2012) . Recently,  Juditsky et al. (2019)  established similar bounds under weaker assumptions by truncating the gradient. However, as indicated by the authors, their technique cannot be used for accelerated algorithms due to the accumulation of bias.

Section Title: Understandings
  Understandings Theorem 1a gives the expected performance in terms of full-batch loss F (x) − F (x ), from which the trade-off of m is clear: Increasing m improves the dependence on variance σ but deteriorates the O(L/K 2 ) term (i.e., the acceleration). Based on this trade-off, we can under- stand the empirical results in Figure 2b: the faster convergence in the early stage could be the result of a better control on σ and the slightly lowered final accuracy is possibly caused by the reduced acceleration effect. Theorem 1b provides the probability of the full-batch loss deviating from its expected performance (i.e., K 0 (m)). It is clear that increasing m leads to smaller deviations with the same probability, which sheds light on the understanding of the increased robustness observed in  Figure 2 . Since the theorem is built on the full-batch loss, we did an experiments based on this Under review as a conference paper at ICLR 2020 metric in Figure 4 & Table 2. Here we choose training a smaller ResNet18 with pre-activation ( He et al., 2016a ) on CIFAR-10 as the case study (the test accuracy is reported in Appendix A.5). For AM2-SGD, we only give the expected convergence results as follows. Theorem 2. For the reformulated AM2-SGD, if we choose Remark: In comparison with Theorem 1a, Theorem 2 has an additional term F (x 0 ) − F (x ) in the upper bound, which is inevitable. This difference comes from different restrictions on the choice of m. For AM2-SGD, m ≥ 1 is the only requirement. Since it is impossible to let m K to obtain an improved rate, this additional term is inevitable. As a sanity check, we can let m → ∞ to obtain a point table with almost all x 0 , and then the upper bound becomes exactly F (x 0 ) − F (x ). In some cases, there exists an optimal choice of m > 1 in Theorem 2. However, the optimal choice could be messy and thus we omit the discussion here.

Section Title: Understanding
  Understanding Comparing the rates, we see that when using the same m, AM2-SGD has slightly better dependence on σ, which is related to the observation in  Figure 5  that AM2-SGD is always slightly faster than AM1-SGD. This difference is suggesting that randomly incorporating past iter- ates beyond m iterations helps. If m = O(1), Theorems 1 and 2 establish the optimal O(L/K 2 + (σ + M )/ √ K) rate in the convex setting (see  Lan (2012)  for optimality), which verifies AM1-SGD and AM2-SGD as variants of the Nesterov's method ( Nesterov, 1983 ; 2013b). From the above analysis, the effect of m can be understood as trading acceleration for variance control. However, since both acceleration and variance control boost the convergence speed, the reduced final performance observed in the CIFAR experiments may not always be the case as will be shown in  Figure 5  and  Table 3 .

Section Title: Connections with Katyusha
  Connections with Katyusha Our original inspiration of AM1-SGD comes from the construction of Katyusha ( Allen-Zhu, 2018 ), the recent breakthrough in finite-sum convex optimization, which uses a previously calculated "snapshot" point to provide momentum, i.e., Katyusha momentum. AM1-SGD also uses an aggregated point to provide momentum and it shares many structural simi- larities with Katyusha. We refer the interested readers to Appendix B.3.

Section Title: PERFORMANCE EVALUATION
  PERFORMANCE EVALUATION In this section, we evaluate AM1-SGD and AM2-SGD on more deep learning tasks. Our goal is to show their potentials of serving as alternatives for M-SGD. Regarding the options: for AM1-SGD, Option I is a nice choice, which has slightly better final performance as shown in Table 1; for AM2- SGD, Option I is not recommended as mentioned before. Here we choose to evaluate Option II for both methods for consistency, which also corresponds to the analysis in Section 4. AM1-SGD and AM2-SGD use exactly the same values for (η, β) as M-SGD, which was tuned to optimize the performance of M-SGD. We set m = 5 for AM1-SGD and AM2-SGD. We trained ResNet50 and ResNet152 ( He et al., 2016b ) on the ILSVRC2012 dataset ("ImageNet") ( Russakovsky et al., 2015 ) shown in Figure 5b. For this task, we used 0.1 initial learning rate and 0.9 momentum for all methods, which is a typical choice. We performed a restart after each learning rate reduction as discussed in Appendix A.4. We believe that this helps the training process and also does not incur any additional overhead. We report the final accuracy in  Table 3 . We also did a language model experiment on Penn Treebank dataset ( Marcus et al., 1993 ). We used the LSTM ( Hochreiter & Schmidhuber, 1997 ) model defined in  Merity et al. (2017)  and followed the experimental setup in its released code. We only changed the learning rate and momentum in Under review as a conference paper at ICLR 2020 the setup. The baseline is SGD+ASGD 11 ( Polyak & Juditsky, 1992 ) with constant learning rate 30 as used in  Merity et al. (2017) . For the choice of (η, β), following  Lucas et al. (2019) , we chose β = 0.99 and used the scheduler that reduces the learning rate by half when the validation loss has not decreased for 15 epochs. We swept η from {5, 2.5, 1, 0.1, 0.01} and found that η = 2.5 resulted in the lowest validation perplexity for M-SGD. We thus ran AM1-SGD and AM2-SGD with this (η, β) and m = 5. Due to the small decay factor, we did not restart AM1-SGD and AM2-SGD after learning rate reductions. The validation perplexity curve is plotted in Figure 5a. We report validation perplexity and test perplexity in  Table 3 . This experiment is directly comparable with the one in  Lucas et al. (2019) . Extra results are provided in the appendices for interested readers: the robustness when using large β (Appendix A.2), a CIFAR-100 experiment (Appendix A.6) and comparison with classical momen- tum ( Polyak, 1964 ), AggMo ( Lucas et al., 2019 ) and QHM ( Ma & Yarats, 2019 ) (Appendix A.3).

Section Title: CONCLUSIONS
  CONCLUSIONS We presented Amortized Nesterov's Momentum, which is a special variant of Nesterov's momentum that utilizes several past iterates to provide the momentum. Based on this idea, we designed two different realizations, namely, AM1-SGD and AM2-SGD. Both of them are simple to implement with little-to-no additional tuning overhead over M-SGD. Our empirical results demonstrate that switching to AM1-SGD and AM2-SGD produces faster early convergence and comparable final generalization performance. AM1-SGD is lightweight and has more robust iterates than M-SGD, and thus can serve as a favorable alternative to M-SGD in large-scale deep learning tasks. AM2-SGD could be favorable for more restrictive tasks (e.g., asynchronous training) due to its extensibility and good performance. Both the methods are proved optimal in the convex case, just like M-SGD. Based on the intuition from convex analysis, the proposed methods are trading acceleration for variance control, which provides hints for the hyper-parameter tuning.
  To implement Option II, we can either maintain another identical network for the shifted pointx or tem- porarily change the network parameters in the evaluation phase.

```
