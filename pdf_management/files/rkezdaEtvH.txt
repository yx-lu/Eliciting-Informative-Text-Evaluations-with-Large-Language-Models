Title:
```
Under review as a conference paper at ICLR 2020 HYPERBOLIC DISCOUNTING AND LEARNING OVER MULTIPLE HORIZONS
```
Abstract:
```
Reinforcement learning (RL) typically defines a discount factor (γ) as part of the Markov Decision Process. The discount factor values future rewards by an exponential scheme that leads to theoretical convergence guarantees of the Bell- man equation. However, evidence from psychology, economics and neuroscience suggests that humans and animals instead have hyperbolic time-preferences ( 1 1+kt for k > 0). Here we extend earlier work of Kurth-Nelson and Redish and propose an efficient deep reinforcement learning agent that acts via hyperbolic discounting and other non-exponential discount mechanisms. We demonstrate that a simple approach approximates hyperbolic discount functions while still using familiar temporal-difference learning techniques in RL. Additionally, and independent of hyperbolic discounting, we make a surprising discovery that simultaneously learn- ing value functions over multiple time-horizons is an effective auxiliary task which often improves over state-of-the-art methods. 1 Time-preference reversals are one implication. Consider two hypothetical choices: (1) a stranger offers $1M now or $1.1M dollars tomorrow (2) a stranger instead offers $1M in 99 days versus $1.1M in 100 days.
```

Figures/Tables Captions:
```
Figure 1: Hyperbolic versus exponential discount- ing. Humans and animals often exhibit hyperbolic discounts (blue curve) which have shallower dis- count declines for large horizons. In contrast, RL agents often optimize exponential discounts (or- ange curve) which drop at a constant rate regard- less of how distant the return.
Figure 2: The Pathworld. Each state (white circle) indicates the accompanying reward r and the distance from the starting state d. From the start state, the agent makes a single action: which which path to follow to the end. Longer paths have a larger rewards at the end, but the agent incurs a higher risk on a longer path.
Figure 3: In each episode of Pathworld an unobserved hazard λ ∼ p(λ) is drawn and the agent is subject to a total risk of the reward not being realized of (1 − e −λ ) d(a) where d(a) is the path length. When the agent's hazard prior matches the true hazard distribution, the value estimate agrees well with the theoretical value. Exponentially discounted values fail to approximate the true value (Table 1).
Figure 4: Multi-horizon model predicts Q-values for n γ separate discount functions thereby modeling different effective horizons. Each Q-value is a lightweight computation, an affine transformation off a shared representation. By modeling over multiple time-horizons, we now have the option to construct policies that act according to a particular value or a weighted combination.
Figure 5: We compare the Hyper-Rainbow (in blue) agent versus the Multi-Rainbow (orange) agent on a random subset of 19 games from ALE (3 seeds each). For each game, the percentage performance improvement for each algorithm against Rainbow is recorded. There is no significant difference whether the agent acts according to hyperbolically-discounted (Hyper-Rainbow) or exponentially- discounted (Multi-Rainbow) Q-values suggesting the performance improvement in ALE emerges from the multi-horizon auxiliary task.
Figure 6: Performance improvement over Rainbow using the multi-horizon auxiliary task in Atari Learning Environment (3 seeds each).
Figure 7: Measuring the Rainbow improvements on top of the Multi-C51 baseline on a subset of 10 games in the Arcade Learning Environment (3 seeds each). On this subset, we find that the multi- horizon auxiliary task interfaces well with n-step methods (top right) but poorly with a prioritized replay buffer (bottom left).
Table 1: The average mean squared error (MSE) over each of the paths in Figure 3 showing that our approximation scheme well-approximates the true value- profile.
```

Main Content:
```

Section Title: INTRODUCTION
  INTRODUCTION The standard treatment of the reinforcement learning (RL) problem is the Markov Decision Process (MDP) which includes a discount factor 0 ≤ γ ≤ 1 that exponentially reduces the present value of future rewards (Bellman, 1957; Sutton & Barto, 1998). A reward r t received in t-time steps is devalued to γ t r t , a discounted utility model introduced by Samuelson (1937). This establishes a time- preference for rewards realized sooner rather than later. The decision to exponentially discount future rewards by γ leads to value functions that satisfy theoretical convergence properties (Bertsekas, 1995). The magnitude of γ also plays a role in stabilizing learning dynamics of RL algorithms (Prokhorov & Wunsch, 1997; Bertsekas & Tsitsiklis, 1996) and has recently been treated as a hyperparameter of the optimization (OpenAI, 2018; Xu et al., 2018). However, both the magnitude and the functional form of this discounting function establish priors over the solutions learned. The magnitude of γ chosen establishes an effective horizon for the agent of 1/(1 − γ), far beyond which rewards are neglected (Kearns & Singh, 2002). This effectively imposes a time-scale of the environment, which may not be accurate. Further, the exponential discounting of future rewards is consistent with a prior belief that there is a known constant per-time-step hazard rate (Sozou, 1998) or probability of dying of 1 − γ (Lattimore & Hutter, 2011). Additionally, discounting future values exponentially and according to a single discount factor γ does not harmonize with the measured value preferences in humans 1 and animals (Mazur, 1985; 1997; Ainslie, 1992; Green & Myerson, 2004; Maia, 2009). A wealth of empirical evidence has been amassed that humans, monkeys, rats and pigeons instead discount future returns hyperbolically, where d k (t) = 1 1+kt , for some positive k > 0 (Ainslie, 1975; 1992; Mazur, 1985; 1997; Frederick et al., 2002; Green et al., 1981; Green & Myerson, 2004). This discrepancy between the time-preferences of animals from the exponential discounted measure of value might be presumed irrational. But Sozou (1998) showed that hyperbolic time-preferences is mathematically consistent with the agent maintaining some uncertainty over the prior belief of the hazard rate in the environment. Hazard rate h(t) measures the per-time-step risk the agent incurs as it acts in the environment due to a potential early death. Precisely, if s(t) is the probability that the Under review as a conference paper at ICLR 2020 agent is alive at time t then the hazard rate is h(t) = − d dt lns(t). We consider the case where there is a fixed, but potentially unknown hazard rate h(t) = λ ≥ 0. The prior belief of the hazard rate p(λ) implies a specific discount function Sozou (1998). Under this formalism, the canonical case in RL of discounting future rewards according to d(t) = γ t is consistent with the belief that there exists a single hazard rate λ = e −γ known with certainty. Further details are available in Appendix A. Common RL environments are also character- ized by risk, but often in a narrower sense. In deterministic environments like the original Ar- cade Learning Environment (ALE) (Bellemare et al., 2013) stochasticity is often introduced through techniques like no-ops (Mnih et al., 2015) and sticky actions (Machado et al., 2018) where the action execution is noisy. Physics sim- ulators may have noise and the randomness of the policy itself induces risk. But even with these stochastic injections the risk to reward emerges in a more restricted sense. In Section 2 we show that a prior distribution reflecting the uncertainty over the hazard rate, has an as- sociated discount function in the sense that an MDP with either this hazard distribution or the discount function, has the same value function for all policies. This equivalence implies that learning policies with a discount function can be interpreted as making them robust to the as- sociated hazard distribution. Thus, discounting serves as a tool to ensure that policies deployed in the real world perform well even under risks they were not trained under. We propose an algorithm that approximates hyperbolic discounting while building on successful Q- learning (Watkins & Dayan, 1992) tools and their associated theoretical guarantees. We show learning many Q-values, each discounting exponentially with a different discount factor γ, can be aggregated to approximate hyperbolic (and other non-exponential) discount factors. We demonstrate the efficacy of our approximation scheme in our proposed Pathworld environment which is characterized both by an uncertain per-time-step risk to the agent. Conceptually, Pathworld emulates a foraging environment where an agent must balance easily realizable, small meals versus more distant, fruitful meals. We then consider higher-dimensional deep RL agents in the ALE, where we measure the benefits of hyperbolic discounting. This approximation mirrors the work of Kurth-Nelson & Redish (2009); Redish & Kurth-Nelson (2010) which empirically demonstrates that modeling a finite set of µAgents simultaneously can approximate hyperbolic discounting function. Our method then generalizes to other non-hyperbolic discount functions and uses deep neural networks to model the different Q-values from a shared representation. Surprisingly and in addition to enabling new non-exponential discounting schemes, we observe that learning a set of Q-values is beneficial as an auxiliary task (Jaderberg et al., 2016). Adding this multi-horizon auxiliary task often improves over a state-of-the-art baseline, Rainbow (Hessel et al., 2018) in the ALE (Bellemare et al., 2013). This work questions the RL paradigm of learning policies through a single discount function which exponentially discounts future rewards through the following contributions: 1. Hazardous MDPs. We formulate MDPs with hazard present and demonstrate an equivalence between undiscounted values learned under hazards and (potentially non- exponentially) discounted values without hazard. 2. Hyperbolic (and other non-exponential)-agent. A practical approach for training an agent which discounts future rewards by a hyperbolic (or other non-exponential) discount function and acts according to this. 3. Multi-horizon auxiliary task. A demonstration of multi-horizon learning over many γ simultaneously as an effective auxiliary task.

Section Title: HAZARD IN MDPS
  HAZARD IN MDPS To study MDPs with hazard distributions and general discount functions we introduce two modifica- tions. The hazardous MDP now is defined by the tuple < S, A, R, P, H, d >. In standard form, the state space S and the action space A may be discrete or continuous. The learner observes samples from the environment transition probability P (s t+1 |s t , a t ) for going from s t ∈ S to s t+1 ∈ S given a t ∈ A. We will consider the case where P is a sub-stochastic transition function, which defines an episodic MDP. The environment emits a bounded reward r : S × A → [r min , r max ] on each transition. In this work we consider non-infinite episodic MDPs. The first difference is that at the beginning of each episode, a hazard λ ∈ [0, ∞) is sampled from the hazard distribution H. This is equivalent to sampling a continuing probability γ = e −λ . During the episode, the hazard modified transition function will be P λ , in that P λ (s |s, a) = e −λ P (s |s, a). The second difference is that we now consider a general discount function d(t). This differs from the standard approach of exponential discounting in RL with γ according to d(t) = γ t , which is a special case. This setting makes a close connection to partially observable Markov Decision Process (POMDP) (Kaelbling et al., 1998) where one might consider λ as an unobserved variable. However, the classic POMDP definition contains an explicit discount function γ as part of its definition which does not appear here. A policy π : S → A is a mapping from states to actions. The state action value function Q H,d π (s, a) is the expected discounted rewards after taking action a in state s and then following policy π until termination.

Section Title: EQUIVALENCE BETWEEN HAZARD AND DISCOUNTING
  EQUIVALENCE BETWEEN HAZARD AND DISCOUNTING In the hazardous MDP setting we observe the same connections between hazard and discount functions delineated in Appendix A. This expresses an equivalence between the value function of an MDP with a discount and MDP with a hazard distribution. For example, there exists an equivalence between the exponential discount function d(t) = γ t to the undiscounted case where the agent is subject to a (1 − γ) per time-step of dying (Lattimore & Hutter, 2011). The typical Q-value (left side of Equation 2) is when the agent acts in an environment without hazard λ = 0 or H = δ(0) and discounts future rewards according to d(t) = γ t = e −λt which we denote as Q δ(0),γ t π (s, a). The alternative Q-value (right side of Equation 2) is when the agent acts under hazard rate λ = − ln γ but does not discount future rewards which we denote as We also show a similar equivalence between hyperbolic discounting and the specific hazard distribu- tion p k (λ) = 1 k exp(−λ/k), where again, λ ∈ [0, ∞) in Appendix E. For notational brevity later in the paper, we will omit the explicit hazard distribution H-superscript if the environment is not hazardous. This formulation builds upon Sozou (1998)'s relate of hazard rate and discount functions and shows that this holds for generalized Q-values in reinforcement learning.

Section Title: COMPUTING NON-EXPONENTIAL Q-VALUES
  COMPUTING NON-EXPONENTIAL Q-VALUES We now show how one can re-purpose exponentially-discounted Q-values to compute hyperbolic (and other-non-exponential) discounted Q-values. The central challenge with using non-exponential discount strategies is that most RL algorithms use some form of TD learning (Sutton, 1988). This family of algorithms exploits the Bellman equation (Bellman, 1958) which, when using exponential discounting, relates the value function at one state with the value at the following state. Q γ t π (s, a) = E π,P [R(s, a) + γQ π (s , a )] (3) where expectation E π,P denotes sampling a ∼ π(·|s), s ∼ P (·|s, a), and a ∼ π(·|s ). Being able to reuse TD methods without being constrained to exponential discounting is thus an important challenge. We propose here a scheme to deduce hyperbolic as well as other non-exponentially discounted Q-values when our discount function has a particular form. Lemma 3.1. Let Q H,γ π (s, a) be the state action value function under exponential discounting in a hazardous MDP < S, A, R, P, H, γ t > and let Q H,d π (s, a) refer to the value function in the same MDP except for new discounting < S, A, R, P, H, d >. If there exists a function w : [0, 1] → R such that d(t) = 1 0 w(γ)γ t dγ (4) which we will refer to as the exponential weighting condition, then The exchange in the above proof is valid if ∞ t=0 γ t R(s t , a t ) < ∞. The exponential weighting condition is satisfied for hyperbolic discounting and other discounting that we might want to consider (see Appendix F for examples). As an example, the hyperbolic discount can be expressed as the integral of a function f (γ, t) for γ = [0, 1) in Equation 9. This equationn tells us an integral over a function f (γ, t) = 1 k γ 1/k+t−1 = w(γ)γ t yields the desired hyperbolic discount factor Γ k (t) = 1 1+kt . This integral can be derived by Sozou's Laplace transform of the hazard rate prior H = p(λ) in Equation 18 and then applying our change of variables γ = e −λ relating RL discount factors to hazard rates. The computation of hyperbolic and other discount functions is demonstrated in detail in Appendix F. This prescription gives us a tool to produce general forms of non-exponentially discounted Q-values using our familiar exponentially discounted Q-values traditionally learned in RL (Sutton, 1988; Sutton & Barto, 1998).

Section Title: APPROXIMATING HYPERBOLIC Q-VALUES
  APPROXIMATING HYPERBOLIC Q-VALUES Section 3 describes an equivalence between hyperbolically-discounted Q-values and integrals of exponentially-discounted Q-values, however, the method required evaluating an infinite set of value functions. We therefore present a practical approach to approximate discounting Γ(t) = 1 1+kt using a finite set of functions learned via standard Q-learning (Watkins & Dayan, 1992). To avoid estimating an infinite number of Q γ π -values we introduce a free hyperparameter (n γ ) which is the total number of Q γ π -values to consider, each with their own γ. We use a practically-minded approach to choose G that emphasizes evaluating larger values of γ rather than uniformly choosing points and empirically performs well as seen in Section 5. Our approach is described in Appendix G. Each Q γi π computes the discounted sum of returns according to that specific discount factor Q γi π (s, a) = E π [ t (γ i ) t r t |s 0 = s, a 0 = a]. We previously proposed two equivalent approaches for computing hyperbolic Q-values, but for simplicity we consider the one presented in Lemma 3.1. The set of Q-values permits us to estimate the integral through a Riemann sum (Equation 11) which is described in further detail in Appendix I. Q Γ π (s, a) = 1 0 w(γ)Q γ π (s, a)dγ (11) ≈ γi∈G (γ i+1 − γ i ) w(γ i ) Q γi π (s, a) (12) where we estimate the integral through a lower bound. We consolidate this entire process in Figure 11 where we show the full process of rewriting the hyperbolic discount rate, hyperbolically-discounted Q-value, the approximation and the instantiated agent. This approach is similar to that of Kurth- Nelson & Redish (2009) where each µAgent models a specific discount factor γ. However, this differs in that our final agent computes a weighted average over each Q-value rather than a sampling operation of each agent based on a γ-distribution.

Section Title: HYPERBOLIC RESULTS
  HYPERBOLIC RESULTS

Section Title: WHEN TO DISCOUNT HYPERBOLICALLY?
  WHEN TO DISCOUNT HYPERBOLICALLY? The benefits of hyperbolic discounting will be greatest under two conditions: uncertain hazard and non-trivial intertemporal decisions. The first condition can arise under a unobserved hazard-rate variable λ drawn independently at the beginning of each episode from H = p(λ). The second condition emerges with a choice between a smaller nearby rewards versus larger distant rewards. 2 In the absence of both properties we would not expect any advantage to discounting hyperbolically. To see why, if there is a single-true hazard rate λ env , than an optimal γ * = e −λenv exists and future rewards should be discounted exponentially according to it. Further, if there is a single path through the environment with perfect alignment of short- and long-term objectives, all discounting schemes yield the same optimal policy.

Section Title: PATHWORLD EXPERIMENTS
  PATHWORLD EXPERIMENTS We note two sources for discounting rewards in the future: time delay and survival probability (Section 2). In Pathworld we train to maximize hyperbolically discounted returns ( t Γ k (t)R(s t , a t )) under no hazard (H = δ(λ − 0)) but then evaluate the undiscounted returns d(t) = 1.0 ∀ t with the paths subject to hazard H = 1 k exp(−λ/k). Through this procedure, we are able to train an agent that is robust to hazards in the environment. The agent makes one decision in Pathworld ( Figure 2 ): which of the N paths to investigate. Once a path is chosen, the agent continues until it reaches the end or until it dies. This is similar to a multi-armed bandit, with each action subject to dynamic risk. The paths vary quadratically in length with the index d(i) = i 2 but the rewards increase linearly with the path index r(i) = i. This presents Under review as a conference paper at ICLR 2020 ... a non-trivial decision for the agent. At deployment, an unobserved hazard λ ∼ H is drawn and the agent is subject to a per-time-step risk of dying of (1 − e −λ ). This environment differs from the adjusting-delay procedure presented by Mazur (1987) and then later modified by Kurth-Nelson & Redish (2009). Rather then determining time-preferences through variable-timing of rewards, we determine time-preferences through risk to the reward.  Figure 3  validates that our approach well-approximates the true hyperbolic value of each path when the hazard prior matches the true distribution. Agents that discount exponentially according to a single γ (the typical case in RL) incorrectly value the paths. We examine further the failure of exponential discounting in this hazardous setting. For this environment, the true hazard parameter in the prior was k = 0.05 (i.e. λ ∼ 20exp(−λ/0.05)). Therefore, at deployment, the agent must deal with dynamic levels of risk and faces a non-trivial decision of which path to follow. Even if we tune an agent's γ = 0.975 such that it chooses the correct arg-max path, it still fails to capture the functional form ( Figure 3 ) and it achieves a high error over all paths ( Table 1 ). If the arg-max action was not available or if the agent was proposed to evaluate non-trivial intertemporal decisions, it would act sub-optimally. In Appendix B we consider additional experiments where the agent's prior over hazard more realistically does not exactly match the environment true hazard rate and demonstrate the benefit of appropriate priors.

Section Title: ATARI 2600 EXPERIMENTS
  ATARI 2600 EXPERIMENTS With our approach validated in Pathworld, we now move to the high-dimensional environment of Atari 2600, specifically, ALE. We use the Rainbow variant from Dopamine (Castro et al., 2018) which implements three of the six considered improvements from the original paper: distributional RL, predicting n-step returns and prioritized replay buffers. The agent ( Figure 4 ) maintains a shared representation h(s) of state, but computes Q-value logits for each of the N γ i via Q (i) π (s, a) = W i h(s) + b i where W i and b i are the learnable parameters of the affine transformation for that head. A ReLU-nonlinearity is used within the body of the network (Nair & Hinton, 2010). Hyperparameter details are provided in Appendix K and when applicable, they default to the standard Dopamine values. We find strong performance improvements of the hyperbolic agent built on Rainbow (Hyper-Rainbow; blue bars) on a random subset of Atari 2600 games in  Figure 5 .

Section Title: MULTI-HORIZON AUXILIARY TASK RESULTS
  MULTI-HORIZON AUXILIARY TASK RESULTS To dissect the Hyper-Rainbow improvements, recognize that two properties from the base Rainbow agent have changed: 1. Behavior policy, µ. The agent acts according to hyperbolic Q-values computed by our approximation described in Section 4 2. Learn over multiple horizons. The agent simultaneously learns Q-values over many γ rather than a Q-value for a single γ On this subset of 19 games, Hyper-Rainbow improves upon 14 games and in some cases, by large margins. But we seek here a more complete understanding of the underlying driver of this improvement in ALE through an ablation study. The second modification can be regarded as introducing an auxiliary task (Jaderberg et al., 2016). Therefore, to attribute the performance of each properly we construct a Rainbow agent augmented with the multi-horizon auxiliary task (referred to as Multi-Rainbow and shown in orange) but have it still act according to the original policy. That is, Multi-Rainbow acts to maximize expected rewards discounted by a fixed γ action but now learns over multiple horizons as shown in  Figure 4 . We find that the Multi-Rainbow agent performs nearly as well on these games, suggesting the effectiveness of this as a stand-alone auxiliary task. This is not entirely unexpected given the rather special-case of hazard exhibited in ALE through sticky-actions (Machado et al., 2018). We examine further and investigate the performance of this auxiliary task across the full Arcade Learning Environment (Bellemare et al., 2017) using the recommended evaluation by (Machado et al., 2018). Doing so we find strong empirical benefits of the multi-horizon auxiliary task over the state-of-the-art Rainbow agent as shown in  Figure 6 .

Section Title: ANALYSIS AND ABLATION STUDIES
  ANALYSIS AND ABLATION STUDIES To understand the interplay of the multi-horizon auxiliary task with other improvements in deep RL, we test a random subset of 10 Atari 2600 games against improvements in Rainbow (Hessel et al., 2018). On this set of games we measure a consistent improvement with multi-horizon C51 (Multi-C51) in 9 out of the 10 games over the base C51 agent (Bellemare et al., 2017) in  Figure 7 .  Figure 7  indicates that the current implementation of Multi-Rainbow does not generally build successfully on the prioritized replay buffer. On the subset of ten games considered, we find that four out of ten games (Pong, Venture, Gravitar and Zaxxon) are negatively impacted despite (Hessel et al., 2018) finding it to be of considerable benefit and specifically beneficial in three out of these Under review as a conference paper at ICLR 2020 four games (Venture was not considered). The current prioritization scheme simply averaged the temporal-difference errors over all Q-values to establish priority. Alternative prioritization schemes are resulted in comparable performance indicating this is an open issue (Appendix J).

Section Title: RELATED WORK
  RELATED WORK Hyperbolic discounting in economics. Hyperbolic discounting is well-studied in the field of eco- nomics (Sozou, 1998; Dasgupta & Maskin, 2005). Dasgupta and Maskin (2005) proposes a softer interpretation than Sozou (1998) (which produces a per-time-step of death via the hazard rate) and demonstrates that uncertainty over the timing of rewards can also give rise to hyperbolic discount- ing and preference reversals, a hallmark of hyperbolic discounting. Hyperbolic discounting was initially presumed to not lend itself to TD-based solutions (Daw & Touretzky, 2000) but the field has evolved on this point. Maia (2009) proposes solution directions that find models that discount quasi-hyperbolically even though each learns with exponential discounting (Loewenstein, 1996) but reaffirms the difficulty. Finally, Alexander and Brown (2010) proposes hyperbolically discounted temporal difference (HDTD) learning by making connections to hazard. Behavior RL and hyperbolic discounting in neuroscience. TD-learning has long been used for modeling behavioral reinforcement learning (Montague et al., 1996; Schultz et al., 1997; Sutton & Barto, 1998). TD-learning computes the error as the difference between the expected value and actual value (Sutton & Barto, 1998; Daw, 2003) where the error signal emerges from unexpected rewards. However, these computations traditionally rely on exponential discounting as part of the estimate of the value which disagrees with empirical evidence in humans and animals (Strotz, 1955; Mazur, 1985; 1997; Ainslie, 1975; 1992). Hyperbolic discounting has been proposed as an alternative to exponential discounting though it has been debated as an accurate model (Kacelnik, 1997; Frederick et al., 2002). Naive modifications to TD-learning to discount hyperbolically present issues since the simple forms are inconsistent (Daw & Touretzky, 2000; Redish & Kurth-Nelson, 2010) RL models have been proposed to explain behavioral effects of humans and animals (Fu & Anderson, 2006; Under review as a conference paper at ICLR 2020 Rangel et al., 2008) but Kurth-Nelson & Redish (2009) demonstrated that distributed exponential discount factors can directly model hyperbolic discounting. This work proposes the µAgent, an agent that models the value function with a specific discount factor γ. When the distributed set of µAgent's votes on the action, this was shown to approximate hyperbolic discounting well in the adjusting-delay assay experiments (Mazur, 1987). Using the hazard formulation established in Sozou (1998), we demonstrate how to extend this to other non-hyperbolic discount functions and demonstrate the efficacy of using a deep neural network to model the different Q-values from a shared representation. Towards more flexible discounting in reinforcement learning. RL researchers have recently adopted more flexible versions beyond a fixed discount factor (Feinberg & Shwartz, 1994; Sutton, 1995; Sutton et al., 2011; White, 2017). Optimal policies are studied in Feinberg & Shwartz (1994) where two value functions with different discount factors are used. Introducing the discount factor as an argument to be queried for a set of timescales is considered in both Horde (Sutton et al., 2011) and γ-nets (Sherstan et al., 2018). Reinke et al. (2017) proposes the Average Reward Independent Gamma Ensemble framework which imitates the average return estimator. Lattimore and Hutter (2011) generalizes the original discounting model through discount functions that vary with the age of the agent, expressing time-inconsistent preferences as in hyperbolic discounting. The need to increase training stability via effective horizon was addressed in François-Lavet, Fonteneau, and Ernst (2015) who proposed dynamic strategies for the discount factor γ. Meta-learning approaches to deal with the discount factor have been proposed in Xu, van Hasselt, and Silver (2018). Finally, Pitis (2019) characterizes rational decision making in sequential processes, formalizing a process that admits a state-action dependent discount rates. Operating over multiple time scales has a long history in RL. Sutton (1995) generalizes the work of Singh (1992) and Dayan and Hinton (1993) to formalize a multi-time scale TD learning model theory. Previous work has been explored on solving MDPs with multiple reward functions and multiple discount factors though these relied on separate transition models (Feinberg & Shwartz, 1999; Dolgov & Durfee, 2005). Edwards, Littman, and Isbell (2015) considers decomposing a reward function into separate components each with its own discount factor. In our work, we continue to model the same rewards, but now model the value over different horizons. Recent work in difficult exploration games demonstrates the efficacy of two different discount factors (Burda et al., 2018) one for intrinsic rewards and one for extrinsic rewards. Finally, and concurrent with this work, Romoff et al. (2019) proposes the TD(∆)-algorithm which breaks a value function into a series of value functions with smaller discount factors.

Section Title: Auxiliary tasks in reinforcement learning
  Auxiliary tasks in reinforcement learning Finally, auxiliary tasks have been successfully employed and found to be of considerable benefit in RL. Suddarth and Kergosien (1990) used auxiliary tasks to facilitate representation learning. Building upon this, work in RL has consistently demonstrated benefits of auxiliary tasks to augment the low-information coming from the environment through extrinsic rewards (Lample & Chaplot, 2017; Mirowski et al., 2016; Jaderberg et al., 2016; Veeriah et al., 2018; Sutton et al., 2011)

Section Title: DISCUSSION AND FUTURE WORK
  DISCUSSION AND FUTURE WORK This work builds on a body of work that questions one of the basic premises of RL: one should maximize the exponentially discounted returns via a single discount factor. By learning over multiple horizons simultaneously, we have broadened the scope of our learning algorithms. Through this we have shown that we can enable acting according to new discounting schemes and that learning multiple horizons is a powerful stand-alone auxiliary task. Our method well-approximates hyperbolic discounting and performs better in hazardous MDP distributions. This may be viewed as part of an algorithmic toolkit to model alternative discount functions. However, this work still does not fully capture more general aspects of risk since the hazard rate may be a function of time. Further, hazard may not be an intrinsic property of the environment but a joint property of both the policy and the environment. If an agent purses a policy leading to dangerous state distributions then it will naturally be subject to higher hazards and vice-versa - this creates a complicated circular dependency. We would therefore expect an interplay between time-preferences and policy. This is not simple to deal with but recent work proposing state-action dependent discounting (Pitis, 2019) may provide a formalism for more general time-preference schemes.
  2 A trivial intertemporal decision is one between small distant rewards versus large close rewards. For example, the choice between $100 now versus $10 tomorrow.

```
