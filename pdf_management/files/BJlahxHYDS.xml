<article article-type="research-article"><front><article-meta><title-group><article-title>Published as a conference paper at ICLR 2020 CONSERVATIVE UNCERTAINTY ESTIMATION BY FITTING PRIOR NETWORKS</article-title></title-group><contrib-group content-type="author"><contrib contrib-type="person"><name><surname>Ciosek</surname><given-names>Kamil</given-names></name></contrib><contrib contrib-type="person"><name><surname>Fortuin</surname><given-names>Vincent</given-names></name></contrib><contrib contrib-type="person"><name><surname>Tomioka</surname><given-names>Ryota</given-names></name></contrib><contrib contrib-type="person"><name><surname>Hofmann</surname><given-names>Katja</given-names></name></contrib><contrib contrib-type="person"><name><surname>Turner</surname><given-names>Richard</given-names></name></contrib></contrib-group><abstract><p>Obtaining high-quality uncertainty estimates is essential for many applications of deep neural networks. In this paper, we theoretically justify a scheme for esti- mating uncertainties, based on sampling from a prior distribution. Crucially, the uncertainty estimates are shown to be conservative in the sense that they never un- derestimate a posterior uncertainty obtained by a hypothetical Bayesian algorithm. We also show concentration, implying that the uncertainty estimates converge to zero as we get more data. Uncertainty estimates obtained from random priors can be adapted to any deep network architecture and trained using standard su- pervised learning pipelines. We provide experimental evaluation of random priors on calibration and out-of-distribution detection on typical computer vision tasks, demonstrating that they outperform deep ensembles in practice. Contributions We provide a sound theoretical frame- work for obtaining uncertainty estimates by fitting ran- dom priors, a method previously lacking a principled justification. Specifically, we justify estimates in the uncertainty of the output of neural networks with any architecture. In particular, we show in Lemma 1 and Proposition 1 that these uncertainty estimates are con- servative, meaning they are never more certain than a Bayesian algorithm would be. Moreover, in Proposi- tion 2 we show concentration, i.e. that the uncertainties become zero with infinite data. Empirically, we evalu- ate the calibration and out-of-distribution performance of our uncertainty estimates on typical computer vision tasks, showing a practical benefit over deep ensembles and MC dropout.</p></abstract></article-meta></front><body><sec><title>INTRODUCTION</title><p>Deep learning has achieved huge success in many applications. In particular, increasingly often, it is used as a component in decision-making systems. In order to have confidence in decisions made by such systems, it is necessary to obtain good uncertainty estimates, which quantify how certain the network is about a given output. In particular, if the cost of failure is large, for example where the automated system has the capability to accidentally hurt humans, the availability and quality of uncertainty estimates can determine whether the system is safe to deploy at all (<xref ref-type="bibr" rid="b0">Carvalho, 2016</xref>; <xref ref-type="bibr" rid="b0">Leibig et al., 2017</xref>; <xref ref-type="bibr" rid="b0">Michelmore et al., 2018</xref>). Moreover, when decisions are made sequentially, good uncertainty estimates are crucial for achieving good performance quickly (<xref ref-type="bibr" rid="b0">Bellemare et al., 2016</xref>; <xref ref-type="bibr" rid="b0">Houthooft et al., 2016</xref>; <xref ref-type="bibr" rid="b0">Ostrovski et al., 2017</xref>; <xref ref-type="bibr" rid="b0">Burda et al., 2018</xref>).</p><p>Because any non-Bayesian inference process is potentially sub-optimal (<xref ref-type="bibr" rid="b0">De Finetti, 1937</xref>), these uncertainty estimates should ideally be relatable to Bayesian inference with a useful prior. Deep en- sembles (<xref ref-type="bibr" rid="b0">Lakshminarayanan et al., 2017</xref>), one of the most popular methods available for uncertainty estimation in deep networks today, struggle with this requirement. While deep ensembles can be related (<xref ref-type="bibr" rid="b0">Rubin, 1981</xref>) to Bayesian inference in settings where the individual models are trained on subsets of the data, this is not how they are used in practice. In order to improve data efficiency, all ensembles are typically trained using the same data (<xref ref-type="bibr" rid="b0">Lakshminarayanan et al., 2017</xref>), resulting in a method which does not have a theoretical justification. Moreover, deep ensembles can give overconfident uncertainty estimates in practice. On the other hand, Monte-Carlo dropout can be viewed (<xref ref-type="bibr" rid="b0">Gal &amp; Ghahramani, 2016</xref>) as a certain form of Bayesian inference. However, doing so re- quires requires either a limit to be taken or a generalization of variational inference to a quasi-KL divergence (<xref ref-type="bibr" rid="b0">Hron et al., 2018</xref>). In practice, MC dropout can give arbitrarily overconfident estimates (<xref ref-type="bibr" rid="b0">Foong et al., 2019</xref>). More broadly, a category of approaches, known as Bayesian Neural Networks (<xref ref-type="bibr" rid="b0">Blundell et al., 2015</xref>; <xref ref-type="bibr" rid="b0">Welling &amp; Teh, 2011</xref>; <xref ref-type="bibr" rid="b0">Neal, 1996</xref>), maintains a distribution over the weights of the neural network. These methods have a sound Bayesian justification, but training them is both difficult and carries an accuracy penalty, particularly for networks with convolutional architectures (<xref ref-type="bibr" rid="b0">Osawa et al., 2019</xref>). Moreover, tuning BNNs is hard and achieving a good approximation to the posterior is difficult (<xref ref-type="bibr" rid="b0">Brosse et al., 2018</xref>).</p><p>We use another way of obtaining uncertainties for deep networks, based on fitting ran- dom priors (<xref ref-type="bibr" rid="b0">Osband et al., 2018</xref>; <xref ref-type="bibr" rid="b0">2019</xref>). Random priors are easy to train and were found to work very well in practice (<xref ref-type="bibr" rid="b0">Burda et al., 2018</xref>). To obtain the uncertainty estimates, Affiliations: 1. Microsoft Research Cambridge; 2. ETH Zurich; 3. University of Cambridge. The second author was an intern at Microsoft when contributing to this work. we first train a predictor network to fit a prior. Two ex- amples of prior-predictor pairs are shown in the top two plots of <xref ref-type="fig" rid="fig_0">Figure 1</xref>.Faced with a novel input point, we obtain an uncertainty (<xref ref-type="fig" rid="fig_0">Figure 1</xref>, bottom plot) by mea- suring the error of the predictor network against this pattern. Intuitively, these errors will be small close to the training points, but large far from them. The pat- terns themselves are drawn from randomly initialized (and therefore untrained) neural networks. While this way of estimating uncertainties was known before (<xref ref-type="bibr" rid="b0">Os- band et al., 2019</xref>), it did not have a theoretical justifi- cation beyond Bayesian linear regression, which is too limiting for modern applications.</p></sec><sec><title>PRELIMINARIES</title><p>We are going to reason about uncertainty within the for- mal framework of stochastic processes. We now intro- duce the required notations.</p><p>A stochastic process is a collection of random variables {f (x)}. We consider processes where x &#8712; R K and the random-variable f (x) takes values in R M . A stochastic process has exchangeable outputs if the dis- tribution does not change when permuting the M entries in the output vector. Allowing a slight abuse of notation, we denote the finite-dimensional distribution of the process {f (x)} for the set X = {x i } i=1,...,N as f (x 1 , . . . , x N ) = f (X). In practice, the finite-dimensional distribution reflects the idea of restricting the process to points x 1 , . . . , x N and marginalizing over all the other points. Infer- ence can be performed on stochastic processes similarly to probability distributions. In particular, we can start with some prior process {f (x)}, observe a set of N training points X = {x i } i=1,...,N and labels y = {y i } i=1,...,N and then consider the posterior process {f Xy (x)}, whose finite-dimensional distributions are given by f Xy (x 1 . . . x N ) = f (x 1 . . . x N |x 1 , . . . , x N , y 1 , . . . , y N ) for any set of testing points x 1 . . . x N . We use subscripts to denote conditioning on the dataset throughout the paper. We denote the variance of f Xy (x ) with &#963; 2 Xf (x ). A stochastic process is called Gaussian if if all its finite-dimensional distributions are Gaussian. Given a test point x , we denote the posterior GP mean with &#181; Xy (x ) and posterior GP variance with &#963; 2 X (x ). We provide more background on GPs in Appendix D.</p></sec><sec><title>ESTIMATING UNCERTAINTY FROM RANDOM PRIORS</title><p>Intuition Uncertainties obtained from random priors have an appealing intuitive justification. Consider the networks in the top part of <xref ref-type="fig" rid="fig_0">Figure 1</xref>. We start with a randomly initialized prior network, shown in red. Whenever we see a datapoint, we train the predictor network (green) to match this Published as a conference paper at ICLR 2020 prior. Uncertainties can then be obtained by considering the squared error between the prior and the predictor at a given point. An example uncertainty estimate is shown as the shaded blue area in the bottom of <xref ref-type="fig" rid="fig_0">Figure 1</xref>. While it may at first seem that the squared error is a poor measure of uncertainty because it can become very small by random chance, we formally show in Section 4.1 that this is very improbable. In Section 4.2, we show that this error goes down to zero as we observe more data. Similarly to GP inference, uncertainty estimation in our framework does not depend on the regression label. The prediction mean (blue curve in the bottom part of <xref ref-type="fig" rid="fig_0">Figure 1</xref>) is obtained by fitting a completely separate neural network. In section 6, we discuss how this framework avoids the overconfidence characteristic of deep ensembles (<xref ref-type="bibr" rid="b0">Lakshminarayanan et al., 2017</xref>).</p></sec><sec><title>Prior</title><p>The process of obtaining network uncertainties involves randomly initialized prior networks, which are never trained. While this may at first appear very different from they way deep learning is normally done, these random networks are a crucial component of our method. We show in Section 4.1 that the random process that corresponds to initializing these networks can be interpreted as a prior of a Bayesian inference procedure. A prior conveys the information about how the individual data points are related. The fact that we are using random networks has both practical and theo- retical benefits. Practically, since the prior does not depend on the data, there is no way that it can overfit. The use of random priors also has strong empirical support - randomly initialized networks have been recently used as priors to obtain state-of-the-art performance on computer vision tasks (<xref ref-type="bibr" rid="b0">Ulyanov et al., 2018</xref>; <xref ref-type="bibr" rid="b0">Cheng et al., 2019</xref>). Theoretically, using random priors satisfies the likelihood principle (<xref ref-type="bibr" rid="b0">Robert, 2007</xref>). Moreover, random priors can be viewed as a safe choice since they make the minimum reasonable assumption that the network architecture is appropriate for the task. In fact, whenever deep learning is used, with or without uncertainty estimates, practitioners are already implicitly making that assumption.</p></sec><sec><title>Algorithm 1 Training the predictors.</title></sec><sec><title>Algorithm</title><p>The process of training the predictor networks is shown in Algorithm 1. The function TRAIN-UNCERTAINTIES first generates random priors, i.e. neural networks with random weights. In our notation, it corresponds to sampling functions from the prior process {f (x)}. These priors, eval- uated at points from the dataset X = {x i } i=1,...,N are then used as labels for supervised learning, per- formed by the function FIT. After training, when we want to obtain an uncertainty estimate &#966; at a given test point x , we use the formul&#226;</p><p>Here, the quantity&#963; 2 &#181; is the sample mean of the squared error. We will show in Section 4 that it is an unbiased estimator of a variable that models the uncertainty. On the other hand,v &#963; is the sample- based estimate of the standard deviation of squared error across bootstraps, needed to quantify our uncertainty about what the uncertainty is. The hyper-parameter &#946; controls the degree to which this uncertainty is taken into account. Formally, the quantities are defined a&#349;</p><p>In the above equations, B is the number of prior functions and each prior and predictor network has M outputs. Because the predictors are trained independently, uncertainty estimates obtained from each of the B predictor-prior pairs are independent. We defer the discussion of details of network architecture to Section 5. Our experiments (Section 7) show that it is often sufficient to use B = 1 in practice.</p></sec><sec><title>THEORETICAL RESULTS</title><p>In Section 3, we introduced a process for obtaining uncertainties in deep learning. We now seek to provide a formal justification. We define the expected uncertainties as</p><p>In other words,&#963; 2 &#181; is the expected version of the sample-based uncertainties&#963; 2 &#181; (x ) introduced in equation 2. Since Bayesian inference is known to be optimal (<xref ref-type="bibr" rid="b0">De Finetti, 1937</xref>; <xref ref-type="bibr" rid="b0">Jaynes, 2003</xref>; <xref ref-type="bibr" rid="b0">Robert, 2007</xref>), the most appealing way of justifying uncertainty estimates&#963; 2 &#181; and&#963; 2 &#181; is to relate them to a Bayesian posterior &#963; 2 Xf (x ). We do this in two stages. First, in Section 4.1, we prove that the obtained uncertainties are larger than ones arrived at by Bayesian inference. This means that our uncertainties are conservative, ensuring that our algorithm is never more certain than it should be. Next, in Section 4.2, we show that uncertainties concentrate, i.e., they become small as we get more and more data. These two properties are sufficient to justify the use of our uncertainties in many applications.</p></sec><sec><title>UNCERTAINTIES FROM RANDOM PRIORS ARE CONSERVATIVE</title><p>From the point of view of safety, it is preferable to overestimate the ground truth uncertainty than to underestimate it. We now show that this property holds for uncertainties obtained from random priors. We first justify conservatism for the expected uncertainty&#963; 2 &#181; defined in equation 4 and then for the sampled uncertainty&#963; 2 &#181; defined in equation 2.</p></sec><sec><title>Amortized Conservatism</title><p>We first consider a weak form of this conservatism, which we call amortized. It guarantees that&#963; 2 &#181; is never smaller than the average posterior uncertainty across labels sampled from the prior. Formally, amortized conservatism holds if for any test point x we hav&#7869;</p><p>Here &#963; 2 Xf corresponds to the second moment of the posterior process {f Xf (x)}. We will introduce a stronger version of conservatism, which does not have an expectation on the right-hand side, later in this section (eq. 8). For now, we concentrate on amortized conservatism. In Lemma 1 (proof in appendix), we show that it holds under very general conditions.</p><p>Lemma 1. For any function h : R N &#215;(K+1) &#8594; R M , for any test point x &#8712; R K and for any stochastic process {f (x)} x&#8712;R K with all second moments finite and exchangeable outputs</p><p>Relation to a GP Lemma 1 holds for any prior process {f (x)}. However, the prior process used by Algorithm 1 is not completely arbitrary. The fact that prior samples are obtained by initializing neural networks with independently sampled weights gives us additional structure. In fact, it can be shown that randomly initialized neural networks become close to GPs as the width of the layers increases. While the original result due to <xref ref-type="bibr" rid="b0">Neal (1996)</xref> held for a simple network with one hidden layer, it has been extended to a wide class of popular architectures, including to CNNs and RNNs of arbitrary depth (<xref ref-type="bibr" rid="b0">Matthews et al., 2018</xref>; <xref ref-type="bibr" rid="b0">Lee et al., 2018</xref>; <xref ref-type="bibr" rid="b0">Novak et al., 2019</xref>; <xref ref-type="bibr" rid="b0">Williams, 1997</xref>; <xref ref-type="bibr" rid="b0">Le Roux &amp; Bengio, 2007</xref>; <xref ref-type="bibr" rid="b0">Hazan &amp; Jaakkola, 2015</xref>; <xref ref-type="bibr" rid="b0">Daniely et al., 2016</xref>; <xref ref-type="bibr" rid="b0">Garriga-Alonso et al., 2019</xref>). Recently, it has been shown to hold for a broad class of functions trainable by gradient descent (<xref ref-type="bibr" rid="b0">Yang, 2019</xref>). While the precise statement of these results involves technicalities which fall beyond the scope of this paper, we recall the key insight. For a family of neural networks {f W (x)}, where the weights are sampled independently and W is the width of the hidden layers, there exists a limiting kernel function k &#8734; such that</p><p>In other words, as the size of the hidden layers increases, the stochastic process obtained by initializ- ing networks randomly converges in distribution to a GP. In the context of our uncertainty estimates, this makes it reasonable for W large enough to consider the prior to be a GP. We stress that the GP assumption has to hold only for the prior network, which is never trained. We do not make any assumptions about connections between the predictor training process and GPs. Strict Conservatism Denoting the posterior GP variance with &#963; 2 X (x ), we define uncertainty es- timates to be strictly conservative when&#963;</p><p>This statement is stronger than the amortized conservatism in equation 5. Intuitively, equation 8 can be interpreted as saying that our uncertainty estimates are never too small. This confirms the intuition expressed by <xref ref-type="bibr" rid="b0">Burda et al. (2018)</xref> that random priors do not overfit. Below, in Proposition 1, we outline how to guarantee strict conservatism formally. It is proved in Appendix F.1. Proposition 1 (Strict Conservatism in Expectation). Assume that f is a GP. Then for any function h : R N &#215;K &#8594; R M , we hav&#7869;</p><p>Moreover, equality holds if and only if h Xf (x ) = &#181; Xf (x ).</p><p>Conservatism with Finite Bootstraps Lemma 1 above shows conservatism for expected uncer- tainties, i.e.&#963; 2 &#181; introduced in equation 5. However, in practice we have to estimate this expectation using a finite number of bootstraps, and use the sampled uncertainties&#963; 2 &#181; defined in equation 2. We now state a conservatism guarantee that holds even in the case of just one bootstrap (B = 1). The proof is deferred to Appendix F.1.</p><p>Corollary 1 (Strict Conservatism for Finite Bootstraps). Assume that f is a GP. Assume that the random variable&#963; 2 &#181; (x ) has finite variance upper bounded by v UB . Then with probability 1 &#8722; &#948;, for any function h : R N &#215;K &#8594; R M , we hav&#234;</p><p>However, applying Corollary 1 requires the knowledge of v UB . We now provide an upper bound.</p><p>Lemma 2. Assume that the GP {f (x)} is zero mean with exchangeable outputs and the function h Xf takes values in [&#8722;U, U ] M . Assume that permuting the outputs of f produces the same permu- tation in the outputs of h Xf . With probability 1 &#8722; &#948;, we have Var f1,...,f B &#963; 2 &#181; (x ) &#8804; v UB , (11) where v UB is expressible in terms of observable quantities.</p><p>The proof and the explicit formula for v UB is deferred to Appendix F.1. In cases where conser- vatism is desired, but not absolutely essential, we can avoid the torturous calculation of Lemma 2 and replace v UB with the sample-based estimatev &#963; (x ), defined in equation 2. In this case, the con- servatism guarantee is only approximate. This is how we obtained equation 1, used by the algorithm in practice.</p></sec><sec><title>UNCERTAINTIES FROM RANDOM PRIORS CONCENTRATE</title><p>While the conservatism property in Proposition 1 is appealing, it is not sufficient on its own for the uncertainty estimates to be useful. We also need concentration, i.e. a guarantee that the uncertaintie&#349; &#963; 2 become small with more data. We can gurantee this formally by assuming that the class of neural networks being fitted is Lipschitz-continuous and bounded. Intuitively, by assumption of Lipschitz continuity, the predictors h Xf cannot behave very differently on points from the training and test sets, since both come from the same data distribution. We can then show concentration by using standard Rademacher tools to obtain a bound on the expected uncertainty in terms of the squared error on the training set. This process is formalized in Proposition 2. The proof and the technical conditions are given in Appendix F. Proposition 2 assumes that the training error is zero for arbitrarily large training sets, which might at first seem unrealistic. We argue that this assumption is in fact reasonable. The architecture of our predictor networks (<xref ref-type="fig" rid="fig_1">Figure 2</xref>, right diagram) is a superset of the prior architecture (<xref ref-type="fig" rid="fig_1">Figure 2</xref>, left diagram), guaranteeing the existence of weight settings for the predictor that make the training loss zero. Recent results on deep learning optimization (<xref ref-type="bibr" rid="b0">Du et al., 2019</xref>; <xref ref-type="bibr" rid="b0">Allen-Zhu et al., 2019</xref>) have shown that stochastic gradient descent can in general be expected to find representable functions.</p></sec><sec><title>PRACTICAL CONCLUSIONS FROM THE THEORY</title><p>We now re-visit the algorithm we defined in Section 3, with the aim of using the theory above to obtain practical improvements in the quality of the uncertainty estimates.</p></sec><sec><title>Architecture and Choosing the Number of Bootstraps</title><p>Our conservatism guarantee in Proposi- tion 1 holds for any architecture for the predictor h Xf . In theory, the predictor could be completely arbitrary and does not even have to be a deep network. In particular, there is no formal requirement for the predictor architecture to be the same as the prior. On the other hand, to show concentration in Proposition 2, we had to ensure that the prior networks are representable by the predictor. In practice, we use the architecture shown in <xref ref-type="fig" rid="fig_1">Figure 2</xref>, where the predictor mirrors the prior, but has additional layers, giving it more representational power. Moreover, the architecture requires choos- ing the number of bootstraps B. Our experiments in Section 7 show that even using B = 1, i.e. one bootstrap, produces uncertainty estimates of high quality in practice.</p><p>Modeling Epistemic and Aleatoric Uncertainty Proposition 1 and Proposition 2 hold for any Gaussian Process prior. By choosing the process appropriately, we can model both epistemic and aleatoric uncertainty. Denote by {n(x)} a stochastic process obtained by randomly initial- izing neural networks and denote by { (x)&#963; 2 A } the noise term, modeling the aleatoric (observa- tion) noise, where samples are obtained from (x) &#8764; N (0, 1) at each x independently (see Ap- pendix D for more background on aleatoric noise). We can now choose the prior process as a sum {f (x)} = {n(x) + (x)&#963; 2 A } of epistemic component {n(x)} and the noise term. The amount of aleatoric uncertainty can be adjusted by choosing &#963; 2 A .</p></sec><sec><title>Prior Choice, Weight Copying and Conservatism</title><p>One question that can be asked about our architecture (<xref ref-type="fig" rid="fig_1">Figure 2</xref>) is whether it is possible for the predictor to exactly copy the prior weights, giving zero uncertainty everywhere. A useful edge case to consider here is when we are solving a one-dimensional regression problem, &#963; 2 A = 0 and the both the priors and predictors are linear functions. In this case, after training on two points, the predictors will agree with the priors every- where and uncertainty estimates will be zero. However, this is still consistent with our conservatism guarantee The reason for this is once we assume such a linear prior, we are comparing to a GP with a linear kernel. But a GP with that kernel will also have zero uncertainty after seeing two samples.</p></sec><sec><title>Published as a conference paper at ICLR 2020</title><p>In practice, this means that we have to choose the architecture of the prior networks be expressive enough, which is no different from choosing a reasonable prior for Bayesian inference. Empirically, the tested network architecture did not show weight copying.</p></sec><sec><title>PRIOR WORK</title></sec><sec><title>Randomized Prior Functions (RPFs)</title><p>Our work was inspired by, and builds on, Randomised Prior Functions (<xref ref-type="bibr" rid="b0">Osband et al., 2019</xref>; 2018), but it is different in two important respects. First, the existing theoretical justification for RPFs only holds for Bayesian linear regression (<xref ref-type="bibr" rid="b0">Osband et al., 2018</xref>, equation 3) with non-zero noise 1 added to the priors. In contrast, our results are much more general and hold for any deep network with or without added aleatoric noise. Second, we are target- ing a different setting. While RPFs were designed as a way of sampling functions from the posterior, we provide estimates of posterior uncertainty at a given test point. Our algorithm is based on the work by <xref ref-type="bibr" rid="b0">Burda et al. (2018)</xref>, who applied RPFs to exploration in MDPs, obtaining state-of-the art results, but without justifying their uncertainty estimates formally. Our paper provides this miss- ing justification, while also introducing a way of quantifying the error in estimating the uncertainty itself. Moreover, since <xref ref-type="bibr" rid="b0">Burda et al. (2018)</xref> focused on the application of RPFs to Reinforcement Learning, they only performed out-of-distribution evaluation on the relatively easy MNIST dataset (<xref ref-type="bibr" rid="b0">LeCun, 1998</xref>). In contrast, in Section 7 we evaluate the uncertainties on more complex vision tasks. The term prior networks has also been used (<xref ref-type="bibr" rid="b0">Malinin &amp; Gales, 2018</xref>) to denote deep networks that output the parameters of a prior distribution, an approach fundamentally different from our work.</p></sec><sec><title>Deep Ensembles</title><p>The main alternative approach for obtaining uncertainties in deep learning are deep ensembles (<xref ref-type="bibr" rid="b0">Lakshminarayanan et al., 2017</xref>). Building on the bootstrap (<xref ref-type="bibr" rid="b0">Efron &amp; Tibshirani, 1994</xref>), deep ensembles maintain several models and quantify epistemic uncertainty by measuring how their outputs vary. Crucially, deep ensembles use representations trained on regression labels, and tend to learn similar representations for different inputs with similar labels, which can lead to over-fitting the uncertainty estimates. A useful edge case to consider is if the each of the models in the ensemble is convex in the weights. In this case, models in a deep ensemble will all converge to the same weights and produce zero uncertainty. While deep learning models used in practice aren't normally convex, we show empirically in section 7 that deep ensembles can give overconfident un- certainty estimates in practical vision tasks, particularly on points that have the same label as points in the training set. Since our method avoids overconfidence, it can be understood as complementary to deep ensembles, to be used in situations where obtaining conservative estimates is more important than the representational benefit of using labels. In practice, deep ensembles also require using more bootstraps to achieve the same OOD performance. Moreover, they do not have theoretical support in the case when all the members of the ensemble are trained on the same data, which is how they are used in practice (<xref ref-type="bibr" rid="b0">Lakshminarayanan et al., 2017</xref>).</p></sec><sec><title>Dropout</title><p>In cases where it is not economical to train more than one network, uncertainties can be obtained with dropout (<xref ref-type="bibr" rid="b0">Srivastava et al., 2014</xref>; <xref ref-type="bibr" rid="b0">Gal &amp; Ghahramani, 2016</xref>). Monte-Carlo dropout can be viewed (<xref ref-type="bibr" rid="b0">Gal &amp; Ghahramani, 2016</xref>) as a form of approximate Bayesian inference. However, to do so requires a rather unnatural approximating family from the perspective of approximate inference. Also, one has then either to take a limit or generalize variational inference to a quasi-KL (<xref ref-type="bibr" rid="b0">Hron et al., 2018</xref>) divergence. In addition, dropout can be interpreted in terms of MAP inference (<xref ref-type="bibr" rid="b0">Nalis- nick et al., 2019</xref>). Another alternative view of MC dropout is as an ensemble method in which the ensemble members have shared parameters (which means they are trained together) and where the ensembling is applied at test time too. This latter view is arguably as natural as the Bayesian inter- pretation. For this reason we discuss MC dropout separately from BNNs. Since dropout implicitly approximates non-Gaussian weight distribution with Gaussians, it exhibits spurious patterns in the obtained uncertainties, which can lead to arbitrarily overconfident estimates (<xref ref-type="bibr" rid="b0">Foong et al., 2019</xref>). In contrast, due to the conservatism property, random priors avoid such overconfidence.</p><p>Bayesian Neural Networks (BNNs) Bayesian Neural Networks (<xref ref-type="bibr" rid="b0">Blundell et al., 2015</xref>; <xref ref-type="bibr" rid="b0">Kingma &amp; Welling, 2014</xref>; <xref ref-type="bibr" rid="b0">Rezende et al., 2014</xref>; <xref ref-type="bibr" rid="b0">Welling &amp; Teh, 2011</xref>; <xref ref-type="bibr" rid="b0">Brosse et al., 2018</xref>) explicitly model the Published as a conference paper at ICLR 2020 distribution over weights of a neural network. While BNNs provide a link between deep learning and Bayesian inference, they are very slow to train. Even recent tuned implementations of BNNs (<xref ref-type="bibr" rid="b0">Osawa et al., 2019</xref>) are several times slower than supervised learning. This happens despite using a battery of technical optimizations, including distributed training and batch normalization. Moreover, modern convolutional BNNs still carry a significant accuracy penalty when deployed with realistic settings of prior variance.</p></sec><sec><title>EXPERIMENTS</title><p>Encouraged by the huge empirical success of random priors in Reinforcement Learning (<xref ref-type="bibr" rid="b0">Burda et al., 2018</xref>), we wanted to provide an evaluation in a more typical supervised learning setting. We tested the uncertainties in two ways. First, we investigated calibration, i.e. whether we can expect a higher accuracy for more confident estimates. Next, we checked whether the uncertainties can be used for out-of-distribution detection. We compared to two competing approaches for uncertainty detection: deep ensembles (<xref ref-type="bibr" rid="b0">Lakshminarayanan et al., 2017</xref>) and spatial concrete dropout (<xref ref-type="bibr" rid="b0">Gal et al., 2017</xref>). The same ResNet architecture served as a basis for all methods. Details of the implementation are provided in Appendix A.</p></sec><sec><title>Out-Of-Distribution Detection</title><p>We evalu- ated the uncertainty estimates on out-of- distribution detection. To quantify the results, we evaluated the area under the ROC curve (AUROC) for the task of deciding whether a given image comes from the same distribution or not. All methods were trained on four classes from the CIFAR-10 (<xref ref-type="bibr" rid="b0">Krizhevsky et al., 2009</xref>) dataset (training details are provided in Ap- pendix A). We then tested the resulting net- works on images from withheld classes and on the SVHN dataset (<xref ref-type="bibr" rid="b0">Netzer et al., 2011</xref>), which contains completely different images. Results are shown in <xref ref-type="table" rid="tab_0">Table 1</xref>. Considering the statisti- cal errors (see Appendix B), random priors per- formed slightly better than deep ensembles with adversarial training for B = 1 and about the same for B = 10. For dropout, B refers to the number of dropout samples. Dropout per- formed worse, but was cheaper to train. In order to gain a more finely-grained insight into the quality of the uncertainties, we also show uncertainty histograms in <xref ref-type="fig" rid="fig_2">Figure 3</xref>. The figure shows the distribution of uncertainty estimates for seen data (top row) vs. unseen data (bottom row) for bootstrap sizes B = {1, 5, 10}. The main conclusion is that uncertainties obtained from random pri- ors are already well-separated with B = 1, while deep ensembles need more bootstraps to achieve the full separation between test and train examples. We provide additional experimental results, showing OOD accuracy and an evaluation on CIFAR 100 in Appendix B.</p></sec><sec><title>Calibration</title><p>Good uncertainty estimates have the property that accuracy increases as we become more certain, a property known as calibration. We measured it by evaluating average accuracy on the subset of images with uncertainty smaller than a given value. We trained on four classes from the CIFAR-10 (<xref ref-type="bibr" rid="b0">Krizhevsky et al., 2009</xref>) dataset. We then tested the resulting networks on the whole dataset, which included both the seen and unseen classes. Results are shown in <xref ref-type="fig" rid="fig_3">Figure 4</xref>. Ideally, in a calibrated method, these curves should be increasing, indicating that a method always becomes more accurate as it becomes more confident. In coarse terms, <xref ref-type="fig" rid="fig_3">Figure 4</xref> confirms that all methods except a degenerate deep ensemble with only one bootstrap are roughly monotonic. However, uncertainty estimates from random priors are more stable, showing monotonicity on a finer scale as well as on a large scale. Interestingly, calibration improved only slightly when increasing the number of bootstraps B.</p></sec><sec><title>Subsampling Ablation</title><p>In the previous ex- periment, we kept the architectural and opti- mization choices fixed across algorithms. This ensured a level playing field, but meant that we were not able to obtain zero training error on the predictor networks used by random priors. However, we also wanted to evaluate random priors in the setting of near-zero training error. To do this, we used a smaller set of training im- ages, while still keeping the network architec- ture the same. This allowed us to obtain near- complete convergence (details in Appendix A). Results of this ablation are shown in <xref ref-type="fig" rid="fig_4">Figures 5</xref> and 6, as well as <xref ref-type="table" rid="tab_1">Table 2</xref>, analogous to our results on the full dataset presented above. In this sub-sampled regime, the random prior method easily outper- formed competing approaches, showing better calibration (<xref ref-type="fig" rid="fig_4">Fig. 5</xref>). The histograms in <xref ref-type="fig" rid="fig_5">Figure 6</xref> also demonstrate good separation between seen and unseen data. In the out-of-distribution benchmarks reported in <xref ref-type="table" rid="tab_1">Table 2</xref>, the random prior method has comfortably outperformed the baselines. While this training regime is not practical for real-life tasks, it demonstrates the potential performance of random priors when trained to full convergence.</p></sec><sec><title>Sensitivity to Initialization Scale</title><p>We performed an ablation to test the robustness of our algorithm to the scaling of the weight initialization in the prior. Results are shown in <xref ref-type="fig" rid="fig_6">Figure 7</xref>, where we plot the relationship between initialization scale (taken from the set {0.01, 0.1, 1.0, 2.0, 5.0, 10.0}) and AUROC performance on the CIFAR-10 task. OOD performance is relatively robust with respect to the weight initialization within one order of magnitude.</p></sec><sec><title>Summary of experiments</title><p>We have shown that uncertainties obtained from random priors achieve competitive performance with fewer boot- straps in a regime where the network architecture is typical for standard supervised learning workloads. Random priors showed superior perfor- mance in a regime where the predictors can be trained to near-zero loss.</p></sec><sec><title>CONCLUSIONS</title><p>We provided a theoretical justification for the use of random priors for obtaining uncertainty estimates in the context of deep learning. We have shown that the obtained uncertainties are conservative and that they con- centrate for any neural network architecture. We performed an extensive empirical comparison, showing that random priors perform similarly to deep ensembles in a typical supervised training setting, while outperforming them in a regime where we are able to accomplish near-zero training loss for the predictors.</p></sec><sec id="figures"><title>Figures</title><fig id="fig_0"><object-id>fig_0</object-id><label>Figure 1:</label><caption><title>Figure 1:</title><p>On top, two predictors (green) were trained to fit two randomly- generated priors (red). On the bottom, we obtain uncertainties from the difference between predictors and priors. Dots cor- respond to training points x i .</p></caption><graphic /></fig><fig id="fig_1"><object-id>fig_1</object-id><label>Figure 2:</label><caption><title>Figure 2:</title><p>Architecture of the random prior networks f and predictor networks h Xf . The predictor networks h Xf typically share the same architectural core, but have additional layers relative to the prior networks. Both the green and red parts of the predictor networks are trained.</p></caption><graphic /></fig><table-wrap id="tab_0"><label>Table 1:</label><caption><title>Table 1:</title><p>Out-of-distribution AUROC for random priors (RP), deep ensembles (DE), deep ensem- bles with adversarial training (DE+AT) and spa- tial concrete dropout (DR). Estimated confidence intervals are provided in Appendix B.</p></caption><table><tbody><tr><td /></tr></tbody></table></table-wrap><fig id="fig_2"><object-id>fig_2</object-id><label>Figure 3:</label><caption><title>Figure 3:</title><p>Distribution of uncertainty estimates for various algorithms. Top row shows seen data, bottom row shows unseen data from CIFAR-10. For random priors (RP), uncertainties are&#963; 2 . For other algorithms, they are 1 &#8722; max(p &#181; ), where p &#181; is the averaged output of models in ensemble (Lakshminarayanan et al., 2017).</p></caption><graphic /></fig><fig id="fig_3"><object-id>fig_3</object-id><label>Figure 4:</label><caption><title>Figure 4:</title><p>Calibration curves showing the relationship between uncertainty (horizontal axis) and accuracy (vertical axis) for B = 1, 5, 10 on CIFAR-10.</p></caption><graphic /></fig><table-wrap id="tab_1"><label>Table 2:</label><caption><title>Table 2:</title><p>Out-of-distribution AUROC for the same models as above (see Tab. 1) on subsampled data. Numbers are accurate up to &#177;0.01.</p></caption><table><tbody><tr><td /></tr></tbody></table></table-wrap><fig id="fig_4"><object-id>fig_4</object-id><label>Figure 5:</label><caption><title>Figure 5:</title><p>The relationship between uncertainty (horizontal axis) and accuracy (vertical axis) for B = 1, 5, 10 on a subset of 75 samples from CIFAR-10. In well-calibrated models, accuracy increases as uncertainty declines.</p></caption><graphic /></fig><fig id="fig_5"><object-id>fig_5</object-id><label>Figure 6:</label><caption><title>Figure 6:</title><p>Distribution of uncertainty estimates for various algorithms. Top row shows seen data, bottom row shows unseen data from CIFAR-10, where we trained on a sample of 75 images from the training set. For random priors (RP), uncertainties are&#963; 2 . For other algorithms, they are 1 &#8722; max(p &#181; ), where p &#181; is the averaged output of models in ensemble (Lakshminarayanan et al., 2017).</p></caption><graphic /></fig><fig id="fig_6"><object-id>fig_6</object-id><label>Figure 7:</label><caption><title>Figure 7:</title><p>Robustness of OOD perfromance to initialization scale. Conf. bars present, but small, denoting high confidence. Horizontal axis is logarithmic.</p></caption><graphic /></fig></sec></body><back><ref-list id="ref-list-1"><ref id="b0"><element-citation publication-type="journal"><fpage>242</fpage><lpage>252</lpage><person-group person-group-type="author"><name><surname>References Zeyuan Allen-Zhu</surname><given-names>Yuanzhi</given-names></name><name><surname>Li</surname><given-names>Zhao</given-names></name><name><surname>Song</surname><given-names /></name><name><surname>Beach</surname><given-names>Long</given-names></name><name><surname>California</surname><given-names /></name><name><surname>Usa</surname><given-names /></name></person-group></element-citation></ref></ref-list></back></article>