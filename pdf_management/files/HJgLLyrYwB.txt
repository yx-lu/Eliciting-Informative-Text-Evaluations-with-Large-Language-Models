Title:
```
Published as a conference paper at ICLR 2020 STATE-ONLY IMITATION WITH TRANSITION DYNAM- ICS MISMATCH
```
Abstract:
```
Imitation Learning (IL) is a popular paradigm for training agents to achieve com- plicated goals by leveraging expert behavior, rather than dealing with the hard- ships of designing a correct reward function. With the environment modeled as a Markov Decision Process (MDP), most of the existing IL algorithms are con- tingent on the availability of expert demonstrations in the same MDP as the one in which a new imitator policy is to be learned. This is uncharacteristic of many real-life scenarios where discrepancies between the expert and the imitator MDPs are common, especially in the transition dynamics function. Furthermore, obtain- ing expert actions may be costly or infeasible, making the recent trend towards state-only IL (where expert demonstrations constitute only states or observations) ever so promising. Building on recent adversarial imitation approaches that are motivated by the idea of divergence minimization, we present a new state-only IL algorithm in this paper. It divides the overall optimization objective into two subproblems by introducing an indirection step and solves the subproblems it- eratively. We show that our algorithm is particularly effective when there is a transition dynamics mismatch between the expert and imitator MDPs, while the baseline IL methods suffer from performance degradation. To analyze this, we construct several interesting MDPs by modifying the configuration parameters for the MuJoCo locomotion tasks from OpenAI Gym 1 .
```

Figures/Tables Captions:
```
Figure 1: (a) Different amount of gravitation pull is one example of transition dynamics mismatch between the expert and the imitator MDPs. (b) An expert policy π * e trained in T exp transfer poorly to an environment with dissimilar dynamics T pol (gravity 0.5×). (c) IL performance with GAIL degrades when T exp = T pol , compared to the conventional IL setting of imitating in the same environment as the expert.
Figure 2: Environments for training an imitator policy are obtained by changing the default Gym configuration settings, one at a time.
Figure 3: Training progress for I2L and GAIL-S when the imitator and expert MDPs differ in the configuration of the gravity parameter. Gravity in T pol is 0.5× the gravity in T exp .
Figure 4: Training progress for I2L and GAIL-S when the imitator and expert MDPs differ in the configuration of the density parameter. Density of the bot in T pol is 2× the density in T exp .
Figure 5: Training progress for I2L and GAIL-S when the imitator and expert MDPs differ in the configuration of the friction parameter. The friction coefficient on all the joints of the bot in T pol is 3× the coefficient in T exp .
Figure 6: Ablation on capac- ity of buffer B using low-gravity Half-Cheetah.
Table 1: Average episodic re- turns when T exp = T pol .
Table 2: Comparing performance of I2L with GAIfO (Torabi et al., 2018b), a state-only IL baseline.
```

Main Content:
```

Section Title: INTRODUCTION
  INTRODUCTION In the Reinforcement Learning (RL) framework, the objective is to train policies that maximize a certain reward criterion. Deep-RL, which combines RL with the recent advances in the field of deep- learning, has produced algorithms demonstrating remarkable success in areas such as games ( Mnih et al., 2015 ;  Silver et al., 2016 ), continuous control ( Lillicrap et al., 2015 ), and robotics ( Levine et al., 2016 ), to name a few. However, the application of these algorithms beyond controlled simulation environments has been fairly modest; one of the reasons being that manual specification of a good reward function is a hard problem. Imitation Learning (IL) algorithms (Pomerleau, 1991;  Ng et al., 2000 ;  Ziebart et al., 2008 ;  Ho & Ermon, 2016 ) address this issue by replacing reward functions with expert demonstrations, which are easier to collect in most scenarios. The conventional setting used in most of the IL literature is the availability of state-action trajectories from the expert, τ := {s 0 , a 0 , . . . s T , a T }, collected in an environment modeled as a Markov deci- sion process (MDP) with transition dynamics T exp . These dynamics govern the distribution over the next state, given the current state and action. The IL objective is to leverage τ to train an imitator pol- icy in the same MDP as the expert. This is a severe requirement that impedes the wider applicability of IL algorithms. In many practical scenarios, the transition dynamics of the environment in which the imitator policy is learned (henceforth denoted by T pol ) is different from the dynamics of the environment used to collect expert behavior, T exp . Consider self-driving cars as an example, where the goal is to learn autonomous navigation on a vehicle with slightly different gear-transmission characteristics than the vehicle used to obtain human driving demonstrations. We therefore strive Published as a conference paper at ICLR 2020 (a) (b) (c) for an IL method that could train agents under a transition dynamics mismatch, T exp = T pol . We assume that other MDP attributes are the same for the expert and imitator environments. Beyond the dynamics equivalence, another assumption commonly used in IL literature is the avail- ability of expert actions (along with the states). A few recent works ( Torabi et al., 2018a ;b;  Sun et al., 2019 ) have proposed "state-only" IL algorithms, where expert demonstrations do not include the actions. This opens up the possibility of employing IL to situations such as kinesthetic teaching in robotics and learning from weak-supervision sources such as videos. Moreover, if T exp and T pol differ, then the expert actions, even if available, are not quite useful for imitation anyway, since the application of an expert action from any state leads to different next-state distributions for the expert and the imitator. Hence, our algorithm uses state-only expert demonstrations. We build on previous IL literature inspired by GAN-based adversarial learning - GAIL ( Ho & Er- mon, 2016 ) and AIRL ( Fu et al., 2017 ). In both these methods, the objective is to minimize the distance between the visitation distributions (ρ) induced by the policy and expert, under some suit- able metric d, such as Jensen-Shannon divergence. We classify GAIL and AIRL as direct imitation methods as they directly reduce d(ρ π , ρ * ). Different from these, we propose an indirect imitation approach which introduces another distributionρ as an intermediate or indirection step. In slight detail, starting with the Max-Entropy Inverse-RL objective ( Ziebart et al., 2008 ), we derive a lower bound which transforms the overall IL problem into two sub-parts which are solved iteratively: the first is to train a policy to imitate a distributionρ represented by a trajectory buffer, and the second is to move the buffer distribution closer to expert's (ρ * ) over the course of training. The first part, which is policy imitation by reducing d(ρ π ,ρ) is done with AIRL, while the second part, which is reducing d(ρ, ρ * ), is achieved using a Wasserstein critic ( Arjovsky et al., 2017 ). We abbreviate our approach as I2L, for indirect-imitation-learning. We test the efficacy of our algorithm with continuous-control locomotion tasks from MuJoCo. Fig- ure 1a depicts one example of the dynamics mismatch which we evaluate in our experiments. For the Ant agent, an expert walking policy π * e is trained under the default dynamics provided in the OpenAI Gym, T exp = Earth. The dynamics under which to learn the imitator policy are curated by modifying the gravity parameter to half its default value (i.e. 9.81 2 ), T pol = PlanetX. Figure 1b plots the average episodic returns of π * e in the original and modified environments, and proves that direct policy transfer is infeasible. For Figure 1c, we just assume access to state-only expert demon- strations from π * e , and do IL with the GAIL algorithm. GAIL performs well if the imitator policy is learned in the same environment as the expert (T exp = T pol = Earth), but does not succeed un- der mismatched transition dynamics, (T exp = Earth, T pol = PlanetX). In our experiments section, we consider other sources of dynamics mismatch as well, such as agent-density and joint-friction. We show that I2L trains much better policies than baseline IL algorithms in these tasks, leading to successful transfer of expert skills to an imitator in an environment dissimilar to the expert. We start by reviewing the relevant background on Max-Entropy IRL, GAIL and AIRL, since these methods form an integral part of our overall algorithm.

Section Title: BACKGROUND
  BACKGROUND An RL environment modeled as an MDP is characterized by the tuple (S, A, R, T , γ), where S is the state-space, and A is the action-space. Given an action a t ∈ A, the next state is governed by the tran- sition dynamics s t+1 ∼ T (s t+1 |s t , a t ), and reward is computed as r t = R(r t |s t , a t ). The RL ob- jective is to maximie the expected discounted sum of rewards, η(π θ ) = E p0,T ,π ∞ t=0 γ t r(s t , a t ) , where γ ∈ (0, 1] is the discount factor, and p 0 is the initial state distribution. We define the unnor- malized γ-discounted state-visitation distribution for a policy π by ρ π (s) = ∞ t=0 γ t P (s t =s|π), where P (s t =s|π) is the probability of being in state s at time t, when following policy π and start- ing state s 0 ∼ p 0 . The expected policy return η(π θ ) can then be written as E ρπ(s,a) [r(s, a)], where ρ π (s, a) = ρ π (s)π(a|s) is the state-action visitation distribution (also referred to as the occupancy measure). For any policy π, there is a one-to-one correspondence between π and its occupancy measure ( Puterman, 1994 ).

Section Title: MAXIMUM ENTROPY IRL
  MAXIMUM ENTROPY IRL Designing reward functions that adequately capture the task intentions is a laborious and error-prone procedure. An alternative is to train agents to solve a particular task by leveraging demonstrations of that task by experts. Inverse Reinforcement Learning (IRL) algorithms ( Ng et al., 2000 ;  Rus- sell, 1998 ) aim to infer the reward function from expert demonstrations, and then use it for RL or planning. The IRL method, however, has an inherent ambiguity, since many expert policies could explain a set of provided demonstrations. To resolve this,  Ziebart (2010)  proposed the Maximum Causal Entropy (MaxEnt) IRL framework, where the objective is to learn a reward function such that the resulting policy matches the provided expert demonstrations in the expected feature counts f , while being as random as possible: max π H(π) s.t. E s,a∼π [f (s, a)] =f demo where H(π) = E π [− log π(a|s)] is the γ-discounted causal entropy, andf demo denotes the empirical feature counts of the expert. This constrained optimization problem is solved by minimizing the Lagrangian dual, resulting in the maximum entropy policy: π θ (a|s) = exp(Q soft θ (s, a) − V soft θ (s)), where θ is the Lagrangian multiplier on the feature matching constraint, and Q soft θ , V soft θ are the soft value functions such that the following equations hold (please see Theorem 6.8 in  Ziebart (2010) ): Inspired by the energy-based formulation of the maximum entropy policy described above, π θ (a|s) = exp(Q soft θ (s, a) − V soft θ (s)), recent methods ( Finn et al., 2016 ;  Haarnoja et al., 2017 ;  Fu et al., 2017 ) have proposed to model complex, multi-modal action distributions using energy-based policies, π(a|s) ∝ exp(f ω (s, a)), where f ω (s, a) is represented by a universal function approxima- tor, such as a deep neural network. We can then interpret the IRL problem as a maximum likelihood estimation problem:

Section Title: ADVERSARIAL IRL
  ADVERSARIAL IRL An important implication of casting IRL as maximum likelihood estimation is that it connects IRL to adversarial training. We now briefly discuss AIRL ( Fu et al., 2017 ) since it forms a compo- nent of our proposed algorithm. AIRL builds on GAIL ( Ho & Ermon, 2016 ), a well-known ad- versarial imitation learning algorithm. GAIL frames IL as an occupancy-measure matching (or divergence minimization) problem. Let ρ π (s, a) and ρ E (s, a) represent the state-action visitation distributions of the policy and the expert, respectively. Minimizing the Jenson-Shanon divergence min π D JS [ρ π (s, a) || ρ E (s, a)] recovers a policy with a similar trajectory distribution as the expert. GAIL iteratively trains a policy (π θ ) and a discriminator (D ω : S × A → (0, 1)) to optimize the min-max objective similar to GANs ( Goodfellow et al., 2014 ): GAIL attempts to learn a policy that behaves similar to the expert demonstrations, but it bypasses the process of recovering the expert reward function.  Finn et al. (2016)  showed that imposing a special structure on the discriminator makes the adversarial GAN training equivalent to optimizing the MLE objective (Equation 1). Furthermore, if trained to optimality, it is proved that the expert reward (up to a constant) can be recovered from the discriminator. They operate in a trajectory- centric formulation which can be inefficient for high dimensional state- and action-spaces.  Fu et al. (2017)  present AIRL which remedies this by proposing analogous changes to the discriminator, but operating on a single state-action pair: Similar to GAIL, the discriminator is trained to maximize the objective in Equation 2; f ω is learned, whereas the value of π(a|s) is "filled in". The policy is optimized jointly using any RL algorithm with log D ω − log(1 − D ω ) as rewards. When trained to optimality, exp(f ω (s, a)) = π * (a|s) = exp(A * soft (s, a)/α); hence f ω recovers the soft advantage of the expert policy (up to a constant).

Section Title: STATE-ONLY IMITATION
  STATE-ONLY IMITATION State-only IL algorithms extend the scope of applicability of IL by relieving the need for expert ac- tions in the demonstrations. The original GAIL approach could be modified to work in the absence of actions. Specifically, Equation 2 could be altered to use a state-dependent discriminator D ω (s), and state-visitation (instead of state-action-visitation) distributions ρ E (s) and ρ π θ (s). The AIRL al- gorithm, however, requires expert actions due to the special structure enforced on the discriminator (Equation 3), deeming it incompatible with state-only IL. This is because, even though f ω could potentially be made a function of only the state s, actions are still needed for the "filled in" π θ (a|s) component. Inspired by GAIL,  Torabi et al. (2018b)  proposed GAIfO for state-only IL. The moti- vation is to train the imitator to perform actions that have similar effects in the environment, rather than mimicking the expert action. Algorithmically, GAIL is modified to make the discriminator a function of state transitions D ω (s, s ), and include state-transition distributions ρ(s, s ).

Section Title: INDIRECT IMITATION LEARNING (I2L)
  INDIRECT IMITATION LEARNING (I2L) We now detail our I2L algorithm which alters the standard IL routine (used by GAIL, AIRL) by introducing an intermediate or indirection step, through a new distribution represented by a trajec- tory buffer. For this section, we ignore the properties of the transition dynamics for the expert and the imitator MDPs (T exp , T pol ); they can be the same or different, I2L has no specific dependence on this. τ denotes a trajectory, which is a sequence of state-action pairs, {s 0 , a 0 , . . . , s T , a T }. We begin with the expert's (unknown) trajectory distribution, although our final algorithm works with state-only expert demonstrations. Let the trajectory distribution induced by the expert be p * (τ ), and its state-action visitation distribu- tion be ρ * (s, a). Using the parameterization from Equation 1, the likelihood objective to maximize for reward learning in MaxEnt-IRL can be written as (ignoring constants w.r.t ω): As alluded to in Sections 2.2-2.3, if expert actions were available, one could optimize for ω by solving an equivalent adversarial min-max objective, as done in AIRL. To handle state-only IL, we proceed to derive a lower bound to this objective and optimize that instead. Let there be a surrogate policyπ with a state-action distributionρ(s, a). The following proposition provides a lower bound to the likelihood objective in Equation 4.

Section Title: Proposition
  Proposition where L is the Lipschitz constant, and W 1 (ρ * ,ρ) is the 1-Wasserstein (or Earth Mover's) distance between the state-action distributions. Published as a conference paper at ICLR 2020 Proof. Let x := s ⊕ a denote the concatenation of state and action. Under Lipschitz continuity assumption for f ω (x), for any two inputs x ∼ X and x ∼ X , we have Let µ(X, X ) be any joint distribution over the random variables representing the two inputs, such that the marginals are ρ * (X) andρ(X ). Taking expectation w.r.t µ on both sides, we get Since the above inequality holds for any µ, it also holds for µ * = arg min µ E µ (x − x) 1 , which gives us the 1-Wasserstein distance We can therefore lower bound the likelihood objective (Equation 4) as: E τ ∼p*(τ ) [log p ω (τ )] ≥ E τ ∼p(τ ) [log p ω (τ )] − LW 1 (ρ * ,ρ) wherep(τ ) is the trajectory distribution induced by the surrogate policyπ. Since the original opti- mization (Equation 1) is infeasible under the AIRL framework in the absence of expert actions, we instead maximize the lower bound, which is to solve the surrogate problem: This objective can be intuitively understood as follows. Optimizing w.r.t ω recovers the reward (or soft advantage) function of the surrogate policyπ, in the same spirit as MaxEnt-IRL. Optimizing w.r.tρ brings the state-action distribution ofπ close (in 1-Wasserstein metric) to the expert's, along with a bias term that increases the log-likelihood of trajectories fromπ, under the current reward model ω. We now detail the practical implementation of these optimizations.

Section Title: Surrogate policy
  Surrogate policy We do not use a separate explicit parameterization forπ. Instead,π is implicitly represented by a buffer B, with a fixed capacity of k trajectories 2 . In this way,π can be viewed as a mixture of deterministic policies, each representing a delta distribution in trajectory space. B is akin to experience replay ( Lin, 1992 ), in that it is filled with trajectories generated from the agent's interaction with the environment during the learning process. The crucial difference is that inclusion to B is governed by a priority-based protocol (explained below). Optimization w.r.t ω can now be done using adversarial training (AIRL), since the surrogate policy actions are available in B. Following Equation 3, the objective for the discriminator is: max ω E (s,a)∼B log e fω(s,a) e fω(s,a) + π θ (a|s) + E (s,a)∼π θ log π θ (a|s) e fω(s,a) + π θ (a|s) (6) where π θ is the learner (imitator) policy that is trained with log D ω − log(1 − D ω ) as rewards.

Section Title: Optimizingρ
  Optimizingρ Sinceρ is characterized by the state-action tuples in the buffer B, updatingρ amounts to refreshing the trajectories in B. For the sake of simplicity, we only consider the Wasserstein distance objective and ignore the other bias term, when updating forρ in Equation 5. Note that ρ * ,ρ denote the state-action visitation distributions of the expert and the surrogate, respectively. Since we have state-only demonstrations from the expert (no expert actions), we minimize the Wasserstein distance between state visitations, rather than state-action visitations. Following the approach in WGANs ( Arjovsky et al., 2017 ), we estimate W 1 using the Kantorovich-Rubinstein duality, and train a critic network g φ with Lipschitz continuity constraint, The empirical estimate of the first expectation term is done with the states in the provided expert demonstrations; for the second term, the states in B are used. With the trained critic g φ , we obtain a 2 k = 5 in all our experiments Published as a conference paper at ICLR 2020 Update policy θ with PPO using log D ω − log(1 − D ω ) as rewards 11 end score for each trajectory generated by the agent. The score is calculated as 1 |τ | s∈τ g φ (s), where |τ | is the length of the trajectory. Our buffer B is a priority-queue structure of fixed number of trajectories, the priority value being the score of the trajectory. This way, over the course of training, B is only updated with trajectories with higher scores, and by construction of the score function, these trajectories are closer to the expert's in terms of the Wasserstein metric. Further details on the update algorithm for the buffer and its alignment with the Wasserstein distance minimization objective are provided in Appendix 7.3.

Section Title: Algorithm
  Algorithm The major steps of the training procedure are outlined in Algorithm 1. The policy parameters (θ) are updated with the clipped-ratio version of PPO ( Schulman et al., 2017 ). State- value function baselines and GAE ( Schulman et al., 2015 ) are used for reducing the variance of the estimated policy-gradients. The priority buffer B uses the heap-queue algorithm (Appendix 7.3). The Lipschitz constant L in Equation 5 is unknown and task-dependent. If f ω is fairly smooth, L is a small constant that can be treated as a hyper-parameter and absorbed into the learning rate. Please see Appendix 7.2 for details on the hyper-parameters.

Section Title: RELATED WORK
  RELATED WORK There is an extensive amount of literature on IL with state-action expert demonstrations, and also on integrating IL and RL to bootstrap learning ( Billard et al., 2008 ;  Argall et al., 2009 ). Our work is most closely related to state-only IL and adversarial Inverse-RL methods discussed in Section 2. Here, we mention other related prior literature. BCO ( Torabi et al., 2018a ) is a state-only IL approach that learns an inverse dynamics model p(a|s, s ) by running a random exploration policy. The inverse model is then applied to infer actions from the state-only demonstrations, which in turn are used for imitation via Behavioral Cloning, making the approach vulnerable to the well-known issue of compounding errors ( Ross et al., 2011 ).  Kimura et al. (2018)  learn an internal model p(s |s) on state-only demonstrations; the imitator policy is then trained with RL using rewards derived from the model. Imitation under a domain shift has been considered in  Stadie et al. (2017) ;  Liu et al. (2018) . These methods incorporate raw images as observations and are designed to handle differences in context (such as viewpoints, visual appearance, object positions, surroundings) between the expert and the imitator environments.  Gupta et al. (2017)  propose learning invariant feature mappings to transfer skills from an expert to an imitator with a different morphology. However, the reward function for such a transfer is contingent on the assumption of time-alignment in episodic tasks. In our Algorithm 1, the adversarial training between the policy and buffer trajectories (AIRL, Line 9) bears some resemblance to the adversarial self-imitation approaches in ( Guo et al., 2018 ; Gangwani

Section Title: EXPERIMENTS
  EXPERIMENTS In this section, we compare the performance of I2L to baseline methods for state-only IL from Sec- tion 2.3, namely GAIL with state-dependent discriminator, denoted by GAIL-S, and GAIfO ( Torabi et al., 2018b ). We do the evaluation by modifying the continuous-control locomotion task from Mu- JoCo to introduce various types of transition dynamics mismatch between the expert and the imitator MDPs (T exp = T pol ). It should be noted that other aspects of the MDP (S, A, R, γ) are assumed to be the same 3 . We, therefore, use dynamics and MDP interchangeably in this section. While the expert demonstrations are collected under the default configurations provided in OpenAI Gym, we construct the environments for the imitator by changing some parameters independently: a.) gravity in T pol is 0.5× the gravity in T exp , b.) density of the bot in T pol is 2× the density in T exp , and c.) the friction coefficient on all the joints of the bot in T pol is 3× the coefficient in T exp .  Figure 2  has a visual. For all our experiments and tasks, we assume a single expert state-only demonstration of length 1000. We do not assume any access to the expert MDP beyond this. Performance when T exp = T pol .  Table 1  shows the average episodic returns for a policy trained for 5M timesteps using GAIL-S and I2L in the standard IL setting. The policy learning curves are included in Appendix 7.1. All our experiments average 8 independent runs with random seeds. Both the algorithms work fairly well in this scenario, though I2L achieves higher scores in 3 out of 4 tasks. These numbers serve as a benchmark when we evaluate performance with transition dynamics mismatch. The table also contains the expert demonstration score for each task. Performance when T exp = T pol .  Figures 3 , 4 and 5 plot the training progress (mean and standard deviation) with GAIL-S and I2L under mismatched transition dynamics with low gravity, high den- sity and high friction settings, respectively, as described above. We observe that I2L achieves faster learning and higher final scores than GAIL-S in most of the situations. GAIL-S degrades severely in some cases. For instance, for Half-Cheetah under high density, GAIL-S drops to 923 (compared to 5974 with no dynamics change,  Table 1 ), while I2L attains a score of 3975 (compared to 5240 with no dynamics change). Similarly, with Hopper under high friction, GAIL-S score reduces to 810 (from 2130 with no dy- namics change), and the I2L score is 2084 (2751 with no dynamics change). The plots also indicate the final average performance achieved using the original GAIL (marked as GAIL-SA) and AIRL algorithms. Both of these methods require extra supervision in the form of expert actions. Even so, they generally perform worse than I2L, which can be attributed to the fact that the expert actions generated in T exp are not very useful when the dynamics shift to T pol .

Section Title: Comparison with GAIfO baseline
  Comparison with GAIfO baseline GAIfO ( Torabi et al., 2018b ) is a recent state-only IL method which we discuss in Section 2.3.  Table 2  contrasts the performance of I2L with GAIfO for imitation tasks both with and without transition dynamics mismatch. We find GAIfO to be in the same ballpark as GAIL-S. It can learn good imitation policies if the dynamics are the same between the expert and the imitator, but loses performance with mismatched dynamics. Learning curves for GAIfO are included in Appendix 7.7. Furthermore, in Appendix 7.6, we compare to BCO ( Torabi et al., 2018a ).

Section Title: Ablation on buffer capacity
  Ablation on buffer capacity Algorithm 1 uses priority-queue buffer B of fixed number of trajec- tories to represent the surrogate state-action visitationρ. All our experiments till this point fixed the buffer capacity to 5 trajectories. To gauge the sensitivity of our approach to the capacity |B|, we ablate on it and report the results in  Figure 6 . The experiment is done with the low-gravity Half- Cheetah environment. We observe that the performance of I2L is fairly robust to |B|. Surprisingly, even a capacity of 1 trajectory works well, and having a large buffer (|B| = 50) also does not hurt performance much. The GAIL-S baseline on the same task is included for comparison. Empirical measurements of the lower-bound and Wasserstein approximations. Section 3 in- troduces a lower bound on the expected value of a function f ω (s, a) under the expert's state-action visitation. In Appendix 7.4, we analyze the quality of the lower bound by plotting the approximation- gap for the differentρ distributions obtained during training. We observe that the gap generally re- duces. Finally, in Appendix 7.5, we plot the empirical estimate of the Wasserstein distance between the state-visitations of the buffer distribution and the expert, and note that this value also typically decreases over the training iterations.

Section Title: CONCLUSION
  CONCLUSION In this paper, we presented I2L, an indirect imitation-learning approach that utilizes state-only expert demonstrations collected in the expert MDP, to train an imitator policy in an MDP with a dissimilar transition dynamics function. We derive a lower bound to the Max-Ent IRL objective that transforms it into two subproblems. We then provide a practical algorithm that trains a policy to imitate the dis- tribution characterized by a trajectory-buffer using AIRL, whilst reducing the Wasserstein distance between the state-visitations of the buffer and expert, over the course of training. Our experiments in a variety of MuJoCo-based MDPs indicate that I2L is an effective mechanism for successful skill transfer from the expert to the imitator, especially under mismatched transition dynamics.
  A can also be made different between the MDPs without requiring any modifications to the algorithm.

```
