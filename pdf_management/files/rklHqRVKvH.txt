Title:
```
Published as a conference paper at ICLR 2020 HARNESSING STRUCTURES FOR VALUE-BASED PLANNING AND REINFORCEMENT LEARNING
```
Abstract:
```
Value-based methods constitute a fundamental methodology in planning and deep reinforcement learning (RL). In this paper, we propose to exploit the underlying structures of the state-action value function, i.e., Q function, for both planning and deep RL. In particular, if the underlying system dynamics lead to some global structures of the Q function, one should be capable of inferring the function better by leveraging such structures. Specifically, we investigate the low- rank structure, which widely exists for big data matrices. We verify empirically the existence of low-rank Q functions in the context of control and deep RL tasks. As our key contribution, by leveraging Matrix Estimation (ME) techniques, we propose a general framework to exploit the underlying low-rank structure in Q functions. This leads to a more efficient planning procedure for classical control, and additionally, a simple scheme that can be applied to value-based RL techniques to consistently achieve better performance on "low-rank" tasks. Extensive experiments on control tasks and Atari games confirm the efficacy of our approach.
```

Figures/Tables Captions:
```
Figure 1: The approximate rank and MSE of Q (t) during value iteration. (a) & (b) use vanilla value iteration; (c) & (d) use online reconstruction with only 50% observed data each iteration.
Figure 2: An illustration of the proposed SVP algorithm for leveraging low-rank structures.
Figure 3: Performance comparison between optimal policy and the proposed SVP policy.
Figure 4: Approximate rank of different Atari games: histogram (red) and empirical CDF (blue).
Figure 5: An illustration of the proposed SV-RL scheme, compared to the original value-based RL.
Figure 6: Results of SV-RL on various value-based deep RL techniques. First row: results on DQN. Second row: results on double DQN. Third row: results on dueling DQN.
Figure 7: Interpretation of deep RL results. We plot games where the SV-based method performs differently. More structured games (with lower rank) can achieve better performance with SV-RL.
```

Main Content:
```

Section Title: INTRODUCTION
  INTRODUCTION Value-based methods are widely used in control, planning, and reinforcement learning ( Gorodetsky et al., 2018 ;  Alora et al., 2016 ;  Mnih et al., 2015 ). To solve a Markov Decision Process (MDP), one common method is value iteration, which finds the optimal value function. This process can be done by iteratively computing and updating the state-action value function, represented by Q(s, a) (i.e., the Q-value function). In simple cases with small state and action spaces, value iteration can be ideal for efficient and accurate planning. However, for modern MDPs, the data that encodes the value function usually lies in thousands or millions of dimensions ( Gorodetsky et al., 2018 ;  2019 ), including images in deep reinforcement learning ( Mnih et al., 2015 ;  Tassa et al., 2018 ). These practical constraints significantly hamper the efficiency and applicability of the vanilla value iteration. Yet, the Q-value function is intrinsically induced by the underlying system dynamics. These dynamics are likely to possess some structured forms in various settings, such as being governed by partial differential equations. In addition, states and actions may also contain latent features (e.g., similar states could have similar optimal actions). Thus, it is reasonable to expect the structured dynamic to impose a structure on the Q-value. Since the Q function can be treated as a giant matrix, with rows as states and columns as actions, a structured Q function naturally translates to a structured Q matrix. In this work, we explore the low-rank structures. To check whether low-rank Q matrices are common, we examine the benchmark Atari games, as well as 4 classical stochastic control tasks. As we demonstrate in Sections 3 and 4, more than 40 out of 57 Atari games and all 4 control tasks exhibit low-rank Q matrices. This leads us to a natural question: How do we leverage the low-rank structure in Q matrices to allow value-based techniques to achieve better performance on "low-rank" tasks? We propose a generic framework that allows for exploiting the low-rank structure in both classical planning and modern deep RL. Our scheme leverages Matrix Estimation (ME), a theoretically guar- anteed framework for recovering low-rank matrices from noisy or incomplete measurements (Chen & Chi, 2018). In particular, for classical control tasks, we propose Structured Value-based Planning (SVP). For the Q matrix of dimension |S| × |A|, at each value iteration, SVP randomly updates a small portion of the Q(s, a) and employs ME to reconstruct the remaining elements. We show that planning problems can greatly benefit from such a scheme, where fewer samples (only sample around 20% of (s, a) pairs at each iteration) can achieve almost the same performance as the optimal policy. For more advanced deep RL tasks, we extend our intuition and propose Structured Value-based Deep RL (SV-RL), applicable for deep Q-value based methods such as DQN ( Mnih et al., 2015 ). Here, instead of the full Q matrix, SV-RL naturally focuses on the "sub-matrix", corresponding to the sampled batch of states at the current iteration. For each sampled Q matrix, we again apply ME to represent the deep Q learning target in a structured way, which poses a low rank regularization on this "sub-matrix" throughout the training process, and hence eventually the Q-network's predictions. Intuitively, as learning a deep RL policy is often noisy with high variance, if the task possesses a low-rank property, this scheme will give a clear guidance on the learning space during training, after which a better policy can be anticipated. We confirm that SV-RL indeed can improve the performance of various value-based methods on "low-rank" Atari games: SV-RL consistently achieves higher scores on those games. Interestingly, for complex, "high-rank" games, SV-RL performs comparably. ME naturally seeks solutions that balance low rank and a small reconstruction error (cf. Section 3.1). Such a balance on reconstruction error helps to maintain or only slightly degrade the performance for "high-rank" situation. We summarize our contributions as follows: • We are the first to propose a framework that leverages matrix estimation as a general scheme to exploit the low-rank structures, from planning to deep reinforcement learning. • We demonstrate the effectiveness of our approach on classical stochastic control tasks, where the low-rank structure allows for efficient planning with less computation. • We extend our scheme to deep RL, which is naturally applicable for value-based techniques. Across a variety of methods, such as DQN, double DQN, and dueling DQN, experimental results on all Atari games show that SV-RL can consistently improve the performance of value-based methods, achieving higher scores for tasks when low-rank structures are confirmed to exist.

Section Title: WARM-UP: DESIGN MOTIVATION FROM A TOY EXAMPLE
  WARM-UP: DESIGN MOTIVATION FROM A TOY EXAMPLE To motivate our method, let us first investigate a toy example which helps to understand the structure within the Q-value function. We consider a simple deterministic MDP, with 1000 states, 100 actions and a deterministic state transition for each action. The reward r(s, a) is randomly generated first for each (s, a) pair, and then fixed throughout. A discount factor γ = 0.95 is used. The deterministic nature imposes a strong relationship among connected states. In this case, our goal is to explore: (1) what kind of structures the Q function may contain; and (2) how to effectively exploit such structures. The Low-rank Structure Under this setting, Q-value could be viewed as a 1000 × 100 matrix. To probe the structure of the Q-value function, we perform the standard Q-value iteration as follows: Q (t+1) (s, a) = s ∈S P (s |s, a) r(s, a) + γ max a ∈A Q (t) (s , a ) , ∀ (s, a) ∈ S × A, (1) where s denotes the next state after taking action a at state s. We randomly initialize Q (0) . In  Fig. 1 , we show the approximate rank of Q (t) and the mean-square error (MSE) between Q (t) and the optimal Q * , during each value iteration. Here, the approximate rank is defined as the first k singular values (denoted by σ) that capture more than 99% variance of all singular values, i.e., k i=1 σ 2 i / j σ 2 j ≥ 0.99. As illustrated in Fig. 1(a) and 1(b), the standard theory guarantees the Published as a conference paper at ICLR 2020 convergence to Q * ; more interestingly, the converged Q * is of low rank, and the approximate rank of Q (t) drops fast. These observations give a strong evidence for the intrinsic low dimensionality of this toy MDP. Naturally, an algorithm that leverages such structures would be much desired.

Section Title: Efficient Planning via Online Reconstruction with Matrix Estimation
  Efficient Planning via Online Reconstruction with Matrix Estimation The previous results moti- vate us to exploit the structure in value function for efficient planning. The idea is simple: If the eventual matrix is low-rank, why not enforcing such a structure throughout the iterations? In other words, with the existence of a global structure, we should be capable of exploiting it during intermediate updates and possibly also regularizing the results to be in the same low-rank space. In particular, at each iteration, instead of every (s, a) pair (i.e., Eq. (1)), we would like to only calculate Q (t+1) for some (s, a) pairs and then exploit the low-rank structure to recover the whole Q (t+1) matrix. We choose matrix estimation (ME) as our reconstruction oracle. The reconstructed matrix is often with low rank, and hence regularizing the Q matrix to be low-rank as well. We validate this framework in Fig. 1(c) and 1(d), where for each iteration, we only randomly sample 50% of the (s, a) pairs, calculate their corresponding Q (t+1) and reconstruct the whole Q (t+1) matrix with ME. Clearly, around 40 iterations, we obtain comparable results to the vanilla value iteration. Importantly, this comparable performance only needs to directly compute 50% of the whole Q matrix at each iteration. It is not hard to see that in general, each vanilla value iteration incurs a computation cost of O(|S| 2 |A| 2 ). The complexity of our method however only scales as O(p|S| 2 |A| 2 )+O M E , where p is the percentage of pairs we sample and O M E is the complexity of ME. In general, many ME methods employ SVD as a subroutine, whose complexity is bounded by O(min{|S| 2 |A|, |S||A| 2 }) ( Trefethen & David Bau, 1997 ). For low-rank matrices, faster methods can have a complexity of order linear in the dimensions ( Mazumder et al., 2010 ). In other words, our approach improves computational efficiency, especially for modern high-dimensional applications. This overall framework thus appears to be a successful technique: it exploits the low-rank behavior effectively and efficiently when the underlying task indeed possesses such a structure.

Section Title: STRUCTURED VALUE-BASED PLANNING
  STRUCTURED VALUE-BASED PLANNING Having developed the intuition underlying our methodology, we next provide a formal description in Sections 3.1 and 3.2. One natural question is whether such structures and our method are general in more realistic control tasks. Towards this end, we provide further empirical support in Section 3.3.

Section Title: MATRIX ESTIMATION
  MATRIX ESTIMATION ME considers about recovering a full data matrix, based on potentially incomplete and noisy observa- tions of its elements. Formally, consider an unknown data matrix X ∈ R n×m and a set of observed entries Ω. If the observations are incomplete, it is often assumed that each entry of X is observed independently with probability p ∈ (0, 1]. In addition, the observation could be noisy, where the noise is assumed to be mean zero. Given such an observed set Ω, the goal of ME is to produce an estimatorM so that ||M − X|| ≈ 0, under an appropriate matrix norm such as the Frobenius norm. The algorithms in this field are rich. Theoretically, the essential message is: exact or approximate recovery of the data matrix X is guaranteed if X contains some global structure ( Candès & Recht, 2009 ;  Chatterjee et al., 2015 ; Chen & Chi, 2018). In the literature, most attention has been focusing on the low-rank structure of a matrix. Correspondingly, there are many provable, practical algorithms to achieve the desired recovery. Early convex optimization methods ( Candès & Recht, 2009 ) seek to minimize the nuclear norm, ||M || * , of the estimator. For example, fast algorithms, such as the Soft-Impute algorithm ( Mazumder et al., 2010 ) solves the following minimization problem: Since the nuclear norm || · || * is a convex relaxation of the rank, the convex optimization approaches favor solutions that are with small reconstruction errors and in the meantime being relatively low-rank, which are desirable for our applications. Apart from convex optimization, there are also spectral methods and even non-convex optimization approaches ( Chatterjee et al., 2015 ;  Chen & Wainwright, 2015 ; Ge et al., 2016). In this paper, we view ME as a principled reconstruction oracle to effectively exploit the low-rank structure. For faster computation, we mainly employ the Soft-Impute algorithm.

Section Title: OUR APPROACH: STRUCTURED VALUE-BASED PLANNING
  OUR APPROACH: STRUCTURED VALUE-BASED PLANNING We now formally describe our approach, which we refer as structured value-based planning (SVP).  Fig. 2  illustrates our overall approach for solving MDP with a known model. The approach is based on the Q-value iteration. At the t-th iteration, instead of a full pass over all state-action pairs: 1. SVP first randomly selects a subset Ω of the state-action pairs. In particular, each state-action pair in S × A is observed (i.e., included in Ω) independently with probability p. 2. For each selected (s, a), the intermediateQ(s, a) is computed based on the Q-value iteration: 3. The current iteration then ends by reconstructing the full Q matrix with matrix estimation, from the set of observations in Ω. That is, Q (t+1) = ME {Q(s, a)} (s,a)∈Ω . Overall, each iteration reduces the computation cost by roughly 1 − p (cf. discussions in Section 2). In Appendix A, we provide the pseudo-code and additionally, a short discussion on the technical difficulty for theoretical analysis. Nevertheless, we believe that the consistent empirical benefits, as will be demonstrated, offer a sound foundation for future analysis.

Section Title: EMPIRICAL EVALUATION ON STOCHASTIC CONTROL TASKS
  EMPIRICAL EVALUATION ON STOCHASTIC CONTROL TASKS We empirically evaluate our approach on several classical stochastic control tasks, including the Inverted Pendulum, the Mountain Car, the Double Integrator, and the Cart-Pole. Our objective is to demonstrate, as in the toy example, that if the optimal Q * has a low-rank structure, then the proposed SVP algorithm should be able to exploit the structure for efficient planning. We present the evaluation on Inverted Pendulum, and leave additional results on other planning tasks in Appendix B and C.

Section Title: Inverted Pendulum
  Inverted Pendulum In this classical continuous task, our goal is to balance an inverted pendulum to its upright position, and the performance metric is the average angular deviation. The dynamics is described by the angle and the angular speed, i.e., s = (θ,θ), and the action a is the torque applied. We discretize the task to have 2500 states and 1000 actions, leading to a 2500 × 1000 Q-value matrix. For different discretization scales, we provide further results in Appendix G.1.

Section Title: The Low-rank Structure
  The Low-rank Structure We first verify that the optimal Q * indeed contains the desired low-rank structure. We run the vanilla value iteration until it converges. The converged Q matrix is found to have an approximate rank of 7. For further evidence, in Appendix C, we construct "low-rank" policies directly from the converged Q matrix, and show that the policies maintain the desired performance. The SVP Policy Having verified the structure, we would expect our approach to be effective. To this end, we apply SVP with different observation probability p and fix the overall number of iterations to be the same as the vanilla Q-value iteration.  Fig. 3  confirms the success of our approach. Fig. 3(a), 3(b) and 3(c) show the comparison between optimal policy and the final policy based on SVP. We further illustrate the performance metric, the average angular deviation, in Fig. 3(d). Overall, much fewer samples are needed for SVP to achieve a comparable performance to the optimal one.

Section Title: STRUCTURED VALUE-BASED DEEP REINFORCEMENT LEARNING
  STRUCTURED VALUE-BASED DEEP REINFORCEMENT LEARNING So far, our focus has been on tabular MDPs where value iteration can be applied straightforwardly. However, the idea of exploiting structure is much more powerful: we propose a natural extension of our approach to deep RL. Our scheme again intends to exploit and regularize structures in the Q-value function with ME. As such, it can be seamlessly incorporated into value-based RL techniques that include a Q-network. We demonstrate this on Atari games, across various value-based RL techniques.

Section Title: EVIDENCE OF STRUCTURED Q-VALUE FUNCTION
  EVIDENCE OF STRUCTURED Q-VALUE FUNCTION Before diving into deep RL, let us step back and review the process we took to develop our intuition. Previously, we start by treating the Q-value as a matrix. To exploit the structure, we first verify that certain MDPs have essentially a low-rank Q * . We argue that if this is indeed the case, then enforcing the low-rank structures throughout the iterations, by leveraging ME, should lead to better algorithms. A naive extension of the above reasoning to deep RL immediately fails. In particular, with images as states, the state space is effectively infinitely large, leading to a tall Q matrix with numerous number of rows (states). Verifying the low-rank structure for deep RL hence seems intractable. However, by definition, if a large matrix is low-rank, then almost any row is a linear combination of some other rows. That is, if we sample a small batch of the rows, the resulting matrix is most likely low-rank as well. To probe the structure of the deep Q function, it is, therefore, natural to understand the rank of a randomly sampled batch of states. In deep RL, our target for exploring structures is no longer the optimal Q * , which is never available. In fact, like SVP, the natural objective should be the converged values of the underlying algorithm, which in "deep" scenarios, are the eventually learned Q function. With the above discussions, we now provide evidence for the low-rank structure of learned Q function on some Atari games. We train standard DQN on 4 games, with a batch size of 32. To be consistent, the 4 games all have 18 actions. After the training process, we randomly sample a batch of 32 states, evaluate with the learned Q network and finally synthesize them into a matrix. That is, a 32 × 18 data matrix with rows the batch of states, the columns the actions, and the entries the values from the learned Q network. Note that the rank of such a matrix is at most 18. The above process is repeated for 10,000 times, and the histogram and empirical CDF of the approximate rank is plotted in  Fig. 4 . Apparently, there is a strong evidence supporting a highly structured low-rank Q function for those games - the approximate ranks are uniformly small; in most cases, they are around or smaller than 3.

Section Title: OUR APPROACH: STRUCTURED VALUE-BASED RL
  OUR APPROACH: STRUCTURED VALUE-BASED RL Having demonstrated the low-rank structure within some deep RL tasks, we naturally seek approaches that exploit the structure during the training process. We extend the same intuitions here: if eventually, the learned Q function is of low rank, then enforcing/regularizing the low rank structure for each iteration of the learning process should similarly lead to efficient learning and potentially better performance. In deep RL, each iteration of the learning process is naturally the SGD step where one would update the Q network. Correspondingly, this suggests us to harness the structure within the batch of states. Following our previous success, we leverage ME to achieve this task. We now formally describe our approach, referred as structured value-based RL (SV-RL). It exploits the structure for the sampled batch at each SGD step, and can be easily incorporated into any Q-value based RL methods that update the Q network via a similar step as in Q-learning. In particular, Q-value based methods have a common model update step via SGD, and we only exploit structure of the sampled batch at this step - the other details pertained to each specific method are left intact. Precisely, when updating the model via SGD, Q-value based methods first sample a batch of B transitions, {(s (i) t , r (i) t , a (i) t , s (i) t+1 )} B i=1 , and form the following updating targets: For example, in DQN,Q is the target network. The Q network is then updated by taking a gradient step for the loss function B i=1 y (i) − Q(s (i) t , a (i) t ; θ) 2 , with respect to the parameter θ. To exploit the structure, we then consider reconstructing a matrix Q † fromQ, via ME. The recon- structed Q † will replace the role ofQ in Eq. (3) to form the targets y (i) for the gradient step. In particular, the matrix Q † has a dimension of B × |A|, where the rows represent the "next states" {s (i) t+1 } B i=1 in the batch, the columns represent actions, and the entries are reconstructed values. Let S B {s (i) t+1 } B i=1 . The SV-RL alters the SGD update step as illustrated in Algorithm 1 and  Fig. 5 . Note the resemblance of the above procedure to that of SVP in Section 3.2. When the full Q matrix is available, in Section 3.2, we sub-sample the Q matrix and then reconstruct the entire matrix. When only a subset of the states (i.e., the batch) is available, naturally, we look at the corresponding sub-matrix of the entire Q matrix, and seek to exploit its structure.

Section Title: EMPIRICAL EVALUATION WITH VARIOUS VALUE-BASED METHODS
  EMPIRICAL EVALUATION WITH VARIOUS VALUE-BASED METHODS

Section Title: Experimental Setup
  Experimental Setup We conduct extensive experiments on Atari 2600. We apply SV-RL on three representative value-based RL techniques, i.e., DQN, double DQN and dueling DQN. We fix the total number of training iterations and set all the hyper-parameters to be the same. For each experiment, averaged results across multiple runs are reported. Additional details are provided in Appendix D.

Section Title: Consistent Benefits for "Structured" Games
  Consistent Benefits for "Structured" Games We present representative results of SV-RL applied to the three value-based deep RL techniques in  Fig. 6 . These games are verified by method mentioned in Section 4.1 to be low-rank. Additional results on all Atari games are provided in Appendix E. The figure reveals the following results. First, games that possess structures indeed benefit from our approach, earning mean rewards that are strictly higher than the vanilla algorithms across time. More importantly, we observe consistent improvements across different value-based RL techniques. This highlights the important role of the intrinsic structures, which are independent of the specific techniques, and justifies the effectiveness of our approach in consistently exploiting such structures. Further Observations Interestingly however, the performance gains vary from games to games. Specifically, the majority of the games can have benefits, with few games performing similarly or slightly worse. Such observation motivates us to further diagnose SV-RL in the next section.

Section Title: DIAGNOSE AND INTERPRET PERFORMANCE IN DEEP RL
  DIAGNOSE AND INTERPRET PERFORMANCE IN DEEP RL So far, we have demonstrated that games which possess structured Q-value functions can consistently benefit from SV-RL. Obviously however, not all tasks in deep RL would possess such structures. As such, we seek to diagnose and further interpret our approach at scale.

Section Title: Diagnosis
  Diagnosis We select 4 representative examples (with 18 actions) from all tested games, in which SV-RL performs better on two tasks (i.e., FROSTBITE and KRULL), slightly better on one task (i.e., ALIEN), and slightly worse on the other (i.e., SEAQUEST). The intuitions we developed in Section 4 Published as a conference paper at ICLR 2020 incentivize us to further check the approximate rank of each game. As shown in  Fig. 7 , in the two better cases, both games are verified to be approximately low-rank (∼ 2), while the approximate rank in ALIEN is moderately high (∼ 5), and even higher in SEAQUEST (∼ 10).

Section Title: Consistent Interpretations
  Consistent Interpretations As our approach is designed to exploit structures, we would expect to attribute the differences in performance across games to the "strength" of their structured properties. Games with strong low-rank structures tend to have larger improvements with SV-RL (Fig. 7(a) and 7(b)), while moderate approximate rank tends to induce small improvements (Fig. 7(c)), and high approximate rank may induce similar or slightly worse performances (Fig. 7(d)). The empirical results are well aligned with our arguments: if the Q-function for the task contains low-rank structure, SV-RL can exploit such structure for better efficiency and performance; if not, SV-RL may introduce slight or no improvements over the vanilla algorithms. As mentioned, the ME solutions balance being low rank and having small reconstruction error, which helps to ensure a reasonable or only slightly degraded performance, even for "high rank" games. We further observe consistent results on ranks vs. improvement across different games and RL techniques in Appendix E, verifying our arguments.

Section Title: RELATED WORK
  RELATED WORK Structures in Value Function Recent work in the control community starts to explore the structure of value function in control/planning tasks ( Ong, 2015 ;  Alora et al., 2016 ;  Gorodetsky et al., 2015 ; 2018). These work focuses on decomposing the value function and subsequently operating on the reduced-order space. In spirit, we also explore the low-rank structure in value function. The central difference is that instead of decomposition, we focus on "completion". We seek to efficiently operate on the original space by looking at few elements and then leveraging the structure to infer the rest, which allows us to extend our approach to modern deep RL. In addition, while there are few attempts for basis function learning in high dimensional RL ( Liang et al., 2016 ), functions are hard to generate in many cases and approaches based on basis functions typically do not get the same performance as DQN, and do not generalize well. In contrast, we provide a principled and systematic method, which can be applied to any framework that employs value-based methods or sub-modules. Finally, we remark that at a higher level, there are studies exploiting different structures within a task to design effective algorithms, such as exploring the low-dimensional system dynamics in predictive state representation ( Singh et al., 2012 ) or exploring the so called low "Bellman rank" property ( Jiang et al., 2017 ). We provide additional discussions in Appendix F. Value-based Deep RL Deep RL has emerged as a promising technique, highlighted by key successes such as solving Atari games via Q-learning ( Mnih et al., 2015 ) and combining with Monte Carlo tree search ( Browne et al., 2012 ;  Shah et al., 2019 ) to master Go, Chess and Shogi ( Silver et al., 2017a ; b ). Methods based on learning value functions are fundamental components in deep RL, exemplified by the deep Q-network ( Mnih et al., 2013 ;  2015 ). Over the years, there has been a large body of literature on its variants, such as double DQN ( Van Hasselt et al., 2016 ), dueling DQN ( Wang et al., 2015 ), IQN (Dabney et al., 2018) and other techniques that aim to improve exploration ( Osband et al., 2016 ;  Ostrovski et al., 2017 ). Our approach focuses on general value-based RL methods, rather than specific algorithms. As long as the method has a similar model update step as in Q-learning, our approach can leverage the structure to help with the task. We empirically demonstrate that deep RL tasks that have structured value functions indeed benefit from our scheme. Matrix Estimation ME is the primary tool we leverage to exploit the low-rank structure in value functions. Early work in the field is primarily motivated by recommendation systems. Since then, the techniques have been widely studied and applied to different domains ( Abbe & Sandon, 2015 ;  Borgs et al., 2017 ), and recently even in robust deep learning ( Yang et al., 2019 ). The field is relatively mature, with extensive algorithms and provable recovery guarantees for structured matrix ( Davenport & Romberg, 2016 ; Chen & Chi, 2018). Because of the strong promise, we view ME as a principled reconstruction oracle to exploit the low-rank structures within matrices.

Section Title: CONCLUSION
  CONCLUSION We investigated the structures in value function, and proposed a complete framework to understand, validate, and leverage such structures in various tasks, from planning to deep reinforcement learning. The proposed SVP and SV-RL algorithms harness the strong low-rank structures in the Q function, showing consistent benefits for both planning tasks and value-based deep reinforcement learning techniques. Extensive experiments validated the significance of the proposed schemes, which can be easily embedded into existing planning and RL frameworks for further improvements.

```
