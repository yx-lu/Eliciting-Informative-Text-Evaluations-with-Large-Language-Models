Title:
```
Published as a conference paper at ICLR 2020 LEARNING EXPENSIVE COORDINATION: AN EVENT-BASED DEEP RL APPROACH
```
Abstract:
```
Existing works in deep Multi-Agent Reinforcement Learning (MARL) mainly fo- cus on coordinating cooperative agents to complete certain tasks jointly. However, in many cases of the real world, agents are self-interested such as employees in a company and clubs in a league. Therefore, the leader, i.e., the manager of the company or the league, needs to provide bonuses to followers for efficient coordi- nation, which we call expensive coordination. The main difficulties of expensive coordination are that i) the leader has to consider the long-term effect and predict the followers' behaviors when assigning bonuses, and ii) the complex interactions between followers make the training process hard to converge, especially when the leader's policy changes with time. In this work, we address this problem through an event-based deep RL approach. Our main contributions are threefold. (1) We model the leader's decision-making process as a semi-Markov Decision Process and propose a novel multi-agent event-based policy gradient to learn the leader's long-term policy. (2) We exploit the leader-follower consistency scheme to design a follower-aware module and a follower-specific attention module to predict the followers' behaviors and make accurate response to their behaviors. (3) We propose an action abstraction-based policy gradient algorithm to reduce the followers' decision space and thus accelerate the training process of follow- ers. Experiments in resource collections, navigation, and the predator-prey game reveal that our approach outperforms the state-of-the-art methods dramatically.
```

Figures/Tables Captions:
```
Figure 1: Overview of our framework. The details of the leader's module and the follower's module can be found in Section 4.2 and Section 4.3, respectively. The implement details of each module can be found in Appendix D.2.1.
Figure 2: An example and a probabilistic graphical model to illustrate our method.
Figure 3: Leader's reward curves for different tasks (rule-based followers).
Figure 4: The final reward for RL-based followers. No ab- straction means the vanilla RL-based followers.
Table 1: Robustness results in multi-bound resource collections. b% is the probability that followers randomly choose actions.
```

Main Content:
```

Section Title: INTRODUCTION
  INTRODUCTION Deep Multi-Agent Reinforcement Learning (MARL) has been widely used in coordinating cooper- ative agents to jointly complete certain tasks where the agent is assumed to be selfless (fully coop- erative), i.e., the agent is willing to sacrifice itself to maximize the team reward. However, in many cases of the real world, the agents are self-interested, such as taxi drivers in a taxi company (fleets) and clubs in a league. For instance, in the example of taxi fleets (Miao et al., 2016), drivers may pre- fer to stay in the area with high customer demand to gain more reward. It is unfair and not efficient to compel the taxi driver to selflessly contribute to the company, e.g., to stay in the low customer demand area. Forcing the drivers to selflessly contribute may increase the income for the company in a short-term but it will finally causes the low efficient and unsustainable of that company in the Published as a conference paper at ICLR 2020 long run because the unsatisfied drivers may be demotivated and even leave the company. Another important example is that the government wants some companies to invest on the poverty area to achieve the fairness of the society, which may inevitably reduce the profits of companies. Similar to previous example, the companies may leave when the government forces them to invest. A better way to achieve coordination among followers and achieve the leader's goals is that the manager of the company or the government needs to provide bonuses to followers, like the taxi company pays extra bonuses for serving the customers in rural areas and the government provides subsidies for investing in the poverty areas, which we term as expensive coordination. In this paper, we solve the large-scale sequential expensive coordination problem with a novel RL training scheme. There are several lines of works related to the expensive coordination problem, including mecha- nism design (Nisan & Ronen, 2001) and the principal-agent model (Laffont & Martimort, 2009). However, these works focus more on static decisions (each agent only makes a single decision). To consider sequential decisions, the leader-follower MDP game (Sabbadin & Viet, 2013; 2016) and the RL-based mechanism design (Tang, 2017; Shen et al., 2017) are introduced but most of their works only focus on matrix games or small-scale Markov games, which cannot be applied to the case with the large-scale action or state space. The most related work is M 3 RL (Shu & Tian, 2019) where the leader assigns goals and bonuses by using a simple attention mechanism (summing/averaging the features together) and mind (behaviors) tracking to predict the followers' behaviors and makes response to the followers' behaviors. But they only consider the rule-based followers, i.e., followers with fixed preference, and ignore the followers' behaviors responding to the leader's policy, which significantly simplifies the problem and leads the unreasonability of the model. In the expensive coordination problem, there are two critical issues which should be considered: 1) the leader's long-term decision process where the leader has to consider both the long-term effect of itself and long-term behaviors of the followers when determining his action to incentivise the coordination among followers, which is not considered in (Sabbadin & Viet, 2013; Mguni et al., 2019); and 2) the complex interactions between the leader and followers where the followers will adapt their policies to maximize their own utility given the leader's policy, which makes the training process unstable and hard, if not unable, to converge in large-scale environment, especially when the leader changes his actions frequently, which is ignored by (Tharakunnel & Bhattacharyya, 2007; Shu & Tian, 2019). In this work, we address these two issues in the expensive coordination problem through an abstraction-based deep RL approach. Our main contributions are threefold. (1) We model the leader's decision-making process as a semi- Markov Decision Process (semi-MDP) and propose a novel event-based policy gradient to learn the leader's policy considering the long-term effect (leader takes actions at important points rather than at each step to avoid myopic decisions.) (Section 4.1). (2) A well-performing leader's policy is also highly dependent on how well the leader knows the followers. To predict the followers' behaviors precisely, we show the leader-follower consistency scheme. Based on the scheme, the follower-aware module, the follower-specific attention module, and the sequential decision module are proposed to capture these followers' behaviors and make accurate response to their behaviors (Section 4.2). (3) To accelerate the training process, we propose an action abstraction-based policy gradient algorithm for the followers. This approach is able to reduce followers' decision space and thus simplifies the interaction between the leader and followers as well as accelerates the training process of followers (Section 4.3). Experiments in resource collections, navigation and predator- prey show that our method outperforms the state-of-the-art methods dramatically.

Section Title: RELATED WORKS
  RELATED WORKS Our works are closely related to leader-follower RL, temporal abstraction RL, and event-based RL.

Section Title: Leader-follower RL
  Leader-follower RL The leader-follower RL targets at addressing the issue of expensive coordina- tion where the leader wants to maximize the social benefit (or the leader's self-benefit) by coor- dinating non-cooperative followers through providing them bonuses. Previous works have investi- gated different approaches to solve the expensive coordination, including the vanilla leader-follower MARL (Sabbadin & Viet, 2013; Laum√¥nier & Chaib-draa, 2005), leader semi-MDP (Tharakunnel & Bhattacharyya, 2007), multiple followers and sub-followers MARL (Cheng et al., 2017), follow- ers abstraction (Sabbadin & Viet, 2016), and Bayesian optimization (Mguni et al., 2019). But most of them focus on simple tabular games or small-scale Markov games. The most related work (Shu Published as a conference paper at ICLR 2020 & Tian, 2019) leverages the deep RL approach to compute the leader's policy of assigning goals and bonuses to rule-based followers. But their method performs poorly when the followers are RL- based. In this work, we aim to compute the leader's policy against the RL-based followers in the complex and sequential scenarios.

Section Title: Temporal abstraction RL
  Temporal abstraction RL Our methods are also related to temporal abstraction method (Sutton et al., 1998; Daniel et al., 2016; Bacon et al., 2017; Smith et al., 2018; Zhang & Whiteson, 2019; Vezhn- evets et al., 2016). The basic idea of temporal abstraction is to divide the original one-level decision process into a two-level decision process where the high-level part is to decide the meta goal while the low-level policy is to select the primitive actions. Our leader's decision process is different from those methods mentioned above because the leader's policy can naturally form as an intermittent (temporal abstraction) decision process (semi-MDP) (Tharakunnel & Bhattacharyya, 2007) and it is unnecessary to design the two-level decision process for the leader (since the low-level decision process is the follower). Based on the nature of the leader, a novel training method is introduced.

Section Title: Event-based RL & Planning
  Event-based RL & Planning Previous studies also focus on using events to capture important elements (e.g., whether agent reaches a goal) during the whole episode. Upadhyay et al. (2018) regard the leader's action and the environment feedback as events in the continuous time environ- ment. Becker et al. (2004); Gupta et al. (2018) leverage events to capture the fact that an agent has accomplished some goals. We adopt this idea by depicting the event as the actions taken by the leader at some time steps and design a novel event-based policy gradient to learn the long-term leader's policy.

Section Title: STACKELBERG MARKOV GAMES
  STACKELBERG MARKOV GAMES Our research focuses on single-leader multi-follower Stackelberg Markov Games (SMG) (Mguni et al., 2019; Sabbadin & Viet, 2013), which can be formulated as a tuple G = N , S, A, ‚Ñ¶, P, R, Œ≥ . N is the set of N followers, i.e., |N | = N . S is the set of states. s 0 ‚àà S 0 ‚äÇ S is an initial state and S 0 is the set of initial states. A = √ó k‚ààN A k is the set of joint actions for followers where a k ‚àà A k is an action for the k-th follower. œâ ‚àà ‚Ñ¶ = √ó k‚ààN ‚Ñ¶ k is an action for the leader and œâ k = {g k , b k } ‚àà ‚Ñ¶ k is a goal and a bonus that the leader assigns to the k-th follower. P : S √ó A ‚Üí ‚àÜ(S) is the transition function 1 and R = √ó k‚ààN r k √ó r l is the reward function set where r k : S √ó A √ó ‚Ñ¶ ‚Üí R is the reward function for the k-th follower and r l : S √ó A √ó ‚Ñ¶ ‚Üí R is the reward function for the leader. Œ≥ is the discount factor and a is a joint action of followers. The leader's policy is defined as ¬µ = ¬µ k k‚ààN where ¬µ k : ‚Ñ¶ √ó S ‚Üí ‚àÜ(‚Ñ¶ k ) is the leader's action to the k-th follower given the leader's action in the previous timestep œâ t‚àí1 and the current state s t . ‚àÜ(¬∑) is a probability distribution. The followers' joint policy is defined as œÄ = œÄ k where œÄ k : ‚Ñ¶ k √ó S ‚Üí ‚àÜ(A k ) is the k-th follower policy given the leader's action œâ k t and the current state s t . Given the policy profile of the leader and followers ¬µ, œÄ , the follower's utility is defined as We assume that the leader and followers aim to maximize their own utilities. We define the trajectory œÑ as a sequence of state, leader's action, and followers' actions œâ ‚àí1 , (s t , a t , œâ t ) T t=0 where œâ ‚àí1 is the first step leader's action and is set to zero. (b) The probabilistic graphical model of the pro- posed framework. Dotted line means that Œ≤ affects the final result of œâ indirectly. œâ‚àí1 is set to be zero. In this section, we propose a novel training scheme to train a well-performing leader policy against both rule-based and RL-based followers in the expensive coordination problem. We address the two issues, the leader's long-term decision process and the complex interactions between the leader and followers, with three key steps: (a) we model the leader's decision-making process as a semi- Markov Decision Process (semi-MDP) and propose a novel event-based policy gradient to take actions only at important time steps to avoid myopic policy; (b) to accurately predict followers' be- haviors, we construct a follower-aware module based on the leader-follower consistency, including a novel follower-specific attention mechanism, and a sequential decision module to predict followers' behaviors precisely and make accurate response to these behaviors; and (c) an action abstraction- based policy gradient method for followers is proposed to simplify the decision process for the followers and thus simplify the interaction between leader and followers, and accelerate the conver- gence of the training process.

Section Title: EVENT-BASED TRAJECTORY OPTIMIZATION FOR LEADER
  EVENT-BASED TRAJECTORY OPTIMIZATION FOR LEADER We first describe the event-based trajectory optimization for the leader. As we mentioned above, the leader's decision process can be naturally formulated as a semi-MDP (Tharakun- nel & Bhattacharyya, 2007). Therefore, we firstly describe the basic ideas of semi- MDP using the modified option structure. We define the modified option as a tuple: ¬µ, Œ≤ k k‚ààN where ¬µ is the leader's policy as we defined above and Œ≤ k (s t , œâ t‚àí1 ) : S √ó ‚Ñ¶ ‚Üí [0, 1] is the termination function for the k-th follower, to indicate the probabil- ity whether the leader's action to the k-th follower changes (œâ k t‚àí1 = œâ k t ). Based on these definitions, we formulate the one-step option-state transition function with decay as: t is the joint policy for followers. Notice that this is an extension of the aug- mented process mentioned in (Bacon et al., 2017). Differently, we do not have the low-level policy here (the low-level policy is the follower) and since we only focus on the finite time horizon, Œ≥ is set to be 1. Our modified option is used to depict the long-term decision process for the leader as shown in  Fig. 2 . Now we start to discuss our leader's policy gradient. In fact, it is not easy to directly optimize the leader's utility based on this multi-agent option-state transition function since this form in- cludes leader's different action stages to different followers. Notice that for a sampled trajec- tory, the occurrence of the leader actions is deterministic. Therefore, we can regard the time step and the action the leader takes at that step as an event and define the (universal) event set U T = { t i , œâ k ti |t i ‚â§ T, k ‚àà N }. We use the notation e k i = t i , œâ k ti to represent the leader's action to the k-th follower at step t i , i is the index of the event. Since we focus on the change of the actions from the leader, we further define a set that represents a collection of new actions (œâ k t = œâ k t‚àí1 ) taken by the leader within that trajectory: A T = {e k i |œâ k ti = œâ k ti‚àí1 , t i ‚â§ T, k ‚àà N } ‚äÜ U T , where t i ‚àí 1 is the previous time step. A T represents when and how the leader commits to a new action (an example can be found in Fig. 2a). For brevity, e k j ‚àà A T means e k j ‚àà U T \A T . The probability of Published as a conference paper at ICLR 2020 A T can be represented as: where t j ‚àí 1 is the previous time step for t j . This equation illustrates that the probability of the occurrence of a certain leader's event set within a trajectory. Concretely, the leader changes action to the k-th follower at t i ‚àà e k i while maintaining the same action within the interval from t i ‚àí 1 ‚àà e k i‚àí1 to t i (s.t., t i ‚àà e k i ). Similarly, we can further define the probability of the whole trajectory œÑ as: Comparing with P (A T ), P (œÑ ) includes the probability of the followers as well as the state transition. Do note that our goal is to maximize max A T E P (œÑ ) [R œÑ (T )] , indicating that the leader is required to select an action that can maximize the accumulated reward, where R œÑ (T ) = T t=0 Œ≥ t r l t is the accumulated reward and œÑ is to stress that its accumulated reward is from the trajectory œÑ . Following the REINFORCE trick (Sutton & Barto, 1998), the policy gradient for the termination function and the leader's policy function can be formulated under the following proposition: Proposition 1. The policy gradients for the termination function Œ≤ k (s ti , œâ ti ) and leader's policy function ¬µ k œâ k ti |s ti , œâ ti‚àí1 can be written as: ‚àá Œ∏ J(Œ∏) ‚âà E œÑ ‚àºpœÑ (¬∑) k‚ààN T i=0 I(e k i ) R œÑ (T ) ; ‚àá œë J(œë) ‚âà E œÑ ‚àºpœÑ (¬∑) k‚ààN T i=0 I (e k i ) R œÑ (T ) ; where Œ∏ and œë are the parameters for the termination function Œ≤ k Œ∏ and leader's policy ¬µ k œë . I(¬∑) and I (¬∑) are the piece-wise functions: All the proofs can be found in Appendix A. Proposition 1 implies that under the event-based method, whether the leader's commitment to a new action will induce different policy gradients for both termination function and the policy function. However, from the empirical results, we find that the leader's policy function updates rarely during the whole episode because the policy only updates when the leader commits to a new action, which causes the sample inefficiency. Notice that in fact the leader commits to the same action when e k i / ‚àà A T . Therefore, the policy indication function I (¬∑) can be formulated in an alternative way: This form considers both committing to a new action and maintaining the same actions (Details can be found in Remark 2), which we call the Event-Based Policy Gradient (EBPG) and the previous one as the sparse EBPG respectively. Intuitively, the dense EBPG is better than the sparse EBPG because it updates the leader's policy function more frequently than the sparse one. For example, in time step t, supposing that the leader chooses a wrong action for follower k and receives a negative reward. Then, the leader should learn to diminish the action chosen that state by EBPG. The sparse EBPG only do one PG during before terminating the action (at the committing action step) while the dense one does PG in each step before terminating the action. The latter can provide more signal to correct the wrong action. Experiments also reveal that the dense one is better (Sec. D.3.3).

Section Title: NEURAL NETWORK BASED LEADER
  NEURAL NETWORK BASED LEADER The EBPG approach is able to improve leader's performance. However, it is still very hard for the leader to choose actions considering long-term effect only based on the current state informa- tion. This is because the followers change their behaviors over time according to the leader's policy.

Section Title: Published as a conference paper at ICLR 2020
  Published as a conference paper at ICLR 2020 Therefore, we introduce new modules and training schemes so as to capture the change of the fol- lowers' behaviors as well as the global state. To abstract the complicated state information, we use neural networks to learn the state representation. To capture the followers' behaviors and make accurate response to their behaviors, we design three modules: (1) we exploit the leader-follower consistency under game regularization and policy bound conditions, (2) based on the consistency, a follower-aware module is introduced and (3) based on the follower-aware module, a novel attention mechanism, and sequential decision making module is designed to make accurate response to these followers' behaviors as shown in  Fig. 1 .

Section Title: Leader-Follower Consistency
  Leader-Follower Consistency In previous works, a surge of researches focus on predicting other agents' behaviors through historical information, where the other agents are assumed to be oppo- nents of that agent, which is only suitable for zero-sum games (Zheng et al., 2018; Foerster et al., 2018; He et al., 2016). However, these methods cannot be directly applied to our case because SMG is not zero-sum. We note that Shu & Tian (2019) attempt to directly use the followers' behavior prediction module (use the history of the followers to predict their future actions) but do not analyze when and how it works. To ensure that the leader can predict the followers' behaviors, we introduce the following assumptions.

Section Title: Assumption 1. (Game regularization)
  Assumption 1. (Game regularization) The leader-follower state-action space (A √ó ‚Ñ¶ √ó S) is com- pact and r k is a continuous function w.r.t. ¬µ bounded by R max . This assumption is inspired by (Antos et al., 2008). We only extend it into the multi-agent forms. This assumption indicates that the action and states space should be limited and the reward function for the leader action should be smooth. Assumption 2. (Policy Bound) For any agent k, reward function r k and policy is consistency, i.e., Where C 2 is a constant that satisfies C 2 > 0. œÄ k is the k-th follower's new policy. ¬µ is the leader's new policy. This assumption is inspired by (Mguni et al., 2019). œÄ ‚àík indicates the joint policy without the k-th agent's. This assumption indicates that the change of the leader causes only slightly changes on each followers policy. Based on these two assumptions, we propose a proposition here: Proposition 2. (Leader-Follower Consistency.) If both the assumptions of game regularization and policy bound are satisfied, for ‚àÄ > 0, k ‚àà N , there exists Œ¥ > 0, such that |¬µ ‚àí ¬µ | ‚â§ implies œÄ k ‚àí œÄ k ‚â§ Œ¥, where ¬µ and œÄ k are the new policies for the leader and the k-th follower respectively. This proposition reveals that the change of the leader causes only slightly changes on each follower's policy under the game regularization assumption and the policy bound assumption, which is fun- damental to follower-aware learning. Roughly speaking, the game regularization requires that the states, actions, and rewards are bounded while the policy bound states that a little change of a fol- lower's policy does not change its utility so much. The former is from the game itself and we only focus on the latter. To satisfy the latter, one possible method is to make the ¬µ and ¬µ close because the followers always find the best response to the leader's policy and if the leader changes a little, the followers do not change too much since the new best response to ¬µ is not far away from best response to ¬µ. One direct method is to slow down the learning rate of the leader to make ¬µ and ¬µ close. Moreover, for the leader part, taking the right actions is also an important way to guarantee the second assumption because the taken action will more precisely decrease the probability of huge change of the whole process and stabilize the training process. There is an interesting phenomenon that on one hand, knowing more about the followers can diminish the wrong decision and thus aids the establishment of the consistency. On the other hand, the consistency will further guarantee the accuracy of the follower-aware module. Therefore, they form a positive feedback.

Section Title: Follower-Aware Module
  Follower-Aware Module Based on the leader-follower consistency, we can safely implement the follower-aware module to our network. Before we discuss this module in details, we first define the history for both the leader and followers. For the k-th follower, its history at time step t is a sequence of states, its own actions, and the leader's actions to it, i.e., h k t = (s t , a k t , œâ k t ) t ‚â§t ‚àà H k t Published as a conference paper at ICLR 2020 while the leader's history is the stack of all followers histories h t = h k t k‚ààN ‚àà H l t . Then, we define the history-based leader's policy as: t , where Z is the normalization term, p k is the predicted action probability of the k-th follower and√¢ defines the predicted action (predicted by the leader). p is to stress the output is a probability. Since we cannot directly obtain an accurate estimation of a k , we adopt an alternative way to leverage history information and imitation learning to make a pre- diction of other agents' action probability function p k √¢ k t |s t , h k t (Implementation details can be found in Appendix B) and p(œâ k t |s t ,√¢ t , h t ) is designed using the attention mechanism as well as the sequential decision module presented below.

Section Title: Follower-Specified Attention Mechanism
  Follower-Specified Attention Mechanism Inspired by (Chen et al., 2018), we introduce a follower-specific attention mechanism to identify the important followers where the important fol- lowers are followers who has just finished a task and the leader has to commit new actions to these followers. The attention mechanism is as follows: Where w k is the weight of the k-th follower, A(¬∑) : R ds√ód a k t √ód h k t ‚Üí R dc is a function to blend various information of an agent together (d c means the dimension of the output of the A(¬∑)), f (¬∑) : R dc ‚Üí R 1 is a function to map the blending information to a real number, and c k t is the k-th agent attention value (the compression of history, states and actions for follower k as well as other followers).√¢ k t is the output of p k √¢ k t |s t , h k t , the predicted the k-th follower's action. This attention mechanism is better because it quantifies the importance of each follower in each state through learning while the original methods only adds/averages all the features (s t ,√¢ k t , h k t ) together (Shu & Tian, 2019). Another advantage is that its weights can be visualized to see which follower is important to the leader at current step. (Details can be found in Appendices D.2.1 & D.3.4). c k t then is used by Œ≤ k and ¬µ k .

Section Title: Sequentially Determining Goals and Bonuses
  Sequentially Determining Goals and Bonuses Also notice that the goal and the bonus are sequen- tially correlated. Therefore, it is better for the leader to choose the bonus and the goal sequentially rather than select them independently. Therefore, to consider the goal and bonus jointly when mak- ing a decision, we build a probabilistic graph-based model as: p(œâ k t |s t ,√¢ t , h t ) ‚âà p(g k t ; b k t |c k t ) ‚àù p(b k t |g k t , c k t ) √ó p(g k t |c k t ), the first approximate equation is established because c k t is the compression of (s t ,√¢ t , h t ). p(b k t |g k t , c k t ) and p(g k t |c k t ) means the policy for bonuses and goals (Implementation details can be found in Appendix D.2.1).

Section Title: FOLLOWER ACTION ABSTRACTION POLICY GRADIENT
  FOLLOWER ACTION ABSTRACTION POLICY GRADIENT These methods mentioned above are fully implemented can enhance the performance dramatically. But when facing the RL-based followers, the SMG is still hard to converge. This is because in SMG, the policies of the leader and followers are always changing depending on other agents' performance. To guarantee convergence, the leader can only update its policy when the followers reach (or are near to) the best response policy (Fiez et al., 2019). However, when the followers are RL-based agents, there is no way to ensure the followers' policies are (near) the best response policies in large-scale SMG and the commonly-seen idea is to provide enough training time but it is unbearable in practice due to the limitation of computing power (Mguni et al., 2019). To accelerate the training process, inspired by the action abstraction approach which is commonly- seen in Poker (Brown & Sandholm, 2019; Tuyls et al., 2018) and action abstraction RL (Chandak et al., 2019), we collect the followers' primitive actions sharing the same properties together as a meta policy. Then, the followers only need to select the meta action to make a decision. Therefore, the original game is converted into a meta game, which is easy to solve. Specifically, we define the policy for the k-th follower as: œÄ k t (a k |≈ù) = z œÄ k meta (z|≈ù)œÄ k lower (a|≈ù, z), where≈ù = s, œâ k is the augmented state for the follower (the combination of current state and the leader's action). œÄ k meta (z|≈ù) is the meta policy for the k-th follower and z is the high-level (meta) action. We hypothesize that the lower-level policy (the policy to choose the primitive actions) is already known (rule-based) and deterministic, i.e., œÄ k lower (a k |≈ù, z) = 1. For instance, given the example of the navigation task, the œÄ k meta can be the selection to which landmark to explore while œÄ k lower is a specific route planning algorithm (such Published as a conference paper at ICLR 2020 In this section, we discuss how to design the leader's and followers' loss functions. Loss Functions for the Leaders. The basic structure for the leader is the actor-critic structure (Sut- ton & Barto, 1998). We find that adding regularizers can enhance the leader's performance and we implement the maximum entropy for the leader's policy function as well as the L2 regularization for the termination function, i.e., L enp = ‚àí k œâ k ¬µ k (œâ k |s, h) log ¬µ k (œâ k |s, h) and L reg = Œ≤ 2 . We also use imitation learning to learn the predicted action function p k . Following the same logic of (Shu & Tian, 2019), two baseline functions œÜ g (c t ) and œÜ b (c t ) are also introduced to further reduce the variance. Details can be found in Appendix B.

Section Title: Loss Functions for the RL-Based Followers
  Loss Functions for the RL-Based Followers The basic structure for each follower is also based on the actor-critic structure. We leverage the action abstraction policy gradient as we mentioned above. The learning rate between the leader and follower should satisfy the two time-scale principle (Roughly speaking, the leader learns slower than the follower(s)), similar to (Borkar, 1997). Details can be found in Appendix B and the pseudo-code can be found in Appendix C.

Section Title: EXPERIMENTAL RESULTS
  EXPERIMENTAL RESULTS

Section Title: SETUP
  SETUP

Section Title: Tasks.
  Tasks. We evaluate the follow- ing tasks to testify the performance of our proposed method. All of these tasks are based on SMG men- tioned above. (1) resource collec- tions: each follower collects three types of resources including its pre- ferred one and the leader can choose two bonuses levels (Shu & Tian, 2019); (2) multi-bonus resources col- lections: based on (1), the leader can choose four bonuses levels; (3) modified navigation: followers are required to navigate some landmarks and after one of the landmarks is reached, the reached land- mark disappears and new landmark will appear randomly. (4) modified predator-prey: followers are required to capture some randomly moving preys, prizes will be given after touching them. Both (3) and (4) are based on (Lowe et al., 2017) and we modify them into our SMG setting. Moreover, to increase the difficulty, in each episode, the combinations of the followers will change, i.e., in each task, there are 40 different followers and at each episode, we randomly choose some followers to play the game. More details can be found in Appendix D.

Section Title: Baselines & Ablations
  Baselines & Ablations To evaluate our method, we compare a recently proposed method as our baseline: M 3 RL (Shu & Tian, 2019). We do not include other baselines because other methods Published as a conference paper at ICLR 2020 cannot be used in our problems, as justified in (Shu & Tian, 2019). For the ablations of the leader part, we choose: (1) ours: the full implementation of our method. (2) ours w/o EBPG: removing the event-based policy gradient part; (3) ours w/o Attention: replacing follower-specified attention model by the original attention model mentioned in (Shu & Tian, 2019). For the follower part, we choose (a) with rule-based follower (b) with vanilla RL-based follower, and (c) with action abstraction RL-based follower to testify the ability of our methods when facing different followers.

Section Title: Hyper-Parameters
  Hyper-Parameters Our code is implemented in Pytorch (Paszke et al., 2017). If no special men- tion, the batch size is 1 (online learning). Similar to (Shu & Tian, 2019), we set the learning rate as 0.001 for the leader's critic and followers while 0.0003 for the leader's policy. The optimization al- gorithm is Adam (Kingma & Ba, 2014). Our method takes less than two days to train on a NVIDIA Geforce GTX 1080Ti GPU in each experiment. For the loss function, we set the Œª 1 = 0.01 and Œª 2 = 0.001. The total training episode is 250, 000 for all the tasks (including both the rule-based followers and the RL-based followers). To encourage exploration, we use the Œπ-greedy 2 . For the leader, the exploration rate is set to 0.1 and slightly decreases to zero (5000 episode). For the followers, the exploration rate for each agent is always 0.3 (except for the noise experiments).

Section Title: LEARNING EFFICIENCY
  LEARNING EFFICIENCY The quantitative results with dif- ferent tasks are shown in  Figs. 3  & 4. For the rule-based fol- lowers, from  Fig. 3 , we find that our method outperforms the state-of-the-art method in all the tasks, showing that our method is sample efficient and fast to coverage. There is an interesting phenomenon that in the task of multi-bonus resource collections and navigation, only our method obtains a positive reward, indi- cating that our method can work well in complicated environments. For ablations, we can see that ours w/o attention and ours w/o EBPG are worse than ours, representing these components do en- hance the performance. For the RL-based followers, from  Fig. 4 , we observe that when facing the RL-based method with action abstraction, our approach outperforms the baseline method in all the tasks (in predator-prey game, the reward for ours is twice as that of the state-of-the-art). We also find that without action abstraction, the reward is less than zero, revealing that the abstraction does play a crucial role in stabilizing training.

Section Title: ROBUSTNESS
  ROBUSTNESS This experiment is to evaluate whether our method is robust to the noise, i.e., the follower randomly takes actions. We make this experiment by introducing noise into the follower decision. From  Table 1 , we can find that our method reaches a higher total reward (more than 5) among all the environment with noise than the state-of-the-art, indicating that our method is robust to the noise. We also observe that the total reward for the baseline method becomes lower with the increase of the noise while our method is more robust to the change. Moreover, for the incentive (the total gain), we find that our method gains much more incentive than the state-of-the-art method, showing that our method coordinates have a better coordination the followers than the state-of-the-art method.

Section Title: MORE EXPERIMENTS
  MORE EXPERIMENTS We also do a substantial number of experiments. However, due to the space limitation, we can only provide some results here: (1) The total incentives: incentive can reveal the performance of successful rate interacting with the followers. Our method outperforms the state-of-the-art method, indicating that our method has a better ability to interact with the followers. (2) Sparse EBPG: we compare the performance gap between sparse EBPG and (dense) EBPG. This results show that the sparse one is worse than the dense one, supporting the assumption that the dense signal can improve 2 Normally it is called the decayed -greedy. We use Œπ instead of to avoid notation abuse. Published as a conference paper at ICLR 2020 the sample efficiency. (3) Visualizing attention: We visualize the attention module to find what it ac- tually learns and the result indicates that our attention mechanism does capture the followers whom the leader needs to assign bonuses to. (4) Two time-scale training: We testify whether our two time- scale training scheme works and the ablation shows that this scheme does play an important role in improving the performance of both the leader and the followers. (5) The committing interval: We observe that the dynamic committing interval (our method) performs better than the one with fixed committing intervals. (6) Reward for RL-based followers: we show the reward for the followers, which can provide the situation of the followers. The result represents that our method aids the followers to gain more than the state-of-the-art method. (7) Number of RL-based followers: finally, we testify our method in cases with different number of RL-based followers. The result shows that our method always performs well. The full results can be found in Appendix D.

Section Title: CONCLUSION REMARKS
  CONCLUSION REMARKS This paper proposes a novel RL training scheme for Stackelberg Markov Games with single leader and multiple self-interested followers, which considers the leader's long-term decision process and complicated interaction between followers with three contributions. 1) To consider the long-term effect of the leader's behavior, we develop an event-based policy gradient for the leader's policy. 2) To predict the followers' behaviors and make accurate response to their behaviors, we exploit the leader-follower consistency to design a novel follower-aware module and follower-specific attention mechanism. 3) We propose an action abstraction-based policy gradient algorithm to accelerate the training process of followers. Experiments in resource collections, navigation, and predator-prey game reveal that our method outperforms the state-of-the-art methods dramatically. We are willing to highlight that SMGs contribute to the RL (especially MARL) community with three key aspects: 1). As we mentioned in the Introduction, most of the existing MARL methods assume that all the agents are willing to sacrifice themselves to maximize the total rewards, which is not true in many real-world non-cooperative scenarios. On the contrary, our proposed method realistically assumes that agents are self-interested. Thus, SMGs provide a new scheme focusing more on the self-interested agents. We think this aspect is the most significant contribution to the RL community. 2). The SMGs can be regarded as the multi-agent system with different roles (the leader and the followers) (Wilson et al., 2008) and our method provides a solution to that problem. 3). Our methods also contribute to the hierarchical RL, i.e., it provides a non-cooperative training scheme between the high-level policy (the leaders) and the low-level policy (the followers), which plays an important role when the followers are self-interested. Moreover, our EBPG also propose an novel policy gradient method for the temporal abstraction structure. There are several directions we would like to investigate to further extend our SMG model: i) we will consider multiple cooperative/competitive leaders and multiple self-interested followers, which is the case in the labor market, ii) we will consider multi-level leaders, which is the case in the hierarchical organizations and companies and iii) we will consider the adversarial attacks to our SMG model, which may induce extra cost to the leader for efficient coordination. We believe that our work is a preliminary step towards a deeper understanding of the leader-follower scheme in both research and the application to society.
  1 Notice that the transition function does not depend on the leader's action.

```
