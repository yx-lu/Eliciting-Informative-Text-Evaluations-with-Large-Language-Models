Title:
```
None
```
Abstract:
```
Reinforcement learning agents that operate in diverse and complex environments can benefit from the structured decomposition of their behavior. Often, this is addressed in the context of hierarchical reinforcement learning, where the aim is to decompose a policy into lower-level primitives or options, and a higher-level meta-policy that triggers the appropriate behaviors for a given situation. However, the meta-policy must still produce appropriate decisions in all the states. In this work, we propose a policy design that decomposes into primitives, similarly to hierarchical reinforcement learning, but without an explicit high-level meta-policy. Instead, each primitive can decide for themselves whether they wish to act in the current state. We use an information-theoretic mechanism for enabling this decentralized decision: each primitive chooses how much information it needs about the current state to make a decision and the primitive that requests the most information about the current state acts in the environment. Regularizing the primitives to use as little information as possible leads to natural competition and specialization. We experimentally demonstrate that this policy architecture improves over both flat and hierarchical policies in terms of generalization.
```

Figures/Tables Captions:
```
Figure 1: Illustration of our model (Left): An intrinsic competition mechanism, based on the amount of information each primitive requests, is used to select a primitive to be active for a given input. Each primitive focuses on distinct features of the environment; in this case, one policy focuses on boxes, a second one on gates, and the third one on spheres. Right: The primitive-selection mechanism of our model. The primitive with most information acts in the environment and gets the reward.
Figure 2: Snapshots of motions learned by the policy. Top: Reference motion clip. Middle: Simulated character imitating the reference motion. Bottom: Probability of selecting each primitive. Baselines. We compare our proposed method to the following baselines: a) Option Critic (Bacon et al., 2017) - We extended the author's implementation of the Option Critic architecture and experimented with multiple variations in terms of hyperparameters and state/goal encoding. None of these yielded reasonable performance in partially observed tasks, so we omit it from the results. b) MLSH (Meta-Learning Shared Hierarchy) (Frans et al., 2017) - This method uses meta-learning to learn sub-policies that are shared across tasks along with learning a task-specific high-level master. It also requires a phase-wise training schedule between the master and the sub-policies to stabilize training. We use the MLSH implementation provided by the authors. c) Transfer A2C: In this method, we first learn a single policy on the one task and then transfer the policy to another task, followed by fine-tuning in the second task.
Figure 3: Convergence of four primitives on Four Room Maze: Left: We trained four primitives on the Four Room Maze task, where the goal was sampled from one of the two fixed goals. We see that the proposed algorithm is able to learn four primitives. Right: We transfer the learned primitives to the scenario where the goal is sampled from one of the four possible goals. The checkpointed model is ran on 100 different episodes (after a fixed number of steps/updates) and the normalized frequency of activation of the different primitives is plotted.
Figure 4: Embeddings visualizing the states (S) and goals (G) which each primitive is active in, and the actions (A) proposed by the primitives for the motion imitation tasks. A total of four primitives are trained. The primitives produce distinct clusters.
Figure 5: Multitask training. Each panel corresponds to a different training setup, where different tasks are denoted A, B, C, ..., and a rectangle with n circles corresponds to an agent composed of n primitives trained on the respective tasks. Top row: activation of primitives for agents trained on single tasks. Bottom row: Retrain: Two primitives are trained on task A and transferred to task B. The results (success rates) indicate that the multi-primitive model is substantially more sample efficient than the baseline (transfer A2C). Copy and Combine: More primitives are added to the model over time in a plug-and-play fashion (two primitives are trained on task A; the model is extended with a copy of the two primitives; the resulting four-primitive model is trained on task B.) This is more sample efficient than other strong baselines, such as (Frans et al., 2017; Bacon et al., 2017). Zero-Shot Generalization: A set of primitives is trained on task C, and zero-shot generalization to task A and B is evaluated. The primitives learn a form of spatial decomposition which allows them to be active in both target tasks, A and B. The checkpointed model is ran on 100 different episodes, and the normalized frequency of activation of the different primitives is plotted.
Figure 6: Continual Learning Scenario: The plot on the left shows that the primitives remain activated. The solid green line shows the boundary between the tasks. The plot on the right shows the number of samples required by our model and the transfer baseline model across different tasks. We observe that the proposed model takes fewer steps than the baseline (an A2C policy trained in a similar way), and the gap in terms of the number of samples keeps increasing as tasks become harder. The checkpointed model is ran on 100 different episodes (after a fixed number of steps/updates) and the normalized frequency of activation of the different primitives is plotted.
Figure 7: Left: Multitask setup, where we show that we are able to train eight primitives when training on a mixture of four tasks in the Minigrid environment. Here, the x-axis denotes the number of frames (timesteps). Right: Success rates of the different methods on the Ant Maze tasks. Success rate is measured as the number of times the ant is able to reach the goal (based on 500 sampled trajectories).
```

Main Content:
```

Section Title: INTRODUCTION
  INTRODUCTION Learning policies that generalize to new environments or tasks is a fundamental challenge in re- inforcement learning. While deep reinforcement learning has enabled training powerful policies, which outperform humans on specific, well-defined tasks ( Mnih et al., 2015 ), their performance often diminishes when the properties of the environment or the task change to regimes not encountered during training. This is in stark contrast to how humans learn, plan, and act: humans can seamlessly switch between different aspects of a task, transfer knowledge to new tasks from remotely related but essentially distinct prior experience, and combine primitives (or skills) used for distinct aspects of different tasks in meaningful ways to solve new problems. A hypothesis hinting at the reasons for this discrepancy is that the world is inherently compositional, such that its features can be described by compositions of small sets of primitive mechanisms ( Parascandolo et al., 2017 ). Since humans seem to benefit from learning skills and learning to combine skills, it might be a useful inductive bias for the learning models as well. This is addressed to some extent by the hierarchical reinforcement learning (HRL) methods, which focus on learning representations at multiple spatial and temporal scales, thus enabling better explo- ration strategies and improved generalization performance ( Dayan & Hinton, 1993 ;  Sutton et al., 1999b ;  Dietterich, 2000 ;  Kulkarni et al., 2016 ). However, hierarchical approaches rely on some form of learned high-level controller, which decides when to activate different components in the hierarchy. While low-level sub-policies can specialize to smaller portions of the state space, the top-level controller (or master policy) needs to know how to deal with any given state. That is, it should provide optimal behavior for the entire accessible state space. As the master policy is trained Published as a conference paper at ICLR 2020 on a particular state distribution, learning it in a way that generalizes to new environments effectively becomes the bottleneck for such approaches ( Sasha Vezhnevets et al., 2017 ;  Andreas et al., 2017 ). We argue, and empirically show, that in order to achieve better generalization, the interaction between the low-level primitives and the selection thereof should itself be performed without requiring a single centralized network that understands the entire state space. We, therefore, propose a decentralized approach as an alternative to standard HRL, where we only learn a set of low-level primitives without learning an explicit high-level controller. In particular, we construct a factorized representation of the policy by learning simple primitive policies, which focus on distinct regions of the state space. Rather than being gated by a single meta-policy, the primitives directly compete with one another to determine which one should be active at any given time, based on the degree to which their state encoders "recognize" the current state input. While, technically, the competition between primitives implicitly realizes a global selection mechanism, we consider our model decentralized in the sense that individual primitives can function on their own, and can be combined in new ways, without relying on an explicit high-level controller. We frame the problem as one of information transfer between the current state and a dynamically selected primitive policy. Each policy can, by itself, decide to request information about the current state, and the amount of information requested is used to determine which primitive acts in the current state. Since the amount of state information that a single primitive can access is limited, each primitive is encouraged to use its resources wisely. Constraining the amount of accessible information in this way naturally leads to a decentralized competition and decision mechanism where individual primitives specialize in smaller regions of the state space. We formalize this information-driven objective based on the variational information bottleneck. The resulting set of competing primitives achieves both a meaningful factorization of the policy and an effective decision mechanism for which primitives to use. Importantly, not relying on a centralized meta-policy enables the individual primitive mechanisms can be recombined in a plug-and-play fashion, and the primitives can be transferred seamlessly to new environments.

Section Title: Contributions
  Contributions In summary, the contributions of our work are as follows: (1) We propose a method for learning and operating a set of functional primitives in a decentralized way, without requiring an explicit high-level meta-controller to select the active primitives (see  Fig. 1  for illustration). (2) We introduce an information-theoretic objective, the effects of which are twofold: a) it leads to the specialization of individual primitives to distinct regions of the state space, and b) it enables a competition mechanism, which is used to select active primitives in a decentralized manner. (3) We demonstrate the superior transfer learning performance of our model, which is due to the flexibility of the proposed framework regarding the dynamic addition, removal, and recombination of primitives. Decentralized primitives can be successfully transferred to larger or previously unseen environments, and outperform models with an explicit meta-controller for primitive selection.

Section Title: PRELIMINARIES
  PRELIMINARIES We consider a Markov decision process (MDP) defined by the tuple (S, A, P, r, γ), where the state space S and the action space A may be discrete or continuous. The environment emits a bounded reward r : S × A → [r min , r max ] on each transition and γ ∈ [0, 1) is the discount factor. π(.|s) denotes a policy over the actions given the current state s. R(π) = E π [ t γ t r(s t )] denotes the expected total return when an agent follows the policy π. The standard objective in reinforcement learning is to maximize the expected total return R(π). We use the concept of the information bottleneck ( Tishby et al., 2000 ) to learn compressed representations. The information bottleneck objective is formalized as minimizing the mutual information of a bottleneck representation layer with the input while maximizing its mutual information with the corresponding output. This type of input compression has been shown to improve generalization ( Achille & Soatto, 2016 ;  Alemi et al., 2016 ).

Section Title: INFORMATION-THEORETIC LEARNING OF DISTINCT PRIMITIVES
  INFORMATION-THEORETIC LEARNING OF DISTINCT PRIMITIVES Our goal is to learn a policy, composed of multiple primitive sub-policies, to maximize the expected reward over T -step interactions for a distribution of tasks. Simple primitives which focus on solving a part of the given task (and not the complete task) should generalize more effectively, as they can be applied to similar aspects of different tasks (subtasks) even if the overall objective of the tasks are drastically different. Learning primitives in this way can also be viewed as learning a factorized representation of a policy, which is composed of several independent policies. Our proposed approach consists of three mechanisms: 1) a mechanism for restricting a particular primitive to a subset of the state space; 2) a competition mechanism between primitives to select the most effective primitive for a given state; 3) a regularization mechanism to improve the generalization performance of the policy as a whole. We consider experiments with both fixed and variable sets of primitives and show that our method allows for primitives to be added or removed during training, or recombined in new ways. Each primitive is represented by a differentiable, parameterized function approximator, such as a neural network.

Section Title: PRIMITIVES WITH AN INFORMATION BOTTLENECK
  PRIMITIVES WITH AN INFORMATION BOTTLENECK To encourage each primitive to encode information from a particular part of state space, we limit the amount of information each primitive can access from the state. In particular, each primitive has an information bottleneck with respect to the input state, preventing it from using all the information from the state. We define the overall policy as a mixture of primitives, π(a | s) = k c k π k (a | s) , where π k (a | s) denotes the k th primitive and c k = δ kk for k ∼ p(k | s). We denote the probability of selecting the k th primitive as α k (s) := p(k | s). Rather than learning an explicit model for p(k | s), however, we impose an information-based mechanism for selecting primitives, wherein we limit the amount of information each primitive can contain and select the ones that request the most information about the state. To implement an information bottleneck, we design each of the K primitives to be composed of an encoder p enc (z k | s) and a decoder p dec (a | z k ), together forming the primitive policy, The encoder output z k is meant to represent the information about the current state s that an individual primitive k believes is important to access in order to perform well. The decoder takes this encoded information and produces a distribution over the actions a. Following the variational information bottleneck objective ( Alemi et al., 2016 ), we penalize the KL divergence of p enc (z k |s) and a prior p(z), In practice, we estimate the marginalization over z using a single sample throughout our experiments.

Section Title: Published as a conference paper at ICLR 2020
  Published as a conference paper at ICLR 2020 In other words, a primitive pays an "information cost" proportional to L k for accessing the information about the current state. In the experiments below, we fix the prior to be a unit Gaussian. In the general case, we can learn the prior as well and include its parameters in θ. The information bottleneck encourages each primitive to limit its knowledge about the current state, but it will not prevent multiple primitives from specializing to similar parts of the state space. To mitigate this redundancy, and to make individual primitives focus on different regions of the state space, we introduce an information-based competition mechanism to encourage diversity among the primitives.

Section Title: COMPETING INFORMATION-CONSTRAINED PRIMITIVES
  COMPETING INFORMATION-CONSTRAINED PRIMITIVES We can use the information measure from equation 1 to define a selection mechanism for the primitives without having to learn a centralized meta-policy. The intuition is that the information content of an individual primitive encodes its effectiveness in a given state s such that the primitive with the highest value L k should be activated in that particular state. In particular, we set α k = Z −1 exp(βL k ) to obtain a distribution over k as a function of the information content, activating the primitives with the highest information content. Here, Z = k exp(βL k ) is a normalization constant. This mechanism enables competition between primitives, leading them to focus on parts of the state space that they "understand" well and letting others act in other parts.

Section Title: Trading reward and information
  Trading reward and information To perform proper credit assignment, the environment reward is distributed to primitives according to their participation in the global decision, i.e. the reward r k given to the k th primitive is weighted by its selection coefficient, such that r k = α k r, with r = k r k . Hence, a primitive can potentially get a higher reward when deciding to act, but it also pays a higher price for accessing more information about the current state. The information bottleneck and the competition mechanism, when combined with the overall reward maximization objective, will lead to specialization of individual primitives to distinct regions in the state space. That is, each primitive should specialize in a part of the state space that it can reliably associate rewards with. Since the entire ensemble still needs to understand all of the state space for the given task, different primitives need to encode and focus on different parts of the state space.

Section Title: REGULARIZING PRIMITIVE SELECTION
  REGULARIZING PRIMITIVE SELECTION The objective described above will optimize the expected return while minimizing the information content of individual primitives. This is not sufficient, however, as it might lead to highly unbalanced outcomes: some primitives might be more active initially and learn to become even more active, completely disabling other primitives. Thus, in addition to minimizing each primitive's absolute information content, we need to normalize their activity with respect to each other. To do so, we penalize their information content in proportion to their activation by adding a regularization term of the form Note that this can be rewritten (see Appendix A) as L reg = −H(α) + LSE(L 1 , . . . , L K ) , where H(α) is the entropy of α, and LSE is the LogSumExp function, LSE(x) = log( j e xj ). Thus, minimizing L reg increases the entropy of α, leading to a diverse set of primitive selections, in turn, ensuring that different combinations of the primitives are used. Similarly, LSE approximates the maximum of its arguments, LSE(x) ≈ max j x j , and, therefore, penalizes the dominating L k terms, thus equalizing their magnitudes.

Section Title: OBJECTIVE AND ALGORITHM SUMMARY
  OBJECTIVE AND ALGORITHM SUMMARY Our overall objective function consists of 3 terms, 1. The expected return from the standard RL objective, R(π) which is distributed to the primitives according to their participation, Published as a conference paper at ICLR 2020 2. The individual bottleneck terms leading the individual primitives to focus on specific parts of the state space, L k for k = 1, . . . , K, 3. The regularization term applied to the combined model, L reg . The overall objective for the k th primitive thus takes the form: J k (θ) ≡ E π θ [r k ] − β ind L k − β reg L reg , (3) where E π θ denotes an expectation over the state trajectories generated by the agent's policy, r k = α k r is the reward given to the kth primitive, and β ind , β reg are the parameters controlling the impact of the respective terms.

Section Title: Implementation
  Implementation In our experiments, the encoders p enc (z k | s) and decoders p dec (a | z k ) (see.  Fig. 1 ) are represented by neural networks, the parameters of which we denote by θ. Actions are sampled through each primitive every step. While our approach is compatible with any RL method, we maximize J(θ) computed on-policy from the sampled trajectories using a score function estimator ( Williams, 1992 ;  Sutton et al., 1999a ) specifically A2C ( Mnih et al., 2016 ) (unless otherwise noted). Every experimental result reported has been averaged over 5 random seeds. Our model introduces 2 extra hyper-parameters β ind , β reg .

Section Title: RELATED WORK
  RELATED WORK There are a wide variety of hierarchical reinforcement learning approaches( Sutton et al., 1998 ;  Dayan & Hinton, 1993 ;  Dietterich, 2000 ). One of the most widely applied HRL framework is the Options framework (( Sutton et al., 1999b )). An option can be thought of as an action that extends over multiple timesteps, thus providing the notion of temporal abstraction or subroutines in an MDP. Each option has its own policy (which is followed if the option is selected) and the termination condition (to stop the execution of that option). Many strategies are proposed for discovering options using task-specific hierarchies, such as pre-defined sub-goals ( Heess et al., 2017 ), hand-designed features ( Florensa et al., 2017 ), or diversity-promoting priors ( Daniel et al., 2012 ;  Eysenbach et al., 2018 ). These approaches do not generalize well to new tasks.  Bacon et al. (2017)  proposed an approach to learn options in an end-to-end manner by parameterizing the intra-option policy as well as the policy and termination condition for all the options. Eigen-options ( Machado et al., 2017 ) use the eigenvalues of the Laplacian (for the transition graph induced by the MDP) to derive an intrinsic reward for discovering options as well as learning an intra-option policy. In this work, we consider a sparse reward setup with high dimensional action spaces. In such a scenario, performing unsupervised pretraining or using auxiliary rewards leads to much better performance ( Frans et al., 2017 ;  Florensa et al., 2017 ;  Heess et al., 2017 ). Auxiliary tasks such as motion imitation have been applied to learn motor primitives that are capable of performing a variety of sophisticated skills ( Liu & Hodgins, 2017 ;  Peng et al., 2017 ;  Merel et al., 2019b ;a). Our work is also related to the Neural Module Network family of architectures ( Andreas et al., 2017 ;  Johnson et al., 2017 ;  Rosenbaum et al., 2019 ) where the idea is to learn modules that can perform some useful computation like solving a subtask and a controller that can learn to combine these modules for solving novel tasks. More recently,  Wu et al. (2019)  proposed a framework for using diverse suboptimal world models to learn primitive policies. The key difference between our approach and all the works mentioned above is that we learn functional primitives without requiring any explicit high-level meta-controller or master policy.

Section Title: EXPERIMENTAL RESULTS
  EXPERIMENTAL RESULTS In this section, we briefly outline the tasks that we used to evaluate our proposed method and direct the reader to the appendix for the complete details of each task along with the hyperparameters used for the model. We designed experiments to address the following questions: a) Learning primitives - Can an ensemble of primitives be learned over a distribution of tasks? b) Transfer Learning using primitives - Can the learned primitives be transferred to unseen/unsolvable sparse environments? c) Comparison to centralized methods - How does our method compare to approaches where the primitives are trained using an explicit meta-controller, in a centralized way?

Section Title: LEARNING ENSEMBLES OF FUNCTIONAL PRIMITIVES
  LEARNING ENSEMBLES OF FUNCTIONAL PRIMITIVES We evaluate our approach on a number of RL environments to demonstrate that we can indeed learn sets of primitive policies focusing on different aspects of a task and collectively solving it.

Section Title: Four Room Maze
  Four Room Maze We consider the Four-rooms gridworld environment ( Sutton et al., 1999c ) where the agent has to navigate its way through a grid of four interconnected rooms to reach a goal position within the grid. We consider the scenario where the starting position of the agent is fixed, but the goal is sampled from a discrete set.  Fig. 3  shows that the proposed algorithm can learn four primitives. Refer to Appendix F for more details.

Section Title: Motion Imitation
  Motion Imitation To evaluate the proposed method in terms of scalability, we present a series of tasks from the motion imitation domain, showing that we can use a set of distinct primitives for imitation learning. In these tasks, we train a simulated 2D biped character to perform a variety of highly dynamic skills by imitating motion capture clips recorded from human actors. Each mocap https://github.com/jeanharb/option_critic https://github.com/openai/mlsh Published as a conference paper at ICLR 2020 clip is represented by a target state trajectory τ * = {s * 0 , s * 1 , ..., s * T }, where s * t denotes the target state at timestep t. The input to the policy is augmented with a goal g t = {s * t+1 , s * t+2 }, which specifies the the target states for the next two timesteps. Both the state s t and goal g t are then processed by the encoder p enc (z t |s t , g t ). The repertoire of skills consists of 8 clips depicting different types of walks, runs, jumps, and flips. The motion imitation approach closely follows  Peng et al. (2018) . To analyze the specialization of the various primitives, we computed 2D embeddings of states and goals which each primitive is active in, and the actions proposed by the primitives.  Fig. 4  illustrates the embeddings computed with t-SNE ( van der Maaten & Hinton, 2008 ). The embeddings show distinct clusters for the primitives, suggesting a degree of specialization of each primitive to certain states, goals, and actions. We evaluate our model in a partially- observable 2D multi-task environment called Minigrid, similar to the one intro- duced in ( Chevalier-Boisvert et al., 2018 ). The environment is a two-dimensional grid with a single agent, impassable walls, and many objects scattered in the environment. The agent is provided with a natural lan- guage string that specifies the task that the agent needs to complete. The setup is partially observable, and the agent only gets the small, egocentric view of the grid (along with the natural language task de- scription). We consider three tasks here: the Pickup task (A), where the agent is re- quired to pick up an object specified by the goal string, the Unlock task (B) where the agent needs to unlock the door (there could be multiple keys in the environment, and the agent needs to use the key which matches the color of the door) and the UnlockPickup task (C), where the agent first needs to unlock a door that leads to another room. In this room, the agent needs to find and pick up the object specified by the goal string. Additional implementation details of the environment are provided in appendix D. Details on the agent model can be found in appendix D.3. We train agents with varying numbers of primitives on various tasks - concurrently, as well as in transfer settings. The different experiments are summarized in  Figs. 5  and 7. An advantage of the multi-task setting is that it allows for quantitative interpretability as to when and which primitives are being used. The results indicate that a system composed of multiple primitives generalizes more easily to a new task, as compared to a single policy. We further demonstrate that several primitives can be combined dynamically and that the individual primitives respond to stimuli from new environments when trained on related environments.

Section Title: DO LEARNED PRIMITIVES HELP IN TRANSFER LEARNING?
  DO LEARNED PRIMITIVES HELP IN TRANSFER LEARNING? We evaluate our approach in the settings where the adaptation to the changes in the task is vital. The argument in favor of modularity is that it enables better knowledge transfer between related tasks. Naturally, the transfer is easier when the tasks are closely related, as the model will only need to learn how to compose the already-learned primitives. In general, it is difficult to determine how closely related two tasks are, however, and the inductive bias of modularity could even be harmful if the two tasks are very different. In such cases, we could add new primitives (which would need to be learned) and still obtain a sample-efficient transfer, as some part of the task structure would already have been captured by the pretrained primitives. This approach can be extended towards adding primitives during training, providing a seamless way to combine knowledge about different tasks to solve more complex tasks. We investigate here the transfer properties of a primitive trained in one environment and transferred to a different one. Results are shown in  Fig. 5 .

Section Title: Continuous control for ant maze
  Continuous control for ant maze chosen from a set of available goals at the start of each episode. We pretrain a policy (see model details in Appendix G.1) with a motion reward in an environment which does not have any walls (similar to  Haarnoja et al. (2018) ), and then transfer the policy to the second task where the ant has to navigate to a random goal chosen from one of the 3 (or 10) available goal options. For our model, we make four copies of the pretrained policies and then finetune the model using the pretrained policies as primitives. We compare to both MLSH ( Frans et al., 2017 ) and option-critic ( Bacon et al., 2017 ). All these baselines have been pretrained in the same manner. As evident from  Fig. 7 , our method outperforms the other approaches. The fact that the initial policies successfully adapt to the transfer environment underlines the flexibility of our approach.

Section Title: Zero Shot Generalization
  Zero Shot Generalization The purpose of this experiment is to show that the model consisting of multiple primitives is somewhat able to decompose the task C into its subtasks, A and B. The better this decomposition is the better should the model transfer to the individual subtasks. In order to test this, we trained a set of 4 primitives on task C, and then evaluate them (without finetuning) on tasks A and B. We note that the ensemble is able to solve the transfer tasks, A and B, successfully 72% of the time, while a monolithic policy's success rate is 38%. This further shows that the primitives learn meaningful decompositions.

Section Title: Continual Learning: 4 Rooms Scneario
  Continual Learning: 4 Rooms Scneario positions then transfer (and finetune) on eight-goal positions. The results are shown in  fig. 6 . The proposed method achieves better sample efficiency as compared to training a single monolithic policy.

Section Title: SUMMARY AND DISCUSSION
  SUMMARY AND DISCUSSION We present a framework for learning an ensemble of primitive policies that can collectively solve tasks without learning an explicit master policy. Rather than relying on a centralized, learned meta-controller, the selection of active primitives is implemented through an information-theoretic mechanism. The learned primitives can be flexibly recombined to solve more complex tasks. Our experiments show that, on a partially observed "Minigrid" task and a continuous control "Ant Maze" walking task, our method can enable better transfer than flat policies and hierarchical RL baselines, including the Meta-learning Shared Hierarchies model and the Option-Critic framework. On Minigrid, we show how primitives trained with our method can transfer much more successfully to new tasks. On the Ant Maze, we show that primitives initialized from a pretrained walking control can learn to walk to different goals in a stochastic, multi-modal environment with nearly twice the success rate of a more conventional hierarchical RL approach, which uses the same pretraining but a centralized high-level policy. The proposed framework could be very attractive for continual learning settings, where one could add more primitive policies over time. Thereby, the already learned primitives would keep their focus on particular aspects of the task, and newly added ones could specialize on novel aspects.

Section Title: ACKNOWLEDGEMENTS
  ACKNOWLEDGEMENTS

```
