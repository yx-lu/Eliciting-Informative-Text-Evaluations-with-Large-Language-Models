Title:
```
Under review as a conference paper at ICLR 2020 A FINITE-TIME ANALYSIS OF Q-LEARNING WITH NEURAL NETWORK FUNCTION APPROXIMATION
```
Abstract:
```
Q-learning with neural network function approximation (neural Q-learning for short) is among the most prevalent deep reinforcement learning algorithms. De- spite its empirical success, the non-asymptotic convergence rate of neural Q- learning remains virtually unknown. In this paper, we present a finite-time analy- sis of a neural Q-learning algorithm, where the data are generated from a Markov decision process and the action-value function is approximated by a deep ReLU neural network. We prove that neural Q-learning finds the optimal policy with O(1/ √ T ) convergence rate if the neural function approximator is sufficiently overparameterized, where T is the number of iterations. To our best knowledge, our result is the first finite-time analysis of neural Q-learning under non-i.i.d. data assumption.
```

Figures/Tables Captions:
```
Table 1: Comparison with existing finite-time analyses of Q-learning.
```

Main Content:
```

Section Title: INTRODUCTION
  INTRODUCTION Q-learning has been shown to be one of the most important and effective learning strategies in Reinforcement Learning (RL) over the past decades ( Watkins & Dayan, 1992 ;  Schmidhuber, 2015 ;  Sutton & Barto, 2018 ), where the agent takes an action based on the action-value function (a.k.a., Q- value function) at the current state. Recent advance in deep learning has also enabled the application of Q-learning algorithms to large-scale decision problems such as mastering Go ( Silver et al., 2016 ; 2017), robotic motion control ( Levine et al., 2015 ;  Kalashnikov et al., 2018 ) and autonomous driving ( Shalev-Shwartz et al., 2016 ;  Schwarting et al., 2018 ). In particular, the seminal work by  Mnih et al. (2015)  introduced the Deep Q-Network (DQN) to approximate the action-value function and achieved a superior performance versus a human expert in playing Atari games, which triggers a line of research on deep reinforcement learning such as Double Deep Q-Learning ( Van Hasselt et al., 2016 ) and Dueling DQN ( Wang et al., 2016 ). Apart from its widespread empirical success in numerous applications, the convergence of Q- learning and temporal difference (TD) learning algorithms has also been extensively studied in the literature ( Jaakkola et al., 1994 ;  Baird, 1995 ;  Tsitsiklis & Van Roy, 1997 ;  Perkins & Pendrith, 2002 ;  Melo et al., 2008 ;  Mehta & Meyn, 2009 ;  Liu et al., 2015 ;  Bhandari et al., 2018 ;  Lakshminarayanan & Szepesvari, 2018 ;  Zou et al., 2019b ). However, the convergence guarantee of deep Q-learning algo- rithms remains a largely open problem. The only exceptions are  Yang et al. (2019)  which studied the fitted Q-iteration (FQI) algorithm ( Riedmiller, 2005 ;  Munos & Szepesvári, 2008 ) with action-value function approximation based on a sparse ReLU network, and  Cai et al. (2019a)  which studied the global convergence of Q-learning algorithm with an i.i.d. observation model and action-value func- tion approximation based on a two-layer neural network. The main limitation of the aforementioned work is the unrealistic assumption that all the data used in the Q-learning algorithm are sampled i.i.d. from a fixed stationary distribution, which fails to capture the practical setting of neural Q-learning. In this paper, in order to bridge the gap between the empirical success of neural Q-learning and the theory of conventional Q-learning (i.e., tabular Q-learning, and Q-learning with linear function approximation), we study the non-asymptotic convergence of a neural Q-learning algorithm under non-i.i.d. observations. In particular, we use a deep neural network with the ReLU activation func- tion to approximate the action-value function. In each iteration of the neural Q-learning algorithm, it updates the network weight parameters using the temporal difference (TD) error and the gradient of the neural network function. Our work extends existing finite-time analyses for TD learning ( Bhan- dari et al., 2018 ) and Q-learning ( Zou et al., 2019b ), from linear function approximation to deep Under review as a conference paper at ICLR 2020

Section Title: contributions are summarized as follows
  contributions are summarized as follows • We establish the first finite-time analysis of Q-learning with deep neural network function approx- imation when the data are generated from a Markov decision process (MDP). We show that, when the network is sufficiently wide, neural Q-learning converges to the optimal action-value function up to the approximation error of the neural network function class. • We establish an O(1/ √ T ) convergence rate of neural Q-learning to the optimal Q-value function up to the approximation error, where T is the number of iterations. This convergence rate matches the one for TD-learning with linear function approximation and constant stepsize ( Bhandari et al., 2018 ). Although we study a more challenging setting where the data are non-i.i.d. and the neural network approximator has multiple layers, our convergence rate also matches the O(1/ √ T ) rate proved in  Cai et al. (2019a)  with i.i.d. data and a two-layer neural network approximator. To sum up, we present a comprehensive comparison between our work and the most relevant work in terms of their respective settings and convergence rates in  Table 1 .

Section Title: Notation
  Notation We denote [n] = {1, . . . , n} for n ∈ N + . x 2 is the Euclidean norm of a vector x ∈ R d . For a matrix W ∈ R m×n , we denote by W 2 and W F its operator norm and Frobenius norm respectively. We denote by vec(W) the vectorization of W, which converts W into a column vector. For a semi-definite matrix Σ ∈ R d×d and a vector x ∈ R d , x Σ = √ x Σx denotes the Mahalanobis norm. We reserve the notations {C i } i=0,1,... to represent universal positive constants that are independent of problem parameters. The specific value of {C i } i=1,2,... can be different line by line. We write a n = O(b n ) if a n ≤ Cb n for some constant C > 0 and a n = O(b n ) if a n = O(b n ) up to some logarithmic terms of b n .

Section Title: RELATED WORK
  RELATED WORK Due to the huge volume of work in the literature for TD learning and Q-learning algorithms, we only review the most relevant work here.

Section Title: Asymptotic analysis
  Asymptotic analysis The asymptotic convergence of TD learning and Q-learning algorithms has been well established in the literature ( Jaakkola et al., 1994 ;  Tsitsiklis & Van Roy, 1997 ;  Konda & Tsitsiklis, 2000 ;  Borkar & Meyn, 2000 ;  Ormoneit & Sen, 2002 ;  Melo et al., 2008 ;  Devraj & Meyn, 2017 ). In particular,  Tsitsiklis & Van Roy (1997)  specified the precise conditions for TD learning with linear function approximation to converge and gave counterexamples that diverge.  Melo et al. (2008)  proved the asymptotic convergence of Q-learning with linear function approximation from standard ODE analysis, and identified a critic condition on the relationship between the learning policy and the greedy policy that ensures the almost sure convergence.

Section Title: Finite-time analysis
  Finite-time analysis The finite-time analysis of the convergence rate for Q-learning algorithms has been largely unexplored until recently. In specific,  Dalal et al. (2018) ;  Lakshminarayanan & Szepes- vari (2018)  studied the convergence of TD(0) algorithm with linear function approximation under i.i.d. data assumptions and constant step sizes. Concurrently, a seminal work by  Bhandari et al. (2018)  provided a unified framework of analysis for TD learning under both i.i.d. and Markovian noise assumptions with an extra projection step. The analysis has been extended by  Zou et al. (2019b)  to SARSA and Q-learning algorithms with linear function approximation. More recently, Under review as a conference paper at ICLR 2020  Srikant & Ying (2019)  established the finite-time convergence for TD learning algorithms with lin- ear function approximation and a constant step-size without the extra projection step under non-i.i.d. data assumptions through carefully choosing the Lyapunov function for the associated ordinary dif- ferential equation of TD update. A similar analysis was also extended to Q-learning with linear function approximation ( Chen et al., 2019 ).  Hu & Syed (2019)  further provided a unified analysis for a class of TD learning algorithms using Markov jump linear system.

Section Title: Neural function approximation
  Neural function approximation Despite the empirical success of DQN, the theoretical convergence of Q-learning with deep neural network approximation is still missing in the literature. Following the recent advances in the theory of deep learning for overparameterized networks ( Jacot et al., 2018 ;  Chizat & Bach, 2018 ;  Du et al., 2019b ;a;  Allen-Zhu et al., 2019b ;a;  Zou et al., 2019a ;  Arora et al., 2019 ;  Cao & Gu, 2019a ;  Zou & Gu, 2019 ;  Cai et al., 2019b ), two recent work by  Yang et al. (2019)  and  Cai et al. (2019a)  proved the convergence rates of fitted Q-iteration and Q-learning with a sparse multi-layer ReLU network and two-layer neural network approximation respectively, under i.i.d. observations.

Section Title: PRELIMINARIES
  PRELIMINARIES A discrete-time Markov Decision Process (MDP) is denoted by a tuple M = (S, A, P, r, γ). S and A are the sets of all states and actions respectively. P : S × A → P(S) is the transition kernel such that P(s |s, a) gives the probability of transiting to state s after taking action a at state s. r : S × A → [−1, 1] is a deterministic reward function. γ ∈ (0, 1) is the discounted factor. A policy π : S → P(A) is a function mapping a state s ∈ S to a probability distribution π(·|s) over the action space. Let s t and a t denote the state and action at time step t. Then the transition kernel P and the policy π determine a Markov chain {s t } t=0,1,... For any fixed policy π, its associated value function V π : S → R is defined as the expected total discounted reward: The corresponding action-value function Q π : S × A → R is defined as Q π (s, a) = E[ ∞ t=0 γ t r(s t , a t )|s 0 = s, a 0 = a] = r(s, a) + γ S V π (s )P(s |s, a)ds , for all s ∈ S, a ∈ A. The optimal action-value function Q * is defined as Q * (s, a) = sup π Q π (s, a) for all (s, a) ∈ S × A. Based on Q * , the optimal policy π * can be derived by following the greedy algorithm such that π * (a|s) = 1 if Q(s, a) = max b∈A Q * (s, b) and π * (a|s) = 0 otherwise. We define the optimal Bellman operator T as follows It is worth noting that the optimal Bellman operator T is γ-contractive in the sup-norm and Q * is the unique fixed point of T ( Bertsekas et al., 1995 ).

Section Title: THE NEURAL Q-LEARNING ALGORITHM
  THE NEURAL Q-LEARNING ALGORITHM In this section, we start with a brief review of Q-learning with linear function approximation. Then we will present the neural Q-learning algorithm.

Section Title: Q-LEARNING WITH LINEAR FUNCTION APPROXIMATION
  Q-LEARNING WITH LINEAR FUNCTION APPROXIMATION In many reinforcement learning algorithms, the goal is to estimate the action-value function Q(·, ·), which can be formulated as minimizing the mean-squared Bellman error (MSBE) ( Sutton & Barto, 2018 ): min Q(·,·) E µ,π,P (T Q(s, a) − Q(s, a)) 2 , (4.1) where state s is generated from the initial state distribution µ and action a is chosen based on a fixed learning policy π. To optimize (4.1), Q-learning iteratively updates the action-value function using the Bellman operator in (3.1), i.e., Q t+1 (s, a) = T Q t (s, a) for all (s, a) ∈ S × A. However, due to the large state and action spaces, whose cardinalities, i.e., |S| and |A|, can be infinite for continuous problems in many applications, the aforementioned update is impractical. To address this Under review as a conference paper at ICLR 2020 issue, a linear function approximator is often used ( Szepesvari, 2010 ;  Sutton & Barto, 2018 ), where the action-value function is assumed to be parameterized by a linear function, i.e., Q(s, a; θ) = φ(s, a) θ for any (s, a) ∈ S × A, where φ : S × A → R d maps the state-action pair to a d- dimensional vector, and θ ∈ Θ ⊆ R d is an unknown weight vector. The minimization problem in (4.1) then turns to minimizing the MSBE over the parameter space Θ.

Section Title: NEURAL Q-LEARNING
  NEURAL Q-LEARNING Analogous to Q-learning with linear function approximation, the action-value function can also be approximated by a deep neural network to increase the representation power of the approximator. Specifically, we define a L-hidden-layer neural network as follows f (θ; x) = √ mW L σ L (W L−1 · · · σ(W 1 x) · · · ), (4.2) where x ∈ R d is the input data, W 1 ∈ R m×d , W L ∈ R 1×m and W l ∈ R m×m for l = 2, . . . , L − 1, θ = (vec(W 1 ) , . . . , vec(W L ) ) is the concatenation of the vectorization of all parameter matrices, and σ(x) = max{0, x} is the ReLU activation function. Then, we can parameterize Q(s, a) using a deep neural network as Q(s, a; θ) = f (θ; φ(s, a)), where θ ∈ Θ and φ : S × A → R d is a feature mapping. Without loss of generality, we assume that φ(s, a) 2 ≤ 1 in this paper. Let π be an arbitrarily stationary policy. The MSBE minimization problem in (4.1) can be rewritten in the following form Recall that the optimal action-value function Q * is the fixed point of Bellman optimality operator T which is γ-contractive. Therefore Q * is the unique global minimizer of (4.3). The nonlinear parameterization of Q(·, ·) turns the MSBE in (4.3) to be highly nonconvex, which imposes difficulty in finding the global optimum θ * . To mitigate this issue, we will approximate the solution of (4.3) by project the Q-value function into some function class parameterized by θ, which leads to minimizing the mean square projected Bellman error (MSPBE): min θ∈Θ E µ,π,P (Q(s, a; θ) − Π F T Q(s, a; θ)) 2 , (4.4) where F = {Q(·, ·; θ) : θ ∈ Θ} is some function class parameterized by θ ∈ Θ, and Π F is a projection operator. Then the neural Q-learning algorithm updates the weight parameter θ using the following projected descent step: θ t+1 = Π Θ (θ t − η t g t (θ t )), where the gradient term g t (θ t ) is defined as and ∆ t is the temporal difference (TD) error. It should be noted that g t is not the gradient of the MSPBE nor an unbiased estimator for it. The details of the neural Q-learning algorithm are displayed in Algorithm 1, where θ 0 is randomly initialized, and the constraint set is chosen to be Θ = B(θ 0 , ω), which is defined as follows B(θ 0 , ω) def = {θ = (vec(W 1 ) , . . . , vec(W L ) ) : W l − W (0) l F ≤ ω, l = 1, . . . , L} (4.6) for some tunable parameter ω. It is easy to verify that

Section Title: CONVERGENCE ANALYSIS OF NEURAL Q-LEARNING
  CONVERGENCE ANALYSIS OF NEURAL Q-LEARNING In this section, we provide a finite-sample analysis of neural Q-learning. Note that the optimization problem in (4.4) is nonconvex. We focus on finding a surrogate action-value function in the neural network function class that well approximates Q * .

Section Title: APPROXIMATE STATIONARY POINT IN THE CONSTRAINED SPACE
  APPROXIMATE STATIONARY POINT IN THE CONSTRAINED SPACE To ease the presentation, we abbreviate f (θ; φ(s, a)) as f (θ) when no confusion arises. We define the function class F Θ,m as a collection of all local linearization of f (θ) at the initial point θ 0 F Θ,m = {f (θ 0 ) + ∇ θ f (θ 0 ), θ − θ 0 : θ ∈ Θ}. (5.1) Under review as a conference paper at ICLR 2020

Section Title: Algorithm 1 Neural Q-Learning with Gaussian Initialization
  Algorithm 1 Neural Q-Learning with Gaussian Initialization 1: Input: learning policy π, learning rate {η t } t=0,1,... , discount factor γ, constraint set Θ, Ran- domly generate the entries of W (0) l from N (0, 1/m), l = 1, . . . , m 2: Initialization: θ 0 Following to the local linearization analysis in  Cai et al. (2019a) , we define the approximate station- ary point of Algorithm 1 as follows. Definition 5.1 ( Cai et al. (2019a) ). A point θ * ∈ Θ is said to be the approximate stationary point of Algorithm 1 if for all θ ∈ Θ it holds that

Section Title: Definition 5.1 immediately implies
  Definition 5.1 immediately implies According to Proposition 4.2 in  Cai et al. (2019a) , this further indicates f (θ * ) = Π F Θ,m T f (θ * ). In other words, f (θ * ) is the unique fixed point of the MSPBE in (4.4). Therefore, we can show the convergence of neural Q-learning to the optimal action-value function Q * by first connecting it to the minimizer f (θ * ) and then adding the approximation error of F Θ,m .

Section Title: THE MAIN THEORY
  THE MAIN THEORY Before we present the convergence of Algorithm 1, let us lay down the assumptions used throughout our paper. The first assumption controls the bias caused by the Markovian noise in the observations through assuming the uniform ergodicity of the Markov chain generated by the learning policy π. Assumption 5.2. The learning policy π and the transition kernel P induce a Markov chain {s t } t=0,1,... such that there exist constants λ > 0 and ρ ∈ (0, 1) satisfying Assumption 5.2 also appears in  Bhandari et al. (2018) ;  Zou et al. (2019b) , which is essential for the analysis of the Markov decision process. The uniform ergodicity can be established via the mi- norization condition for irreducible Markov chains ( Meyn & Tweedie, 2012 ;  Levin & Peres, 2017 ). For the purpose of exploration, we also need to assume that the learning policy π satisfies some regularity condition. Denote b max (θ) = argmax b∈A | ∇ θ f (θ 0 ; s, b), θ | for any θ ∈ Θ. Similar to  Melo et al. (2008) ;  Zou et al. (2019b) ;  Chen et al. (2019) , we define Note that Σ π is independent of θ and only depends on the policy π and the initial point θ 0 in the definition of f . In contrast, Σ * π (θ) is defined based on the greedy action under the policy associated with θ. The scaling parameter 1/m is used to ensure that the operator norm of Σ π to be in the order of O(1). It is worth noting that Σ π is different from the neural tangent kernel (NTK) or the Gram matrix in  Jacot et al. (2018) ;  Du et al. (2019a) ;  Arora et al. (2019) , which are n × n matrices defined based on a finite set of data points {(s i , a i )} i=1,...,n . When f is linear, Σ π reduces to the covariance matrix of the feature vector. Assumption 5.3 is also made for Q-learning with linear function approximation in  Melo et al. (2008) ;  Zou et al. (2019b) ;  Chen et al. (2019) .  Moreover, Chen et al. (2019)  presented numerical simulations to verify the validity of Assumption 5.3.  Cai et al. (2019a)  imposed a slightly different assumption but with the same idea that the learning policy π should be not too far away from the greedy policy. The regularity assumption on the learning policy is directly imposed on the action value function in  Cai et al. (2019a) , which can be implied by Assumption 5.3 and thus is slightly weaker. We note that Assumption 5.3 can be relaxed to the one made in  Cai et al. (2019a)  without changing any of our analysis. Nevertheless, we choose to present the current version which is more consistent with existing work on Q-learning with linear function approximation ( Melo et al., 2008 ;  Chen et al., 2019 ). In the following theorem, we show that neural Q-learning converges to the optimal action-value function within finite time if the neural network is overparameterized. Theorem 5.6. Under the same conditions as in Theorem 5.4, with probability at least 1 − 3δ − L 2 exp(−C 0 m 2/3 L) over the randomness of θ 0 , it holds that where all the expectations are taken conditional on θ 0 , Q * is the optimal action-value function, δ ∈ (0, 1) and {C i } i=0,...,2 are universal constants. The optimal policy π * can be obtained by the greedy algorithm derived based on Q * . Remark 5.7. The convergence rate in Theorem 5.6 can be simplifies as follows The first term is the projection error of the optimal Q-value function on to the function class F Θ,m , which decreases to zero as the representation power of F Θ,m increases. In fact, when the width m of the DNN is sufficiently large, recent studies ( Cao & Gu, 2019a ;b) show that f (θ) is almost linear around the initialization and the approximate stationary point f (θ * ) becomes the fixed solution of the MSBE ( Cai et al., 2019a ). Moreover, this term diminishes when the Q function is approximated by linear functions when the underlying parameter has a bounded norm ( Bhandari et al., 2018 ;  Zou et al., 2019b ). As m goes to infinity, we obtain the convergence of neural Q-learning to the optimal Q-value function with an O(1/ √ T ) rate.

Section Title: PROOF OF MAIN RESULTS
  PROOF OF MAIN RESULTS In this section, we provide the detailed proof of the convergence of Algorithm 1. To simplify the presentation, we write f (θ; φ(s, a)) as f (θ; s, a) throughout the proof when no confusion arises. We first define some notations that will simplify the presentation of the proof. Recall the definition of g t (·) in (4.5). For any θ ∈ Θ, we define the following vector-value map g that is independent of the data point. g(θ) = E µ,π,P [∇ θ f (θ; s, a)(f (θ; s, a) − r(s, a) − γmax b∈A f (θ; s , b))], (6.1) where s follows the initial state distribution µ, a is chosen based on the policy π(·|s) and s follows the transition probability P(·|s, a). Similarly, for all θ ∈ Θ, we define the following gradient terms based on the linearized function f ∈ F Θ,m m t (θ) = ∆(s t , a t , s t+1 ; θ)∇ θ f (θ), m(θ) = E µ,π,P ∆(s, a, s ; θ)∇ θ f (θ) , (6.2) where ∆ is defined in (5.3), and a population version based on the linearized function. Now we present the technical lemmas that are useful in our proof of Theorem 5.4. For the gradients g t (·) defined in (4.5) and m t (·) defined in (6.2), we have the following lemma that characterizes the difference between the gradient of the neural network function f and the gradient of the linearized function f . with probability at least 1−2δ−3L 2 exp(−C 6 mω 2/3 L) over the randomness of the initial point, and g t (θ t ) 2 ≤ (2 + γ)C 7 m log(T /δ) holds with probability at least 1 − δ − L 2 exp(−C 6 mω 2/3 L). where {C i > 0} i=0,...,7 are universal constants. The next lemma upper bounds the bias of the non-i.i.d. data for the linearized gradient map. for any fixed t ≤ T , where C 0 > 0 is an universal constant and τ * = min{t = 0, 1, 2, . . . |λρ t ≤ η T } is the mixing time of the Markov chain {s t , a t } t=0,1,... . Since f is a linear function approximator of the neural network function f , we can show that the gradient of f satisfies the following nice property in the constrained set Θ. Now we can integrate the above results and obtain proof of Theorem 5.4. Proof of Theorem 5.4. By Algorithm 1 and the non-expansiveness of projection Π Θ , we have Under review as a conference paper at ICLR 2020 We need to find an upper bound for the gradient norm and a lower bound for the inner prod- uct. According to Definition 5.1, the approximate stationary point θ * of Algorithm 1 satisfies m(θ * ), θ − θ * ≥ 0 for all θ ∈ Θ. The inner product in (6.4) can be decomposed into Combining results from (6.4)and (6.5), we have Recall constraint set defined in (4.6). We have Θ = B(θ 0 , ω) = {θ : W l − W (0) l F ≤ ω, ∀l = 1, . . . , L} and that m and ω satisfy the condition in (6.3). Term I 1 is the error of the local linearization of f (θ) at θ 0 . By Lemma 6.1, with probability at least 1 − 2δ − 3L 2 exp(−C 1 mω 2/3 L) over the randomness of the initial point θ 0 , we have | g t − m t (θ t ), θ t − θ * | ≤ C 2 (2 + γ)m −1/6 log m log(T /δ) (6.7) holds uniformly for all θ t , θ * ∈ Θ, where we used the fact that ω = C 0 m −1/2 L −9/4 . Term I 2 is the bias of caused by the non-i.i.d. data (s t , a t , s t+1 ) used in the update of Algorithm 1. Conditional on the initialization, by Lemma 6.2, we have where τ * = min{t = 0, 1, 2, . . . |λρ t ≤ η T } is the mixing time of the Markov chain {s t , a t } t=0,1,... . Term I 3 is the estimation error for the linear function approximation. By Lemma 6.3, we have m(θ t ) − m(θ * ), θ t − θ * ≥ βE f (θ t ) − f (θ * ) 2 θ 0 , (6.9) where β = (1 − α −1/2 ) ∈ (0, 1) is a constant. Substituting (6.7), (6.8) and (6.9) into (6.6), we have it holds that with probability at least 1 − 2δ − 3L 2 exp(−C 1 mω 2/3 L) over the randomness of the initial point θ 0 . Recall the choices of the step sizes η 0 = . . . = η T = 1/(2βm √ T ) and the radius ω = C 0 m −1/2 L −9/4 . Dividing the above inequality by T and telescoping it from t = 0 to T yields Under review as a conference paper at ICLR 2020 For θ 0 , θ * ∈ Θ, again by ω = Cm −1/2 L −9/4 , we have θ 0 − θ * 2 2 ≤ 1/m. Since f (·) ∈ F Θ,m , by Lemma 6.1, it holds with probability at least 1 − 2δ − 3L 2 exp(−C 0 m 2/3 L) over the randomness of the initial point θ 0 that where we used the fact that γ < 1. This completes the proof.

Section Title: CONCLUSIONS
  CONCLUSIONS In this paper, we provide the first finite-time analysis of Q-learning with neural network function approximation (i.e., neural Q-learning), where the data are generated from a Markov decision pro- cess and the action-value function is approximated by a deep ReLU neural network. We prove that neural Q-learning converge to the optimal action-value function up to the approximation error with O(1/ √ T ) rate, where T is the number of iterations. Our proof technique is of independent interest and can be extended to analyze other deep reinforcement learning algorithms. One interesting fu- ture direction would be to remove the projection step in our algorithm by applying the ODE based analysis in  Srikant & Ying (2019) ;  Chen et al. (2019) .

```
