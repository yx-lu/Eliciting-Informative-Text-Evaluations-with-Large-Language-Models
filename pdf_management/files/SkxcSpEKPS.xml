<article article-type="research-article"><front><article-meta><title-group><article-title>Under review as a conference paper at ICLR 2020 GENERATIVE ADVERSARIAL NETWORKS FOR DATA SCARCITY INDUSTRIAL POSITRON IMAGES WITH ATTENTION</article-title></title-group><abstract><p>In the industrial field, the positron annihilation is not affected by complex envi- ronment, and the gamma-ray photon penetration is strong, so the nondestructive detection of industrial parts can be realized. Due to the poor image quality caused by gamma-ray photon scattering, attenuation and short sampling time in positron process, we propose the idea of combining deep learning to generate positron im- ages with good quality and clear details by adversarial nets. The structure of the paper is as follows: firstly, we encode to get the hidden vectors of medical CT im- ages based on transfer Learning, and use PCA to extract positron image features. Secondly, we construct a positron image memory based on attention mechanism as a whole input to the adversarial nets which uses medical hidden variables as a query. Finally, we train the whole model jointly and update the input param- eters until convergence. Experiments have proved the possibility of generating rare positron images for industrial non-destructive testing using countermeasure networks, and good imaging results have been achieved.</p></abstract></article-meta></front><body><sec><title>INTRODUCTION</title><p>In recent years, with the advancement of science and technology, especially the rapid developmen- t of high-end manufacturing, in the field of industrial non-destructive testing, in many cases, it is necessary to perform defect detection without damaging or affecting the performance and inter- nal structure of the device under test. Therefore, there is an increasing demand for corresponding detection devices.</p><p>In complex industrial environments (such as aviation, internal combustion engines, chemical engi- neering, etc.), it is of great research significance to detect faults and defects in closed chambers. In this paper, the use of positron annihilation gamma photon imaging positron emission imaging technology for industrial nondestructive testing is studied. The occurrence of positron annihilation is not affected by factors such as high temperature, high pressure, corrosion, etc., so it can penetrate the dense metal material cavity, realize the undisturbed and non-destructive trace imaging of the detected object, and obtain the detected object after processing. Describe the image and perform a state analysis. Therefore, the quality of imaging technology directly affects the analysis of fault detection results.</p><p>Positron Emission Tomography (PET) was first used in medical imaging. The principle is that when a radioactive positron nucleus decays, a proton in the nucleus is converted into a neutron, and a positron and a neutron are released. The positron will quickly combine with the electrons in the material in a very short time, causing a positron annihilation phenomenon, producing a pair of gamma photon pairs with opposite directions and energy of 511KeV. Photon pairs are collected, identified, processed, and finally reconstructed to obtain medical images.</p><p>Commonly used PET reconstruction algorithms are analytic method (K, 2000) and statistical method (<xref ref-type="bibr" rid="b1">Shepp &amp; Vardi, 2007</xref>). The currently widely used algorithms are MLEM and OSEM. At present, PET technology has been widely used in the clinical diagnosis of human diseases. The advantages are quite obvious, the imaging quality is higher, and it shows great advantages in medical research. The principle of positron emission in industrial non-destructive fields is similar to medical imaging, but it has its own unique difficulties: the detection environment is more harsh, the sampling time is Under review as a conference paper at ICLR 2020 short, and at the same time, due to the phenomenon of scattering and attenuation of photons, indus- trial positron imaging is obtained. The image quality is even worse. Therefore, the reconstructed image needs to be further processed to obtain a higher quality image.</p><p>In this paper, we propose adversarial networks of positron image memory module based on attention mechanism. Using medical images as basic data sets, introducing knowledge of migration learning, building memory module according to the contribution of detail features to images, a positron image generation network in the field of industrial non-destructive testing is obtained through joint training, thus achieving higher quality generation of industrial positron images.</p><p>In summary, our main contributions in this paper are as follows:</p><p>We are the first to advocate an idea of using Generative Adversarial Networks to enhance the detail of the positron image in the industrial non-destructive field, and realize the generation and processing of the scarce image data in the professional field.</p><p>We use the medical CT image dataset as the basic training sample of the network framework, which is based on the idea of migration learning, and then extract the features of a small number of indus- trial non-destructively detected positron images, which can improve the details of the generated im- ages, and make the network model have better applicability in the field of industrial non-destructive testing.</p><p>We combine the attention-based mechanism in the professional domain image feature extraction. By constructing a memory module containing industrial positron image features, we can generate image generation in a specific domain, and finally conduct an industrial lossless positron image generation model.</p><p>We train the whole network jointly, through the discriminant network of the antagonistic genera- tion network, the front-end network was back-propagated, the input parameters were updated, and the model was optimized. Finally, the convergence was achieved and The Turing test was passed successfully.</p></sec><sec><title>RELATED WORK</title><p>Model of GAN: Since GAN (<xref ref-type="bibr" rid="b2">Goodfellow et al., 2014</xref>) was proposed in 2014, it has become a re- search hotspot in the field of unsupervised learning, and related researches continue to increase. For the improvement of network structure, it mainly focuses on training stability and mode collapse. <xref ref-type="bibr" rid="b3">Radford et al.Radford et al. (2015)</xref> combine CNN with GAN to realize image generation, which makes GAN move from theory to practice, and establishes the framework and training mode of the whole GAN model. <xref ref-type="bibr" rid="b4">Arjovsky et al. M et al. (2017)</xref> propose Wasserstein GAN (WGAN), which use Earth-Mover instead of JS divergence to measure the distance between the real sample and the generated sample distribution, and the problem of pattern collapse was well solved. Mirza proposed <xref ref-type="bibr" rid="b4">Conditional Generative Adversarial Nets (CGAN) in 2014M &amp; S (2014)</xref>, which transformed unsu- pervised tasks into supervised tasks, thus improving the stability of training. <xref ref-type="bibr" rid="b5">Mao, X et al.X et al. (2016)</xref> propose Least Squares Generative Adversarial Networks (LSGAN), which replace the loss function in the original network with the least squares loss function, alleviating the problem of train- ing instability and insufficient diversity of generated images in GAN. <xref ref-type="bibr" rid="b6">Karras T et al.T et al. (2017)</xref> of NVIDIA team propose a progressive structure model to realize the transition from low-resolution to high-resolution image, and the generative model of high definition image can be trained smoothly. DeepMind team (<xref ref-type="bibr" rid="b1">A et al., 2018</xref>) introduce the idea of orthogonal regularization into GAN, which greatly improve the generation performance of GAN by truncating the input prior distribution Z in time, and contribute a lot of experience of parameter adjustment in the process of training mode. EBGAN (J et al., 2016) regards the discriminative network in GAN as an energy function and adds reconstruction error to it, which can solve the problem of mode collapse. Unrolled GAN (<xref ref-type="bibr" rid="b1">L et al., 2016</xref>) modifies the loss function of the generative network so that the generator can take into account the changes of discriminator after training K times and avoid the mode collapse caused by switching between different modes. DRAGAN (<xref ref-type="bibr" rid="b10">N et al., 2017</xref>) introduces the "no regret algorithm" in game theory, and construct gradient penalty scheme to avoid unwanted partial equilibrium. It can solve mode collapse problem and improve training efficiency.</p></sec><sec><title>Under review as a conference paper at ICLR 2020</title><p>Domain Application: The greatest advantage of GAN is that it can generate real-like samples with- out any explicit modeling of data distribution in the whole generation process. Therefore, GAN has achieved good results in many fields such as image, text, voice and so on. Image generation(<xref ref-type="bibr" rid="b11">P et al., 2016</xref>)(<xref ref-type="bibr" rid="b6">T et al., 2017</xref>)(<xref ref-type="bibr" rid="b13">Z et al., 2017</xref>), per resolution processing of image(C et al., 2017), object detection(<xref ref-type="bibr" rid="b1">Y et al., 2018</xref>)(<xref ref-type="bibr" rid="b2">J et al., 2017</xref>), object transfiguration(<xref ref-type="bibr" rid="b0">S et al., 2017</xref>)(<xref ref-type="bibr" rid="b18">Wu H, 2017</xref>), joint im- age generation(<xref ref-type="bibr" rid="b18">Liu &amp; Tuzel, 2016</xref>), video generation(<xref ref-type="bibr" rid="b19">Chen et al., 2018</xref>) (<xref ref-type="bibr" rid="b20">Tulyakov et al., 2017</xref>), text to image(<xref ref-type="bibr" rid="b5">H et al., 2017</xref>)(<xref ref-type="bibr" rid="b1">A et al., 2017</xref>), text generation(J et al., 2018), speech conversion(C et al., 2017), domain adaptation(<xref ref-type="bibr" rid="b2">J et al., 2017</xref>), and so on. These researches provide more possibilities for the practical application of GAN in the field.</p></sec><sec><title>Medical Image Processing</title><p>Image generation in medical field can be divided into two categories: conditional and unconditional. (<xref ref-type="bibr" rid="b2">M et al., 2018</xref>)(<xref ref-type="bibr" rid="b1">A &amp; J, 2017</xref>)(<xref ref-type="bibr" rid="b6">T et al., 2017</xref>) uses DCGAN(<xref ref-type="bibr" rid="b3">Radford et al., 2015</xref>) to realize the batch generation of realistic medical images(prostatic lesion plaques, lung cancer cakes, retina), and the resolution reaches 16*16,56*56 and 64*64 respectively, the results of which can pass the Turing test successfully.(C et al., 2018a) uses DCGAN to learn higher resolution MR images distribution from a small number of samples, and compare them with the real images, so as to achieve the effect of "false and true". (C et al., 2018b) uses PGGAN to synthesize skin le- sion images, and it successfully showed amazing and highly realistic synthetic images. Conditional medical image generation studies are as follows: (<xref ref-type="bibr" rid="b9">D et al., 2017</xref>) realizes CT image synthesis from MR images by means of 3D full convolution network. The loss function of the network is composed of reconstruction loss and gradient loss, and it is also trained by combining additional confronta- tion network to improve the reality of the synthesized images. (<xref ref-type="bibr" rid="b11">P et al., 2017</xref>) compress the images of vascular tree into a multivariate normal distribution by means of the antagonistic automatic en- coder. The normal distribution is sampled to synthesize any high resolution vascular tree image, and then the end-to-end framework for high resolution retinal image synthesis is obtained by using image conversion model(<xref ref-type="bibr" rid="b11">P et al., 2016</xref>).(<xref ref-type="bibr" rid="b6">T et al., 2017</xref>) proposes a two-stage GAN, the first one of which is to synthesize vessel tree images from noise training, and the second network uses image translation framework to generate real, high-resolution pairs of groundtruth vessel segmentation and the corresponding eye fundus image.(<xref ref-type="bibr" rid="b34">W et al., 2018</xref>) uses CGAN as the framework to synthesize 200*200 PET images by using CT image and binary label graph. The researchs contribution lies in the construction of a two-channel generation network to achieve a more realistic global output and it called multi-channel GAN. (<xref ref-type="bibr" rid="b14">F &amp; D, 2018</xref>) sets up a multi-stage generator to obtain speckle images, low-resolution images, and high-resolution images in turn by intra-vascular ultrasound simulation of tissue maps according to different generating networks. (<xref ref-type="bibr" rid="b1">A &amp; G, 2017</xref>) conducts joint learning by adding a specific task network to CGAN, and then obtain a network model that retain specific task characteristics. (<xref ref-type="bibr" rid="b2">M et al., 2018</xref>) uses WGAN as the network framework, and use noise and attribute vectors as inputs to generating high-resolution 3D images.</p><p>In addition, in the aspects of image segmentation, reconstruction, detection, denoising, recognition, classification, etc, and the use of GAN model has also achieved some research results, which pro- vides feasibility for the application of GAN in professional and specific field. Traditional positron image processing in the field of industrial nondestructive processing focuses on the improvement of image reconstruction algorithm, but due to the inherent characteristics of small sampling points and short time, the quality of the obtained image is poor, and the stability of the image affected by the field is poor. Therefore, in this paper, we innovatively introduce the knowledge of neural network for positron image processing. And through the experimental verification, a high quality image is obtained.</p></sec><sec><title>METHOD</title><p>The structure of the whole positron image countermeasure generation network based on memory module is shown in the following <xref ref-type="fig" rid="fig_0">figure 1</xref>:</p></sec><sec><title>STRUCTURE OF GAN</title><p>The original GAN(<xref ref-type="bibr" rid="b2">Goodfellow et al., 2014</xref>) model consists of two parts: the discriminative nets and the generative nets. Through training, the generator generates pseudo-data as close as possible to the real data distribution. The initial input of the network is random noise signal z, then map it to a new data space by generator to get the generated data G(z), after that the discriminator outputs Under review as a conference paper at ICLR 2020 a probability value through a binary classification of real data and generated data. The confidence level indicates the performance of the generator. Through continuous iteration optimization during the whole training process, the default optimization is achieved when the two sets of data cannot be distinguished.</p><p>The basic idea of the GAN comes from the zero-sum game theory. The two networks against each other, in which the discriminator aims to distinguish the real distribution and the generative distri- bution as far as possible, while the generator aims to make the generated data consistent with the real data in the feature space and make the discriminator indistinguishable, finally the whole model reaches the optimum. The mathematical model is expressed as formula equation 1:</p><p>Where x represents real images, z represents noise to the generator, and G(Z) represents the gener- ated images, D(x) represents the probability of judging whether a real image is true.</p></sec><sec><title>ENCODER</title><p>GAN is used to directly fit the distribution of n-dimensional vectors in images, and sample with ran- dom noise, the whole training process is in an unsupervised state, so the direction of data generation is uncontrollable unless the process traverses all initial distributions. At the same time, the generated images may not be the true expression of image meaning due to the excessive pursuit of quality. Under review as a conference paper at ICLR 2020 Therefore, when using the adversarial model to generate positron images in the industrial non- destructive testing, we choose to add a prior to restrict the data generation, so that the generation model can be trained for specific areas.</p><p>Based on the scarcity of industrial positron image data, in this paper, we introduce the knowledge of migration learning and use medical images as training data to construct an encoder, which is based on the variational auto-encoder.</p><p>Firstly, we sample medical image data X to get a series of sample points x 1 , x 2 , x 3 , x n , which makes all the sample data in X fit successfully and obtains a distribution p(x), is described as formula equation 2:</p><p>In order to achieve this goal, the distribution fitting of data sample X is finally realized with the help of implicit variable Z. It is assumed that p(x) describes a probability distribution of X generated by Z, which satisfies the Gauss distribution. Therefore, the whole encoder can be expressed as sampling Z from the standard normal distribution. In the process, we can get Variance and Mean of Sample Data X Sampled by Neural Network. The clustering process can be parametrized as formula equation 3:</p><p>Where the mean and variance of the normal distribution which is exclusive to X k can be obtained. Then Z k can be sampled from this exclusive distribution.</p></sec><sec><title>FEATURE EXTRACTION-MEMORY MODULE</title><p>In order to obtain a more suitable generation model for positron image in the industrial field, we proposes an image feature memory module based on attention mechanism, which is used to extract domain image features. This network is a research focus of this chapter. The basic flow of the whole network is as follows:1) use neural network to extract the feature of the rare positron images, and obtain the images feature vectors. 2) combine the vector and the hidden variable get in Section 3.2 based on attention mechanism to obtain an image memory model. 3)use the memory model as the input of adversarial nets and train jointly with the whole network to obtain positron image generator for the industrial field.</p></sec><sec><title>Positron image feature extraction</title><p>We use the principal component analysis(<xref ref-type="bibr" rid="b5">H et al., 2015</xref>) is used to extract the positron sample data and the vector space transformation is used to reduce the di- mensionality of higher dimensional positron data. Firstly, transform the original data into a new coordinate system by projection according to the new coordinate vector. Secondly, the variance of the first principal component of the projection data in the new coordinate system is the largest. As the dimension increases, the variance decreases in turn and the dimension decreases. is described as formula equation 4:</p><p>m represents the positron sample, n represents the sample dimension and the sample Y = m &#215; n . The data matrix Y is de-averaged so that the mean value of each dimension is 0. Then we should find the most important feature vectors in the images, that is, the data on the coordinate axis represented by the feature fluctuates the most and the sum of squares of all samples projected on unit vector &#181; is Under review as a conference paper at ICLR 2020 the largest. Then we get the value of &#181; using Lagrange theorem. The mathematical expression is as follows equation 5:</p><p>The nets use convolution neural network(CNN) to construct image feature extraction network, the network structure is divided into three layers, namely two convolution layers and one non-linear output layer. Firstly, small image slices are extracted from sample images and the dimension of the slices is the same as convolution core. Then traverse all the pixels in them and perform two- level convolution operation. Finally, hashing operation and histogram statistics are carried out in the output layer to print the feature vector.</p><p>Memory Module Based on Attention Mechanism The obtained positron eigenvector&#374; is fused with the hidden variable Z of the medical image obtained based on the attention mechanism to get the input nets. The purpose is to make the prior knowledge contained in the nets more focused on positron features, so that the features of scarce data can be more applied in the whole training process.</p><p>The basic idea is the global attention and the focus in our model is to extract all positron image features. The specific realization is to align image data vectors, directly use medical images as query vectors, and input positron image feature vectors as hidden state to calculate their weights equation 6.</p><p>Where z t is the medical image distribution, y s are feature vectors extracted from positron images, and score (z t , y s ) is the scoring criterion for the operation.</p><p>We get a constant and normalize it. And the contribution degree of each feature of positron image to the network can be obtained. So the image feature can be fused according to the weight ratio. Finally, the vector containing prior knowledge in the field is obtained as overall input of adversarial nets.</p></sec><sec><title>ADVERSARIAL NETS</title><p>Generative model The generative network is constructed based on DenseNet (<xref ref-type="bibr" rid="b36">G et al., 2017</xref>), and the positron image features can be requisitioned repeatedly in the model. The network can also strengthen the contribution of the characteristics of scarce data so that the generated images closer to real industrial positron images in detail.</p><p>The generative model is as follows: the output of the memory model in chapter 3.3 as a whole input to the net, and the input of each layer is related to the output of all the previous layers, not only related to upper layer. It can be expressed as formula equation 7: X l = H l ([X 0 , X 1 , &#183; &#183; &#183; , X l&#8722;1 ]) (7) X 0 , X 1 , &#183; &#183; &#183; , X (l&#8722;1) is the concatenation to the net. We can group all output feature maps from layer X 0 to X (l&#8722;1) according to different channels and the structure is used to reduce the parameters without losing features randomly, so that the initial input can enter each layers convolution calcula- tion to realize the feature reuse. The basic structure is 33 convolution layer, Batch Normalization(<xref ref-type="bibr" rid="b0">S &amp; C, 2015</xref>)and ReLU non-linear activation layer.</p><p>Feature maps of all previous layers need to be cat in the network. In order to perform the down sampling operation, the net is divided into several Denseblocks and transition layers are used be- tween different them. Referring to the original network, it consists of Batch Normalization layer, Under review as a conference paper at ICLR 2020 1*1 convolution network and 2*2 average-pooling. In the same Denseblock, the state of each lay- er is associated with all previous layers, and the training of each layer is aimed at the global state feedback of the network to update the parameters.</p></sec><sec><title>Discriminative model</title><p>The discriminative net is used to discriminate specific images in a specific domain, in which domain image features can be used as the evaluation criteria for network classifi- cation as much as possible. The net uses Markov Model based on PatchGAN, which is composed of full convolution layers. The output is an n-dimensional matrix, and the mean of the matrix is used as the output of the discriminative network, so that each receptive field in the image can be judged, which is equivalent to the convolution discriminant in batches by layers, and finally fed back to the whole network. The calculation of receptive field is equation 8:</p><p>In the model, the real input samples are medical data sets. Therefore, in order to make the generated data better characterize positron image features, we need to add an additional attention perception loss function to the net. The loss function of the whole net consists of two parts: L G AN and L A P G , the attention loss function L A P G is used to characterize the feature contribution of positron image, which is realized to measure the distribution distance between the generated data and the positron images. The loss function is described as equation 9:</p><p>W i represents number of elements in each layer, and s is the number of layers. The loss function of the whole net can be described as equation 10, and the L G AN is similar as the original GAN.</p></sec><sec><title>EXPERIMENTAL RESULTS</title></sec><sec><title>IMPLEMENTATION DETAILS</title><p>We design the model firstly by using encoder to obtain the medical images hidden vectors and using principal component analysis to reduce positron images dimensionality and extract the main fea- ture. Train memory module and adversarial nets jointly, and in the process of backpropagation, the identification network updates the parameters of the front-end network, so that the feature extrac- tion network extracts the features repeatedly until the whole network achieves the optimal model. Finally, the positron image generator for industrial non-destructive testing is obtained. The discriminator refers to pixel and each batch is 70*70.The learning rate is 0.0002 in the whole net. The model are trained iteratively using Adam algorithm(=0.5).</p></sec><sec><title>DATASET</title><p>DeepLession The dataset(<xref ref-type="bibr" rid="b0">K et al., 2018</xref>) contains more than 32,000 lesions from more than 10,000 case studies. The data is developed by National Institutes of Health Clinical Center(NIHCC), and developed by mining historical medical data from image archives and communication systems. It is now maybe the largest set of CT medical image data available to all. In the experiment, 150,000 images were selected for our training and the image pixels are 256*256.</p></sec><sec><title>Positron images</title><p>The dataset is obtained by the Geant4 Application for Tomographic Emis- sion(GATE). GATE is a simulation software which can simulate the physical process of PET imag- ing system, such as geometric size, material composition, detector movement and so on. In the model design, the anisotropic tube made of aluminium metal is filled with positron nuclide solution. Its activity is 600 Bq, the number of detectors is 184 *64, the sampling time is 0.1 s, the energy resolution is 15 percent, the time resolution is 300 ps, the energy window is 350-650 keV, and the time window is 10 ns. The design sampling time is 0.1s to meet the needs of rapid sampling in Under review as a conference paper at ICLR 2020 In this section, we compare our approach with the commonly used generation model, aiming at the generation of industrial positron images. Quantitative results are presented in <xref ref-type="table" rid="tab_0">Table 1</xref>.</p><p>The metrics to evaluate the performance of our method, namely multi-scale structural similarity(MS- SSIM) (<xref ref-type="bibr" rid="b1">A et al., 2017</xref>) and Frchet Inception Distance (FID)(<xref ref-type="bibr" rid="b2">M et al., 2017</xref>). The SSIM is used to measure the similarity of two images and FID is to measure the Frchet distance of two distributions in feature space.</p><p>By comparing the experimental data, we can clearly see that the confrontation network constructed in this paper has a better effect on the generation of positron images for professional fields, and the generated images are closer to the real images.</p></sec><sec><title>CONCLUSION</title><p>In this paper, we introduce an application of GAN in the field of nondestructive testing for specific industries. We combine the knowledge of transfer learning to make up the problem of insufficien- t data. The key point is to introduce attention mechanism to construct a positron image feature memory module, which can reuse image features under the condition of scarce data. At the same time, an attention loss function is added to the discriminative net to further improve the generator performance. Experiments show that compared with the start-of-the-art generation methods in deep learning, the model in our paper has an obvious improvement in the quality of industrial positron image generation.</p><p>In the future, our focus is to further study the application of generative adversarial networks in industrial positron image processing, and to further improve the quality of domain images.</p></sec><sec id="figures"><title>Figures</title><fig id="fig_0"><object-id>fig_0</object-id><label>Figure 1:</label><caption><title>Figure 1:</title><p>Positron Image Generation Flow Chart.</p></caption><graphic /><graphic /></fig><table-wrap id="tab_0"><label>Table 1:</label><caption><title>Table 1:</title><p>Evaluation results</p></caption><table><tbody><tr><td /></tr></tbody></table></table-wrap></sec></body><back><ack /><sec><title>A APPENDIX</title><p>The Positron image are shown in figure 2, which are simulation diagram with defects. The picture shows one-to-one correspondence between the original images and the generated images.</p></sec><ref-list id="ref-list-1"><ref id="b0"><element-citation publication-type="journal"><source>Medical imaging system</source><year>2000</year><person-group person-group-type="author"><name><surname>References Gao</surname><given-names>S K</given-names></name></person-group></element-citation></ref><ref id="b1"><element-citation publication-type="journal"><article-title>Maximum likelihood reconstruction for emission tomography</article-title><source>IEEE Transactions on Medical Imaging</source><year>2007</year><volume>1</volume><issue>2</issue><fpage>113</fpage><lpage>122</lpage><person-group person-group-type="author"><name><surname>Shepp</surname><given-names>L A</given-names></name><name><surname>Vardi</surname><given-names>Y</given-names></name></person-group></element-citation></ref><ref id="b2"><element-citation publication-type="journal"><source>Advances in neural information processing systems</source><year>2014</year><person-group person-group-type="author"><name><surname>Goodfellow</surname><given-names>Ian</given-names></name><name><surname>Pouget</surname><given-names>Abadie J</given-names></name><name><surname>Mirza</surname><given-names>M</given-names></name></person-group></element-citation></ref><ref id="b3"><element-citation publication-type="journal"><article-title>Unsupervised representation learning with deep convolutional generative adversarial networks</article-title><source>Under review as a conference paper at ICLR 2020</source><year>2015</year><person-group person-group-type="author"><name><surname>Radford</surname><given-names>A</given-names></name><name><surname>Metz</surname><given-names>L</given-names></name></person-group></element-citation></ref><ref id="b4"><element-citation publication-type="journal"><article-title>Wasserstein gan</article-title><source>Mirza M and Osindero S. Conditional generative adversarial nets</source><year>2017</year><person-group person-group-type="author"><name><surname>Arjovsky</surname><given-names>M</given-names></name><name><surname>Chintala</surname><given-names>S</given-names></name><name><surname>Bottou</surname><given-names>L</given-names></name></person-group></element-citation></ref><ref id="b5"><element-citation publication-type="journal"><source>Least squares generative adversarial networks</source><year>2016</year><person-group person-group-type="author"><name><surname>Mao</surname><given-names>X</given-names></name><name><surname>Li</surname><given-names>Q</given-names></name><name><surname>Xie</surname><given-names>H</given-names></name></person-group></element-citation></ref><ref id="b6"><element-citation publication-type="journal"><article-title>Progressive growing of gans for improved quality, stability, and variation</article-title><year>2017</year><person-group person-group-type="author"><name><surname>Karras</surname><given-names>T</given-names></name><name><surname>Laine</surname><given-names>Aila T</given-names></name><name><surname>Lehtinen</surname><given-names>S</given-names></name></person-group></element-citation></ref><ref id="b7"><element-citation publication-type="journal"><article-title>Large scale gan training for high fidelity natural image synthesis</article-title><year>2018</year><person-group person-group-type="author"><name><surname>Brock</surname><given-names>A</given-names></name><name><surname>Donahue</surname><given-names>J</given-names></name><name><surname>Simonyan</surname><given-names>K</given-names></name></person-group></element-citation></ref><ref id="b8"><element-citation publication-type="journal"><source>Energy-based generative adversarial network</source><year>2016</year><person-group person-group-type="author"><name><surname>Zhao</surname><given-names>J</given-names></name><name><surname>Mathieu</surname><given-names>M</given-names></name><name><surname>Lecun</surname><given-names>Y</given-names></name></person-group></element-citation></ref><ref id="b9"><element-citation publication-type="journal"><source>Unrolled generative adversarial networks</source><year>2016</year><person-group person-group-type="author"><name><surname>Metz</surname><given-names>L</given-names></name><name><surname>Poole</surname><given-names>B</given-names></name></person-group></element-citation></ref><ref id="b10"><element-citation publication-type="journal"><source>On convergence and stability of gans</source><year>2017</year><person-group person-group-type="author"><name><surname>Kodali</surname><given-names>N</given-names></name><name><surname>Abernethy</surname><given-names>J</given-names></name><name><surname>Hays</surname><given-names>J</given-names></name></person-group></element-citation></ref><ref id="b11"><element-citation publication-type="journal"><source>Image-to-image translation with conditional adversarial networks</source><year>2016</year><person-group person-group-type="author"><name><surname>Isola</surname><given-names>P</given-names></name><name><surname>Zhu</surname><given-names>J Y</given-names></name><name><surname>Zhou</surname><given-names>T</given-names></name></person-group></element-citation></ref><ref id="b12"><element-citation publication-type="journal"><source>Learning to discover cross-domain relations with generative adversarial networks</source><year>2017</year><person-group person-group-type="author"><name><surname>Kim</surname><given-names>T</given-names></name><name><surname>Cha</surname><given-names>M</given-names></name></person-group></element-citation></ref><ref id="b13"><element-citation publication-type="journal"><source>Dualgan: Unsupervised dual learning for image-to-image translation</source><year>2017</year><person-group person-group-type="author"><name><surname>Yi</surname><given-names>Z</given-names></name><name><surname>Zhang</surname><given-names>H</given-names></name></person-group></element-citation></ref><ref id="b14"><element-citation publication-type="journal"><source>Photo-realistic single image super-resolution using a generative adversarial network</source><year>2017</year><person-group person-group-type="author"><name><surname>Ledig</surname><given-names>C</given-names></name><name><surname>Theis</surname><given-names>L L</given-names></name></person-group></element-citation></ref><ref id="b15"><element-citation publication-type="journal"><article-title>Segan:adversarial network with multi-scale l1 loss for medical image segmentation</article-title><source>Neuroinformatics</source><year>2018</year><volume>16</volume><issue>3-4</issue><fpage>383</fpage><lpage>392</lpage><person-group person-group-type="author"><name><surname>Xue</surname><given-names>Y</given-names></name><name><surname>Xu</surname><given-names>T T</given-names></name><name><surname>Zhang</surname><given-names>H</given-names></name></person-group></element-citation></ref><ref id="b16"><element-citation publication-type="journal"><source>Perceptual generative adversarial networks for small object detection</source><year>2017</year><person-group person-group-type="author"><name><surname>Li</surname><given-names>J</given-names></name><name><surname>Liang</surname><given-names>X</given-names></name></person-group></element-citation></ref><ref id="b17"><element-citation publication-type="journal"><source>Genegan: Learning object transfiguration and attribute subspace from unpaired data</source><year>2017</year><person-group person-group-type="author"><name><surname>Zhou</surname><given-names>S</given-names></name><name><surname>Xiao</surname><given-names>T</given-names></name></person-group></element-citation></ref><ref id="b18"><element-citation publication-type="journal"><article-title>Gp-gan: Towards realistic high-resolution image blending</article-title><source>Ming Yu Liu and Oncel Tuzel. Coupled generative adversarial networks</source><year>2017</year><person-group person-group-type="author"><name><surname>Zhang</surname><given-names>J</given-names></name><name><surname>Wu</surname><given-names>H</given-names></name><name><surname>Zheng</surname><given-names>S</given-names></name></person-group></element-citation></ref><ref id="b19"><element-citation publication-type="journal"><source>Vgan-based image representation learning for privacy-preserving facial expression recognition</source><year>2018</year><person-group person-group-type="author"><name><surname>Chen</surname><given-names>Jiawei</given-names></name><name><surname>Konrad</surname><given-names>Janusz</given-names></name><name><surname>Ishwar</surname><given-names>Prakash</given-names></name></person-group></element-citation></ref><ref id="b20"><element-citation publication-type="journal"><source>Mocogan: Decomposing motion and content for video generation</source><year>2017</year><person-group person-group-type="author"><name><surname>Tulyakov</surname><given-names>Sergey</given-names></name><name><surname>Liu</surname><given-names>Ming Yu</given-names></name><name><surname>Dong</surname><given-names>Xiao</given-names></name></person-group></element-citation></ref><ref id="b21"><element-citation publication-type="journal"><source>Stackgan: Text to photo-realistic image synthesis with stacked generative adversarial networks</source><year>2017</year><person-group person-group-type="author"><name><surname>Zhang</surname><given-names>H</given-names></name><name><surname>Xu</surname><given-names>T</given-names></name></person-group></element-citation></ref><ref id="b22"><element-citation publication-type="journal"><source>Tac-gan-text conditioned auxiliary classifier generative adversarial network</source><year>2017</year><person-group person-group-type="author"><name><surname>Dash</surname><given-names>A</given-names></name><name><surname>Gamboa</surname><given-names>J C B</given-names></name></person-group></element-citation></ref><ref id="b23"><element-citation publication-type="journal"><source>Long text generation via adversarial training with leaked information</source><year>2018</year><person-group person-group-type="author"><name><surname>Guo</surname><given-names>J</given-names></name><name><surname>Lu</surname><given-names>S</given-names></name><name><surname>Cai</surname><given-names>H</given-names></name></person-group></element-citation></ref><ref id="b24"><element-citation publication-type="journal"><article-title>Voice conversion from unaligned corpora using variational autoencoding wasserstein generative adversarial networks</article-title><year>2017</year><person-group person-group-type="author"><name><surname>Hsu</surname><given-names>C C</given-names></name><name><surname>Hwang</surname><given-names>H T</given-names></name><name><surname>Wu</surname><given-names>Y C</given-names></name></person-group></element-citation></ref><ref id="b25"><element-citation publication-type="journal"><source>Cycada: Cycle-consistent adversarial domain adaptation</source><year>2017</year><person-group person-group-type="author"><name><surname>Hoffman</surname><given-names>J</given-names></name><name><surname>Tzeng</surname><given-names>E</given-names></name></person-group></element-citation></ref><ref id="b26"><element-citation publication-type="journal"><article-title>How to fool radiologists with generative adversarial networks? a visual turing test for lung cancer diagnosis</article-title><year>2018</year><person-group person-group-type="author"><name><surname>Chuquicusma</surname><given-names>M J M</given-names></name><name><surname>Hussein</surname><given-names>S</given-names></name></person-group></element-citation></ref><ref id="b27"><element-citation publication-type="journal"><article-title>Deep generative adversarial neural networks for realistic prostate lesion mri synthesis</article-title><source>Under review as a conference paper at ICLR 2020</source><year>2017</year><person-group person-group-type="author"><name><surname>Kitchen</surname><given-names>A</given-names></name><name><surname>Seah</surname><given-names>J</given-names></name></person-group></element-citation></ref><ref id="b28"><element-citation publication-type="journal"><source>Unsupervised anomaly detection with generative adversar- ial networks to guide marker discovery</source><year>2017</year><person-group person-group-type="author"><name><surname>Schlegl</surname><given-names>T</given-names></name><name><surname>Seebck</surname><given-names>P</given-names></name></person-group></element-citation></ref><ref id="b29"><element-citation publication-type="journal"><source>Learning implicit brain mri manifolds with deep learning</source><year>2018</year><person-group person-group-type="author"><name><surname>Bermudez</surname><given-names>C</given-names></name><name><surname>Plassard</surname><given-names>A J</given-names></name></person-group></element-citation></ref><ref id="b30"><element-citation publication-type="journal"><source>Generating highly realistic images of skin lesions with gans</source><year>2018</year><person-group person-group-type="author"><name><surname>Baur</surname><given-names>C</given-names></name><name><surname>Albarqouni</surname><given-names>S</given-names></name></person-group></element-citation></ref><ref id="b31"><element-citation publication-type="journal"><source>Medical image synthesis with context-aware generative adversarial networks</source><year>2017</year><person-group person-group-type="author"><name><surname>Nie</surname><given-names>D</given-names></name><name><surname>Trullo</surname><given-names>R</given-names></name></person-group></element-citation></ref><ref id="b32"><element-citation publication-type="journal"><article-title>End-to-end adversarial retinal image synthesis</article-title><source>IEEE transactions on medical imaging</source><year>2017</year><volume>37</volume><issue>3</issue><fpage>781</fpage><lpage>791</lpage><person-group person-group-type="author"><name><surname>Costa</surname><given-names>P</given-names></name><name><surname>Galdran</surname><given-names>A A</given-names></name></person-group></element-citation></ref><ref id="b33"><element-citation publication-type="journal"><source>Synthetic medical images from dual generative adversarial networks</source><year>2017</year><person-group person-group-type="author"><name><surname>Guibas</surname><given-names>J T</given-names></name><name><surname>Virdi</surname><given-names>T S</given-names></name></person-group></element-citation></ref><ref id="b34"><element-citation publication-type="journal"><source>Learning myelin content in multiple sclerosis from multimodal mri through adversarial training</source><year>2018</year><person-group person-group-type="author"><name><surname>Wei</surname><given-names>W</given-names></name><name><surname>Poirion</surname><given-names>E E</given-names></name><name><surname>Bodini</surname><given-names>B</given-names></name></person-group></element-citation></ref><ref id="b35"><element-citation publication-type="journal"><source>Simulating patho-realistic ultrasound images using deep generative networks with adversarial learning</source><year>2018</year><person-group person-group-type="author"><name><surname>Sheet</surname><given-names>Tom F</given-names></name></person-group></element-citation></ref><ref id="b36"><element-citation publication-type="journal"><article-title>Adversarial stain transfer for histopathology image analysis</article-title><source>IEEE transactions on medical imaging</source><year>2017</year><volume>37</volume><issue>3</issue><fpage>792</fpage><lpage>802</lpage><person-group person-group-type="author"><name><surname>Bentaieb</surname><given-names>A</given-names></name><name><surname>Hamarneh</surname><given-names>G</given-names></name></person-group></element-citation></ref><ref id="b37"><element-citation publication-type="journal"><source>Blood vessel geometry synthesis using generative adversarial networks</source><year>2018</year><person-group person-group-type="author"><name><surname>Wolterink</surname><given-names>J M</given-names></name><name><surname>Leiner</surname><given-names>T</given-names></name></person-group></element-citation></ref><ref id="b38"><element-citation publication-type="journal"><article-title>Pcanet: A simple deep learning baseline for image classification?</article-title><source>IEEE transactions on image processing</source><year>2015</year><volume>24</volume><issue>12</issue><fpage>5017</fpage><lpage>5032</lpage><person-group person-group-type="author"><name><surname>Chan</surname><given-names>T H</given-names></name><name><surname>Jia</surname><given-names>K</given-names></name></person-group></element-citation></ref><ref id="b39"><element-citation publication-type="journal"><source>Densely connected convolutional networks</source><year>2017</year><person-group person-group-type="author"><name><surname>Huang</surname><given-names>G</given-names></name><name><surname>Liu</surname><given-names>Z</given-names></name><name><surname>Van Der Maaten</surname><given-names>L</given-names></name></person-group></element-citation></ref><ref id="b40"><element-citation publication-type="journal"><source>Batch normalization: Accelerating deep network training by reducing inter- nal covariate shift</source><year>2015</year><person-group person-group-type="author"><name><surname>Szegedy</surname><given-names>Ioffe S</given-names></name></person-group></element-citation></ref><ref id="b41"><element-citation publication-type="journal"><article-title>Deeplesion: automated mining of large-scale lesion annotations and universal lesion detection with deep learning</article-title><source>Journal of Medical Imaging</source><year>2018</year><volume>5</volume><issue>3</issue><fpage>036501</fpage><lpage>036501</lpage><person-group person-group-type="author"><name><surname>Yan</surname><given-names>K</given-names></name><name><surname>Wang</surname><given-names>X</given-names></name></person-group></element-citation></ref><ref id="b42"><element-citation publication-type="journal"><article-title>Conditional image synthesis with auxiliary classifier gans</article-title><source>ICML</source><year>2017</year><fpage>26422651</fpage><lpage>26422651</lpage><person-group person-group-type="author"><name><surname>Odena</surname><given-names>A</given-names></name><name><surname>Olah</surname><given-names>C</given-names></name><name><surname>Shlens</surname><given-names>J</given-names></name></person-group></element-citation></ref><ref id="b43"><element-citation publication-type="journal"><article-title>Gans trained by a two time-scale update rule converge to a local nash equilibrium</article-title><source>NIPS</source><year>2017</year><fpage>66266637</fpage><lpage>66266637</lpage><person-group person-group-type="author"><name><surname>Heusel</surname><given-names>M</given-names></name><name><surname>Ramsauer</surname><given-names>H</given-names></name></person-group></element-citation></ref></ref-list></back></article>