<article article-type="research-article"><front><article-meta><title-group><article-title /></title-group><abstract><p>Quantifying, enforcing and implementing fairness emerged as a major topic in machine learning. We investigate these questions in the context of deep learning. Our main algorithmic and theoretical tool is the computational estimation of similarities between probability, "&#224; la Wasserstein", using adversarial networks. This idea is flexible enough to investigate different fairness constrained learning tasks, which we model by specifying properties of the underlying data generative process. The first setting considers bias in the generative model which should be filtered out. The second model is related to the presence of nuisance variables in the observations producing an unwanted bias for the learning task. For both models, we devise a learning algorithm based on approximation of Wasserstein distances using adversarial networks. We provide formal arguments describing the fairness enforcing properties of these algorithm in relation with the underlying fairness generative processes. Finally we perform experiments, both on synthetic and real world data, to demonstrate empirically the superiority of our approach compared to state of the art fairness algorithms as well as concurrent GAN type adversarial architectures based on Jensen divergence.</p></abstract></article-meta></front><body><sec><title>INTRODUCTION</title><p>Along the last few years, much emphasis has been laid on fairness issues in machine learning. Actually, when the learning sample presents biases, these are learnt by algorithms based on loss functions promoting closeness to observed data. Using such models for decision making generalizes biases to the whole population. This drawback of machine learning, also known as unfairness, has become a major challenge in the domain. For a recent survey on this topic we refer to <xref ref-type="bibr" rid="b10">Dwork et al. (2012)</xref>; <xref ref-type="bibr" rid="b10">Zemel et al. (2013)</xref> or Friedler et al. and references therein.</p><p>Fairness usually deals with situations where an algorithm exhibits a different behavior for two different subgroups of the population, while these subgroups should not play any role in its outcome. This situation is often modeled as follows : the algorithm should aims at forecasting a variable Y based on observations X. Fairness is then defined with respect to a protected variable, called protected attribute, S which represents membership to each population subgroup. The algorithm is called fair if its predictions does not depend too much on S.</p><p>Defining and quantifying this notion of dependency is a complicated task and has received much attention. One of the main tools is the so-called disparate impact which measures if the decision taken by an algorithm differs from one group to another. Absence of disparate impact is called demographic parity. Another measure of fairness is given by the dependency of prediction error with respect to S. The independent case is a form of fairness called equality of odds. We refer for instance to <xref ref-type="bibr" rid="b5">Chouldechova (2017)</xref>, <xref ref-type="bibr" rid="b12">Friedler et al. (2016)</xref> or Besse et al. (2018) and references therein. Both situations amounts to considering that either the distribution of the prediction, or its conditional distribution given the target variable Y , does not depend on S. Hence fairness quantification can be naturally implemented using distance between conditional distributions.</p><p>This point of view has been extensively studied when trying to "repair" data sets as described for instance in <xref ref-type="bibr" rid="b12">Feldman et al. (2015)</xref>, <xref ref-type="bibr" rid="b20">Johndrow &amp; Lum (2017)</xref>, <xref ref-type="bibr" rid="b17">Hacker &amp; Wiedemann (2017)</xref> or <xref ref-type="bibr" rid="b12">Friedler et al. (2019)</xref>. This solution consists in changing the input data so that predictability of the protected Under review as a conference paper at ICLR 2020 attribute is impossible. The data will be blurred in order to obtain a fair treatment of the protected class. The natural distance to measure the difference between the conditional distributions is the so-called Wasserstein distance, which provides an alternative framework to measure the dependency of the decision rule with respect to the protected attribute as shown in <xref ref-type="bibr" rid="b1">Barrio et al. (2019a)</xref> or <xref ref-type="bibr" rid="b1">Barrio et al. (2019b)</xref>. Yet previous methods face the difficulty of computing the Wasserstein distance which is a challenging task as shown in <xref ref-type="bibr" rid="b23">Peyr&#233; &amp; Cuturi (2019)</xref>.</p><p>In this work, we aim at building fair classifiers by considering a Wasserstein type constraint. Adding constraints to the classifiers to get fair behavior has been studied in several papers. We refer to <xref ref-type="bibr" rid="b12">Friedler et al. (2019)</xref>, <xref ref-type="bibr" rid="b28">Zafar et al. (2017a)</xref> and references therein. Our approach, yet sharing some similarities with <xref ref-type="bibr" rid="b11">Edwards &amp; Storkey (2016)</xref>, based on <xref ref-type="bibr" rid="b16">Ganin et al. (2016)</xref>, is more flexible and enables to solve wider classes of fairness problems based on different adversarial architecture resulting in more suited loss functions. Wasserstein constraint for fairness has also been considered in <xref ref-type="bibr" rid="b19">Jiang et al. (2019)</xref> for binary logistic regression. In the following, we provide algorithms which, for both demographic parity and equality of odds, can incorporate fairness constraints based on the 1-Wasserstein distance. Here we will consider two different mathematical models, describing the relationships between the variables X the target variable Y and the protected variable S. Computing Wasserstein type constraints is difficult, we use neural networks as they have been proved useful to estimate Wasserstein type distances as discussed in <xref ref-type="bibr" rid="b0">Arjovsky et al. (2017)</xref>. The proposed approach can be combined with any kind of neural network predictor. Hence we are able to manage a large variety of input data structure (e.g. images) as well as output labels (multiclass, regression, images . . . ). We demonstrate on fairness benchmark datasets that the proposed Wasserstein approximation framework outperforms both classical fair algorithms (e.g fair SVM) as well as similar adversarial architectures based on Jensen / GAN losses very close to the approaches described in <xref ref-type="bibr" rid="b4">Beutel et al. (2017)</xref>; Madras et al. (2018).</p><p>The paper falls into the following parts. Section 2 is devoted to the presentation of Wasserstein distance, approximation schemes and applications to fair modeling. Section 3 describes a first model of fairness related to demographic parity. Section 4 considers a second option connected to equality of odds. For both models, we propose an adversarial network methodology to obtain a fair classifier for each type of fairness. Section 5 studies these algorithms on real benchmark data sets as well as synthetic simulations.</p></sec><sec><title>FAIRNESS : DEFINITIONS AND METRICS</title></sec><sec><title>FRAMEWORK</title><p>The statistical model we consider is the following. The problem consists in forecasting a binary variable Y &#8712; {0, 1}, using observed covariates X &#8712; R d , d &#8805; 1. We assume moreover that the population can be divided into two categories that represent a bias, modeled by a variable S &#8712; {0, 1}. This variable is called the protected, or sensitive, attribute which takes the values S = 0 for the " minority" class and S = 1 for the " majority " class. 1 We observe n joint realizations of these variables D = {(X i , S i , Y i ), i &#8712; {1, . . . , n}}. We use the following notations</p><p>The fair classification problem aims at predicting Y from the variables X, using a family of binary classifiers g &#8712; G : R d &#8594; {0, 1} without using the information conveyed by S. For every g &#8712; G, the outcome of the classification will be the prediction&#374; = g(X). We consider in the following that the classifier g comes from a score given by a predictor F : R d &#8594; R such that&#374; = 1 F (X)&gt;&#951; for a chosen threshold &#951; &gt; 0.</p><p>Different criteria have been proposed for measuring the fairness off depending of the context. The disparate impact (DI) measures the sensitivity of the predicted values&#374; with respect to S.</p></sec><sec><title>Under review as a conference paper at ICLR 2020</title><p>The most favorable situation in terms of fairness with respect to the protected attribute S, is achieved when DI(&#374; , S) = 0 (i.e. P (&#374; = 1|S = 0) = P (&#374; = 1|S = 1)), which corresponds to the situation known as demographic parity. In this case, the predicted class is independent from S. While its interpretation is clear, the mathematical properties of the disparate impact measure are not favorable, in particular it lacks robustness and smoothness features which would be necessary to blend algorithmic practice and mathematical theory. In the following, we propose an alternative measure of equality of opportunity which features smoothness properties and comes with a strong mathematical background. Given a score function F , we set L 1 (F (X)) = L(F (X)|S = 1) and L 0 (F (X)) = L(F (X)|S = 0) the laws of conditional distribution of the score for each class and denote the corresponding quantile functions by Q 0,F and Q 1,F . Independence of the decision with the variable S would entail that the repartition of the scores is similar for the two subgroups. So the distance between the quantiles of these two distributions acts as a measure of fairness measuring that the repartition of the score is spread in a similar ways whatever the values of the protected attribute, hence acting as a sensitivity index of the predicted values F (X) with respect to S. Namely define</p><p>which corresponds to the so-called earth-mover or W 1 Wasserstein distance between the conditional distributions. Clearly W(L 0 (F (X)), L 1 (F (X))) = 0 implies that DI(&#374; , S) = 0. So Wasserstein distance appears in this framework as a smooth criterion to assess the sensitivity w.r.t to the protected variable. This criterion corresponds to the quantity that is used to measure fairness in <xref ref-type="bibr" rid="b1">Barrio et al. (2019b)</xref> and <xref ref-type="bibr" rid="b1">Barrio et al. (2019a)</xref>. Note that Wasserstein distance for fairness has been also considered in the seminal paper by <xref ref-type="bibr" rid="b12">Feldman et al. (2015)</xref>.</p><p>Another important criterion is the equality of odds, which measures the influence of the S on the accuracy of the algorithm. For this the prediction errors across the different class groups are compared and this notion of fairness is achieved when P (&#374; = 1|Y = 1, S = 0) = P (&#374; = 1|Y = 1, S = 1) and P (&#374; = 0|Y = 0, S = 0) = P (&#374; = 0|Y = 0, S = 1). Here again, this condition can be interpreted as a notion of independence of the conditional distributions defined for (i, s) &#8712; {0, 1} 2 as L i s (f (X)) the distribution of the random variable (f (X)|Y = i, S = s). Hence as we exposed for the notion of equality of opportunity, fairness will be assessed through the computations of the Wasserstein distances</p><p>Note that in some cases, we are only interested in equality of opportunities. This corresponds to the case where we only require that P (&#374; = 1|Y = 1, S = 0) = P (&#374; = 1|Y = 1, S = 1) as pointed out in <xref ref-type="bibr" rid="b10">Hardt et al. (2016)</xref>. Hence in this case it amounts to control only W(L 1 0 (F (X)), L 1 1 (F (X))).</p></sec><sec><title>WASSERSTEIN DIVERGENCES USING NEURAL NETWORKS AND PROPERTIES</title><p>The earth-mover, or Wasserstein-1 distances between probability distribution is defined as follows : W(L 1 , L 2 ) = inf &#947;&#8712;&#928;(L1,L2) E X,Y &#8764;&#947; X &#8722; Y (2) where &#928;(L 1 , L 2 ) is the set of all probability measures on X, Y with marginals L 1 and L 2 . Disparate Impact is closely related to the notion of unpredictability of the variable S. Hence the aim in this case is to The distance associated to these notions is the total variation distance d TV (L 1 , L 2 but due to intractability of this distance, it has been replaced in the machine learning literature by W. Clearly the independent case is obtained when the distance is null and the decrease of W leads to smaller DI as shown in the experiments in <xref ref-type="bibr" rid="b1">Barrio et al. (2019a)</xref>. Hence a constraint on the Wasserstein distance promotes fairness, also in terms of Disparate Impact.</p><p>Although the infimum in Equation (2) is not tractable in general, it can be approximated by a neural network. The first step is to reformulate (2) using the <xref ref-type="bibr" rid="b0">Kantorovich-Rubinstein duality Villani (2008)</xref>: W(L 1 , L 2 ) = sup f &#8712;F1 E X&#8764;L1 f (X) &#8722; E X&#8764;L2 f (X) (3) where F 1 denotes the space of 1 Lipschitz function. As a second step, the approach proposed in <xref ref-type="bibr" rid="b0">Arjovsky et al. (2017)</xref> is based on estimation of the supremum in (3) by replacing F 1 by the set of functions described by a fixed neural network architecture with spectral normalization Miyato et al. Under review as a conference paper at <xref ref-type="bibr" rid="b16">ICLR 2020 (2018)</xref>. This provides a general methodology to estimate and optimize divergences &#224; la Wasserstein and leads to interesting empirical results <xref ref-type="bibr" rid="b0">Arjovsky et al. (2017)</xref>. Furthermore, we demonstrate empirically that this approach allows to control to some extent the empirical EMD divergence introduced above. The two examples of fairness measures which we have introduced are based on distributional divergence measured using Wasserstein distance and we propose to handle these divergence terms computationally using the dual formulation presented in this paragraph.</p><p>One specificity of the fairness problem which we consider is that, empirically, we only have access to a finite number of samples for each values of the protected attribute (S &#8712; {0, 1}). For example, we only have access to L 0 and L 1 through a fixed finite sample and the Wasserstein terms which we manipulate are only computed on finitely many samples. This raises the following comments. Other divergences, such as <xref ref-type="bibr" rid="b0">Jensen-Shannon Goodfellow et al. (2014)</xref>, KL divergence or total variation <xref ref-type="bibr" rid="b0">Arjovsky et al. (2017)</xref> can be approximated using neural networks. These divergences reflect similarities between mutually absolutely continuous probability measures. However they degenerate when considering singular measures. For the problems which we intend to attack in this work, we aim at enforcing equality of distributions using only a fixed number of samples. Entropy or total variation based divergences fail to capture dissimilarity between singular measures, and in particular they degenerate when considering disjoint finite sample sets. On the other hand, Wasserstein metric is well defined and does not degenerate on empirical distributions given by finite samples.</p><p>A second favorable property of this metric is its continuity features. When considering parametric distributions, this translate into continuity of the metric in the parameter space as remarked by <xref ref-type="bibr" rid="b0">Arjovsky et al. (2017)</xref>, resulting in numerically more favorable situations compared to discontinuous problems. Another important consequence of the continuity properties of Wasserstein distance is that it translates into stable approximation of distribution divergence in the limit of large i.i.d samples (see Appendix). This last property is very desirable since all we can do from an empirical perspective is limited to finite samples.</p></sec><sec><title>TYPE 1 FAIR LEARNING : DEMOGRAPHIC PARITY</title><p>The first case where fairness is desirable corresponds to the situation where the target variable is biased (....). For instance, it is well-known that the income of a people is biased by the gender. The situation doesn't arise from a biased gathering of data but from bias that exist in the real data and that we don't want to reproduce in our model. Thus, a fair model in this case will change the prediction in order to make them independent from the protected variable. A suitable objective for this problem is to obtain of disparate impact (or the SDI) as close as possible to 1. This situation is formally represented in <xref ref-type="fig" rid="fig_0">Figure 1</xref>. In this situation we require that : X &#8869; &#8869; S|Y and Y * &#8869; &#8869; S|Y where Y * is not observed. Note that, as it intuitively expected, Y is not independent from S (even conditionally to X). In this example, Y * could represent an ideal case where the income level reflect the proficiency and not the gender.</p></sec><sec><title>TYPE 1 FAIR NEURAL NETWORKS</title><p>The following can be applied either in multivariate regression, Y &#8712; R d , or classification Y = {0, 1}, we consider the type 1 configuration described in <xref ref-type="fig" rid="fig_0">Figure 1</xref>. We propose a neural network model with adversarial Wasserstein constraints on the output as described in <xref ref-type="fig" rid="fig_2">Figure 3</xref> In this networks, the function F is a classifier or regressor. In order to have a prediction independent from S, we add Under review as a conference paper at ICLR 2020</p><p>update F by gradient descent :</p><p>9: end for Wasserstein penalization reflecting the dependency between F (X) and S, we obtain the following optimization:</p><p>where l is the loss function for the problem. Note that the Wasserstein penalty term in (4) is exactly the EM D fairness measure which we introduced in (1). Applying the approximation scheme described in Section 2.2, we obtain the following saddle point problem :</p><p>&#955; &gt; 0 are hyper-parameters and F s represents the set of functions encoded by a fixed architecture neural network with spectral normalization. Approximating expectations using empirical sample, the learning process for this model is described in Alg. 1. As explained in Section 2.2, in the limit of large samples, the maximization in A provide a proxy for the Wasserstein distance between the two conditional distributions. Note that we can use a similar architecture for equality of opportunities.</p></sec><sec><title>TYPE 2 FAIR LEARNING : EQUALITY OF ODDS</title><p>The second case where fairness is desirable corresponds to the situation where the data are subject to a bias nuisance variable which is in principle of no help for the learning task at hand and which influence should be removed. On famous example is the dog vs wolf problem exposed in <xref ref-type="bibr" rid="b24">Ribeiro et al. (2016)</xref>. In this example, that data was heavily biased by the presence, for the wolfs, and the absence, for the dogs, of snow in the picture. Although the presence of snow is not independent from the presence of wolfs, we prefer a model that focuses on animal features rather than background. More generally, this kind of situation appears when the descriptors or target variables show dependency with the protected variable due to a biased data collection process or when we plan to use the model on data that have a different distribution with respect to the protected variable (this could be the case Under review as a conference paper at ICLR 2020 if we want to detect wolfs and dogs in a snow free area). The equality of odds is a suitable objective for this type of fairness.</p><p>We represent the underlying data generation process formally in <xref ref-type="fig" rid="fig_2">Figure 3</xref>, we require the following conditional independence: X * &#8869; &#8869; S|Y, Y &#8869; &#8869; S|X * where X * is not observed. Note that, as it is intuitively expected, Y and X are not independent from S (even conditionally on any other variable in the model). In this context, we suppose that there is a representation of the data X * from which we can build a model to predict Y which will be independent of S given Y . Back to our example for wolf and dog, X could be the pictures, X * could be physical features of the animal. A model learnt from this X * could predict Y = {dog, wolf } from a picture independently of the presence of snow (even if the probability of observing snow is greater when the animal on a picture is a wolf).</p></sec><sec><title>TYPE 2 FAIR NEURAL NETWORKS FOR BINARY CLASSIFICATION</title><p>In the following, we consider the case where Y &#8712; {0, 1} and type 2 configuration. We propose a neural network model with adversarial Wasserstein constraints as described in <xref ref-type="fig" rid="fig_2">Figure 3</xref>.1 In this networks, the function F &#8226; T (X) (or F (Z) with Z = T (X)) is a classifier constructed in two steps : a transformation T : X &#8594; Z and a classifier F : Z &#8594; F (Z) &#8712; 0, 1. We expect to build T such that Z has the same properties as X * (i.e. X * &#8869; &#8869; S|Y ). In order to achieve this goal, we constraint the distribution Z conditionally to Y to be independent of S. Based on this idea, we obtain the following optimization problem : inf F,T E X [l(F (T (X)), Y )] + &#955; W(L 0 0 (T (X)), L 0 1 (T (X))) + W(L 1 0 (T (X)), L 1 1 (T (X))) (5) where l is a given loss function (binary cross entropy in our setting). We then apply the approximation procedure of Wasserstein distance described in Section 2.2 and obtain the following saddle point problem :</p><p>where &#955; &gt; 0 are hyper-parameters of the method and F s describes all functions generated by a given fixed architecture neural network with spectral normalization <xref ref-type="bibr" rid="b22">Miyato et al. (2018)</xref>. Based on finite sample approximation of the various expectations in this formulation, the learning process is similar to Alg. 1 and is fully described in the appendix. The supremum over A 0 and A 1 for finite sample approximation of the expectations is a proxy to the Wasserstein distance between the two conditional distributions of interest in 2.1. Moreover, Property 1 states that in the limit of Wasserstein distance between conditional distribution set to 0 the latent space Z = T (X) satisfies the same properties of conditional Independence as X * . Furthermore any classifier build a posteriori on Z will satisfy equal opportunities with respect to Y .</p></sec><sec><title>Proposition 1 Assume that the deterministic map T satisfies</title><p>then, we have T (X) &#8869; &#8869; S|Y , for any measurable map G, G(T (X)) &#8869; &#8869; S|Y .</p><p>Proof sketch: Both are expressions of equality in distribution. Nullity of Wasserstein distance entails T (X) &#8869; &#8869; S|Y . This implies that for any deterministic map G, G(T (X)) &#8869; &#8869; S|Y . Note that, contrary to other repair procedures for which the transformation must be recomputed for any new observations (in <xref ref-type="bibr" rid="b1">Barrio et al. (2019a)</xref> the transformation relies on the optimal transport map which depends on the observations), here the optimal transformation T can be used directly for all new observations.</p></sec><sec><title>EXPERIMENTATION</title><p>We show in this section, empirical results supporting our theoretical expectations. For simplicity and reproducibility purposes, we keep neural networks as simple as possible and try to use similar architectures as much as we can. For all the experiments, we set the learning rate of Adam to 1e &#8722;4 , and n w to 10 (see algorithm description in Sections 3 and 4). All experiments have been implemented with keras/tensorflow.</p></sec><sec><title>Fairness benchmarks</title><p>We consider three state of the art fairness benchmarks : (i) impact of gender in adult database (predict income&gt;50K, 48842 examples, 16 attributes) (ii) impact of age (boolean 25 &lt; age &gt; 60) in the bank database (predict credit acceptance, 45211 examples, 17 attributes) (iii) impact of the gender on the attractivity in a subset of the celebA dataset (64x64 rgb images, 19670 examples).</p></sec><sec><title>Concurent methods</title><p>We compare our results with the C-SVM implementation proposed by <xref ref-type="bibr" rid="b28">Zafar et al. (2017a</xref>;b) and a classifier based on our neural network architecture without fairness constraint (UnfairClf). We also compare our approach with GAN type Jensen adversarial as in <xref ref-type="bibr" rid="b4">Beutel et al. (2017)</xref>; Madras et al. (2018). Note that the architectures that we use are slightly different from the original papers, this was on purpose to ensure an objective comparison with our approach. Indeed we keep our adversarial architectures and only replace Wasserstein loss and network by a classifier with binary cross entropy and GAN trick for training.</p><p>Results: <xref ref-type="table" rid="tab_0">Table 1</xref> reports accuracy (ACC), DI and EMD in a 70%train-30%test scheme with 10 repetitions to assess variability 2 in both demographic parity and equality of odds scenarios. For equality of odds, we aggregate fairness measures DI and EM D, conditioning on Y and summing over Y = 0, 1, these aggregated measures are denoted by DI Y and EM D Y . For the adversarial approaches we report one result with high fairness constraint and one result with a lesser constraint (obtained by considering different values of &#955;). For the demographic parity constraint, in all examples, our algorithm reduces the DI close to 0 with an acceptable accuracy decrease. <xref ref-type="fig" rid="fig_4">Figure 5</xref> illustrates that our last hyper-parameter &#955; (see Equation (4)) controls the trade-off between accuracy and fairness both in wasserstein and GAN configurations. Our approach clearly dominates concurrent methods in all situations in terms of both accuracy and fairness level. This is further illustrated in the third part of <xref ref-type="fig" rid="fig_4">Figure 5</xref> where the accuracy / fairness tradeoff is very favorable to our wasserstein approach compared to more traditional GAN methods. Finally <xref ref-type="table" rid="tab_0">Table 1</xref> illustrates difficulties for GAN models to enforce hard fairness constraints (DI close to 0). For equality of odds, our method and GAN approach perform similarly. We train a fair auto-encoder with two Wasserstein adversarial networks constraining equality of odds for the decoded images. We observe in <xref ref-type="fig" rid="fig_4">Figure 5</xref> that equality of odds is achieved by assigning the same color to all transformed images. We train a first network (unfairClf) on the biased database. We train a similar network on the fair database, and construct a fair classifier (fairClf) by composition of the second trained classifier and the auto-encoder. As expected unfairClf generalizes better on the biased test set (accuracy 0.94 versus 0.87). However when switching the test color distribution fairClf is far more robust (accuracy 0.82 versus 0.60). This demonstrates that in addition to building a representation which looks fair, our auto-encoder approach is robust to fluctuations of the bias variable distribution.</p></sec><sec><title>CONCLUSIONS</title><p>This work tackles the challenge of incorporating constraints to deal with bias issues in machine learn- ing. We show that Wasserstein is an appropriate choice of distance between conditional distributions to control fairness using adversarial neural networks. We also explicit mathematical models providing abstract frameworks to understand and apply two types of fair constraints (demographic parity and equality of odds). The predictor we obtain prove efficient on well known fairness benchmarks Under review as a conference paper at ICLR 2020 as well as synthetic problems. Our experiments designed with minimal hand tuning to overcome reproducibility issues. As expected adversarial wasserstein constraints are more efficient to enforce fairness than their traditional GAN counterparts.</p></sec><sec id="figures"><title>Figures</title><fig id="fig_0"><object-id>fig_0</object-id><label>Figure 1:</label><caption><title>Figure 1:</title><p>Fairness type 1.</p></caption><graphic /></fig><fig id="fig_1"><object-id>fig_1</object-id><label>Figure 2:</label><caption><title>Figure 2:</title><p>Fairness type 2.</p></caption><graphic /></fig><fig id="fig_2"><object-id>fig_2</object-id><label>Figure 3:</label><caption><title>Figure 3:</title><p>Networks for fairness type 1</p></caption><graphic /></fig><fig id="fig_3"><object-id>fig_3</object-id><label>Figure 4:</label><caption><title>Figure 4:</title><p>Fair networks for fairness type 2</p></caption><graphic /></fig><table-wrap id="tab_0"><label>Table 1:</label><caption><title>Table 1:</title><p>Accuracy, Disparate impact and EMD for adult, bank and CelebA under demographic parity and equality of odds constraint</p></caption><table><tbody><tr><td /></tr></tbody></table></table-wrap><fig id="fig_4"><object-id>fig_4</object-id><label>Figure 5:</label><caption><title>Figure 5:</title><p>Accuracy / fairness tradeoffs between our Wasserstein approach and more traditional GAN approaches similar to Beutel et al. (2017); Madras et al. (2018) for demographic parity.</p></caption><graphic /><graphic /><graphic /></fig><fig id="fig_5"><object-id>fig_5</object-id><label>Figure 6:</label><caption><title>Figure 6:</title><p>Fair auto encoder.a) T-shirt, b) fair representation of T-shirt, c) shirt, d)fair representation of shirt Learning based on fair representations: To illustrate Proposition 1 and the fact that our fairness constrinat can be applied to other type of problem than classification, we consider classification of T-shirts versus shirts (Y = {T shirt, shirt}) in the fashion-MNIST dataset Xiao et al. (2017) (12000 training examples, 2000 test examples). These two classes are known to be the most challenging to distinguish in this dataset (accuracy around 0.9). We bias the dataset by adding a color (S = {turquoise, yellow}) correlated to the class variable Y : P (S = yellow|Y = T &#8722; shirt) = 0.9, P (S = T urquoise|Y = shirt) = 0.9. We apply the following experimental process: train on the biased dataset, and compare validation performances both on the biased test set and the same biased test set, with switched colors: P (S = yellow|Y = T &#8722; shirt) = 0.1 and P (S = T urquoise|Y = shirt) = 0.1.</p></caption><graphic /></fig></sec></body><back><sec><p>Note that in the case where S is not a binary variable but multidimensional or multi-class, we can consider one versus one fairness identifying in each case a "minority".</p></sec><ref-list id="ref-list-1"><ref id="b0"><element-citation publication-type="journal"><article-title>Wasserstein gan</article-title><source>CoRR</source><year>2017</year><person-group person-group-type="author"><name><surname>References Mart&#237;n Arjovsky</surname><given-names>Soumith</given-names></name><name><surname>Chintala</surname><given-names>L&#233;on</given-names></name><name><surname>Bottou</surname><given-names /></name></person-group></element-citation></ref><ref id="b1"><element-citation publication-type="journal"><article-title>Obtaining fairness using optimal transport theory</article-title><source>Proceedings of ICML Conference</source><year>2019</year><person-group person-group-type="author"><name><surname>Del Barrio</surname><given-names>Eustasio</given-names></name><name><surname>Gamboa</surname><given-names>Fabrice</given-names></name><name><surname>Gordaliza</surname><given-names>Paula</given-names></name><name><surname>Loubes</surname><given-names>Jean-Michel</given-names></name></person-group></element-citation></ref><ref id="b2"><element-citation publication-type="journal"><article-title>A central limit theorem for l_p transportation cost with applications to fairness assessment in machine learning</article-title><source>Information and Inference : A journal of IMA</source><year>2019</year><person-group person-group-type="author"><name><surname>Del Barrio</surname><given-names>Eustasio</given-names></name><name><surname>Gordaliza</surname><given-names>Paula</given-names></name><name><surname>Loubes</surname><given-names>Jean-Michel</given-names></name></person-group></element-citation></ref><ref id="b3"><element-citation publication-type="journal"><article-title>Confidence intervals for testing disparate impact in fair learning</article-title><source>arXiv preprint arXiv:1807.06362</source><year>2018</year><person-group person-group-type="author"><name><surname>Besse</surname><given-names>Philippe</given-names></name><name><surname>Barrio</surname><given-names>Eustasio Del</given-names></name><name><surname>Gordaliza</surname><given-names>Paula</given-names></name><name><surname>Loubes</surname><given-names>Jean-Michel</given-names></name></person-group></element-citation></ref><ref id="b4"><element-citation publication-type="journal"><article-title>Data decisions and theoretical implications when adversarially learning fair representations</article-title><source>arXiv preprint arXiv:1707.00075</source><year>2017</year><person-group person-group-type="author"><name><surname>Beutel</surname><given-names>Alex</given-names></name><name><surname>Chen</surname><given-names>Jilin</given-names></name><name><surname>Zhao</surname><given-names>Zhe</given-names></name><name><surname>Chi</surname><given-names>Ed H</given-names></name></person-group></element-citation></ref><ref id="b5"><element-citation publication-type="journal"><article-title>Fair prediction with disparate impact: A study of bias in recidivism prediction instruments</article-title><source>Big Data</source><year>2017</year><volume>5</volume><issue>2</issue><fpage>153</fpage><lpage>163</lpage><person-group person-group-type="author"><name><surname>Chouldechova</surname><given-names>Alexandra</given-names></name></person-group></element-citation></ref><ref id="b6"><element-citation publication-type="journal"><article-title>Central limit theorems for the Wasserstein distance between the empirical and the true distributions</article-title><source>Ann. Probab.</source><year>1999</year><fpage>1009</fpage><lpage>1071</lpage><person-group person-group-type="author"><name><surname>Barrio</surname><given-names>E</given-names></name><name><surname>Gin&#233;</surname><given-names>E</given-names></name><name><surname>Matr&#225;n</surname><given-names>C</given-names></name></person-group></element-citation></ref><ref id="b7"><element-citation publication-type="journal"><article-title>Central limit theorems for empirical transportation cost in general dimension</article-title><source>Ann. Probab.</source><year>2019</year><volume>47</volume><issue>2</issue><fpage>926</fpage><lpage>951</lpage><person-group person-group-type="author"><name><surname>Del Barrio</surname><given-names>Eustasio</given-names></name><name><surname>Loubes</surname><given-names>Jean-Michel</given-names></name></person-group></element-citation></ref><ref id="b8"><element-citation publication-type="journal"><article-title>Asymptotics for transportation cost in high dimensions</article-title><source>Journal of Theoretical Probability</source><year>1995</year><volume>8</volume><person-group person-group-type="author"><name><surname>Dobri&#263;</surname><given-names>V</given-names></name><name><surname>Yukich</surname><given-names>J E</given-names></name></person-group></element-citation></ref><ref id="b9"><element-citation publication-type="journal"><source>Real Analysis and Probability</source><year>2002</year><person-group person-group-type="author"><name><surname>Dudley</surname><given-names>R M</given-names></name></person-group></element-citation></ref><ref id="b10"><element-citation publication-type="journal"><article-title>Fairness through awareness</article-title><source>Proceedings of the 3rd innovations in theoretical computer science confer- ence</source><year>2012</year><fpage>214</fpage><lpage>226</lpage><person-group person-group-type="author"><name><surname>Dwork</surname><given-names>Cynthia</given-names></name><name><surname>Hardt</surname><given-names>Moritz</given-names></name><name><surname>Pitassi</surname><given-names>Toniann</given-names></name><name><surname>Reingold</surname><given-names>Omer</given-names></name><name><surname>Zemel+</surname><given-names>Richard</given-names></name></person-group></element-citation></ref><ref id="b11"><element-citation publication-type="journal"><article-title>Censoring representations with an adversary</article-title><source>International Conference on Learning Representations</source><year>2016</year><person-group person-group-type="author"><name><surname>Edwards</surname><given-names>Harrison</given-names></name><name><surname>Storkey</surname><given-names>Amos J</given-names></name></person-group></element-citation></ref><ref id="b12"><element-citation publication-type="journal"><article-title>Certifying and removing disparate impact</article-title><source>Proceedings of the 21th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining</source><year>2015</year><fpage>259</fpage><lpage>268</lpage><person-group person-group-type="author"><name><surname>Feldman</surname><given-names>Michael</given-names></name><name><surname>Friedler</surname><given-names>Sorelle</given-names></name><name><surname>Moeller</surname><given-names>John</given-names></name><name><surname>Scheidegger</surname><given-names>Carlos</given-names></name><name><surname>Venkatasubra- Manian</surname><given-names>Suresh</given-names></name></person-group></element-citation></ref><ref id="b13"><element-citation publication-type="journal"><article-title>A comparative study of fairness-enhancing interventions in machine learning</article-title><source>ArXiv e-prints</source><person-group person-group-type="author"><name><surname>Friedler</surname><given-names>Sorelle</given-names></name><name><surname>Scheidegger</surname><given-names>Carlos</given-names></name><name><surname>Venkatasubramanian</surname><given-names>Suresh</given-names></name><name><surname>Choudhary</surname><given-names>Sonam</given-names></name><name><surname>Hamil- Ton</surname><given-names>Evan P</given-names></name></person-group></element-citation></ref><ref id="b14"><element-citation publication-type="journal"><article-title>On the (im)possibility of fairness</article-title><source>arXiv e-prints, art. arXiv:1609.07236</source><year>2016</year><person-group person-group-type="author"><name><surname>Friedler</surname><given-names>Sorelle</given-names></name><name><surname>Scheidegger</surname><given-names>Carlos</given-names></name><name><surname>Venkatasubramanian</surname><given-names>Suresh</given-names></name></person-group></element-citation></ref><ref id="b15"><element-citation publication-type="journal"><source>Proceedings of the Conference on Fairness, Accountability, and Transparency, FAT* '19</source><year>2019</year><fpage>329</fpage><lpage>338</lpage><person-group person-group-type="author"><name><surname>Sorelle</surname><given-names>A</given-names></name><name><surname>Friedler</surname><given-names>Carlos</given-names></name><name><surname>Scheidegger</surname><given-names>Suresh</given-names></name><name><surname>Venkatasubramanian</surname><given-names>Sonam</given-names></name><name><surname>Choudhary</surname><given-names>Evan P</given-names></name><name><surname>Hamilton</surname><given-names>Derek Roth</given-names></name></person-group></element-citation></ref><ref id="b16"><element-citation publication-type="journal"><article-title>Domain-adversarial training of neural networks</article-title><source>Journal of Machine Learning Research</source><year>2016</year><volume>17</volume><issue>59</issue><fpage>1</fpage><lpage>35</lpage><person-group person-group-type="author"><name><surname>Ganin</surname><given-names>Yaroslav</given-names></name><name><surname>Ustinova</surname><given-names>Evgeniya</given-names></name><name><surname>Ajakan</surname><given-names>Hana</given-names></name><name><surname>Germain</surname><given-names>Pascal</given-names></name><name><surname>Larochelle</surname><given-names>Hugo</given-names></name><name><surname>Laviolette</surname><given-names>Fran&#231;ois</given-names></name><name><surname>March</surname><given-names>Mario</given-names></name><name><surname>Lempitsky</surname><given-names>Victor</given-names></name><name><surname>Goodfellow</surname><given-names>Ian</given-names></name><name><surname>Pouget-Abadie</surname><given-names>Jean</given-names></name><name><surname>Mirza</surname><given-names>Mehdi</given-names></name><name><surname>Xu</surname><given-names>Bing</given-names></name><name><surname>Warde-Farley</surname><given-names>David</given-names></name><name><surname>Ozair</surname><given-names>Sherjil</given-names></name><name><surname>Courville</surname><given-names>Aaron</given-names></name><name><surname>Bengio</surname><given-names>Yoshua</given-names></name></person-group></element-citation></ref><ref id="b17"><element-citation publication-type="journal"><article-title>A continuous framework for fairness</article-title><source>arXiv e-prints, art. arXiv:1712.07924</source><year>2017</year><person-group person-group-type="author"><name><surname>Hacker</surname><given-names>Philipp</given-names></name><name><surname>Wiedemann</surname><given-names>Emil</given-names></name></person-group></element-citation></ref><ref id="b18"><element-citation publication-type="journal"><article-title>Equality of opportunity in supervised learning</article-title><source>Advances in neural information processing systems</source><year>2016</year><fpage>3315</fpage><lpage>3323</lpage><person-group person-group-type="author"><name><surname>Hardt</surname><given-names>Moritz</given-names></name><name><surname>Price</surname><given-names>Eric</given-names></name><name><surname>Srebro</surname><given-names>Nati</given-names></name></person-group></element-citation></ref><ref id="b19"><element-citation publication-type="journal"><article-title>Wasserstein fair classification</article-title><source>Uncertainty in AI</source><year>2019</year><person-group person-group-type="author"><name><surname>Jiang</surname><given-names>Ray</given-names></name><name><surname>Pacchiano</surname><given-names>Aldo</given-names></name><name><surname>Stepleton</surname><given-names>Tom</given-names></name><name><surname>Jiang</surname><given-names>Heinrich</given-names></name><name><surname>Chiappa</surname><given-names>Silvia</given-names></name></person-group></element-citation></ref><ref id="b20"><element-citation publication-type="journal"><article-title>An algorithm for removing sensitive information: application to race-independent recidivism prediction</article-title><source>arXiv preprint arXiv:1703.04957</source><year>2017</year><person-group person-group-type="author"><name><surname>James</surname><given-names>E</given-names></name><name><surname>Johndrow</surname><given-names>Kristian</given-names></name><name><surname>Lum</surname><given-names /></name></person-group></element-citation></ref><ref id="b21"><element-citation publication-type="journal"><article-title>Learning adversarially fair and transferable representations</article-title><source>International Conference on Machine Learning</source><year>2018</year><fpage>3381</fpage><lpage>3390</lpage><person-group person-group-type="author"><name><surname>Madras</surname><given-names>David</given-names></name><name><surname>Creager</surname><given-names>Elliot</given-names></name><name><surname>Pitassi</surname><given-names>Toniann</given-names></name><name><surname>Zemel</surname><given-names>Richard</given-names></name></person-group></element-citation></ref><ref id="b22"><element-citation publication-type="journal"><article-title>Spectral normalization for generative adversarial networks</article-title><source>International Conference on Learning Representations</source><year>2018</year><person-group person-group-type="author"><name><surname>Miyato</surname><given-names>Takeru</given-names></name><name><surname>Kataoka</surname><given-names>Toshiki</given-names></name><name><surname>Koyama</surname><given-names>Masanori</given-names></name><name><surname>Yoshida</surname><given-names>Yuichi</given-names></name></person-group></element-citation></ref><ref id="b23"><element-citation publication-type="journal"><article-title>Computational optimal transport</article-title><source>Foundations and Trends R in Machine Learning</source><year>2019</year><volume>11</volume><issue>5-6</issue><fpage>355</fpage><lpage>607</lpage><person-group person-group-type="author"><name><surname>Peyr&#233;</surname><given-names>Gabriel</given-names></name><name><surname>Cuturi</surname><given-names>Marco</given-names></name></person-group></element-citation></ref><ref id="b24"><element-citation publication-type="journal"><year>2016</year><fpage>1135</fpage><lpage>1144</lpage><person-group person-group-type="author"><name><surname>Marco Tulio Ribeiro</surname><given-names>Sameer</given-names></name><name><surname>Singh</surname><given-names>Carlos</given-names></name><name><surname>Guestrin</surname><given-names /></name></person-group></element-citation></ref><ref id="b25"><element-citation publication-type="journal"><source>Probability for Statisticians</source><year>2000</year><person-group person-group-type="author"><name><surname>Shorack</surname><given-names>G R</given-names></name></person-group></element-citation></ref><ref id="b26"><element-citation publication-type="journal"><source>Grundlehren der mathematischen Wissenschaften</source><year>2008</year><person-group person-group-type="author"><name><surname>Villani</surname><given-names>C&#233;dric</given-names></name></person-group></element-citation></ref><ref id="b27"><element-citation publication-type="journal"><source>Fashion-mnist: a novel image dataset for benchmarking machine learning algorithms</source><year>2017</year><person-group person-group-type="author"><name><surname>Xiao</surname><given-names>Han</given-names></name><name><surname>Rasul</surname><given-names>Kashif</given-names></name><name><surname>Vollgraf</surname><given-names>Roland</given-names></name></person-group></element-citation></ref><ref id="b28"><element-citation publication-type="journal"><article-title>Fairness beyond disparate treatment &amp; disparate impact: Learning classification without disparate mistreatment</article-title><year>2017</year><fpage>1171</fpage><lpage>1180</lpage><person-group person-group-type="author"><name><surname>Zafar</surname><given-names>Muhammad B</given-names></name><name><surname>Valera</surname><given-names>Isabel</given-names></name><name><surname>Rodriguez</surname><given-names>Manuel G</given-names></name><name><surname>Gummadi</surname><given-names>Krishna P</given-names></name></person-group></element-citation></ref><ref id="b29"><element-citation publication-type="journal"><source>Proceed- ings of the 20th International Conference on Artificial Intelligence and Statistics, volume 54 of Proceedings of Machine Learning Research</source><year>2017</year><fpage>962</fpage><lpage>970</lpage><person-group person-group-type="author"><name><surname>Zafar</surname><given-names>Muhammad B</given-names></name><name><surname>Valera</surname><given-names>Isabel</given-names></name><name><surname>Rodriguez</surname><given-names>Manuel G</given-names></name><name><surname>Gummadi</surname><given-names>Krishna P</given-names></name></person-group></element-citation></ref><ref id="b30"><element-citation publication-type="journal"><article-title>Learning fair representations</article-title><source>In Proceedings of the 30th International Conference on Machine Learning</source><year>2013</year><volume>28</volume><fpage>325</fpage><lpage>333</lpage><person-group person-group-type="author"><name><surname>Zemel</surname><given-names>Rich</given-names></name><name><surname>Wu</surname><given-names>Yu</given-names></name><name><surname>Swersky</surname><given-names>Kevin</given-names></name><name><surname>Pitassi</surname><given-names>Toni</given-names></name><name><surname>Dwork</surname><given-names>Cynthia</given-names></name></person-group></element-citation></ref></ref-list></back></article>