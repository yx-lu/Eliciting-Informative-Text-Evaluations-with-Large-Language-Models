Title:
```
Under review as a conference paper at ICLR 2020 COLLABORATIVE TRAINING OF BALANCED RANDOM FORESTS FOR OPEN SET DOMAIN ADAPTATION
```
Abstract:
```
In this paper, we introduce a collaborative training algorithm of balanced random forests for domain adaptation tasks which can avoid the overfitting problem. In real scenarios, most domain adaptation algorithms face the challenges from noisy, insuf- ficient training data. Moreover in open set categorization, unknown or misaligned source and target categories adds difficulty. In such cases, conventional methods suffer from overfitting and fail to successfully transfer the knowledge of the source to the target domain. To address these issues, the following two techniques are proposed. First, we introduce the optimized decision tree construction method, in which the data at each node are split into equal sizes while maximizing the information gain. Compared to the conventional random forests, it generates larger and more balanced decision trees due to the even-split constraint, which contributes to enhanced discrimination power and reduced overfitting. Second, to tackle the domain misalignment problem, we propose the domain alignment loss which pe- nalizes uneven splits of the source and target domain data. By collaboratively optimizing the information gain of the labeled source data as well as the entropy of unlabeled target data distributions, the proposed CoBRF algorithm achieves significantly better performance than the state-of-the-art methods. The proposed algorithm is extensively evaluated in various experimental setups in challenging do- main adaptation tasks with noisy and small training data as well as open set domain adaptation problems, for two backbone networks of AlexNet and ResNet-50.
```

Figures/Tables Captions:
```
Figure 1: Split examples in decision tree according to the split functions: (a) the conventional method chooses the split that maximizes the information gain. (b) In contrast, the proposed method additionally enforces the size of child nodes to be equal, resulting in a random balanced tree. Note that CoBRF has far mode nodes which improves the generalization ability for domain adaptation.
Figure 2: Hyperplanes by the proposed methods. (a,b) The hyperplanes estimated by binary pseudo labels followed by translation for even split. Dotted line is the hyperplane estimated using linear SVM. The data are evenly split by the hyperplane shift (solid lines). Among these hyperplanes, the one with maximum information gain is chosen: yellow hyperplane in (a). (c) In CoBRF, both the source information gain and target entropy is considered. The yellow is better in source information gain, the target data split is biased, while the blue splits the source and target evenly well.
Figure 3: Visualization of trees learned by the proposed methods. The white and gray circles at the leaf nodes represent the source and target data fallen into the node, respectively. As a tree without domain alignment only considers the labeled source data, the target data distributions in leaf nodes are not even, whereas that with domain alignment generates more uniform splits. Refer to Sec. 3.2 and Fig. 2c.
Table 1: Ablation study of components for the split function of random forests without the domain alignment. The experiment is performed on Amazon (A), Webcam (W) and DSLR (D) domains of Office-31 with ResNet-50.
Table 2: The effect of λ for domain alignment in the CoBRF. The experiment is performed on Office-31 with ResNet-50.
Table 3: The effect of the tree numbers and maximum depth in training the CoBRF. The experiment is performed on 40 % noisy condition of the Office-31 (W->A) with ResNet-50.
Table 4: Performance comparison of the 60 and 80% Noisy and 10% Small training data protocol on Office-31, ImageCLEF-DA and Office-Home dataset with ResNet-50.
Table 5: Performance comparison of the 40% Noisy protocol on Office-31 with ResNet-50.
Table 6: Performance comparison of the OpenSet1 protocol on the Office-31 dataset with AlexNet Krizhevsky et al. (2012).
Table 7: Performance comparison of the OpenSet2 protocol on the Office-31 dataset with ResNet-50. Results of CoBRF* are from a more challenging setup. Refer to Sec. 4.5.
```

Main Content:
```

Section Title: INTRODUCTION
  INTRODUCTION In recent years, domain adaptation has been researched as it can help to solve major difficulties in the real world. Due to the huge overhead in labeling large-scale training data, it is desirable if an existing network can be adapted to different target domains. More importantly, it is common that the training dataset for adaptation is noisy and small, or the labels in the target domain do not match with the source or even unknown. These are inherent challenges in the domain adaptation problem as in real world it is common for the data to contain such class bias, noise and unlabeled data. However, in practice, since the adapted networks are often overfitted to the provided source data or the data distribution of the target domain is frequently quite different from the source, they do not perform well to the target domain. To properly deal with these real-world conditions with insufficient information, it is critical to learn the shared data distribution that is effective both in the source and target domain. To this end, we propose the collaborative training algorithm of balanced random forest (CoBRF) to mitigate the challenging problems such as noisy labels, lack of training data, and misaligned or unknown categories (open set categorization). In random forests, multiple decision trees are learned by optimizing the information gain for the randomly selected subset features at each node split. Since random forests ensemble the internal decision trees, they are more robust to noise and overfitting problem than single decision trees. To improve the robustness of the random forests, we take one step further by balancing the decision trees, i.e., maximizing the number of leaf nodes for the same tree depth. Our method builds more balanced decision trees by enforcing the sizes of the data in the left and right child nodes to be equal. While this split strategy is not locally optimal in terms of information gain, the resulting decision trees have far more leaf nodes, and it endows more expressive power which can be helpful in dealing with noise and unseen data or classes. It also helps to avoid overfitting as it prevents a node committing too early Under review as a conference paper at ICLR 2020 for a specific pattern, or in other words, it postpones the decision as late as possible so that various discriminant information in the training data can be fully considered. To enforce even splits while maintaining the discriminability, the CoBRF uses the hyperplanes estimated by the linear support vector machine (SVM). First, it randomly assigns the classes in the nodes to binary pseudo labels and equalizes the sizes of two pseudo classes by randomly removing data in the larger class. Then a linear classifier is found by SVM, and its hyperplane is translated until the data sizes on both sides are equal. In a sense, it finds the even split of the data projected onto the normal direction of the hyperplane and places the hyperplane there. The node split by the translated hyperplane is simple yet effective. The ablation study in Sec. 4.2 confirms that the CoBRF boosts the performance compared to the baseline random forests. Since the above training process only considers maximizing the information gain of labeled data in the source domain, which is referred to 'class information gain', it does not resolve the domain misalignment problem between the source and target domain. Because the target labels are not available during training, we try to keep the overall distribution of the target data as close to that of the source data as possible. Since the source data are evenly split, we guide the algorithm to minimize the information gain between the source and target domain, which encourages even split of the target data also. The CoBRF combines the ideas, minimizing the 'domain information gain' between source and target data for the domain alignment while keeping the class information gain to be maximized. Note that the domain alignment term is the same as the negative information gain of the binary domain labels (source/target). Thus, the CoBRF can be seen as an example of adversarial learning, as it considers the domain information gain in an adversarial manner compared to the conventional objective function of the random forest. We summarize the main contributions as three-fold. • We introduce the collaborative training algorithm based balanced random forest (CoBRF) using the discriminative and even node split function. Linear SVM with binary pseudo labeling is used to find the discriminative hyperplane and the even split ensures the decision tree to be balanced. • We also adopt the adversarial learning of domain information gain to align the source and target data distribution. To align two domains, the information gain between the source and target data is minimized, which learns the common data distribution of both the (unlabeled) target domain and the source domain data. • We perform an extensive evaluation of the domain adaptation to show the performance of the proposed method according to various challenging evaluation protocols. Specifically, it is compared to the baseline and state-of-the-art methods using noisy and small training data, and with open-set domain adaptation protocols. In both cases we observe significant performance improvements.

Section Title: RELATED WORK
  RELATED WORK

Section Title: DOMAIN ADAPTATION
  DOMAIN ADAPTATION Recently adversarial learning has been one of the dominant approaches in domain adaptation with deep neural networks. The gradient reversal layer  Ganin & Lempitsky (2015)  is introduced to train the networks so that the discrimination of source and target domains is penalized. It improves the classification performance compared to the networks learned only with the source data.  Tzeng et al. (2017)  suggest the domain adaptation framework based on the discriminative network learning, which assigns individual weights to the source and target domains. In training the networks, they also consider the adversarial weight update to align the domains. Several other domain adaptation papers in adversarial learning using conditional learning  Long et al. (2018) , domain-symmetric  Zhang et al. (2019) , and collaborative  Zhang et al. (2018b)  methods have been introduced. Also, in  Tzeng et al. (2014) ;  Long et al. (2015 ; 2016), maximum mean discrepancy (MMD)-based methods have been studied.  Tzeng et al. (2014)  propose the domain confusion loss to improve domain distribution alignment.  Long et al. (2015)  introduce the task-specific embedding and multiple kernel approach along with MMD to decrease the domain discrepancy. The residual transfer module presented in  Long et al. (2016)  associates the classification ability of the source and target domain. MMD is further extended to multiple domain alignment in the joint adaptation networks (JAN)  Long et al. (2017)  using adversarial learning. The generative adversarial networks  Radford et al. (2015)  are adopted in Under review as a conference paper at ICLR 2020 many domain adaptation methods  Liu & Tuzel (2016) ;  Sankaranarayanan et al. (2018) ;  Volpi et al. (2018) . CoGAN proposed by  Liu & Tuzel (2016)  learns the joint distribution of multiple domains without corresponding image pairs.  Sankaranarayanan et al. (2018)  propose the combined adversarial and discriminative learning method using the generator and discriminator of GAN.

Section Title: EVALUATION PROTOCOLS IN DOMAIN ADAPTATION
  EVALUATION PROTOCOLS IN DOMAIN ADAPTATION Recently, many challenging protocols are introduced to evaluate the domain adaptation in realistic settings. Regarding domain generalization on deep neural networks  Li et al. (2018) ;  Balaji et al. (2018) , they divide multiple domain data into training and test set, then use the leave-one-domain-out scheme for evaluation. The domain adaptation on the partially overlapping source and target domains is presented in  Zhang et al. (2018a) ;  Cao et al. (2018) . Multiple sources and target domains are mixed into the source or target domains in  Zhao et al. (2018) ;  Mancini et al. (2018) ;  Hoffman et al. (2018) . The adaptable model is aimed to be learned using the distribution to the multiple domains of the mixed set. Recently, several works  Saito et al. (2018) ;  Panareda Busto & Gall (2017) ;  Tan et al. (2019)  address the open set domain adaptation. They assume that there exist unknown and partially overlapped known classes between domains. On the other hand, the domain adaptation methods under small training data  Hong et al. (2017)  and the noisy data  Shu et al. (2019)  are studied to address the real-world condition.  Hong et al. (2017)  use single training data per person, and  Shu et al. (2019)  artificially corrupt the class labels or features of the source domain for the robustness evaluation. These protocols are challenging as they pose difficult problems of overfitting, class misalignment, noisy, lack of training data, and little overlap.

Section Title: RANDOM FOREST AS AN ENSEMBLE LEARNING METHOD
  RANDOM FOREST AS AN ENSEMBLE LEARNING METHOD The ensemble of multiple learners has widely been used to avoid the overfitting problem  Singh et al. (2016) ;  Han et al. (2017 ; 2016);  Pi et al. (2016) .  Singh et al. (2016)  introduce the regularization method for network learning, which works with a variety set of network architectures and performs better than the existing regularization methods (i.e.dropout). Branchout  Han et al. (2017)  is devised for layer-level regularization in visual tracking, where multiple branches of fully connected layers are randomly selected in training. Random forest  Breiman (2001)  combines multiple random decision trees to build robust classifier or regressor. Random forests have been applied to many applications such as object tracking  Zhang et al. (2017) , feature point detection  Lindner et al. (2014) , and speech recognition  Black & Muthukumar (2015) , to name a few. However, it should be emphasized that the most important benefit is the mitigation of overfitting by ensembling multiple decision trees. As noticed in the literature  Wyner et al. (2017) ;  Gomes et al. (2017) , the random forests tend not to propagate severe overfitting error even with a large number of trees. There have been many recent works to improve the performance of random forests:  Dheenadayalan et al. (2016)  proposes pruning nodes for efficient learning,  Ristin et al. (2015a)  presents incremental modeling for large scale recognition, and  Probst & Boulesteix (2017)  investigates how to tune the number of trees. SVM  Yao et al. (2011) ;  Ristin et al. (2015b)  or random projection  Bosch et al. (2007) ;  Bossard et al. (2014)  is often used as the binary classifier for better node split. Training balanced decision trees has been also an important topic  Bosch et al. (2007) ;  Bossard et al. (2014) ;  Yao et al. (2011) ;  Lei et al. (2014) ;  Ristin et al. (2015b) . They split a node into child nodes by the binary classifier, which is trained by evenly-divided training data in the node. We argue that training balanced random forests helps to alleviate the overfitting problem since balancing random decision trees avoids the biased distribution in the specific domain but prefers the common representation to any domains. Hence, we introduce the learning algorithm that enforces the even-split constraint by shifting the hyperplane(Sec. 3.1) for balanced random forests. Although there have been studies of the balanced training of random forests, we provide elaborate training process of balanced random forests to learn common representations for the domain adaptation task. The effectiveness of the balanced random forests is shown by extensive domain adaptation experiments.

Section Title: PROPOSED METHOD
  PROPOSED METHOD In this section, we first explain the limitation of the conventional random forests for the domain adaptation task, and then we introduce the even node split function in Sec.3.1 and the domain information gain for selecting the domain-aligned split function in Sec. 3.2.

Section Title: EVEN CONSTRAINED RANDOM FOREST LEARNING
  EVEN CONSTRAINED RANDOM FOREST LEARNING A random forest consists of multiple random decision trees, whose nodes learn a binary classifier for the randomly-selected subset of features to maximize the information gain (IG). We abuse the term node for the training data in the node interchangeably. The entropy of a node n is defined as E C (n) = − c∈C(n) p c (n) · log (p c (n)) , (1) where C(n) represents the set of classes of the data in n, and p c (n) is the probability of class c in n (i.e., the data count of class c divided by |n|). Then the information gain for a node n with the left Under review as a conference paper at ICLR 2020 and right child nodes is defined as Conventionally, the simple split functions that compares only a couple of feature values are used, but recently more elaborate split functions using the linear classifiers are used  Yao et al. (2011) ;  Ristin et al. (2015b) . The hyperplane split function for a node n is written as ν n (x) = go left, if w n · ψ n (x) < k n go right, otherwise, (3) where ψ n (·) is the sub-feature selection function and w n and k n are the hyperplane parameters either randomly set or learned by a linear support vector machine  Cortes & Vapnik (1995) . The hyperplane with the largest information gain is the most discriminative classifier at the given node, but for the entire decision tree and the random forest it may not be the best option, because it causes the learned trees to be skewed and not well balanced (Fig. 1a). We propose to add a hard constraint of equal-size in splitting the node to get more balanced trees. The detailed learning process is as follows. For the SVM to build a binary classifier, the classes in the node are randomly assigned to binary pseudo labels, and the training data for each class are assigned to the corresponding pseudo label. As the data sizes of the pseudo labels will be different, we randomly erase the data in the larger pseudo class to match the sizes. Then the base hyperplane (w n and k n ) is computed to classify the binary pseudo labels. Still, the split of the training data by the hyperplane is not equal-sized; thus we update the bias k n of the hyperplane so that the data size on each side is equal or differs at most by one (||n lef t |−|n right || ≤ 1). Geometrically this process is moving the hyperplane along the normal vector w n , so that it is placed at the even split of the data projected onto the normal direction (Fig. 2a,2b). Among the estimated hyperplanes from randomly selected sub-features, the one that maximizes the information gain IG C (n) is chosen as the node split function. To build a decision tree, like the conventional random forest, the node split is repeatedly applied until the maximum depth is reached or too few data are left in the node (Fig. 1b). Inherently the proposed split method creates balanced trees, and for the same depth, the number of nodes is much larger than that of the conventional random forest. We argue that having more (leaf) nodes in the decision tree has advantages in domain adaptation tasks. The conventional split function is locally optimal, but because of that, it is more susceptible to overfitting by committing too early, and eventually, it decreases the discriminative power of the entire random forest. In the balanced trees, the data sizes in the leaf nodes are almost the same; thus, they represent local data distribution more faithfully. The even-size constraint can be thought of as a regularization in learning decision trees. The experimental results of the ablation study in Sec. 4.2 supports this argument.

Section Title: COLLABORIVE LEARNING OF RANDOM FORESTS
  COLLABORIVE LEARNING OF RANDOM FORESTS Balanced data distribution is a big advantage in domain adaptation. However, as it does not use the unlabeled target data for learning, it still does not correctly align the data distribution of the source and target domain. In other words, the distribution of the target data also needs to be considered in building a random forest. We propose a new collaborative measure for selecting the split function that considers both the conventional IG and the domain distribution of the source and target data together. The collaborative information gain (co-IG) is defined as co-IG(n) = (1 − λ) IG C (n s ) − λ IG D (n), (4) where λ is a user parameter, n s is the labeled training data (in the source domain), and D is the binary domain label {source, target} representing the domain that the data belongs to. More specifically, IG D (n) is the information gain on the domain distribution, when the data labels are either source or target, disregarding the classes in the source domain. The CoBRF chooses the hyperplane that maximizes co-IG when splitting the nodes. Note that the IG on the domain distribution, IG D (n), is subtracted in Eq. 4, to ensure that we prefer even distribution of the source and target data in the child nodes. IG D (n) is minimized when both Under review as a conference paper at ICLR 2020 (a) Without domain alignment (b) With domain alignment source and target data are evenly split into the children, as it maximizes the entropy of the children (Eq. 2). Thus IG D (n) in CoBRF collaboratively enforce the even split of target data also. Fig. 2c illustrates the effect of co-IG compared to conventional IG in split function selection. The yellow line has higher IG as it segments the source data (colored) better, but co-IG also considers the split of target data (gray). Although the blue line has lower IG than the blue, it separates the target data more evenly; thus, the blue line is chosen as the split function. The resulting decision trees by CoBRF are shown in  Fig. 3 . The co-IG is closely related to the adversarial learning of the network backpropagation  Long et al. (2015) ;  Ganin et al. (2016) . In this framework, IG C and IG D can be thought of as the classification and adversarial domain alignment, respectively. Thanks to the domain alignment term (co-IG), the CoBRF learns the robust models even with very noisy or small training data without overfitting. We validate the proposed methods from the moderate challenging condition such as 40% noise data to the very severe condition such as 80% noise or merely 10% training data. Further, we evaluate the open set domain adaptation, which has received attention in recent years, in the following section.

Section Title: EXPERIMENTAL RESULTS
  EXPERIMENTAL RESULTS

Section Title: EXPERIMENTAL SETTING
  EXPERIMENTAL SETTING We use three domain adaptation datasets such as Office-31  Saenko et al. (2010) , ImageCLEF-DA 1 and  Office-Home Venkateswara et al. (2017) . We evaluate algorithms using three challenging protocols: noisy, small training data, and weakly supervised open set domain adaptation. To train the CoBRF, we use 100 trees with a maximum depth of 8. When there is no training data fallen on a node, we prune the tree at that node. The number of randomly selected feature dimension for the SVM training is set to 250. The input feature of SVM is normalized for the stable learning of the hyperplane. We repeat the SVM training 15 times to select the optimal split in each node. Due to the space limitation, only representative results are shown in this section. Refer to the appendix for detailed information of the datasets, metric, and full experimental results.

Section Title: ABLATION STUDY
  ABLATION STUDY We evaluate the effect of components in the CoBRF proposed in this paper. The CoBRF uses the balanced pseudo labeling (mid_pseudo), and the hyperplane shift (h_shift) for even data split. As binary labeling is necessary for hyperplane computation, the pseudo method uses randomly-assigned binary pseudo labels without removing the data to make label sizes equal. Therefore the four Under review as a conference paper at ICLR 2020 combinations of (pseudo, mid_pseudo)×h_shift are tested with Office-31. The baseline is (pseudo + no_h_shift). As shown in  Table 1  and 8 of appendix, both balancing the pseudo labels and enforcing even splits by translating hyperplanes improve the performance.  Table 2  shows the effect of the parameter λ in co-IG formulation (Eq. 4). It confirms that optimizing for the cobalanced distribution helps the alignment of domain distributions.  Table 3  also represents the effect of hyperparameters in training CoBRF. The accuracy of the maximum depth 8 with 100 decision trees shows the best result.

Section Title: NOISY DATA
  NOISY DATA In this experiment, the training labels of the specified portion of the source domain are randomly changed for the noise condition, which is also referred to as the label corruption in  Shu et al. (2019) . Corruption levels are set to 40, 60 and 80% of the source domain (refer to the appendix for full experimental results). We conduct noisy conditions for the Office-31, ImageCLEF-DA and Office-Home datasets in  Table 4  and 5. We test DAN  Long et al. (2015) ,  JAN Long et al. (2017) , and CDAN+E  Long et al. (2018)  Under review as a conference paper at ICLR 2020 algorithms 2 on the same noisy condition for comparison.  Table 5  shows the result of 40% noisy training data for Office-31 with ResNet-50  He et al. (2016) . The proposed CoBRF outperforms all other algorithms in average accuracy. The result confirms that the CoBRF improves the performance in most settings, and interestingly,  Table 4  shows the more severe the noise is, the larger the performance improvement gets.

Section Title: SMALL TRAINING DATA
  SMALL TRAINING DATA In this experiment, we use only 10% of training samples to evaluate the performance against overfit- ting. We perform the experiments on Office-31, Office-Home, and ImageCLEF-DA datasets with ResNet-50. The result of  Table 4  shows the CoBRF achieves favorable performance compared to the other algorithms. Full experimental results are presented in the appendix.

Section Title: OPEN SET EXPERIMENTS
  OPEN SET EXPERIMENTS We perform two open set evaluation protocols proposed in  Saito et al. (2018) ;  Tan et al. (2019) .

Section Title: OpenSet1
  OpenSet1 The first open set protocol  Saito et al. (2018)  uses 11 classes (10 known and 1 unknown) of the Office-31 dataset. The labels from 1 to 10 of both source and target domains are marked as the known class, and all data with label 21∼31 in the target domain are used as one unknown class. According to  Saito et al. (2018)  the unknown class of the source data is not used in training, and the unknown class of the target data is classified by thresholding the class probability. The thresholding value is set to 0.3.  Table 6  shows the result of CoBRF as well as the state-of-the-art methods. The 2 DAN, JAN: https://github.com/thuml/Xlearn , CDAN+E: https://github.com/thuml/CDAN

Section Title: Under review as a conference paper at ICLR 2020
  Under review as a conference paper at ICLR 2020 CoBRF achieves the best performance among all algorithms on Office-31. It also demonstrates the effectiveness of the proposed method under the challenging adaptation condition. OpenSet2: Recently, another open set protocol is proposed in  Tan et al. (2019) , which uses partially overlapping known classes between the source and target domain. Each domain has 5 known-and- common classes, 5 known-but-different classes, and 1 unknown class for all other training data, thus in total there are 15 known and 1 unknown classes. First, according to  Tan et al. (2019) , 3 samples per class per domain and 9 samples in the unknown class per domain are used in training. Hence the total number of training samples is (3 samples × 10 classes/domain + 9 samples_in_unknown) × 2 domains = 78. All other algorithms and CoBRF results in  Table 7  are using this protocol. Additionally, we evaluate more challenging setup, where the training data are sampled regardless of the domain, i.e., the data in common classes (including unknown) are merged before being sampled. In this case, 3 samples × 15 classes + 9 samples_in_unknown = 54 in total are used. The results in CoBRF* rows are acquired in this setup. We confirm that CoBRF works well compared to state-of-the-art methods under the OpenSet2 and more challenging condition.

Section Title: CONCLUSION
  CONCLUSION We propose a novel cobalanced random forest (CoBRF) algorithm for challenging conditions and open set protocols. The CoBRF enhances the discriminative ability of the random forest by building balanced decision trees by the even split. The proposed CoBRF algorithm also employs the adversarial learning for domain alignment and benefits the effectiveness against the overfitting to the labeled source data. We extensively evaluate the proposed algorithms using challenging experimental protocols and demonstrate its superior performance over the baseline and state-of-the-art methods.

```
