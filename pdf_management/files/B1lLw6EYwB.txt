Title:
```
Published as a conference paper at ICLR 2020 GAP-AWARE MITIGATION OF GRADIENT STALENESS
```
Abstract:
```
Cloud computing is becoming increasingly popular as a platform for distributed training of deep neural networks. Synchronous stochastic gradient descent (SSGD) suffers from substantial slowdowns due to stragglers if the environment is non- dedicated, as is common in cloud computing. Asynchronous SGD (ASGD) meth- ods are immune to these slowdowns but are scarcely used due to gradient staleness, which encumbers the convergence process. Recent techniques have had limited suc- cess mitigating the gradient staleness when scaling up to many workers (computing nodes). In this paper we define the Gap as a measure of gradient staleness and propose Gap-Aware (GA), a novel asynchronous-distributed method that penalizes stale gradients linearly to the Gap and performs well even when scaling to large numbers of workers. Our evaluation on the CIFAR, ImageNet, and WikiText-103 datasets shows that GA outperforms the currently acceptable gradient penalization method, in final test accuracy. We also provide convergence rate proof for GA. Despite prior beliefs, we show that if GA is applied, momentum becomes beneficial in asynchronous environments, even when the number of workers scales up.
```

Figures/Tables Captions:
```
Figure 1: Final test and train error for different numbers of asynchronous workers N . The figure shows the average (bold line) and standard deviation (band) of 5 runs on the CIFAR10 dataset using the WideResNet model. The black dashed line is the SGD error using a single worker.
Figure 2: Final test error for different numbers of asynchronous workers N . Each line in the figure represents the average (bold line) and standard deviation (band) of 5 runs on a specific framework. The black dashed line represents the average result of SGD using a single worker.
Table 1: Test accuracy on different frameworks a . N=32.
Table 2: Final test perplexity using Transformer-XL on WikiText-103. (Base- line 24.25. Lower is better).
Table 3: ResNet-50 ImageNet final test accuracy (Baseline 75.64%)
```

Main Content:
```

Section Title: INTRODUCTION
  INTRODUCTION The steady growth of deep neural networks over the years has made it impractical to train them from scratch on a single worker (i.e., computational device). Distributing the computations over several workers can drastically reduce the training time. However, due to the sequential nature of the widely used stochastic gradient descent (SGD) method, distributing the process is not an easy task. Synchronous SGD (SSGD) is the most common method used to distribute the learning process across multiple workers. Several recent works ( Mikami et al., 2018 ;  Ying et al., 2018 ;  Yamazaki et al., 2019 ;  Goyal et al., 2017 ) have shown that SSGD can achieve large speedups while maintaining high accuracy. The major drawback of SSGD is that its speed is confined to the slowest worker in every iteration. This shortcoming is magnified in non-dedicated 1 environments such as cloud computing. For this reason, all the above mentioned works were forced to use homogeneous workers in a dedicated network, which serves to reduce the variance in the workers' iteration times. Unlike cloud computing, dedicated networks are expensive and therefore not available to most users. In asynchronous SGD (ASGD), each worker communicates independently of the others, thereby addressing the major drawback of SSGD. ASGD enjoys linear speedup in terms of the number of workers, even on non-dedicated networks. This makes ASGD a potentially better alternative to SSGD when using cloud computing. Unfortunately, ASGD also has a significant weakness known as gradient staleness; the gradients used to update the parameter server's (master) parameters are Published as a conference paper at ICLR 2020 often based on older parameters and therefore are inaccurate. Prior works have shown that gradient staleness severely hinders the convergence process by reaching reduced final accuracy ( Chen et al., 2016 ;  Cui et al., 2016 ).  Mitliagkas et al. (2016)  showed that gradient staleness also induces implicit momentum, thus the momentum coefficient γ must be decayed when scaling up the number of workers. Most research works measure the gradient staleness of a worker according to the delay: the number of master updates since the worker began calculating the stochastic gradient g, until g is used to update the master. To overcome gradient staleness, Zhang et al. (2015b) proposed Staleness-Aware (SA), which penalizes the step size of stale gradients linearly to their delay. This method was later embraced by other works ( Jiang et al., 2017 ;  Hardy et al., 2017 ;  Giladi et al., 2019 ) and is currently the common method for penalizing stale gradients. Unfortunately, this method suffers from a degradation of the final accuracy, especially when scaling up the number of workers. In Section 4.1, we show that the main reason for this degradation is the over-penalization and under-penalization caused by SA. SSGD and ASGD rely on hyperparameter tuning for every different number of workers ( Shallue et al., 2018 ). Tuning is extremely time-consuming, thus avoiding it is beneficial, whenever possible.

Section Title: Our contribution
  Our contribution To mitigate gradient staleness while minimizing the degradation of final accuracy, we define a measure of gradient staleness we refer to as the Gap. The Gap is based on the difference between the parameters used to calculate the gradient and the parameters on which the gradient is applied. We propose a new method called Gap-Aware (GA) that penalizes the step size of stale gradients linearly to their Gap, while eliminating the over-penalization or under-penalization of SA. No new hyperparameters are introduced using the GA method. • We show that GA out-performs SA, especially as the number of workers scales up. • We prove that the convergence rate of the GA-ASGD algorithm with a non-convex loss function is consistent with SGD: O 1 √ BK where K is the total number of steps and B is the batch size. • We show that penalizing the gradient itself rather than the step-size, eliminates under-penalization. • Our results suggest that GA can be used without re-tuning the hyperparameters. • As opposed to conclusions by  Mitliagkas et al. (2016) , we show that applying momentum in an asynchronous environment is advantageous (using GA), even when multiple workers are used. • We combine GA with Adam ( Kingma & Ba, 2015 ) (Adam-GA), and show that Adam-GA achieves almost two orders of magnitude better perplexity than Adam or Adam-SA (which combines Adam with SA) using several workers on the Transformers-XL model. Our results establish GA as a superior gradient-penalizing option to SA and suggest that using GA is a preferable alternative to SSGD in non-dedicated networks such as cloud computing, even when scaling to large numbers of workers. To validate our claims, we performed experiments on the CIFAR10, CIFAR100 (Krizhevsky, 2012), ImageNet ( Russakovsky et al., 2015 ), and WikiText-103 ( Merity et al., 2016 ) datasets, using several state-of-the-art architectures. A version of GA has reached 72.18% final test accuracy on the ImageNet dataset using 128 simulated asynchronous workers. As far as we know, this is the largest number of asynchronous workers reported to converge on ImageNet.

Section Title: RELATED WORK
  RELATED WORK Eliminating gradient staleness is a challenging task and several papers suggested techniques to reduce its detrimental effects.  Zheng et al. (2017)  proposed DC-ASGD, which uses a Taylor expansion to mitigate the gradient staleness. EASGD (Zhang et al., 2015a) uses a center force to pull the workers' parameters toward the master's parameters. Both DC-ASGD and EASGD achieve high accuracy on small numbers of workers, but fall short when trained on large clusters.  Chan & Lane (2014)  proposed penalizing stale gradients by reducing their size and thus limiting their effect on the learning process. They suggest decaying the learning rate exponentially to the delay; this makes the step size arbitrarily small when the amount of workers grows, virtually ceasing the learning process. As part of their convergence analysis,  Dutta et al. (2018)  suggest a penalizing method that is linear to the norm between the master and worker's parameters. However, their method introduces another hyperparameter, which requires additional time to tune. As opposed to other methods, GA performs well even when the number of workers is large, without introducing new hyperparameters.  Gupta et al. (2016)  as well as  Aji & Heafield (2019)  suggest collecting several gradients before updating the master to reduce the effects of gradient staleness.  Wen et al. (2017)  propose minimizing the size of the gradients to reduce communication times. GA is orthogonal to both of these approaches.

Section Title: BACKGROUND
  BACKGROUND The goal of an optimization procedure is to minimize f (θ), where f is a smooth, but not necessarily convex, objective function (a.k.a. loss) and the vector θ ∈ R d is the model's parameters: θ * = arg min θ∈R d f (θ) := E ξ [F (θ; ξ)] (1) where ξ ∈ Ξ is a random variable from Ξ, the entire set of training samples Ξ = {1, 2, · · · , M }. F (θ; ξ) is the stochastic loss function with respect to the training sample indexed by ξ. SGD is commonly the workhorse behind the optimization of deep neural networks. Denoting η as the learning rate and k as the step number, SGD's iterative update rule is: θ k+1 = θ k − η k ∇f (θ k ). We denote X k as the variable X at the k th step, where X is any variable. Momentum Momentum ( Polyak, 1964 ) is a widely adopted optimization technique due to its accelerated convergence and oscillation reduction ( Sutskever et al., 2013 ). Instead of simply using the gradient, the momentum iterative update rule 2 uses an exponentially-weighted moving average of gradients called the update vector: v k+1 = γv k + ∇f (θ k ). The update rule is: θ k+1 = θ k − η k v k+1 . Nesterov's Accelerated Gradient (NAG) ( Nesterov, 1983 ) is a well-used variation of momentum that has been proven to achieve quadratic speedup in convergence rate compared to SGD.

Section Title: ASYNCHRONOUS SGD (ASGD)
  ASYNCHRONOUS SGD (ASGD) We consider the commonly used ASGD, which operates with a parameter-server (master), used to keep the model's most up-to-date parameters. Each worker maintains a replica of the model's parameters. The workers run in parallel and synchronize with the master independently from each other at the beginning of each batch iteration. We denote τ k as the delay at the k th step. The worker and master algorithms are given by Algorithm 1 and 2, respectively, where B is the batch-size and ξ k,b denotes the b th sample in the batch sampled at the k th iteration. Gradient staleness appears when a gradient g k is computed on parameters θ k−τ k but applied to different parameters θ k . Save and send current parameters θ k+1 to worker i Over-Penalizing Let us assume that at some step k, after τ master updates we get θ k = θ k−τ just as the gradient calculated on θ k−τ is applied. This means there is no gradient staleness for the next gradient update since it was computed using the same parameters on which it is applied. Unfortunately, the delay remains τ > 0, thereby causing over-penalization when SA is used. Additionally, τ scales linearly with N , which dramatically reduces η when N is large. Consequently, on large numbers of workers, the convergence rate of SA is sluggish and its accuracy plummets.

Section Title: Under-Penalizing
  Under-Penalizing The SA method doesn't take into account that when using momentum, the update step also contains past gradients. To emphasize the importance of this issue, let's examine a fictional example: assume that some gradient g k is very stale (τ k is large). Following the SA technique, the update rule is: θ k+1 = θ k − η k τ k · v k+1 = θ k − η k τ k · (γv k + g k ). This means the stale gradient g k is indeed penalized by being multiplied by η k τ k , which is small; thus g k doesn't change the parameters much. We further assume the next iteration is very fast (τ k+1 is small). The next update will be: θ k+2 = θ k+1 − η k+1 τ k+1 · (γ 2 v k + γg k + g k+1 ). The stale gradient g k is multiplied by γ η k+1 τ k+1 , which is large (assuming η k ≈ η k+1 ). Despite the fact that g k was stale, it still has a significant impact on the learning process. In other words, g k is under-penalized. To eliminate this possibility we penalize the stale gradient itself rather than the learning rate. Using this method, the staleness of each gradient is accounted for within the update vector v.

Section Title: GAP-AWARE (GA)
  GAP-AWARE (GA) In this section we propose a new method called Gap-Aware to mitigate over and under-penalization.

Section Title: THE GAP AS A MEASURE OF GRADIENT STALENESS
  THE GAP AS A MEASURE OF GRADIENT STALENESS An intuitive method to measure gradient staleness would be: ∇f (θ k )−∇f (θ k−τ k ) . This essentially measures the difference between the stale gradient and the accurate gradient that is computed on the up-to-date parameters. (Of course, ∇f (θ k ) is never calculated in ASGD algorithms.) Commonly used in deep learning is the Lipschitzian gradients assumption: Setting x = θ k , y = θ k−τ k into Equation 2 we get: ∇f (θ k ) − ∇f (θ k−τ k ) ≤ L θ k − θ k−τ k . This implies that θ k − θ k−τ k is a valid (and easily calculated) measure of the gradient staleness. This measure also addresses the delay's over-penalization; using the same simple example described in Section 4.1, the term θ k − θ k−τ k will now be zero, correctly measuring the gradient staleness. The learning rate η, commonly decays as the training progresses. This decay can be viewed as a built-in penalization to reduce variance. Following this notion, we suggest reducing the staleness penalization as η decays. To accommodate all the attributes above, we define the Gap: Definition 1. G k , the Gap at the k th step, is defined as the minimal number of updates required to traverse the current distance between the master's and worker's parameters using the maximal learning rate and assuming all gradients have an average norm. G k ∈ R is defined as: Where C = η max E k [ ∇f (θ k−τ k ) ] is a constant representing the maximal distance the parameters can travel in a single update, given the gradient's norm is the average gradient norm. Definition 1 means that dividing η by the Gap produces larger steps than those produced by SA This allows exploring more distant minimas while still mitigating the gradient staleness. Note that E[G k ] = τ k occurs only if all previous τ k updates were in the exact same direction, which rarely happens. Empirically, we found that E[G k ] < τ k (See Appendix C.8). To mitigate the gradient staleness, while eliminating the over-penalization and under-penalization, we divide the gradients themselves by their respective Gap. We refer to this method as Gap-Aware (GA).

Section Title: CONVERGENCE ANALYSIS
  CONVERGENCE ANALYSIS In this section we provide the outlines of the convergence analysis. The complete proofs are given in Appendix B. The GA-ASGD update rule (without momentum) is: The convergence rate is the speed (or number of steps) at which a convergent sequence approaches its limit. We follow ideas similar to  Lian et al. (2015)  to show that the upper bound of the convergence rate of GA on a non-convex loss function is similar to that of SGD. Assumption 1. We assume the following, commonly-used assumptions, hold: • Unbiased gradient: The stochastic gradient ∇F (θ; ξ) is unbiased: • Bounded variance: The variance of the stochastic gradient is bounded: • Lipschitzian gradients: See Equation 2. • Independence: All the random variables {ξ k,b } k=1...K;b=1...B , are independent. • Bounded age: All delay variables τ 1 , ...τ K are bounded: Theorem 1. Assume that Assumption 1 holds and the learning rate sequence {η k } k=1···K satisfies: We have the following ergodic convergence rate for the iteration of GA-ASGD: Where E[·] denotes taking expectation in terms of all random variables. To simplify the upper bound in Theorem 1, we observed that setting the learning rate η k such that the expression η k G k is a constant value across all k obtains the following convergence rate: Corollary 1. Assume that Assumption 1 holds and that η k are set such that η k G k is constant for any k as follows: If the maximal delay parameter T satisfies: K ≥ 4BL(T + 1) 2 (f (θ 1 ) − f (θ * )) σ 2 (10) then the output of GA satisfies the following ergodic convergence rate: Corollary 1 claims that if the total iterations K is greater than O(BT 2 ), the convergence rate achieves O(1/ √ BK), which is consistent with the convergence rate of ASGD presented in  Lian et al. (2015) , and with the convergence rate of SGD.

Section Title: GAP-AWARE VERSIONS
  GAP-AWARE VERSIONS We explore three ways to measure G k : • Global: G k ∈ R as defined in Definition 1. • Layer-wise: Every layer is penalized differently and independently. G k ∈ R P where P is the number of layers in the model. We denote 1 S as an S-dimensional vector of ones. We denote any vector X *,p is the p th layer in the vector X * . Every element in G k is calculated per-layer: • Parameter-wise: Every parameter (element in the parameter vector θ) is penalized differently and independently. G k ∈ R d where d is the number of parameters. We denote | · | on vector X, as the absolute value per element of X. Every element in G k is calculated and applied per-element: Where C ∈ R d is also calculated element-wise. Specifically, C = η max E k [|∇f (θ k−τ k )|]. We tested these variations on three different frameworks 3 to determine which technique has the best performance.  Figure 1  demonstrates that the parameter-wise method (equation 13) resulted in the best test and train error. Since this phenomenon repeats across all frameworks, we henceforth use the parameter-wise method in the Gap-Aware algorithm. We denote as an element-wise multiplication between vectors and describe the final GA algorithm of the master as Algorithm 4. The worker algorithm remains the same as in Algorithm 1.

Section Title: EXPERIMENTS
  EXPERIMENTS We simulated multiple distributed workers 4 to measure the final test error, train error, and convergence speed of different cluster sizes. To validate that penalizing linearly to the Gap is the factor that leads to better performance, we used the same hyperparameters across all the tested algorithms (see Appendix C.4). These hyperparameters are the ones tuned for a single worker, suggested by the authors of the respective papers for each framework. We simulated the workers' execution time using a gamma-distributed model ( Ali et al., 2000 ) (see Appendix C.3), where the execution time for each individual batch was drawn from a gamma distribution. The gamma distribution is a well-accepted model for task execution time, which naturally gives rise to stragglers. The importance of asynchronous over synchronous training is explained in Appendix D.

Section Title: Combining GA with DANA
  Combining GA with DANA One way to verify whether it is better to penalize using the Gap or the delay, is to change the Gap while fixing the delay, and examining the results using GA and SA. Momentum generally increases the norm of the update vector; this in turn, increases the effective step size thus increasing the Gap for a given delay.  Hakimi et al. (2019)  introduced DANA, which uses the Published as a conference paper at ICLR 2020 momentum to estimate the master's parameters at the time of the gradient update, thus decreasing the Gap. The combination of decreasing the Gap using DANA and penalizing stale gradients using GA or SA is easily integrated since both methods are orthogonal. In our experiments, we also compared between DANA-Gap-Aware (DANA-GA) and DANA-Staleness-Aware (DANA-SA). We note that since DANA decreases the Gap, DANA-GA penalizes much less than DANA-SA (see Appendix C.8).

Section Title: Algorithms
  Algorithms Our evaluations consist of the following algorithms: • Baseline: Single worker with the same tuned hyperparameters as in the respective framework's paper. See Appendix C.4. • ASGD: ASGD (Algorithm 2) without momentum (γ = 0). • NAG-ASGD: ASGD with momentum (Algorithm 2) using a NAG optimizer. • Staleness-Aware: SA as described in Algorithm 3, using a NAG optimizer. • Gap-Aware: Parameter-wise GA (Algorithm 4) as described in Section 5, using a NAG optimizer. • DANA: DANA (Algorithm 6) as described in Appendix C.1. • DANA-SA: DANA-Staleness-Aware (Algorithm 7) as described in Appendix C.1. • DANA-GA: DANA-Gap-Aware (Algorithm 8) as described in Appendix C.1. • Adam: Adam (Algorithm 9) as described in Appendix C.1. • Adam-SA: Adam-Staleness-Aware (Algorithm 10) as described in Appendix C.1. • Adam-GA: Adam-Gap-Aware (Algorithm 11) as described in Appendix C.1. Our evaluation was extensive on image classification tasks such as CIFAR10, CIFAR100 (Krizhevsky, 2012), and ImageNet ( Russakovsky et al., 2015 ). It also included a language modeling task using the WikiText-103 corpus ( Merity et al., 2016 ). All datasets and models are detailed in Appendix C.2.

Section Title: EVALUATION ON CIFAR
  EVALUATION ON CIFAR

Section Title: Gradient Staleness Effects
  Gradient Staleness Effects In  Figure 2 , NAG-ASGD shows how gradient staleness is exacerbated by momentum. NAG-ASGD yields high accuracy with few workers, but the test error climbs sharply when more than 16 workers are used. On the other hand, ASGD without momentum performs poorly using few workers. When using many workers, ASGD significantly surpasses NAG-ASGD because of the implicit momentum generated in asynchronous training ( Mitliagkas et al., 2016 ). SA & GA  Figure 2  also demonstrates that both staleness penalization methods (GA and SA) out- perform the naive NAG-ASGD. GA results in better final test error than SA across all experiments. This empirically proves that GA is the better method for penalizing the gradients. We claim this occurs mainly because SA over-penalizes the gradients, thereby making it impossible to reach any distant, good minima when the number of steps is limited (for more details see Appendix C.8). DANA Versions  Figure 2  shows that DANA potentially diverges when N grows as opposed to DANA-GA and DANA-SA. This shows that DANA benefits from staleness penalization. Furthermore, DANA-GA out-performs all other methods and remains close to the baseline's error across all frameworks. The fact that DANA-GA out-performs DANA-SA validates that GA is superior to SA.

Section Title: Tuned ASGD
  Tuned ASGD To validate that the staleness penalization helps overcome the gradient staleness and improve the results, we tuned the momentum and learning rate of ASGD using 32 workers on the 3 frameworks shown in  Figure 2 . For each framework, we performed a grid search of 70 perturbations (See Appendix C.12).  Table 1  shows that GA and DANA-GA, using the same hyperparameters as the baseline, provide similar or better results than tuning both γ and η, which is highly time-consuming. According to  Mitliagkas et al. (2016) , if momentum is used, the asynchronous implicit momentum should impede the convergence as N increases. However, GA and DANA-GA, which use a large momentum, generally perform better than the tuned ASGD even when N is large. This phenomenon repeats across all frameworks, which suggests that GA, and especially DANA-GA, can mitigate the asynchronous implicit momentum problem. Tuning these methods should further improve the results. The graphs of the train error also show the same concepts discussed here regarding the test error and are presented in Figure 6, Appendix C.10. The convergence rate analysis appears in Appendix C.11.

Section Title: IMAGENET EXPERIMENTS
  IMAGENET EXPERIMENTS We conducted experiments on the ImageNet dataset using the ResNet-50 model ( He et al., 2016 ). Every asynchronous worker is a machine with 8 GPUs, so the 128 workers in our experiments simulate a total of 1024 GPUs. For reference,  Goyal et al. (2017)  used 256 GPUs synchronously. The hyperparameters we used are those of the tuned single worker (see Appendix C.4).  Table 3  shows that GA out-performs SA due to the high number of workers, which exacerbates the over-penalizing of SA. Unlike SA, GA out-performs NAG-ASGD as N increases due to successful staleness mitigation. DANA-GA remains close to the baseline and better than any other method as N increases. DANA-GA reaches 72.18% final test accuracy when using 128 workers, which is the most asynchronous workers shown to converge on ImageNet as far as we know.

Section Title: NLP EXPERIMENTS
  NLP EXPERIMENTS NLP tasks are usually trained using Adam ( Kingma & Ba, 2015 ). To test SA and GA we implemented a version of Adam-SA and Adam-GA given by Algorithm 10 and 11, respectively (Appendix C.1). Transformer-XL ( Dai et al., 2019 ) is a state-of-the-art model for NLP tasks; however, its sensitivity to gradient staleness is catastrophic ( Aji & Heafield, 2019 ).  Table 2  shows that GA successfully mitigates the gradient staleness and achieves near-baseline perplexity while SA results in a higher perplexity by almost two orders of magnitude. In this scenario, SA completely fails to mitigate the gradient staleness, proving the superiority of GA. (See hyperparameters in Appendix C.4).

Section Title: CONCLUSIONS
  CONCLUSIONS The goal of this work is to mitigate gradient staleness, one of the main challenges of ASGD. We argue that penalizing stale gradients linearly to the delay, as done in the widely used SA method, flounders due to over and under-penalization. We defined the Gap to measure gradient staleness and proposed GA, a novel asynchronous distributed technique that mitigates the gradient staleness by penalizing stale gradients linearly to the Gap. We showed that GA surpasses SA across all frameworks, especially in NLP problems or when the number of workers is large. This presents GA as a superior alternative for staleness penalizing. We further introduced DANA-GA and demonstrated that DANA-GA mitigates gradient staleness better than any of the other methods we compared. Despite prior belief, DANA-GA's superb performance enables the use of momentum in asynchronous environments with many workers; it presents a desirable alternative for parallel training with multiple workers, especially on non-dedicated environments such as cloud computing. In future work, we plan to examine what makes GA perform so well in NLP tasks.

```
