Title:
```
Under review as a conference paper at ICLR 2020 BOOTSTRAPPING THE EXPRESSIVITY WITH MODEL- BASED PLANNING
```
Abstract:
```
We compare the model-free reinforcement learning with the model-based ap- proaches through the lens of the expressive power of neural networks for poli- cies, Q-functions, and dynamics. We show, theoretically and empirically, that even for one-dimensional continuous state space, there are many MDPs whose optimal Q-functions and policies are much more complex than the dynamics. We hypothesize many real-world MDPs also have a similar property. For these MDPs, model-based planning is a favorable algorithm, because the resulting poli- cies can approximate the optimal policy significantly better than a neural network parameterization can, and model-free or model-based policy optimization rely on policy parameterization. Motivated by the theory, we apply a simple multi-step model-based bootstrapping planner (BOOTS) to bootstrap a weak Q-function into a stronger policy. Empirical results show that applying BOOTS on top of model- based or model-free policy optimization algorithms at the test time improves the performance on MuJoCo benchmark tasks. * indicates equal contribution 1 In turn, the dynamics can also be much more complex than the Q-function. Consider the following situa- tion: a subset of the coordinates of the state space can be arbitrarily difficult to express by neural networks, but the reward function can only depend on the rest of the coordinates and remain simple.
```

Figures/Tables Captions:
```
Figure 1: Left: The dynam- ics of two randomly gener- ated MDPs (from the RAND, and SEMI-RAND methods outlined in Section 4.3 and detailed in Appendix D.1). Right: The corresponding Q-functions which are more complex than the dynamics (more details in Section 4.3).
Figure 2: A visualization of the dynamics, the reward function in the MDP defined in Definition 4.1, and the approximation of its optimal Q-function for the effective horizon H = 4. We can also construct slightly more involved construction with Lipschitz dynamics and very similar properties. Please see Appendix C.
Figure 3: (Left): The performance of DQN, SLBO, and MBPO on the bottom dynamics in Figure 1. The number after the acronym is the width of the neural network used in the parameterization of Q. We see that even with sufficiently large neural networks and sufficiently many steps, these algorithms still suffers from bad approximability and cannot achieve optimal reward. (Right): Performance of BOOTS + DQN with various planning steps. A near-optimal reward is achieved with even k = 3, indicating that the bootstrapping with the learned dynamics improves the expressivity of the policy significantly.
Figure 4: Comparison of BOOTS-MBSAC vs MBSAC and BOOTS-SAC vs SAC on Ant and Hu- manoid. Particularly on the Humanoid environment, BOOTS improves the performance signifi- cantly. The test policies for MBSAC and SAC are the deterministic policy that takes the mean of the output of the policy network, because the deterministic policy performs better than the stochastic policy in the test time.
Figure 5: BOOTS-MBSAC or BOOTS- MBPO outperforms previous state-of-the-art algorithms on Humanoid. The results are av- eraged over 5 random seeds and shadow area indicates a single standard deviation from the mean.
```

Main Content:
```

Section Title: INTRODUCTION
  INTRODUCTION Model-based deep reinforcement learning (RL) algorithms offer a lot of potentials in achieving sig- nificantly better sample efficiency than the model-free algorithms for continuous control tasks. We can largely categorize the model-based deep RL algorithms into two types: 1. model-based policy optimization algorithms which learn policies or Q-functions, parameterized by neural networks, on the estimated dynamics, using off-the-shelf model-free algorithms or their variants (Luo et al., 2019; Janner et al., 2019; Kaiser et al., 2019; Kurutach et al., 2018; Feinberg et al., 2018; Buckman et al., 2018), and 2. model-based planning algorithms, which plan with the estimated dynamics Nagabandi et al. (2018); Chua et al. (2018); Wang & Ba (2019). A deeper theoretical understanding of the pros and cons of model-based and the model-free algo- rithms in the continuous state space case will provide guiding principles for designing and applying new sample-efficient methods. The prior work on the comparisons of model-based and model-free algorithms mostly focuses on their sample efficiency gap, in the case of tabular MDPs (Zanette & Brunskill, 2019; Jin et al., 2018), linear quadratic regulator (Tu & Recht, 2018), and contextual decision process with sparse reward (Sun et al., 2019). In this paper, we theoretically compare model-based RL and model-free RL in the continuous state space through the lens of approximability by neural networks, and then use the insight to design practical algorithms. What is the representation power of neural networks for expressing the Q- function, the policy, and the dynamics? How do the model-based and model-free algorithms utilize the expressivity of neural networks? Our main finding is that even for the case of one-dimensional continuous state space, there can be a massive gap between the approximability of Q-function and the policy and that of the dynamics: The optimal Q-function and policy can be significantly more complex than the dynamics. We construct environments where the dynamics are simply piecewise linear functions with constant pieces, but the optimal Q-functions and the optimal policy require an exponential (in the horizon) Under review as a conference paper at ICLR 2020 number of linear pieces, or exponentially wide neural networks, to approximate. 1 The approximabil- ity gap can also be observed empirically on (semi-) randomly generated piecewise linear dynamics with a decent chance. (See  Figure 1  for two examples.) When the approximability gap occurs, any deep RL algorithms with policies parameterized by neural networks will suffer from a sub-optimal performance. These algorithms include both model-free algorithms such as DQN (Mnih et al., 2015) and SAC (Haarnoja et al., 2018), and model-based policy optimization algorithms such as SLBO (Luo et al., 2019) and MBPO (Janner et al., 2019). To validate the intuition, we empirically apply these algorithms to the constructed or the randomly generated MDPs. Indeed, they fail to converge to the optimal rewards even with sufficient samples, which suggests that they suffer from the lack of expressivity. However, in such cases, model-based planning algorithms should not suffer from the lack of ex- pressivity, because they only use the learned, parameterized dynamics, which are easy to express. The policy obtained from the planning is the maximizer of the total future reward on the learned dynamics, and can have an exponential (in the horizon) number of pieces even if the dynamics has only a constant number of pieces. In fact, even a partial planner can help improve the expressivity of the policy. If we plan for k steps and then resort to some Q-function for estimating the total reward of the remaining steps, we can obtain a policy with 2 k more pieces than what Q-function has. We hypothesize that the real-world continuous control tasks also have a more complex optimal Q- function and a policy than the dynamics. The theoretical analysis of the synthetic dynamics suggests that a model-based few-steps planner on top of a parameterized Q-function will outperform the orig- inal Q-function because of the addtional expressivity introduced by the planning. We empirically verify the intuition on MuJoCo benchmark tasks. We show that applying a model-based planner on top of Q-functions learned from model-based or model-free policy optimization algorithms in the test time leads to significant gains over the original Q-function or policy. In summary, our contributions are: 1. We construct continuous state space MDPs whose Q-functions and policies are proved to be more complex than the dynamics (Sections 4.1 and 4.2.) 2. We empirically show that with a decent chance, (semi-)randomly generated piecewise lin- ear MDPs also have complex Q-functions (Section 4.3.) 3. We show theoretically and empirically that the model-free RL or model-based policy op- timization algorithms suffer from the lack of expressivity for the constructed MDPs (Sec- tions 4.3), whereas model-based planning solve the problem efficiently (Section 5.2.) 4. Inspired by the theory, we propose a simple model-based bootstrapping planner (BOOTS), which can be applied on top of any model-free or model-based Q-learning algorithms at Under review as a conference paper at ICLR 2020 the test time. Empirical results show that BOOTS improves the performance on MuJoCo benchmark tasks, and outperforms previous state-of-the-art on MuJoCo humanoid environ- ment.

Section Title: ADDITIONAL RELATED WORK
  ADDITIONAL RELATED WORK Comparisons with Prior Theoretical Work. Model-based RL has been extensively studied in the tabular case (see (Zanette & Brunskill, 2019; Azar et al., 2017) and the references therein), but much less so in the context of deep neural networks approximators and continuous state space. (Luo et al., 2019) give sample complexity and convergence guarantees suing principle of optimism in the face of uncertainty for non-linear dynamics. Below we review several prior results regarding model-based versus model-free dichotomy in var- ious settings. We note that our work focuses on the angle of expressivity, whereas the work below focuses on the sample efficiency.

Section Title: Tabular MDPs
  Tabular MDPs The extensive study in tabular MDP setting leaves little gap in their sample com- plexity of model-based and model-free algorithms, whereas the space complexity seems to be the main difference. (Strehl et al., 2006). The best sample complexity bounds for model-based tabular RL (Azar et al., 2017; Zanette & Brunskill, 2019) and model-free tabular RL (Jin et al., 2018) only differ by a poly(H) multiplicative factor (where H is the horizon.) Linear Quadratic Regulator. Dean et al. (2018) and Dean et al. (2017) provided sample complexity bound for model-based LQR. Recently, Tu & Recht (2018) analyzed sample efficiency of the model- based and model-free problem in the setting of Linear Quadratic Regulator, and proved an O(d) gap in sample complexity, where d is the dimension of state space. Unlike tabular MDP case, the space complexity of model-based and model-free algorithms has little difference. The sample-efficiency gap mostly comes from that dynamics learning has d-dimensional supervisions, whereas Q-learning has only one-dimensional supervision. Contextual Decision Process (with function approximator). Sun et al. (2019) prove an ex- ponential information-theoretical gap between mode-based and model-free algorithms in the fac- tored MDP setting. Their definition of model-free algorithms requires an exact parameterization: the value-function hypothesis class should be exactly the family of optimal value-functions in- duced by the MDP family. This limits the application to deep reinforcement learning where over- parameterized neural networks are frequently used. Moreover, a crucial reason for the failure of the model-free algorithms is that the reward is designed to be sparse.

Section Title: Related Empirical Work
  Related Empirical Work A large family of model-based RL algorithms uses existing model-free algorithms of its variant on the learned dynamics. MBPO (Janner et al., 2019), STEVE (Buckman et al., 2018), and MVE (Feinberg et al., 2018) are model-based Q-learning-based policy optimization algorithms, which can be viewed as modern extensions and improvements of the early model-based Q-learning framework, Dyna (Sutton, 1990). SLBO (Luo et al., 2019) is a model-based policy optimization algorithm using TRPO as the algorithm in the learned environment. Another way to exploit the dynamics is to use it to perform model-based planning. Racanière et al. (2017) and Du & Narasimhan (2019) use the model to generated additional extra data to do plan- ning implicitly. Chua et al. (2018) study how to combine an ensemble of probabilistic models and planning, which is followed by Wang & Ba (2019), which introduces a policy network to distill knowledge from a planner and provides a prior for the planner. Piché et al. (2018) uses methods in Sequential Monte Carlo in the context of control as inference. Oh et al. (2017) trains a Q-function and then perform lookahead planning. Nagabandi et al. (2018) uses random shooting as the plan- ning algorithm. Lowrey et al. (2018) uses the dynamics to improve the performance of model-free algorithms. Heess et al. (2015) backprops through a stochastic computation graph with a stochastic gradient to optimize the policy under the learned dynamics. Levine & Koltun (2013) distills a policy from trajectory optimization. Rajeswaran et al. (2016) trains a policy adversarially robust to the worst dy- namics in the ensemble. Clavera et al. (2018) reformulates the problem as a meta-learning problem and using meta-learning algorithms. Predictron (Silver et al., 2017) learns a dynamics and value function and then use them to predict the future reward sequences.

Section Title: Under review as a conference paper at ICLR 2020
  Under review as a conference paper at ICLR 2020 Another line of work focus on how to improve the learned dynamics model. Many of them use an ensemble of models (Kurutach et al., 2018; Rajeswaran et al., 2016; Clavera et al., 2018), which are further extended to an ensemble of probabilistic models (Chua et al., 2018; Wang & Ba, 2019). Luo et al. (2019) designs a discrepancy bound for learning the dynamics model. Talvitie (2014) augments the data for model training in a way that the model can output a real observation from its own prediction. Malik et al. (2019) calibrates the model's uncertainty so that the model's output distribution should match the frequency of predicted states. Oh et al. (2017) learns a representation of states by predicting rewards and future returns using representation.

Section Title: PRELIMINARIES
  PRELIMINARIES Markov Decision Process. A Markov Decision Process (MDP) is a tuple S, A, f, r, γ , where S is the state space, A the action space, f : S × A → ∆(S) the transition dynamics that maps a state action pair to a probability distribution of the next state, γ the discount factor, and r ∈ R S×A the reward function. Throughout this paper, we will consider deterministic dynamics, which, with slight abuse of notation, will be denoted by f : S × A → S. A deterministic policy π : S → A maps a state to an action. The value function for the policy is defined as is defined V π (s) An RL agent aims to find a policy π that maximizes the expected total reward defined as η(π) def = E s1∼µ [V π (s 1 )] , where µ is the distribution of the initial state. Bellman Equation. Let π be the optimal policy, and V the optimal value function (that is, the value function for policy π ). The value function V π for policy π and optimal value function V satisfy the Bellman equation and Bellman optimality equation, respectively. Let Q π and Q defines the state-action value function for policy π and optimal state-action value function. Then, for a deterministic dynamics f , we have

Section Title: Neural Networks
  Neural Networks We focus on fully-connected neural nets with ReLU function as activations. A one-dimensional input and one-dimensional output ReLU neural net represents a piecewise linear function. A two-layer ReLU neural net with d hidden neurons represents a piecewise linear function with at most (d + 1) pieces. Similarly, an H-layer neural net with d hidden neurons in each layer represents a piecewise linear function with at most (d + 1) H pieces (Pascanu et al., 2013).

Section Title: Problem Setting and Notations
  Problem Setting and Notations In this paper, we focus on continuous state space, discrete action space MDPs with S ⊂ R. We assume the dynamics is deterministic (that is, s t+1 = f (s t , a t )), and the reward is known to the agent. Let x denote the floor function of x, that is, the greatest integer less than or equal to x. We use I[·] to denote the indicator function.

Section Title: APPROXIMABILITY OF Q-FUNCTIONS AND DYNAMICS
  APPROXIMABILITY OF Q-FUNCTIONS AND DYNAMICS We show that there exist MDPs in one-dimensional continuous state space that have simple dynam- ics but complex Q-functions and policies. Moreover, any polynomial-size neural network function approximator of the Q-function or policy will result in a sub-optimal expected total reward, and learning Q-functions parameterized by neural networks requires fundamentally an exponential num- ber of samples (Section 4.2). Section 4.3 illustrates the phenomena that Q-function is more complex than the dynamics occurring frequently and naturally even with random MDP, beyond the theoretical construction.

Section Title: A PROVABLE CONSTRUCTION OF MDPS WITH COMPLEX Q
  A PROVABLE CONSTRUCTION OF MDPS WITH COMPLEX Q Recall that we consider the infinite horizon case and 0 < γ < 1 is the discount factor. Let H = (1 − γ) −1 be the "effective horizon" - the rewards after H steps become negligible due to the discount factor. For simplicity, we assume that H > 3 and it is an integer. (Otherwise we take just take H = (1 − γ) −1 .) Throughout this section, we assume that the state space S = [0, 1) and the action space A = {0, 1}. Definition 4.1. Given the effective horizon H = (1 − γ) −1 , we define an MDP M H as follows. Let κ = 2 −H . The dynamics f by the following piecewise linear functions with at most three pieces. The reward function is defined as The initial state distribution µ is uniform distribution over the state space [0, 1). The dynamics and the reward function for H = 4 are visualized in Figures 2a, 2b. Note that by the definition, the transition function for a fixed action a is a piecewise linear function with at most 3 pieces. Our construction can be modified so that the dynamics is Lipschitz and the same conclusion holds (see Appendix C). Attentive readers may also realize that the dynamics can be also be written succinctly as f (s, 0) = 2s mod 1 and f (s, 1) = 2s + κ mod 1 2 , which are key properties that we use in the proof of Theorem 4.2 below. Optimal Q-function Q and the optimal policy π . Even though the dynamics of the MDP constructed in Definition 4.1 has only a constant number of pieces, the Q-function and policy are very complex: (1) the policy is a piecewise linear function with exponentially number of pieces, (2) the optimal Q-function Q and the optimal value function V are actually fractals that are not continuous anywhere. These are formalized in the theorem below.

Section Title: Under review as a conference paper at ICLR 2020
  Under review as a conference paper at ICLR 2020 And the optimal value function is a fractal with the expression: The close-form expression of Q can be computed by Q (s, a) = r(s, a) + V (f (s, a)), which is also a fractal. We approximate the optimal Q-function by truncating the infinite sum to 2H terms, and visualize it in Figure 2c. We discuss the main intuitions behind the construction in the following proof sketch of the Theorem. A rigorous proof of Theorem 4.2) is deferred to Appendix B.1.

Section Title: Proof Sketch
  Proof Sketch The key observation is that the dynamics f essentially shift the binary representation of the states with some addition. We can verify that the dynamics satisfies f (s, 0) = 2s mod 1 and f (s, 1) = 2s + κ mod 1 where κ = 2 −H . In other words, suppose s = 0.s (1) s (2) · · · is the binary representation of s, and let left-shift(s) = 0.s (2) s (3) · · · . Moreover, the reward function is approximately equal to the first bit of the binary representation (Here the small negative drift of reward for action a = 1, −2(γ H−1 − γ H ), is only mostly designed for the convenience of the proof, and casual readers can ignore it for simplicity.) Ignoring carries, the policy pretty much can only affect the H-th bit of the next state s = f (s, a): the H-th bit of s is either equal to (H + 1)-th bit of s when action is 0, or equal its flip when action is 1. Because the bits will eventually be shifted left and the reward is higher if the first bit of a future state is 1, towards getting higher future reward, the policy should aim to create more 1's. Therefore, the optimal policy should choose action 0 if the (H + 1)-th bit of s is already 1, and otherwise choose to flip the (H + 1)-th bit by taking action 1. A more delicate calculation that addresses the carries properly would lead us to the form of the optimal policy (Equation (2).) Computing the total reward by executing the optimal policy will lead us to the form of the optimal value function (equation (3).) (This step does require some elementary but sophisticated algebraic manipulation.) With the form of the V , a shortcut to a formal, rigorous proof would be to verify that it satisfies the Bellman equation, and verify π is consistent with it. We follow this route in the formal proof of Theorem 4.2) in Appendix B.1.

Section Title: THE APPROXIMABILITY OF Q-FUNCTION
  THE APPROXIMABILITY OF Q-FUNCTION A priori, the complexity of Q or π does not rule out the possibility that there exists an approxi- mation of them that do an equally good job in terms of maximizing the rewards. However, we show that in this section, indeed, there is no neural network approximation of Q or π with a polynomial width. We prove this by showing any piecewise linear function with a sub-exponential number of pieces cannot approximate either Q or π with a near-optimal total reward. Theorem 4.3. Let M H be the MDP constructed in Definition 4.1. Suppose a piecewise linear policy π has a near optimal reward in the sense that η(π) ≥ 0.92 · η(π ), then it has to have at least Ω (exp(cH)/H) pieces for some universal constant c > 0. As a corollary, no constant depth neural networks with polynomial width (in H) can approximate the optimal policy with near optimal rewards. Consider a policy π induced by a value function Q, that is, π(s) = arg max a∈A Q(s, a). Then,when there are two actions, the number of pieces of the policy is bounded by twice the number of pieces of Q. This observation and the theorem above implies the following inapproximability result of Q . Corollary 4.4. In the setting of Theorem 4.3, let π be the policy induced by some Q. If π is near- optimal in a sense that η(π) ≥ 0.92 · η(π ), then Q has at least Ω (exp(cH)/H) pieces for some universal constant c > 0.

Section Title: Under review as a conference paper at ICLR 2020
  Under review as a conference paper at ICLR 2020 The intuition behind the proof of Theorem 4.3 is as follows. Recall that the optimal policy has the form π (s) = I[s (H+1) = 0]. One can expect that any polynomial-pieces policy π behaves subopti- mally in most of the states, which leads to the suboptimality of π. Detailed proof of Theorem 4.3 is deferred to Appendix B.2. Beyond the expressivity lower bound, we also provide an exponential sample complexity lower bound for Q-learning algorithms parameterized with neural networks (see Appendix B.4).

Section Title: THE APPROXIMABILITY OF Q-FUNCTIONS OF RANDOMLY GENERATED MDPS
  THE APPROXIMABILITY OF Q-FUNCTIONS OF RANDOMLY GENERATED MDPS In this section, we show the phenomena that the Q-function not only occurs in the crafted cases as in the previous subsection, but also occurs more robustly with a decent chance for (semi-) randomly generated MDPs. (Mathematically, this says that the family of MDPs with such a property is not a degenerate measure-zero set.) It is challenging and perhaps requires deep math to characterize the fractal structure of Q-functions for random dynamics, which is beyond the scope of this paper. Instead, we take an empirical ap- proach here. We generate random piecewise linear and Lipschitz dynamics, and compute their Q-functions for the finite horizon, and then visualize the Q-functions or count the number of pieces in the Q-functions. We also use DQN algorithm (Mnih et al., 2015) with a finite-size neural network to learn the Q-function. We set horizon H = 10 for simplicity and computational feasibility. The state and action space are [0, 1) and {0, 1} respectively. We design two methods to generate random or semi-random piecewise dynamics with at most four pieces. First, we have a uniformly random method, called RAND, where we independently generate two piecewise linear functions for f (s, 0) and f (s, 1), by generating random positions for the kinks, generating random outputs for the kinks, and connecting the kinks by linear lines (See Appendix D.1 for a detailed description.) In the second method, called SEMI-RAND, we introduce a bit more structure in the generation process, towards increasing the chance to see the phenomenon. The functions f (s, 0) and f (s, 1) have 3 pieces with shared kinks. We also design the generating process of the outputs at the kinks so that the functions have more fluctuations. The reward for both of the two methods is r(s, a) = s, ∀a ∈ A. (See Appendix D.1 for a detailed description.)  Figure 1  illustrates the dynamics of the generated MDPs from SEMI-RAND. More details of empir- ical settings can be found in Appendix D.1. The optimal policy and Q can have a large number of pieces. Because the state space has one dimension, and the horizon is 10, we can compute the exact Q-functions by recursively applying Bellman operators, and count the number of pieces. We found that, 8.6% fraction of the 1000 MDPs independently generated from the RAND method has policies with more than 100 pieces, much larger than the number of pieces in the dynamics (which is 4). Using the SEMI-RAND method, a 68.7% fraction of the MDPs has polices with more than 10 3 pieces. In Section D.1, we plot the histogram of the number of pieces of the Q-functions.  Figure 1  visualize the Q-functions and dynamics of two MDPs generated from RAND and SEMI-RAND method. These results suggest that the phenomenon that Q-function is more complex than dynamics is not a degenerate phenomenon and can occur with non-zero measure. For more empirical results, see Appendix D.2.

Section Title: Model-based policy optimization methods also suffer from a lack of expressivity
  Model-based policy optimization methods also suffer from a lack of expressivity As an impli- cation of our theory in the previous section, when the Q-function or the policy are too complex to be approximated by a reasonable size neural network, both model-free algorithms or model-based policy optimization algorithms will suffer from the lack of expressivity, and as a consequence, the sub-optimal rewards. We verify this claim on the randomly generated MDPs discussed in Sec- tion 4.3, by running DQN (Mnih et al., 2015), SLBO (Luo et al., 2019), and MBPO (Janner et al., 2019) with various architecture size. For the ease of exposition, we use the MDP visualized in the bottom half of  Figure 1 . The optimal policy for this specific MDP has 765 pieces, and the optimal Q-function has about 4 × 10 4 number of pieces, and we can compute the optimal total rewards. First, we apply DQN to this environment by using a two-layer neural network with various widths to parameterize the Q-function. The training curve is shown in  Figure 3  (Left). Model-free algorithms Under review as a conference paper at ICLR 2020 using a zero-th order optimization algorithm (which only requires oracle query of the function value) such as cross-entropy method or random shooting. can not find near-optimal policy even with 2 14 hidden neurons and 1M trajectories, which suggests that there is a fundamental approximation issue. This result is consistent with Fu et al. (2019), in a sense that enlarging Q-network improves the performance of DQN algorithm at convergence. Second, we apply SLBO and MBPO in the same environment. Because the policy network and Q-function in SLOBO and MBPO cannot approximate the optimal policy and value function, we see that they fail to achieve near-optimal rewards, as shown in  Figure 3  (Left).

Section Title: MODEL-BASED BOOTSTRAPPING PLANNER
  MODEL-BASED BOOTSTRAPPING PLANNER Our theory and experiments in Section 4.2 and 4.3 demonstrate that when the Q-function or the policy is complex, model-free or model-based policy optimization algorithms will suffer from the lack of expressivity. The intuition suggests that model-based planning algorithms will not suffer from the lack of expressivity because the final policy is not represented by a neural network. For the construction in Section 4.1, we can actually prove that even a few-steps planner can bootstrap the expressivity of the Q-function (formalized in Theorem 5.1 below). Inspired the theoretical result, we apply a simple k-step model-based bootstrapping planner on top of existing Q-functions (trained from either model-based or model-free approach) in the test time, on either the one-dimensional MDPs considered in Section 4 or the continuous control benchmark tasks in MuJoCo. The bootstrapping planner is reminiscent of MCTS using in AlphaGo (Silver et al., 2016; 2018). However, here, we use the learned dynamics and deal with the continuous state space.

Section Title: BOOTSTRAPPING THE Q-FUNCTION
  BOOTSTRAPPING THE Q-FUNCTION Given a function Q that is potentially not expressive enough to approximate the optimal Q-function, we can apply the Bellman operator with a learned dynamicsf for k times to get a bootstrapped version of Q: Given the bootstrapped version, we can derive a greedy policy w.r.t it: Algorithm 1, called BOOTS summarizes how to apply the planner on top of any RL algorithm with a Q-function (straightforwardly). For the MDPs constructed in Section 4.1, we can prove that representing the optimal Q-function by B k f [Q] requires fewer pieces in Q than representing the optimal Q-function by Q directly. Theorem 5.1. Consider the MDP M H defined in Definition 4.1. There exists a constant-piece piece- wise linear dynamicsf and a 2 H−k+1 -piece piecewise linear function Q, such that the bootstrapped policy π boots k,Q,f (s) achieves the optimal total rewards. By contrast, recall that in Theorem 4.3, we show that approximating the optimal Q-function directly with a piecewise linear function requires ≈ 2 H piecewise. Thus we have a multiplicative factor of 2 k gain in the expressivity by using the bootstrapped policy. Here the exponential gain is only magnificent enough when k is close to H because the gap of approximability is huge. However, in more realistic settings - the randomly-generated MDPs and the MuJoCo environment - the bootstrapping planner improvs the performance significantly as shown in the next subsection.

Section Title: EXPERIMENTS
  EXPERIMENTS BOOTS on random piecewise linear MDPs. We implement BOOTS (Algorithm 1) with various steps of planning and with the learned dynamics. 4 . The planner is an exponential-time planner which enumerates all the possible future sequence of actions. We also implement bootstrapping with partial planner with varying planning horizon. As shown in  Figure 3 , BOOTS + DQN not only has the best sample-efficiency, but also achieves the optimal reward. In the meantime, even a partial planner helps to improve both the sample-efficiency and performance. More details of this experiment are deferred to Appendix D.3.

Section Title: BOOTS on MuJoCo environments
  BOOTS on MuJoCo environments We work with the OpenAI Gym environments (Brockman et al., 2016) based on the Mujoco simulator (Todorov et al., 2012) with maximum horizon 1000 and discount factor 1. We apply BOOTS on top of three algorithms: (a) SAC (Haarnoja et al., 2018), the state-of-the-art model-free RL algorithm; (b) MBPO (Janner et al., 2019), a model-based Q- learning algorithm, and an extension of Dyna (Sutton, 1990); (c) a computationally efficient variant of MBPO that we develop using ideas from SLBO (Luo et al., 2019), which is called MBSAC. The main difference here from MBPO and other works such as (Wang & Ba, 2019; Kurutach et al., 2018) is that we don't use model ensemble. Instead, we occasionally optimize the dynamics by one step of Adam to introduce stochasticity in the dynamics, following the technique in SLBO Luo et al. (2019). Our algorithm is a few times faster than MBPO in wall-clock time. It performs similarly to MBPO on Humanoid, but generally worse than MBPO on other environments. See Appendix A.1 for details. We use k = 4 steps of planning unless explicitly mentioned otherwise in the ablation study (Sec- tion A.2). In  Figure 4 , we compare BOOTS+SAC with SAC, and BOOTS + MBSAC with MBSAC on Gym Ant and Humanoid environments, and demonstrate that BOOTS can be used on top of ex- isting strong baselines. We found that BOOTS has little help for other simpler environments, and we suspect that those environments have much less complex Q-functions so that our theory and intuitions do not necessarily apply. (See Section A.2 for more ablation study.) In  Figure 5 , we compare BOOTS+MBSAC and BOOTS+MBPO with other MBPO, SAC, and STEVE (Buckman et al., 2018) 5 on the humanoid environment. We see a strong performance sur- passing the previous state-of-the-art MBPO.

Section Title: CONCLUSION
  CONCLUSION Our study suggests that there exists a significant representation power gap of neural networks be- tween for expressing Q-function, the policy, and the dynamics in both constructed examples and em- pirical benchmarking environments. We show that our model-based bootstrapping planner BOOTS helps to overcome the approximation issue and improves the performance in synthetic settings and in the difficult MuJoCo environments. We raise some interesting open questions. • Can we theoretically generalize our results to high-dimensional state space, or continuous actions space? Can we theoretically analyze the number of pieces of the optimal Q-function of a stochastic dynamics? • In this paper, we measure the complexity by the size of the neural networks. It's conceivable that for real-life problems, the complexity of a neural network can be better measured by its weights norm. Could we build a more realistic theory with another measure of complexity? • The BOOTS planner comes with a cost of longer test time. How do we efficiently plan in high-dimensional dynamics with a long planning horizon? • The dynamics can also be more complex (perhaps in another sense) than the Q-function in certain cases. How do we efficiently identify the complexity of the optimal Q-function, policy, and the dynamics, and how do we deploy the best algorithms for problems with different characteristics?

```
