Title:
```
Under review as a conference paper at ICLR 2020 UNSUPERVISED DOMAIN ADAPTATION WITH IMPUTATION
```
Abstract:
```
Motivated by practical applications, we consider unsupervised domain adaptation for classification problems, in the presence of missing data in the target domain. More precisely, we focus on the case where there is a domain shift between source and target domains, while some components in the target data are systematically absent. We propose a way to impute non-stochastic missing data for a classi- fication task by leveraging supervision from a complete source domain through domain adaptation. We introduce a single model performing joint domain adap- tation, imputation and classification which is shown to perform well under var- ious representative divergence families (H-divergence, Optimal Transport). We perform experiments on two families of datasets: a classical digit classification benchmark commonly used in domain adaptation papers and real world digital advertising datasets, on which we evaluate our model's classification performance in an unsupervised setting. We analyze its behavior showing the benefit of explic- itly imputing non-stochastic missing data jointly with domain adaptation.
```

Figures/Tables Captions:
```
Figure 1: Adaptation-Imputation model: (a) training, (b) inference.
Figure 2: Missing patch size study
Figure 3: ADV-MSE weighting on ads-kaggle
Table 1: Classification accuracy performance in % on digits for the two training criteria on the target domain test set. Standard deviation is in %.
Table 2: CE on ads for ADV models
Table 3: Accuracy on digits and CE on ads-kaggle for ADV Adaptation-Imputation
```

Main Content:
```

Section Title: INTRODUCTION
  INTRODUCTION When dealing with machine learning applications in the real world, data usually come with several imperfections that make classical algorithms hardly deployable. One of these issues is that data are often incomplete. Typically, while capturing data coming from different locations with several sen- sors per location, a sensor may randomly fail or even may be just missing at a given location. Such a situation can also occur in disease diagnosis in multi-modal medical imaging where one of the modalities fails or is not available; for example the positron emission tomography (PET) modality which reveals metabolic information for clinical tests requires ingesting a radioactive tracer which poses health risks and is often missing (Cai et al., 2018). Similarly, in computational advertising ap- plications, information is missing for users who do not have a prior history on a merchant's website while their global clicking behavior across websites may be known. Another common issue is that data used for training and deployment may differ in their generation process: data may be collected on different devices, background noise or compression schemes may affect differently training or deployment data, leading to a shift in data distribution. This has given rise to the important literature of Domain Adaptation (Pan & Yang, 2010; Kouw & Loog, 2019). These two issues are usually inde- pendently addressed by developing models handling only the missing data or the domain adaptation problem. In this paper, motivated by practical advertising applications, we consider unsupervised domain adaptation (i.e. labels are not available in the target domain) for classification when (1) part of input data is missing in the target domain thus requiring some form of imputation, (2) there is no possible supervision in the target domain for imputation thus requiring indirect supervision from the source domain, and (3) there exists a domain shift between the source and target distributions requiring domain adaptation. More precisely we consider this adaptation-imputation setting for non- stochastic missing data, i.e. when the same features are missing for all target samples. This contrasts with many imputation problems which take benefit of stochasticity in missing features. We propose a model that handles unsupervised domain shift and missing data assuming non- stochastic missing data in the target domain. The model learns to perform imputation for the target domain while aligning the distributions of the source and target domains in a latent space, thus going beyond the simple juxtaposition of a data imputation module followed by a domain-invariant feature Under review as a conference paper at ICLR 2020 representation learning module. Imputation makes use of an indirect supervision from the complete source domain. This key property allows us to handle non-stochastic missing data, while satisfying the constraints related to adaptation and to the classification objective. The imputation process plays several roles in our global architecture as it provides us with information about the missing data for the target domain while contributing to the domain-invariant loss and the reconstruction loss. Ex- tensive empirical evidence on handwritten digits and Click-Through-Rate prediction (CTR) domain adaptation problems illustrate the benefit of the proposed model. The original contributions are the following: • We introduce a new problem : joint unsupervised domain adaptation and imputation for classifi- cation motivated by practical applications; • We propose a new model for handling the problem end-to-end. It learns to generate relevant missing information while aligning source and target distributions in a latent space and to classify source instances; • We evaluate the model not only on academic benchmarks but also on challenging real world advertising data.

Section Title: RELATED WORK
  RELATED WORK We review below typical related work for domain adaptation and data imputation.

Section Title: UNSUPERVISED DOMAIN ADAPTATION (UDA)
  UNSUPERVISED DOMAIN ADAPTATION (UDA) A number of shallow learning methods approach Domain Adaptation by weighting individual obser- vations during training. These methods focus either on data importance-weighting (Cortes & Mohri, 2014; Zadrozny, 2014) or on class importance-weighting (Z. Lipton & Smola, 2018). Recent deep learning methods try to align the distributions of the two domains, for example by embedding them in a joint latent space. There are two main directions for learning joint embeddings. One is based on adversarial training, making use of GAN extensions. The other one directly exploits explicit distance measures between distributions such as Wasserstein or Maximum Mean Discrepancy (MMD). For the former, the seminal work of Ganin & Lempitsky (2015) learns to map source and target domains onto a common embedding space, by optimizing a double objective: on the one hand they minimize an approximation of the H-divergence between the source and target embeddings via adversarial training, on the other hand they learn to classify the source data embeddings. This influential work has been followed by several extensions and variants. ADDA (Tzeng et al., 2017) advocates the use of two different mappings for the source and the target domains based on the argument that this is more suitable when the marginals are different in the two domains. Liu & Tuzel (2016) trains coupled generative adversarial network (CoGAN) for learning a joint distribution of multi-domain images, that can be used for UDA. Bousmalis et al. (2017) use a generator to map the source to the target domain while training the classifier on the learned representations using source labels. CDAN (Long et al., 2017) improves the domain discriminator by conditioning it on classifier predictions. A second family of approaches proposes metric based divergences such as MMD (Long et al., 2015) for measuring the loss between source and target representations. DeepJDOT (Damodaran & Kel- lenberger, 2018) makes use of an optimal transport formulation to align the joint distributions in a latent space. In addition to feature alignment they perform label distribution alignment following Courty et al. (2017). All these works rely on the assumption of covariate shift and consider that full input data is available for both source and target domains. Our two models (ADV and OT) can be seen respectively as extensions of Ganin & Lempitsky (2015) and Damodaran & Kellenberger (2018) for the missing data problem.

Section Title: IMPUTATION
  IMPUTATION Data imputation is a classical problem addressed by several methods (Little & Rubin, 2002; Van Bu- uren, 2018; Murray, 2018). The usual setting is different from ours since it considers reconstructing the whole missing data in the input space, while we consider 1) reconstruction in a latent space and 2) partial reconstruction since we are interested in the information relevant to the classification task Under review as a conference paper at ICLR 2020 only. Recent generative models like GANs (Goodfellow et al., 2014) or VAEs (Kingma & Welling, 2013; Rezende et al., 2014) have been adapted for data imputation in Yoon et al. (2018) and Mattei & Frellsen (2019) respectively. GAIN (Yoon et al., 2018) is an extension of conditional GANs where the generator takes as input an incomplete data and performs imputation while the discriminator is trained to guess for each sample if each variable is original or imputed. Mattei & Frellsen (2019) suggests a method based on deep latent variable models and importance sampling that offers tighter likelihood bound compared to the standard VAE bound. Most approaches consider a supervised setting where 1) paired complete and incomplete data are available and 2) missingness corresponds to a stochastic process (e.g. a mask distribution for tabular data), 3) imputation is performed in the original feature space. Note that this is different from our setting where there is no direct su- pervision (supervision is only provided indirectly through the source domain) and missingness is non-stochastic which makes the problem harder since one cannot compute statistics on different incomplete samples. The general approach with generative models is to learn a distribution over imputed data which is similar to the one of plain data. This comes in many different instances and usually, generative training alone is not sufficient; additional loss terms are often used. In paired problems, i.e. when each missing datum is associated to a plain version of the datum, these addi- tional terms consist of a reconstruction term imposed by a MSE contraint (Isola et al., 2016b). In unpaired problems a cycle-consistency loss is imposed as in Zhu et al. (2017). Li et al. (2019); Pajot et al. (2019) are among the very few approaches addressing unsupervised imputation in which full instances are never directly used. Both extend the AmbientGAN (Bora et al., 2018) framework and consider stochastic missingness. Our imputation problem is closer to the ones addressed in some forms of inpainting or for multi- modality missing data. The former problem is addressed e.g. in Pathak et al. (2016) who proposes an encoder-decoder model trained according to a joint reconstruction and adversarial loss. The latter is addressed in Cai et al. (2018) who considers the case of multi-modality when one or more modalities are systematically absent, but they do not consider adaptation. They propose to learn to reconstruct the missing modality distribution conditionally to the observed one. Both approaches are fully supervised. Ding et al. (2014) is the only paper we are aware of that considers imputation as we do. Their approach is based on low rank constraints and dictionary learning to guide the transfer between domains. We do not use this method as a baseline due to the complexity and running time of this method which relies on singular value decompositions and dictionnary learning.

Section Title: PROBLEM DEFINITION
  PROBLEM DEFINITION Let us denote respectively (x S , y S ) ∈ R n × R and (x T , y T ) ∈ R n × R, data from the source and target domains where x − is an input, y − the associated label and n is the dimension of the input space. "−" holds for either source S or target T . The joint distribution on each domain is denoted respectively p S (X, Y ) and p T (X, Y ). We consider that x − has two components, x − = (x − 1 , x − 2 ). The problem we address is Unsupervised Domain Adaptation (UDA) with missing features in the target domain. More precisely, we make the following hypotheses. Missingness: We assume that features are the same across domains and that source features x S = (x S1 , x S2 ) are always available while in the target domain only x T1 is available and x T2 is systematically missing. For advertising applications for example, x would characterize the user browsing behaviour on merchant sites; x − 1 characterizes global user features aggregated over his navigation history, which are known for all users; x − 2 characterizes user history on a target merchant site. Source domain would consist of all users who already visited this merchant site and target domain of users who never visited this site. UDA: we assume that source labels y S are available whereas target labels y T are unknown. Covariate shift: we assume covariate shift as in most UDA papers e.g. Ganin & Lempitsky (2015).

Section Title: ADAPTATION-IMPUTATION MODEL
  ADAPTATION-IMPUTATION MODEL As in many generative approaches to UDA, the objective is to project source and target data onto a common latent space in which data distributions from the two domains match, and to learn a clas- sifier on the source data that performs well on the target domain. The novelty of our approach is to offer a solution to deal with datasets in which some information, x T2 , is systematically missing in the target domain. Our model, denoted Adaptation-Imputation, performs three opera- tions jointly: imputation of missing information for the target data, alignment of the distributions Under review as a conference paper at ICLR 2020 of source and target, and classification of source instances. The three operations are performed in a joint embedding space and all the model's components are trained together. The term imputation is used here in a broad sense: our goal is not to recover the whole missing x T2 , but to recover informa- tion from x T2 that will be useful for adaptation and for the target data classification objective. This is achieved via a generative model, which for a given datum in the target domain and conditionally on the available information x T1 , attempts to generate the required missing information. Because x T2 is systematically missing for target data, there is no possible supervision in the target domain; instead we use distant supervision from the source data while transferring to the target domain. We consider two variants of the same model based on different divergence measures between source and target distributions: the Wasserstein distance and the H-divergence approximated through adversar- ial training. For simplicity we describe in the main text the adversarial version ADV and defer the Optimal Transport OT description to Appendix B. We report the results obtained with both models in Section 5.

Section Title: TRAINING
  TRAINING Our model is composed of three different modules responsible for adaptation, imputation and clas- sification, that share parameters and are trained in parallel. For simplicity, we describe each compo- nent in turn, but it should be reminded that they all interact and that their parameters are all optimized according to the three objectives mentioned above. The interaction is discussed after the individual module descriptions. The model's components are illustrated in  Figure 1 (a) .

Section Title: Adaptation
  Adaptation The latent space representations of source and target domains are denoted with a tilde notation: x S = ( x S1 , x S2 ) and x T = ( x T1 , x T2 ). Referring to  Figure 1 (a) , x − 1 = g 1 (x − 1 ), ( x − 1 denotes either x S1 or x T1 ) is the mapping of the observed component x − 1 onto the latent space and x − 2 = h • g 1 (x − 1 ) is the second component's latent representation generated from x − 1 . This generation mechanism will be described later. Adaptation aligns the distributions ( x S1 , x S2 ) and ( x T1 , x T2 ) in the latent space. For the ADV model, alignment is performed via a classical adversarial loss operating on the latent representations: L 1 = E x∼p S (X) log D 1 ( x S ) + E x∼p T (X) log(1 − D 1 ( x T )) (1) where D 1 ( x) represents the probability that x comes from the source rather than the target.

Section Title: Imputation
  Imputation Imputation amounts at generating an encoding x T2 , in the latent space, for the missing information in the target data, conditioned on the available information x T1 . Our objective here is to generate missing information which is relevant for the classification objective. Since we never have access to any target component x T2 , we learn to perform imputation based on the source data. More precisely, we learn to generate x S2 from x S1 through the generator h, x S2 = h•g 1 (x S1 ), as depicted in  Figure 1 . We want h to generate the missing information x S2 associated to the observed x S1 . For that we perform two operations in parallel. First, we align the distribution of x S2 with the distribution of x S2 = g 2 (x S2 ), that is a direct mapping of x S2 onto the shared latent space, using an adversarial loss described below. The intuition is that both g 1 and g 2 are simple mappings operating respectively on x S1 and x S2 while h acts as a generator conditioned on x S1 for generating x S2 . Moreover, we not only impose this distribution alignment, but would also like x S2 to represent missing information relative to x S2 and associated to a specific x S1 . For that, we use a reconstruction term in parallel to the above alignment, in our case a MSE distance between x S2 and x S2 . This MSE term guarantees that the imputed x S2 truly represents information present in x S2 . Similar ideas combine distribution matching and MSE conditioning and have been used e.g. in Isola et al. (2016a); Pathak et al. (2016). The learned mappings are used to perform imputation on the target data x T2 = h • g 1 (x T1 ). The imputation loss has thus two components. The first is the adversarial term L ADV responsible for aligning x S2 and x S2 , L ADV = E x2∼p S (X2) log D 2 ( x S2 ) + E x1∼p S (X1) log(1 − D 2 ( x S2 )). The second is the reconstruction term L M SE = E x∼p S (X) x S2 − x S2 2 2 . The total imputation loss is then: (2) L 2 = λ ADV × L ADV + λ M SE × L M SE where λ ADV , λ M SE are hyperparameters. The two processes of imputation and adaptation influ- ence each other. Both are also influenced by the classification process described below. Its effect Under review as a conference paper at ICLR 2020 on imputation is to force the generated x S2 to contain information about x S2 relevant for the clas- sification task. This information is transferred via adaptation to the target domain when generating x T2 .

Section Title: Classification
  Classification The last component of the model is a classifier f , trained on the source domain mapping x S for the classification task as classically done for UDA. The corresponding loss is: L 3 = E x∼p S (X) L Disc (f ( x S ), y S ) (3) where L Disc is typically a cross-entropy loss.

Section Title: Overall loss
  Overall loss The overall loss function L will be the weighted sum of the adaptation, imputation and classification losses: where λ 1 , λ 2 , λ 3 are hyperparameters and the final optimization problem is: Interaction between the model's components Both mappings g 1 , g 2 and generator h appear in the three terms of the loss function in Equation 4, meaning that they should learn to perform the three tasks simultaneously. g 1 learns to map the x S1 and x T1 components onto the latent space, the mappings being denoted respectively x S1 and x T1 . h learns to generate missing information x T2 from x T1 . The formed x − is generated such that it fulfills the classification objective. g 2 on its side should fulfill the imputation objective while preserving part of the information present in x S2 . Note that our model makes use of a unique mapping g 1 for both source and target domains. Separate mappings could have been used for the two domains, but the proposed solution was found to be more robust and to reduce the number of parameters during learning.

Section Title: Implementation
  Implementation Let us now detail the implementation of this model. For adversarial training, discriminators D 1 (adaptation) and D 2 (imputation) will be implemented by binary classifiers. D 1 is trained to distinguish between source x S and target x T mappings while D 2 is trained to separate Under review as a conference paper at ICLR 2020 imputed x S2 , generated from x S1 , from x S2 a direct embedding of x S2 . We use the gradient reversal trick in Ganin & Lempitsky (2015) for implementing the min-max condition and define two gradient reversal networks on D 1 and D 2 . We use an adaptive update of the scale of the gradients in D 1 and D 2 and optimize L 1 , L 2 and L 3 jointly as synthesized in Algorithm 1 in the Appendix. In practise we fix all hyperparameters but λ M SE to 1, additional tuning could yield improved performance.

Section Title: INFERENCE
  INFERENCE At inference, given x T1 , we generate x T = ( x T1 , x T2 ) with x T1 = g 1 (x T1 ) an embedding of x T1 and a generated x T2 , encoding part of the missing information x T2 in x T , as illustrated in  Figure 1 (b) . We use for the latter the following mapping: x T2 = h • g 1 (x T1 ) where g 1 is as above and h is the generative mapping conditioned on x T1 . Finally x T is used as input to the classifier f .

Section Title: EXPERIMENTS
  EXPERIMENTS

Section Title: DATASETS AND EXPERIMENTAL SETTING
  DATASETS AND EXPERIMENTAL SETTING

Section Title: Datasets
  Datasets Experiments are performed on two types of datasets. The first one is a classical digits classification benchmark used in many domain adaptation studies which we will refer to as digits and transformed to fit our missing data setting. The second one corresponds to advertising datasets. The task here is binary classification: one wants to predict Click-Through-Rate (CTR) or Conversion Rate (CR) given user behavior. This is one of the problem that has initially motivated our adaptation- imputation framework. We use two such datasets: ads-kaggle is a public kaggle dataset 1 , while ads-real has been gathered internally and corresponds to real advertising traffic. Further details on datasets and preprocessing are presented in Sections 5.2, 5.3 and in Appendix C.

Section Title: Baselines
  Baselines We report results for the following models: • Full models: Source-Full is trained without adaptation on the full x S and tested on full x T when the latter is available (digits); Adaptation-Full adds adaptation to this model. • Missing models: Source-Missing and Adaptation-Missing do the same but consid- ering full x S while x T is incomplete: x T = (x T1 , 0), i.e. x T2 is set to 0. • Partial models: Source-Partial and Adaptation-Partial is a variant of the above setting where only the first component x − 1 for source and target are considered for adaptation and classification while the second ones x − 2 are simply ignored. • Imputation models: Adaptation-Imputation corresponds to our model. • Naive model: Naive is used for ads-kaggle to provide a reference loss value for this dataset. It predicts for all examples the mean CTR value as computed using source training data only. Adaptation-Full is an upper bound of the performance of Adaptation-Imputation since it uses full information while x T2 is not available in practice. Adaptation-Missing and Adaptation-Partial can be considered as lower bounds for our model since they only perform adaptation and no imputation. Parameters and architecture of the neural networks used for the different models and experiments are presented in Appendix D. Hyperparameters are chosen using the Deep Embedded Validation estimator introduced in You et al. (2019) combined with heuristics and typical UDA values. Further details are given in Appendix D.2.1. We present the results for digits and ads respectively in Sections 5.2, 5.3. Section 5.4 presents ablation studies. Reported results are mean value and standard deviation over five different initial- izations; best results are indicated in bold.

Section Title: DIGITS
  DIGITS Description For digits, we consider the unsupervised adaptation between two datasets among MNIST (LeCun et al. (1998)), USPS (Hull (1994)), SVHN (Netzer et al. (2011)) and MNIST-M Under review as a conference paper at ICLR 2020 (Ganin & Lempitsky (2015)). The direction MNIST → SVHN is not considered as the task is difficult even for traditional UDA (Ganin & Lempitsky, 2015). All tasks are 10-class classification problems. From the complete image datasets, we build datasets with missing input values.

Section Title: Half digit missing
  Half digit missing In a first series of experiments, we removed one half of each image, the horizontal bottom part. We report classification accuracy in  Table 1  for the different adaptation problems and models (ADV and OT). Removing half of the image leads to a strong performance decrease for Source-Partial and Source-Missing with respect to the upper bounds pro- vided by Source-Full, respectively between 10 and 20 points of accuracy, and between 15 and 45 points. This is partially recovered when training with adaptation (Adaptation- Partial, Adaptation-Missing, for both ADV or OT). But the gap is still important with respect to the upper bound, i.e. Adaptation-Full. In all cases, Adaptation-Imputation clearly in- creases the performance; between 10 and 25 points of accuracy over Adaptation-Missing and 2 to 20 points over Adaptation-Partial. This is a very significant improvement which vali- dates the importance of the imputation component. In Section 5.4 we show that the simultaneous use of imputation and adaptation is required for reaching this level of performance. Imputation or adap- tation alone are well behind the jointly trained instance of the model. However, it does not reach the upper bound performance of Adaptation-Full where the difference lies between 10 and 25 ac- curacy points. Moreorever, Adaptation-Imputation beats the non-adapted Source-Full baseline on several datasets. Both the ADV and OT versions exhibit the same general behavior. In the reported results in  Table 1 , ADV performance is higher than OT. This is because performance is highly dependent on the NN architectures and we tuned our NNs for ADV. OT models may reach performance similar to ADV but we find that it requires models with an order of magnitude more parameters. To keep the comparison fair, we thus used the same NN models for both ADV and OT. Imputation models achieve their highest performance when the adaptation task between domains is complex (MNIST → MNIST-M, SVHN → MNIST) illustrating the importance of imputation when transfer is difficult. In all experiments, the performance of --Partial model where "-" refers to Source or Adaptation, are usually higher than the --Missing model. Our understand- ing is that setting missing components to zero tends to increase distance between source and target distributions, compared to just ignoring them, making the classification and adaptation problems harder.

Section Title: Missing patch size
  Missing patch size In a second group of experiments on digits, we analyze the evolution of the performance of the models with respect to the size of the missing in- formation in the target domain. For that, we vary the size of the missing patch re- moving a percentage of the image p with p ∈ {30%, 40%, 50%, 60%, 70%} on SVHN → MNIST for ADV models, keeping the same hyperparameters as the ones used for p = 50%. We report the mean val- ues over five runs in  Figure 2 . We notice that Adaptation-Imputation constantly beats the other baselines regardless of the miss- ing patch size. The figure exhibits borderline cases when the size of the missing patch be- Under review as a conference paper at ICLR 2020 comes very small (< 30%) or very large (> 65%). When the missing patch is too small most of the information for predicting the target label is already available thus simple models perform already well; while when it becomes too big, too few information is available to guarantee efficient reconstructions from the non-missing patch.

Section Title: ADS
  ADS

Section Title: Description
  Description We performed a second series of tests on two advertising datasets: ads-kaggle and ads-real. The ads datasets correspond to binary classification problems; the task is to predict the probability that a user exposed to an ad from a target partner (e.g. Booking) on a publisher (e.g. NY Times) will click (ads-kaggle) or make a purchase (ads-real) conditioned on the user history. A row in the dataset corresponds to a display i.e. an ad opportunity of a click or purchase for a given (user, partner) pair at a given time on a given publisher site. The source domain is composed of users who already had interactions with a target partner. The target domain is composed of users with no history on a target partner. We consider all partners in a given traffic. For the two domains, x − 1 features correspond to aggregated user features on all the partners, while x − 2 corresponds to user - target partner specific interaction which is known for the source domain but unknown for the target domain. Note that besides missingness, there is also an adaptation problem since statistics for new users are usually different from those of known users (e.g. in terms of frequency of a partner's website visits) as seen in Appendix E. In real datasets, traffic in the source domain is usually abundant while scarce in the target domain. Statistics for each dataset are provided in Table 5 in the Appendix; exact preprocessing used is provided in Appendix C.

Section Title: Results
  Results For this group of experiments, we report the results only for ADV models since the trend has been observed to be similar on digits for both ADV and OT. For ads datasets, missing features do not exist, so we do not report the --Full models' results on these datasets. The classes being imbalanced, accuracy is not relevant here so we report another performance measure, cross-entropy (CE) between the predicted values and the true labels on the target domain which is considered as the most reliable metric to estimate revenue. Note that given the test set size of ads-kaggle, an improvement of 0.001 in logloss is considered as practically significant (Wang et al., 2017). For the ads problem and for large user bases, a small improvement in prediction accuracy can lead to a large increase in a company's revenue. For all experiments, we report in  Table 2  CE on target test for ads-kaggle and ads-real. A first observation is that Adaptation-Imputation is signif- icantly better than the baselines on both datasets ( Table 2 ). For ads-kaggle it improves by 2.3% the best adaptation model (Adaptation-Missing) while for ads-real the improvement reaches 6.3% over the best second which happens to be Source-Partial. A second observation is that for any model, adaptation consistently improves over the same model without adaptation. The only exception is the --Partial setting in ads-real. A third observation is that the missing component indeed contains relevant infor- mation: CE performance on source data (not reported in  Table 2 ) shows that Source-Missing which exploits the x − 2 component is consistently higher than Source-Partial which does not exploit this component, leading to relative gains of the former over the latter of 5.6% on ads-kaggle and 8.2% on ads-real. Adaptation-Imputation is able to generate and to exploit this information.

Section Title: ABLATION ANALYSIS
  ABLATION ANALYSIS We analyze now the role and importance of the different components of our model, and compare with the results from  Tables 1  and 2. We perform experimentation on the public datasets, digits and ads-kaggle and report results in  Table 3  and  Figure 3 .

Section Title: Importance of adaptation
  Importance of adaptation We compare the performance of the model with and without the adap- tation term L 1 in Equation 4. When removing adaptation, inference is performed as before, by feeding x T = ( x T1 , x T2 ) to the classifer f . This means that we only rely on the imputation and classification losses to learn the parameters of the model. Results appear on the top of  Table 3 . For all datasets, the adaptation component considerably increases the performance, from 10 to 30 points for digits and by a significant 0.009 CE value on ads-kaggle.

Section Title: Imputation mechanism
  Imputation mechanism Imputation, cf. Equation 2, combines adversarial training (ADV) and conditioning on the input datum via the MSE loss (MSE). The objective is to learn from x S1 x S1 = g 1 (x S1 ) and to generate missing information in x S2 , x S2 = h( x S1 ). ADV aligns the distributions of x S2 and x S2 while MSE can be thought as performing some form of regression. For a given partial information x S1 , there are possibly several potential x S2 and thus x S2 . ADV allows to focus on a specific mode of x S2 , while MSE will favour a mean value of the distribution. Results on  Table 3 , second group of rows, show that for digits, combining the two influences (MSE and ADV) leads to improved results compared to using separately each loss. MSE alone already provides good performance, while using only ADV is clearly below. For this classification task, identifying the most relevant mode improves the performance over simple regression (L M SE ). Note that reconstruction is an ill posed problem since the task is inherently ambiguous - different digits may be reconstructed from one half of an image. We performed tests with a stochastic input component in order to recover different modes, but the performance was broadly similar. Achieving diversity with Conditional GANs remains an open research topic (Yang et al., 2019). For the ads-kaggle dataset, the perfor- mance of MSE and MSE + ADV are similar. This is analyzed deeper in an additional se- ries of experiments with several weighted com- binations of MSE and ADV. Results are pro- vided in  Table 3  third group of rows, for both digits and ads-kaggle and are plotted for ads-kaggle in  Figure 3 . For digits, this confirms that the equal weights selected for our experiments are indeed generally a good choice reducing the burden of hyperparame- ter selection, while for ads-kaggle perfor- mance could be slightly improved with other weightings. One can see on  Figure 3  that ADV induces a high variance in the results (left part of x-axis) while MSE stabilizes the performance (right part of x-axis). The former allows for bet- ter maximum performance but with high variance: performance ranges from 0.35 to 0.7 on the target domain. A small contribution from MSE (here λ M SE = 0.005) stabilizes the results.

Section Title: CONCLUSION
  CONCLUSION We have proposed a new model to solve unsupervised adaptation problems in the presence of non- stochastic noise in the target domain, by using distant supervision from a complete source domain Under review as a conference paper at ICLR 2020 through domain adaptation and imputing missing values on the target domain in a latent space. This method uses only labelled source instances and leads to important gains on classical adaptation benchmarks over baseline models for two representative families of divergences (optimal transport, adversarial training). We have demonstrated on real world advertising datasets that these meth- ods can be used for problems with missing features in advertising. Potential follow-ups include: extending this method to a semi or fully supervised setting on the target domain; considering simul- taneously domain and target shift which frequently occurs in real world problems while still being an open problem; introducing increased diversity in the generation process.

```
