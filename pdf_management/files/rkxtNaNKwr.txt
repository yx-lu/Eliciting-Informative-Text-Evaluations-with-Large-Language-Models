Title:
```
Under review as a conference paper at ICLR 2020 EVOLUTIONARY REINFORCEMENT LEARNING FOR SAMPLE-EFFICIENT MULTIAGENT COORDINATION
```
Abstract:
```
Many cooperative multiagent reinforcement learning environments provide agents with a sparse team-based reward, as well as a dense agent-specific reward that incentivizes learning basic skills. Training policies solely on the team-based reward is often difficult due to its sparsity. Also, relying solely on the agent-specific reward is sub-optimal because it usually does not capture the team coordination objective. A common approach is to use reward shaping to construct a proxy reward by combining the individual rewards. However, this requires manual tuning for each environment. We introduce Multiagent Evolutionary Reinforcement Learning (MERL), a split-level training platform that handles the two objectives separately through two optimization processes. An evolutionary algorithm maximizes the sparse team-based objective through neuroevolution on a population of teams. Concurrently, a gradient-based optimizer trains policies to only maximize the dense agent-specific rewards. The gradient-based policies are periodically added to the evolutionary population as a way of information transfer between the two optimization processes. This enables the evolutionary algorithm to use skills learned via the agent-specific rewards toward optimizing the global objective. Results demonstrate that MERL significantly outperforms state-of-the-art methods, such as MADDPG, on a number of difficult coordination benchmarks.
```

Figures/Tables Captions:
```
Figure 1: Team represented as multi-headed policy net π
Figure 2: High level schematic of MERL highlighting the integration of local and global reward functions Policy Gradient: The procedure described so far resembles a standard EA except that each agent k stores each of its experiences in its associated replay buffer (R k ) instead of just discarding it. However, unlike EA, which only learns based on the low-fidelity global reward, MERL also learns from the experiences within episodes of a rollout us- ing policy gradients. To enable this kind of "local learning", MERL initializes one multi-headed policy network π pg and one critic Q. A noisy version of π pg is then used to conduct its own set of rollouts in the environment, storing each agent k's ex- periences in its corresponding buffer (R k ) similar to the evolutionary rollouts.
Figure 3: Illustration of environments tested (Lowe et al., 2017; Rahmattalabi et al., 2016)
Figure 4: Performance on Predator-Prey where the prey is 30% faster (left) and 100% faster (right), respectively.
Figure 5: Performance on Physical Deception (left) and Keep-Away (right)
Figure 6: Performance on the Rover Domain.
Figure 7: MATD3's performance for dif- ferent scalarization coefficients
Figure 8: Agent trajectories for coupling = 3. Red/black squares are observed/unobserved POIs respectively
Figure 9: Selection rate for migrat- ing policies
```

Main Content:
```

Section Title: INTRODUCTION
  INTRODUCTION Cooperative multiagent reinforcement learning (MARL) studies how multiple agents can learn to coordinate as a team toward maximizing a global objective. Cooperative MARL has been applied to many real world applications such as air traffic control ( Tumer and Agogino, 2007 ), multi-robot coordination ( Sheng et al., 2006 ;  Yliniemi et al., 2014 ), communication and language ( Lazaridou et al., 2016 ;  Mordatch and Abbeel, 2018 ), and autonomous driving ( Shalev-Shwartz et al., 2016 ). Many such environments endow agents with a team reward that reflects the team's coordination objective, as well as an agent-specific local reward that rewards basic skills. For instance, in soccer, dense local rewards could capture agent-specific skills such as passing, dribbling and running. The agents must then coordinate when and where to use these skills in order to optimize the team objective, which is winning the game. Usually, the agent-specific reward is dense and easy to learn from, while the team reward is sparse and requires the cooperation of all or most agents. Having each agent directly optimize the team reward and ignore the agent-specific reward usually fails or is sample-inefficient for complex tasks due to the sparsity of the team reward. Conversely, having each agent directly optimize the agent-specific reward also fails because it does not capture the team's objective, even with state of the art multiagent RL algorithms such as MADDPG ( Lowe et al., 2017 ). One solution to this problem is to use reward shaping, where extensive domain knowledge about the task is used to create a proxy reward function ( Rahmattalabi et al., 2016 ). Constructing this proxy reward function is difficult in complex environments, and is domain-dependent. Apart from requiring domain knowledge and manual tuning, this approach also poses risks of changing the underlying problem itself ( Ng et al., 1999 ). Simple approaches to creating a proxy reward via linear combinations of the two objectives also fail to solve or generalize to complex coordination tasks ( Devlin et al., 2011 ;  Williamson et al., 2009 ).

Section Title: Under review as a conference paper at ICLR 2020
  Under review as a conference paper at ICLR 2020 In this paper, we introduce Multiagent Evolutionary Reinforcement Learning (MERL), a state-of- the-art algorithm for cooperative MARL that does not require reward shaping. MERL is a split-level training platform that combines gradient-based and gradient-free optimization. The gradient-free optimizer is an evolutionary algorithm that maximizes the team objective through neuroevolution. The gradient-based optimizer is a policy gradient algorithm that maximizes each agent's dense, local rewards. These gradient-based policies are periodically copied into the evolutionary population. The two processes operate concurrently and share information through a shared replay buffer. A key strength of MERL is that it is a general method which does not require domain-specific reward shaping. This is because MERL optimizes the team objective directly while simultaneously leveraging agent-specific rewards to learn basic skills. We test MERL in a number of multiagent coordination benchmarks. Results demonstrate that MERL significantly outperforms state-of-the-art methods such as MADDPG, while using the same observations and reward functions. We also demonstrate that MERL scales gracefully to increasing complexity of coordination objectives where MADDPG and its variants fail to learn entirely.

Section Title: BACKGROUND AND RELATED WORK
  BACKGROUND AND RELATED WORK Markov Games: A standard reinforcement learning (RL) setting is often formalized as a Markov Decision Process (MDP) and consists of an agent interacting with an environment over a finite number of discrete time steps. This formulation can be extended to multiagent systems in the form of partially observable Markov games ( Littman, 1994 ;  Lowe et al., 2017 ). An N -agent Markov game is defined by a global state of the world, S, and a set of N observations {O i } and N actions {A i } corresponding to the N agents. At each time step t, each agent observes its corresponding observation O t i and maps it to an action A t i using its policy π i . Each agent receives a scalar reward r t i based on the global state S t and joint action of the team. The world then transitions to the next state S t+1 which produces a new set of observations {O i }. The process continues until a terminal state is reached. R i = T t=0 γ t r t i is the total return for agent i with discount factor γ ∈ (0, 1]. Each agent aims to maximize its expected return. TD3: Policy gradient (PG) methods frame the goal of maximizing the expected return as the minimization of a loss function. A widely used PG method for continuous, high-dimensional action spaces is DDPG ( Lillicrap et al., 2015 ). Recently, ( Fujimoto et al., 2018 ) extended DDPG to Twin Delayed DDPG (TD3), addressing its well-known overestimation problem. TD3 is the state-of-the-art, off-policy algorithm for model-free DRL in continuous action spaces. TD3 uses an actor-critic architecture ( Sutton and Barto, 1998 ) maintaining a deterministic policy (actor) π : S → A, and two distinct critics Q : S × A → R i . Each critic independently approximates the actor's action-value function Q π . A separate copy of the actor and critics are kept as target networks for stability and are updated periodically. A noisy version of the actor is used to explore the environment during training. The actor is trained using a noisy version of the sampled policy gradient computed by backpropagation through the combined actor-critic networks. This mitigates overfitting of the deterministic policy by smoothing the policy gradient updates. Evolutionary Reinforcement Learning (ERL) is a hybrid algorithm that combines Evolutionary Algorithms (EAs) ( Floreano et al., 2008 ;  Lüders et al., 2017 ;  Fogel, 2006 ;  Spears et al., 1993 ), with policy gradient methods ( Khadka and Tumer, 2018 ). Instead of discarding the data generated during a standard EA rollout, ERL stores this data in a central replay buffer shared with the policy gradient's own rollouts - thereby increasing the diversity of the data available for the policy gradient learners. Since the EA directly optimizes for episode-wide return, it biases exploration towards states with higher long-term returns. The policy gradient algorithm which learns using this state distribution inherits this implicit bias towards long-term optimization. Concurrently, the actor trained by the policy gradient algorithm is inserted into the evolutionary population allowing the EA to benefit from the fast gradient-based learning. Related Work:  Lowe et al. (2017)  introduced MADDPG which tackled the inherent non-stationarity of a multiagent learning environment by leveraging a critic which had full access to the joint state and action during training.  Foerster et al. (2018b)  utilized a similar setup with a centralized critic across agents to tackle StarCraft micromanagement tasks. An algorithm that could explicitly model other agents' learning was investigated in  Foerster et al. (2018a) . However, all these approaches rely Under review as a conference paper at ICLR 2020 on a dense agent reward that properly captures the team objective. Methods to solve for these types of agent-specific reward functions were investigated in  Li et al. (2012)  but were limited to tasks with strong simulators where tree-based planning could be used. A closely related work to MERL is ( Liu et al., 2019 ) where Population-Based Training (PBT) (Jader- berg et al., 2017) is used to optimize the relative importance between a collection of dense, shaped rewards automatically during training. This can be interpreted as a singular central reward function constructed by scalarizing a collection of reward signals where the scalarization coefficients are adaptively learned during training. In contrast, MERL optimizes its reward functions independently with information transfer across them facilitated through shared replay buffers and policy migra- tion directly. This form of information transfer through a shared replay buffer has been explored extensively in recent literature ( Colas et al., 2018 ;  Khadka et al., 2019 ).

Section Title: MULTIAGENT EVOLUTIONARY REINFORCEMENT LEARNING
  MULTIAGENT EVOLUTIONARY REINFORCEMENT LEARNING MERL leverages both agent-specific and team objectives through a hybrid algorithm that combines gradient-free and gradient-based optimization. The gradient-free optimizer is an evolutionary algo- rithm that maximizes the team objective through neuroevolution. The gradient-based optimizer trains policies to maximize agent-specific rewards. These gradient-based policies are periodically added to the evolutionary population and participate in evolution. This enables the evolutionary algorithm to use agent-specific skills learned by training on the agent-specific rewards toward optimizing the team objective without needing to resort to reward shaping. Update π k pg using the sampled policy gradient Policy Topology: We represent our multiagent (team) policies using a multi-headed neural network π as illustrated in  Figure 1 . The head π k represents the k-th agent in the team. Given an incoming observation for agent k, only the output of π k is considered as agent k's response. In essence, all Under review as a conference paper at ICLR 2020 agents act independently based on their own observations while sharing weights (and by extension, the features) in the lower layers (trunk). This is commonly used to improve learning speed ( Silver et al., 2017 ). Further, each agent k also has its own replay buffer (R k ) which stores its experience defined by the tuple (state, action, next state, local reward) for each interaction with the environment (rollout) involving that agent. Team Reward Optimization:  Figure 2  illustrates the MERL algo- rithm. A population of multi-headed teams, each with the same topology, is initialized with random weights. The replay buffer R k is shared by the k-th agent across all teams. The population is then evaluated for each rollout. The team reward for each team is dis- bursed at the end of the episode and is considered as its fitness score. A selection operator selects a portion of the population for survival with probability proportionate to their fitness scores. The weights of the teams in the population are probabilistically perturbed through mutation and crossover operators to create the next generation of teams. A portion of the teams with the highest relative fitness are preserved as elites. At any given time, the team with the highest fitness, or the champion, represents the best solution for the task.

Section Title: Agent-Specific Reward Optimization:
  Agent-Specific Reward Optimization: Crucially, each agent's replay buffer is kept separate from that of every other agent to ensure diversity amongst the agents. The shared critic samples a random mini-batch uniformly from each replay buffer and uses it to update its parameters using gradient descent. Each agent π k pg then draws a mini-batch of experiences from its corresponding buffer (R k ) and uses it to sample a policy gradient from the shared critic. Unlike the teams in the evolutionary population which directly seek to optimize the team reward, π pg seeks to maximize the agent-specific local reward while exploiting the experiences collected via evolution.

Section Title: Skill Migration
  Skill Migration Periodically, the π pg network is copied into the evolving population of teams and can propagate its features by participating in evolution. This is the core mechanism that combines policies learned via agent-specific and team rewards. Regardless of whether the two rewards are aligned, evolution ensures that only the performant derivatives of the migrated network are retained. This mechanism guarantees protection against destructive interference commonly seen when a direct scalarization between two reward functions is attempted. Further, the level of information exchange is automatically adjusted during the process of learning, in contrast to being manually tuned by an expert designer. Algorithm 1 provides a detailed pseudo-code of the MERL algorithm. The choice of hyperparameters is explained in the Appendix. Additionally, our source code 1 is available online. We adopt environments from ( Lowe et al., 2017 ) and ( Rahmattalabi et al., 2016 ) to perform our experiments. Each environment consists of multiple agents and landmarks in a two-dimensional world. Agents take continuous control actions to move about the world.  Figure 3  illustrates the four environments which are described in more detail below.

Section Title: Predator-Prey
  Predator-Prey In this environment, N slower cooperating agents (predators) must chase the faster adversary (prey) around an environment with L large landmarks in randomly-generated locations. The predators get a reward when they catch (touch) the prey while the prey is penalized. The team reward for the predators is the cumulative number of prey-touches in an episode. Each predator can also compute the average distance to the prey and use it as its agent-specific reward. All agents observe the relative positions and velocities of the other agents as well as the positions of the landmarks. The prey can accelerate 33% faster than the predator and has a higher top speed. We tests two versions termed simple and hard predator-prey where the prey is 30% and 100% faster, respectively. Additionally, the prey itself learns dynamically during training. We use DDPG ( Lillicrap et al., 2015 ) as a learning algorithm for training the prey policy. All of our candidate algorithms are tested on their ability to train the team of predators in catching this prey. Physical Deception: N agents cooperate to reach a single target Point of Interest (POI) among N POIs. They are rewarded based on the closest distance of any agent to the target. A lone adversary also desires to reach the target POI. However, the adversary does not know which of the POIs is the correct one. Thus the cooperating agents must learn to spread out and cover all POIs so as to deceive the adversary as they are penalized based on the adversary's distance to the target. The team reward for the agents is then the cumulative reward in an episode. We use DDPG ( Lillicrap et al., 2015 ) to train the adversary policy.

Section Title: Keep-Away
  Keep-Away In this scenario, a team of N cooperating agents must reach a target POI out of L total POIs. Each agent is rewarded based on its distance to the target. We construct the team reward as simply the sum of the agent-specific rewards in an episode. An adversary also has to occupy the target while keeping the cooperating agents from reaching the target by pushing them away. To incentivize this behavior, the adversary is rewarded based on its distance to the target POI and penalized based on the distance of the target from the nearest cooperating agent. Additionally, it does not know which of the POIs is the target and must infer this from the movement of the agents. DDPG ( Lillicrap et al., 2015 ) is used to train the adversary policy.

Section Title: Rover Domain
  Rover Domain This environment is adapted from ( Rahmattalabi et al., 2016 ). Here, N agents must cooperate to reach a set of K POIs. Multiple agents need to simultaneously go to the same POI in order to observe it. The number of agents required to observe a POI is termed the coupling requirement. Agents do not know and must infer the coupling factor from the rewards obtained. If a team with fewer agents than this number go to a POI, no reward is observed. The team's reward is the percentage of POIs observed at the end of an episode. Each agent can also locally compute its distance to its closest POI and use it as its agent-specific reward. Its observation comprises two channels to detect POIs and rovers, respectively. Each channel receives intensity information over 10 • resolution spanning the 360 • around the agent's position loosely based on the characteristic of a Pioneer robot ( Thrun et al., 2000 ). This is similar to a LIDAR. Since it returns the closest reflector, occlusions make the problem partially-observable. A coupling Under review as a conference paper at ICLR 2020 factor of 1 is similar to the cooperative navigation task in  Lowe et al. (2017) . We test coupling factors from 1 to 7 to capture extremely complex coordination objectives.

Section Title: Compared Baselines
  Compared Baselines We compare the performance of MERL with a standard neuroevolutionary algorithm (EA) ( Fogel, 2006 ), MADDPG ( Lowe et al., 2017 ) and MATD3, a variant of MADDPG that integrates the improvements described within TD3 ( Fujimoto et al., 2018 ) over DDPG. Internally, MERL uses EA and TD3 as its team-reward and agent-specific reward optimizer, respectively. MADDPG was chosen as it is the state-of-the-art multiagent RL algorithm. We implemented MATD3 to ensure that the differences between MADDPG and MERL do not originate from having the more stable TD3 over DDPG.

Section Title: Methodology for Reported Metrics
  Methodology for Reported Metrics For MATD3 and MADDPG, the team network was periodi- cally tested on 10 task instances without any exploratory noise. The average score was logged as its performance. For MERL and EA, the team with the highest fitness was chosen as the champion for each generation. The champion was then tested on 10 task instances, and the average score was logged. This protocol shielded the reported metrics from any bias of the population size. We conduct 5 statistically independent runs with random seeds from {2019, 2023} and report the average with error bars showing a 95% confidence interval. All scores reported are compared against the number of environment steps (frames). A step is defined as the multiagent team taking a joint action and receiving a feedback from the environment. To make the comparisons fair across single-team and population-based algorithms, all steps taken by all teams in the population are counted cumulatively.

Section Title: RESULTS
  RESULTS Predator-Prey:  Figure 4  shows the comparative performance in control- ling the team of predators in the Predator-Prey environment. Note that this is an adversarial environment where the prey dynamically adapts against the predators. The prey (con- sidered as part of the environment in this analysis) uses DDPG to learn constantly against our team of preda- tors. This is why predator perfor- mance (measured as number of prey touches) exhibits ebb and flow during learning. MERL outperforms MATD3, EA, and MADDPG across both simple and hard variations of the task. EA seems to be approaching MERL's performance but is significantly slower to learn. This is an expected behavior for neuroevolutionary methods which are known to be sample-inefficient. In contrast, MERL, by virtue of its fast policy-gradient components, learns significantly faster. Physical Deception:  Figure 5  (left) shows the comparative performance in controlling the team of agents in the Physical Deception environment. The performance here is largely based on how close the adversary comes to the target POI. Since the adversary starts out untrained, all compared al- gorithms start out with a fairly high score. As the adversary gradually learns to infer and move towards the target POI, MATD3 and MADDPG demonstrate a gradual decline in performance. However, MERL and EA are able to hold their performance by concocting effective counter-strategies in deceiving the adversary. EA reaches the same performance as MERL but is slower to learn. Keep-Away:  Figure 5  (right) show the comparative performance in Keep-Away. Similar to Physical Deception, MERL and EA are able to hold performance by attaining good counter-measures against Under review as a conference paper at ICLR 2020 the adversary while MATD3 and MADDPG fail to do so. However, EA slightly outperforms MERL on this task. Rover Domain:  Figure 6  shows the comparative performance of MERL, MADDPG, MATD3, and EA tested in the rover domain with coupling fac- tors 1, 3 and 7. In order to benchmark against the proxy reward functions that use scalarized lin- ear combinations, we test MADDPG and MATD3 with two variations of reward functions. Global represents the scenario where only the sparse team reward is used. Mixed represents the scenario where a linear combination of the team-reward and agent-specific reward is used. Each reward is normalized before being combined. A weigh- ing coefficient of 10 is used to amplify the team- reward's influence in order to counter its sparsity. The weighing coefficient was tuned using a grid search (more details in  Figure 7 ). MERL significantly outperforms all baselines across all coupling requirements. The tested base- lines clearly degrade quickly beyond a coupling of 3. The increasing coupling requirement is equiv- alent to increasing difficulty in joint-space exploration and entanglement in the team objective. However, it does not increase the size of the state-space, complexity of perception, or navigation. This indicates that the degradation in performance is strictly due to the increase in complexity of the team objective. Notably, MERL is able to learn on coupling greater than n = 6 where methods without explicit reward shaping have been shown to fail entirely ( Rahmattalabi et al., 2016 ). MERL successfully completes the task using the same set of information and coarse, unshaped reward functions as the other algorithms. The primary mechanism that enables this is MERL's split-level approach that allows it to leverage the agent-specific reward function to solve navigation and perception while concurrently using the team-reward function to learn team formation and effective coordination. Scalarization Coefficients for Mixed Rewards:  Figure 7  shows the performance of MATD3 in optimizing mixed rewards computed with different coefficients used to am- plify the team-reward relative to the agent-reward. The results demonstrate that finding a good balance between these two rewards through linear scalarization is difficult, as all values tested fail to make any progress in the task. This is because a static scalarization cannot capture the dynamic properties of which reward is important when and instead leads to an ineffective proxy. In contrast, MERL is able to leverage both reward functions without the need to explicitly combine them either linearly or via more complex mixing functions. Team Behaviors:  Figure 8  illustrates the trajectories gen- erated for the Rover Domain with a coupling of n = 3. The trajectories for partially and fully trained MERL are shown in  Figure 8 (a)  and (b), respectively. During training, when MERL has not discov- ered team success (no POIs are successfully observed), MERL simply optimizes the agent-specific reward for each agent. This allows it to reach trajectories such as the ones shown in 8(a) where each agent learns to go towards a POI. Since each agent explicitly aims to reach a POI, the probability 3 agents congregating to the same POI is higher compared to random undirected exploration by each agent without the dense agent-specific reward. Once this scenario is discovered, the team reward optimizer (EA) within MERL explicitly selects for agent policies that jointly lead to such team-forming behaviors. Eventually it succeeds Under review as a conference paper at ICLR 2020 as shown in Figure 8(b). Here, team formation and collaborative pursuit of the POIs is immediately apparent. Two teams of 3 agents each form at the start of the episode. Further, the two teams also coordinate to pursue different POIs in order to maximize the team reward. While not perfect (the bottom POI is left unobserved), they do succeed in observing 3 out of the 4 POIs. In contrast, MATD3-mixed fails to observe any POI. From the trajectories, it is apparent that the agents have successfully learned to perceive and navigate to reach POIs. However, they are unable to use this skill to- wards fulfilling the team objec- tive. Instead each agent is rather split on the objective that it is optimizing. Some agents seem to be in sole pursuit of POIs without any regard for team for- mation or collaboration while others seem to exhibit random movements. The primary reason for this is the mixed reward function that directly combines the agent-specific and team reward functions. Since the two reward functions have no guarantees of alignment across the state-space of the task, they invariably lead to learning these sub-optimal joint-behaviors that solve a certain form of scalarized mixed objective. In contrast, MERL by virtue of its bi-level optimization framework is able to leverage both reward functions without the need to explicitly combine them. This enables MERL to avoid these sub-optimal policies and solve the task without any reward shaping or manual tuning. Selection Rate: We ran experiments tracking whether the poli- cies migrated from the policy gradient learners to the evolution- ary population were selected or discarded during the subsequent selection process ( Figure 9 ). Note that the expected selection rate if chosen at random is 0.1 as 1 policy is migrated into a pop- ulation of 10. In contrast, the selection rate for migrated policies is significantly higher across all benchmarks with the exception of Keep-Away. This is consistent with the performance results seen in Keep-Away where EA initially outperforms MERL. How- ever, in general, these results indicate that MERL's integrative approach in combining the two optimization processes towards optimizing the team objective is crucial.

Section Title: CONCLUSION
  CONCLUSION In this paper, we introduced MERL, a split-level algorithm that leverages both agent-specific and team objectives by combining gradient-based and gradient-free optimization. MERL achieves this by using a fast policy-gradient optimizer to exploit dense agent-specific rewards while concurrently leveraging neuroevolution to tackle the team-objective. Results demonstrate that MERL significantly outperforms MADDPG, the state-of-the-art multiagent RL method, in a wide array of benchmarks. We also tested a modification of MADDPG to integrate TD3 - the state-of-the-art single-agent RL algorithm. These experiments demonstrated that the core improvements of MERL originate from its ability to leverage both team and agent-specific reward functions without the need to explicitly combine them. This differentiates MERL from other approaches like reward scalarization and reward shaping that either require extensive manual tuning or can detrimentally change the MDP ( Ng et al., 1999 ) itself. Future work will explore MERL for adversarial settings such as Pommerman ( Resnick et al., 2018 ), StarCraft (Justesen and Risi, 2017;  Vinyals et al., 2017 ) and RoboCup ( Kitano et al., 1995 ;  Liu et al., 2019 ). Further, extending MERL to general multi-reward settings such as is the case for multitask learning, is another promising area for future work. Under review as a conference paper at ICLR 2020

```
