Title:
```
Under review as a conference paper at ICLR 2020 ADAPT-TO-LEARN: POLICY TRANSFER IN REIN- FORCEMENT LEARNING
```
Abstract:
```
Efficient and robust policy transfer remains a key challenge in reinforcement learning. Policy transfer through warm initialization, imitation, or interacting over a large set of agents with randomized instances, have been commonly applied to solve a variety of Reinforcement Learning (RL) tasks. However, this is far from how behavior transfer happens in the biological world: Humans and animals are able to quickly adapt the learned behaviors between similar tasks and learn new skills when presented with new situations. Here we seek to answer the question: Will learning to combine adaptation reward with environmental reward lead to a more efficient transfer of policies between domains? We introduce a principled mechanism that can "Adapt-to-Learn", that is adapt the source policy to learn to solve a target task with significant transition differences and uncertainties. We show through theory and experiments that our method leads to a significantly re- duced sample complexity of transferring the policies between the tasks.
```

Figures/Tables Captions:
```
Figure 1: Learning curves for locomotion tasks, averaged across three runs of each algorithm with random Initialization for RL, warm initialization using source policy for ATL and Jumpstart(Warm- Start) methods and Source policy performance in Target Task without any adaptation.
Figure 2: Trajectory KL divergence Total Intrinsic Return − e ζt averaged across three runs.
Table 1: Transition Model and environment properties for Source and Target task and % change compared to using a warm-start or standalone RL method. Note that the target domain perturba- tions introduced are significant enough such that source policy alone without any adaptation in the target domain produced no meaningful results (Figure-1). This notion of adaptation in the face of uncertainty is a key advancement over traditional policy transfer, meta-learning, or adversarial RL methods aiming to improve performance by learning a policy over a set of lightly perturbed tasks.
```

Main Content:
```

Section Title: INTRODUCTION
  INTRODUCTION Lack of principled mechanisms to quickly and efficiently transfer policies learned between domains has become the major bottleneck in Reinforcement Learning (RL). This inability to transfer or adapt policies is one major reason why RL has still not proliferated physical application like robotics. Since RL agents cannot quickly transfer policies, the agent is forced to learn every task from scratch, which is both time and sample expensive. Warm-start, a method in which weights from one neural network are transferred to another, has been reasonably successful for supervised learning. However, this method can often lead to mixed and even negative results in RL ( Joshi & Chowdhary, 2018 ;  Taylor & Stone, 2009 ). Our main contribution is an algorithm to transfer policies between tasks with significant differences in state transitions via a policy adaptation mechanism. Unlike the majority of existing work in trans- fer learning for RL, our approach does not merely use the transferred policy to warm start (initialize the parameter of the target network with learned source network) policy learning in the target do- main. Neither does it rely on a multitude of simulations across randomly generated source domains. Instead, we combine supervised reference trajectory tracking and unsupervised reinforcement learn- ing to adapt the source policy to the target domain directly. We show through theory and experiments that our method enjoys significantly reduced sample complexity in solving the task. Adapt-to-Learn is inspired by the fact that combined adaptation of behaviors and learning through experience is a primary mechanism of learning in biological creatures ( Krakauer & Mazzoni, 2011 ;  Fryling et al., 2011 ).Inspired by this ability of biological creatures, we seek to answer the question: Will learning to combine intrinsic adaptation reward with environment reward lead to more efficient transfer of policies between domains? Imitation Learning (IL) ( Duan et al., 2017 ;  Zhu et al., 2018 ) seems to play a crucial part in biological learning, and as such has been widely studied in RL. How- ever, the key is, when presented with a new situation, animals do not just imitate, but quickly adapt existing behaviors, and improve them through further experience. In particular, an animal learning to walk on a different terrain does not just imitate its existing gait, but adapts it to the new environ- ment.The theory behind such adaptation in reference tracking control problems has been typically restricted to deterministic dynamical systems with well-defined reference trajectories ( Åström & Wittenmark, 2013 ;  Chowdhary et al., 2013 ). This ability to adapt and incorporate further learning through optimization on the environment reward is one key difference between our method and ex- Under review as a conference paper at ICLR 2020 isting imitation learning and Guided Policy Search (GPS) methods ( Levine & Koltun, 2013 ). Unlike IL and GPS, our method transfers policies between task with significant differences in the transition models. Moreover, by mixing environment reward with intrinsic adaptation rewards, we ensure that the agent quickly adapts and also learns to acquire skills beyond what the source policy can teach. We posit that the presented method can be the foundation of a broader class of RL algorithms that can choose seamlessly between learning through RL to supervised adaptive imitation. Our empir- ical results show that approach is capable of robustly transferring policies between tasks, even in the presence of nonlinear and time-varying differences in the dynamic model of the systems. In particular, we show that it suffices to execute adapted greedy policies to ensure −optimal behavior in the target domain. Related work: D-RL has recently enabled agents to learn policies for complex robotic tasks in simulation ( Peng et al., 2016 ;  2017b ;  Liu & Hodgins, 2017 ;  Heess et al., 2017 ). However, D-RL has been plagued by the curse of sample complexity. Therefore, the capabilities demonstrated in the simulated environment are hard to replicate in the real world. This learning inefficiency of RL has led to significant work in the field of TL ( Taylor & Stone, 2009 ). A significant body of literature on transfer in RL is focused on initialized RL in the target domain using learned source policy; known as jump-start/warm-start methods ( Taylor et al., 2005 ;  Ammar et al., 2012 ;  2015 ). Some examples of these transfer architectures include transfer between similar tasks ( Banerjee & Stone, 2007 ), transfer from human demonstrations ( Peters & Schaal, 2006 ) and transfer from simulation to real ( Peng et al., 2017a ;  Ross et al., 2011 ;  Yan et al., 2017 ). Efforts have also been made in exploring accelerated learning directly on real robots, through Guided Policy Search (GPS) ( Levine et al., 2015 ) and parallelizing the training across multiple agents using meta-learning ( Levine et al., 2016 ;  Nagabandi et al., 2018 ;  Zhu et al., 2018 ). Sim-to-Real transfers have been widely adopted in the recent works and can be viewed as a subset of same domain transfer problems. Daftry et al. ( Daftry et al., 2016 ) demonstrated the policy transfer for control of aerial vehicles across different vehicle models and environments. Christiano et al. ( Christiano et al., 2016 ) transferred policies from simulation to real using an inverse dynamics model estimated interacting with the real robot. Through learning over an adversarial loss, the agents are trained to achieve robust policies across various environments ( Wulfmeier et al., 2017 ). However, these and other reported architectures do not necessarily lead to improved sample efficiency, handle relatively minor changes in the transition model, and are even known to cause negative transfer. In contrast, our approach directly adapts the source policies to target with significant transition model difference while interacting with the environment. It enjoys empirically and rigorously proven sample efficiency guarantees of order O(nH), depending polynomially on the horizon length "H".

Section Title: PRELIMINARIES
  PRELIMINARIES Consider a finite horizon MDP defined as a tuple M = (S, A, P, R, ρ 0 , γ), where S denote set of continuous states; A is a set of continuous bounded actions, P : S × A × S → R + is state transition probability distribution of reaching s upon taking action a in s, ρ 0 : S → R + is the distribution over initial states s 0 and R : S × A → R + is deterministic reward function and H be the finite horizon of the problem. Let π(a|s) : S × A → [0, 1] be stochastic policy over continuous state and action space. The action from policy is a draw from this distribution a i ∼ π(a i |s i ). The agent's goal is to find a policy π which maximize the total return. The total return starting from states s 0 ∼ ρ 0 under a policy π is We formalize the underlying problem of policy transfer by considering a source and target MDP as follows, M S = (S, A, P, R, ρ 0 , γ) S , M T = (S, A, P, R, ρ 0 , γ) T , each with its own state, action space and transition model respectively. We will mainly focus on the problem of same domain transfer in this paper, where the state and action space are analogous S (S) = S (T ) = S ∈ R m and A (T ) = A (S) = A ∈ R k , but the source and target state transition models differ significantly due to unmodeled dynamics or external environment interactions. Furthermore, an example of how the method can extend to cross-domain transfer using manifold alignment ( Joshi & Chowdhary, Under review as a conference paper at ICLR 2020 2018 ;  Wang & Mahadevan, 2009 ) is in the supplementary. Let π * be a parameterized optimal stochastic policy for source MDP M S . The policy π * can be obtained using any available RL methods ( Sutton et al., 2000 ;  Schulman et al., 2015 ; 2017). In this work, we have used the Proximal Policy Optimization(PPO) ( Schulman et al., 2017 ) algorithm to generate the source optimal policy. We will use the following definition of the state value function V π defined under any policy π V π (s t ) = E at,st+1,at+1,... The associated optimal value function fo source MDP M S is V π * for all s ∈ S. Let T : R n → R n be the Bellman update operator defined as Optimal value function V π * satisfies the Bellman Equation-(2) such that V π * (s) = T V π * (s), ∀ s.

Section Title: ALGORITHMIC CONTRIBUTION: TRANSFER THROUGH ADAPTATION
  ALGORITHMIC CONTRIBUTION: TRANSFER THROUGH ADAPTATION

Section Title: ADAPTATION AS A MECHANISM FOR POLICY TRANSFER IN RL
  ADAPTATION AS A MECHANISM FOR POLICY TRANSFER IN RL In this paper, we approach the problem of transfer learning for RL through adaptation of previously learned policies to related tasks. Our approach to adaptation is by enabling the agent to learn the best mixture of imitation and learning from environment reward.Our method differs from RL transfer methods that rely on jump-starts or direct imitation ( Zhu et al., 2018 ;  Duan et al., 2017 ) in a key way: We do not aim to emulate the source optimal policy itself in the target domain, but the source transitions under optimal policy. To make this point, we demonstrate in our empirical results that the source policy, when used directly, does not produce sensible behaviors of the agent in the target domain when transition models are significantly different.On the other hand, our method uses the source transitions projected onto the target task as reference exploration trajectories, helping to adapt and optimize the source policy to the target domain efficiently. In canonical reinforcement learning, the goal is to learn the optimal policy π * which maximizes the future cumulative reward when the reward function and the transition function of the MDP are unknown. In statistical RL, this leads to the (in)famous explore-exploit tradeoff.Yet, this learn- through-experience approach has demonstrated significant potential to enable general solutions to control of complex physical systems for which first principles-based models are not easily obtain- able. However, one of the key challenges in RL has been the high sample complexity of obtaining a reasonable policy. In other words, RL, and specifically Deep RL, require an absurdly huge amount of interactions with the environment (experience) to find reasonable policies. This high learning inefficiency is in apparent contrast with the efficient learning demonstrated by humans and many biological creatures. For examples, octopuses are known to learn to solve complex problems such as opening jars and squeezing through puzzles with just a few trials. This ability to quickly transfer and adapt learned policies between related tasks is likely a key to general intelligence that seems to be missing from RL. Our goal in the rest of this paper is to show that an algorithm that can judiciously combine adaptation to changes and learning new skills is capable of avoiding brute force random exploration to a large extent and be significantly less sample expensive.

Section Title: ADAPT-TO-LEARN: POLICY TRANSFER
  ADAPT-TO-LEARN: POLICY TRANSFER We begin by mathematically describing adaptation for policy transfer in RL and state all the neces- sary assumptions in Section 3.2.1. We then develop the Adapt-to-Learn algorithm in Section 3.2.2.

Section Title: ADAPTATION
  ADAPTATION The proposed Adapt-to-Learn (ATL) algorithm objective is to accumulate higher total returns through maximizing the likelihood of the target policies π θ , which emulate the optimal source transi- tion behavior in the target MDP. We achieve this objective by minimizing the KL divergence between point-wise local trajectories realized using the target policy and source optimal policy in the target Under review as a conference paper at ICLR 2020 domain (Refer Figure-3 in Appendix-C). This adaptation objective can be formalized as minimizing the KL-divergence between source and target transition trajectories: η KL (π θ , π * ) = E st,at∼τ p π θ (τ ) log p π θ (τ ) q π * (τ ) , (3) where τ is the trajectory in the target domain under the policy π θ defined as τ = (s 0 , a 0 , s 1 , a 1 , . . . ), and probability of the trajectory p π θ (τ ) under policy π θ and target transition p T (.|s t , a t ) can be written as Similarly q π * (τ ) can be defined as probability of trajectory deviations at every state s t ∈ τ , when the source optimal policy π * is used in place of target policy π θ , and the states evolve according to the source transitions model p S (.|s t , a t ) (Refer Figure-3 in Appendix-C): Assumption-1: We assume that optimal source policy is available, such that the source policy vari- ance can be assumed to be zero. That is, the action probabilities π * (a t |s t ) = 1, ∀s t . This is a reasonable assumption, since optimal source policy is available it can be treated as deterministic policy and actions can be chosen greedily. We need this assumption only for deriving the expression for intrinsic reward and theoretical analysis. In the empirical evaluation of the algorithm, we treat π * (.) as a stochastic policy. The transition probabilities in the KL divergence in (3) term are treated as transition likelihoods and the transitioned state s t+1 as a random variable. Since s t+1 is the optimal state reached starting in s t under source transition model using π * (a t |s t ), we try to emulate this behavior in the target domain and hence evaluate the transition likelihoods of the target trajectory at {s i } H i=1 . Using the definition of the probabilities of the trajectories under π θ and π * Equation-(4) & (5) the log term in the KL divergence of the trajectory (3) is simplified as follows Using (6) and with assumption-1 i.e. π * (a t |s t ) = 1∀s t the KL term can simplified as follows If calculating the above expectation is feasible, it is possible to minimize the KL divergence and move the policy parameters in the direction of emulating source transition behavior in the target domain. However, this is not generally the case since the true expectation is intractable. Therefore a common practice is to use an empirical estimate of the expectation to do approximate planning. We minimize the trajectory KL divergence by handling the term ζ t as the intrinsic adaptation reward, which captures the local deviation in trajectory in the form of the shaped reward function. Assumption-2: We do not assume to know the true transition distribution for source and the target, but only have access to simulator model of the source. However, we assume both source and target transition models follow a Gaussian distribution centered at the next propagated state and fixed variance "σ".

Section Title: Under review as a conference paper at ICLR 2020
  Under review as a conference paper at ICLR 2020 Assumption-2 is not very restrictive since we empirically show that for any deterministic model, a bootstrapped Gaussian transition assumption is sufficient for ATL agent to learn the task. Using the above assumption, we can approximate the KL term as The individual terms in the expectation ζ t represent the distance between two transition likelihoods of landing in the next state s t+1 starting in s t and under actions a t , a t . The target agent is encouraged to take actions that lead to states which are close in expectation to a reference state provided by an optimal baseline policy operating on the source model. By doing so, we are providing a possible direction of search for the higher environmental rewards "r t ". We can, therefore, solve the following optimization problem to generate adaptive policy updates: The distance between two transition models ζ t is used as an intrinsic reward to calculate the total trajectory intrinsic return, and further, any policy update algorithm can be used ( Sutton et al., 2000 ;  Schulman et al., 2017 ;  2015 ) to update the policy in direction optimizing this objective. However, it is to be noted that though we use policy gradient kind of update for adaptive policy, the optimization is more akin to supervised learning. For every state-action pair, the KL-distance is the true metric to be minimized, and unlike RL, we do not engage in optimal value search in the adaptation part of learning. Hence the algorithm is more sample efficient compared to any RL policy search methods.

Section Title: ADAPTATION AND LEARNING
  ADAPTATION AND LEARNING We achieve Adaption and Learning simultaneously by augmenting environment reward r t with in- trinsic reward ζ t . By doing so, we achieve transferred policies which try to both optimize optimal reference tracking and also maximize the cumulative future environmental reward to acquire skills beyond what source can teach. This trade-off between learning by exploration and learning by adaptation can be realized as follows: Where the term β is the mixing coefficient. We make a heuristic choice for an appropriate β to start with and annealed over episodes for optimal mixing of adaptation and learning. For consistency of the reward mixing, the rewards r t , ζ t are normalized to form the total reward r t = (1 − β)r t + βζ t . Calculate the KL divergence intrinsic reward term 9: Incrementally store the trajectory for policy update 10: Minimize the loss to obtain a adaptive policy

Section Title: SAMPLE-BASED ESTIMATION OF THE GRADIENT
  SAMPLE-BASED ESTIMATION OF THE GRADIENT The previous section proposed an optimization method to find the adaptive policy using KL- divergence as an intrinsic reward, enforcing the target transition model to mimic the source tran- sitions. This section describes how this objective can be approximated using a Monte Carlo simu- lation. The adaptive policy update methods work by computing an estimator of the gradient of the Under review as a conference paper at ICLR 2020 return and plugging it into a stochastic gradient ascent algorithm. where P Z n is empirical distribution over the data (Z n : {s i , a i , a i } n i ).

Section Title: THEORETICAL BOUNDS ON SAMPLE COMPLEXITY
  THEORETICAL BOUNDS ON SAMPLE COMPLEXITY Although there is some empirical evidence that transfer can improve performance in subsequent reinforcement-learning tasks, there are not many theoretical guarantees. Since many of the existing transfer algorithms approach the problem of transfer as a method of providing good initialization to target task RL, we can expect the sample complexity of those algorithms to still be a function of the cardinality of state-action pairs |N | = |S| × |A|. On the other hand, in supervised learning setting, the theoretical guarantees of most algorithm have no dependency on size (or dimensionality) of the input domain (which is analogous to |N | in RL domains). Having formulated a policy transfer algo- rithm using labeled reference trajectories derived from optimal source policy in ATL, we construct supervised learning like PAC property of the proposed method. For the sample complexity analysis we consider only the adaptation part of learning i.e. β = 1 in Equation-(16). This is because in ATL, the adaptive learning is akin to supervised learning, since the source reference trajectories provide the target states given every (s t , a t ) pair. Suppose we are given the learning problem specified with training set Z n = (Z1, . . . Z n ) where each Z i = ({s i , a i , a i }) H i=0 are independently drawn according to some distribution P . Given the data Z n we can compute the empirical return P Z n (η KL ) for every π θ ∈ Π, and we will show that the following holds: and number of trajectory samples required can be lower bounded as For the proof of the above theorem refer the Appendix-B.

Section Title: -OPTIMALITY RESULT UNDER ADAPTIVE TRANSFER-LEARNING
  -OPTIMALITY RESULT UNDER ADAPTIVE TRANSFER-LEARNING Consider MDP M * andM which differ in their transition models. For the sake of analysis, let M * be the MDP with ideal transition model, such that target follows source transition p * precisely. Letp be the transition model achieved (tracked) using the estimated adapted policy learned over data interacting with the target model and the associated MDP be denoted asM . We analyze the -optimality of return under adapted source optimal policy through proposed policy transfer.

Section Title: Under review as a conference paper at ICLR 2020
  Under review as a conference paper at ICLR 2020 Note for the sake of analysis we are deviating from finite horizon assumption and using discout factor γ. To be consistent with previous section we can assume the horizon length H = log γ v 2Vmax , where v is truncation error in total infinite return. Definition 4.2 Given the value function V * = V π * and model M 1 and M 2 , which only differ in the corresponding transition models p 1 and p 2 . Lets define ∀s, a ∈ S × A The proof of this lemma is based on the simulation lemma (see Appendix-C). Similar results for RL with imperfect models were reported by ( Jiang, 2018 ).

Section Title: POLICY TRANSFER IN SIMULATED ROBOTIC LOCOMOTION TASKS
  POLICY TRANSFER IN SIMULATED ROBOTIC LOCOMOTION TASKS To evaluate Adapt-to-Learn Policy Transfer in reinforcement learning, we design our experiments using sets of tasks based on the continuous control environments in MuJoCo simulator ( Todorov et al., 2012 ). Our experimental results demonstrate that ATL can adapt to significant changes in transition dynamics. Therefore, we perturb the parameters of the simulated target models for the policy transfer experiments (see  Table-1  for original and perturbed parameters of the target mode). To create a challenging training environment, we changed the parameters of the model such that the optimal source policy alone without any learning cannot produce any stable results (see source policy performance in  Figure-1 ). We compare our results against two baselines: (a) Initialized Reinforcement learning (initialized PPO) (Jumpstart-RL ( Wang & Mahadevan, 2009 )) (b) stand- alone reinforcement policy learning (PPO) ( Schulman et al., 2017 ). We experiment with the ATL algorithm on Hopper, Walker2d, and HalfCheetah Environments. The states of the robots are their generalized positions and velocities, and the actions are joint torques. High dimensionality, non-smooth dynamics due to contacts and being under-actuated systems make these tasks very challenging. We use deep neural networks to represent the source and target pol- icy, the details of which are in the Table-2 Appendix-D. The following models are included in our evaluation: Slippery Hopper: is defined through 11-dimensional state space and 3-dimension action space, with reward function defined as r(t) = (s t+1 − s t )/dt − 10 −3 a 2 , and a bonus of +1 for being in a non-terminal state. The simulation is terminated upon reaching 1000 steps or hopper toppling. The target model differs in the floor friction and foot joint damping. Slippery Fat-Walker2d: is defined through 17-dimensional state space and 6-dimension action space, with reward function and termination condition defined same as Hopper. The target model differs in the model density and floor friction. Fat HalfCheetah: is defined through 17-dimensional state space and 6-dimension action space, with reward function defined as r(t) = (s t+1 − s t )/dt − 0.1 a 2 . The simulation is terminated upon reaching 1000 steps. The target model differs in the floor friction coefficient, gravity, and mass. To establish a standard baseline, we also included the classic cart-pole and Inverted pendulum bal- ancing tasks, based on the formulation ( Barto et al., 1983 ). We also demonstrate the cross-domain transfer capabilities using a model-based variant of the proposed algorithm. The results of policy transfer for Cart-Pole to Inverted Pendulum and Inverted Pendulum to Bicycle transfers are provided in the Appendix-D. Learning curves showing the total reward averaged across three runs of each al- gorithm are provided in  Figure-1 . Adapt-to Learn policy transfer solved all the three tasks, yielding quicker learning compared to other baseline methods. These results provide empirical evidence of our hypothesis. Using trajectory KL divergence as intrinsic adaptation reward to adapt source pol- icy to the target, we achieve a more robust and sample efficient policy transfer between two tasks,

Section Title: CONCLUSION
  CONCLUSION We introduced a new transfer learning technique for RL: Adapt-to-Learn, that utilizes adaptation of the source policy to target tasks. We demonstrated on nonlinear and continuous robotic locomotion tasks that learning to adapt source policy to the target domain leads to a significant reduction in sample complexity over the prevalent jump-start based approaches. We further also proved theo- retical guarantees on the reduced sample complexity of our proposed architecture. There are many exciting directions for future work. A network of policies that can generalize across multiple tasks could be learned based on each new adapted policies. How to train this end-to-end is an important question for meta-learning. The ability of Adapt-to-Learn to handle significant perturbations to the Under review as a conference paper at ICLR 2020 transition model indicates that it should naturally extend to sim-to-real transfer. Indeed we argue that such adaptation is necessary for real-world robotics, as has been established previously in classical domains like flight control. Another exciting direction is to extend the work to other combinatorial domains (e.g., multiplayer games). We expect, therefore follow on work will find other exciting ways of exploiting such adaptation in RL and machine learning.

```
