<article article-type="research-article"><front><article-meta><title-group><article-title>Under review as a conference paper at ICLR 2020 MULTIPOLAR: MULTI-SOURCE POLICY AGGREGA- TION FOR TRANSFER REINFORCEMENT LEARNING BE- TWEEN DIVERSE ENVIRONMENTAL DYNAMICS</article-title></title-group><abstract><p>Transfer reinforcement learning (RL) aims at improving learning efficiency of an agent by exploiting knowledge from other source agents trained on relevant tasks. However, it remains challenging to transfer knowledge between different environ- mental dynamics without having access to the source environments. In this work, we explore a new challenge in transfer RL, where only a set of source policies collected under unknown diverse dynamics is available for learning a target task efficiently. To address this problem, the proposed approach, MULTI-source POL- icy AggRegation (MULTIPOLAR), comprises two key techniques. We learn to aggregate the actions provided by the source policies adaptively to maximize the target task performance. Meanwhile, we learn an auxiliary network that predicts residuals around the aggregated actions, which ensures the target policy's expres- siveness even when some of the source policies perform poorly. We demonstrated the effectiveness of MULTIPOLAR through an extensive experimental evaluation across six simulated environments ranging from classic control problems to chal- lenging robotics simulations, under both continuous and discrete action spaces.</p></abstract></article-meta></front><body><sec><title>INTRODUCTION</title><p>We envision a future scenario where a variety of robotic systems, which are each trained or manually engineered to solve a similar task, provide their policies for a new robot to learn a relevant task quickly. For example, imagine various pick-and-place robots working in factories all over the world. Depending on the manufacturer, these robots will differ in their kinematics (e.g., link length, joint orientations) and dynamics (e.g., link mass, joint damping, friction, inertia). They could provide their policies to a new robot (<xref ref-type="bibr" rid="b5">Devin et al., 2017</xref>), even though their dynamics factors, on which the policies are implicitly conditioned, are not typically available (<xref ref-type="bibr" rid="b3">Chen et al., 2018</xref>). Moreover, we cannot rely on a history of their individual experiences, as they may be unavailable due to a lack of communication between factories or prohibitively large dataset sizes. In such scenarios, we argue that a key technique to develop is the ability to transfer knowledge from a collection of robots to a new robot quickly only by exploiting their policies while being agnostic to their different kinematics and dynamics, rather than collecting a vast amount of samples to train the new robot from scratch. The scenario illustrated above poses a new challenge in the transfer learning for reinforce- ment learning (RL) domains. Formally, con- sider multiple instances of a single environment that differ in their state transition dynamics, e.g., independent ant robots with different leg designs in <xref ref-type="fig" rid="fig_0">Figure 1</xref>, which reach different lo- cations by executing the same walking actions. These source agents interacting with one of the environment instances provide their determin- istic policy to a new target agent in another en- vironment instance. Then, our problem is: can we efficiently learn the policy of a target agent given only the collection of source policies? Note that information about source environmental dynamics, such as the exact state transition distribu- Under review as a conference paper at ICLR 2020 tions and the history of environmental states, will not be visible to the target agent as mentioned above. Also, the source policies are neither trained nor hand-engineered for the target environment instance, and therefore not guaranteed to work optimally and may even fail (<xref ref-type="bibr" rid="b3">Chen et al., 2018</xref>). These conditions prevent us from adopting existing work on transfer RL between different envi- ronmental dynamics, as they require access to source environment instances or their dynamics for training a target policy (e.g., <xref ref-type="bibr" rid="b15">Lazaric et al. (2008)</xref>; <xref ref-type="bibr" rid="b3">Chen et al. (2018)</xref>; <xref ref-type="bibr" rid="b35">Yu et al. (2019)</xref>; <xref ref-type="bibr" rid="b32">Tirinzoni et al. (2018)</xref>). Similarly, meta-learning approaches (<xref ref-type="bibr" rid="b33">Vanschoren, 2018</xref>; <xref ref-type="bibr" rid="b22">Saemundsson et al., 2018</xref>; <xref ref-type="bibr" rid="b4">Clavera et al., 2019</xref>) cannot be used here because they typically train an agent on a diverse set of tasks (i.e., environment instances). Also, existing techniques that utilize a collection of source poli- cies, e.g., policy reuse frameworks (<xref ref-type="bibr" rid="b7">Fern&#225;ndez &amp; Veloso, 2006</xref>; <xref ref-type="bibr" rid="b21">Rosman et al., 2016</xref>; <xref ref-type="bibr" rid="b0">Zheng et al., 2018</xref>) and option frameworks (<xref ref-type="bibr" rid="b27">Sutton et al., 1999</xref>; <xref ref-type="bibr" rid="b1">Bacon et al., 2017</xref>; Mankowitz et al., 2018), are not a promising solution because, to our knowledge, they assume source policies have the same environmental dynamics but have different goals.</p><p>As a solution to the problem, we propose a new transfer RL approach named MULTI-source POL- icy AggRegation (MULTIPOLAR). As shown in <xref ref-type="fig" rid="fig_1">Figure 2</xref>, our key idea is twofold; 1) In a target policy, we adaptively aggregate the deterministic actions produced by a collection of source poli- cies. By learning aggregation parameters to maximize the expected return at a target environment instance, we can better adapt the aggregated actions to unseen environmental dynamics of the tar- get instance without knowing source environmental dynamics nor source policy performances. 2) We also train an auxiliary network that predicts a residual around the aggregated actions, which is crucial for ensuring the expressiveness of the target policy even when some source policies are not useful. As another notable advantage, the proposed MULTIPOLAR can be used for both continuous and discrete action spaces with few modifications while allowing a target policy to be trained in a principled fashion. Similar to <xref ref-type="bibr" rid="b0">Ammar et al. (2014)</xref>; <xref ref-type="bibr" rid="b26">Song et al. (2016)</xref>; <xref ref-type="bibr" rid="b3">Chen et al. (2018)</xref>; <xref ref-type="bibr" rid="b32">Tirinzoni et al. (2018)</xref>; <xref ref-type="bibr" rid="b35">Yu et al. (2019)</xref>, our method assumes that the environment structure (state/action space) is identical between the source and target environments, while dynamics/kinematics parameters are different. This assumption holds in many real-world applications such as in sim-to-real tasks (<xref ref-type="bibr" rid="b29">Tan et al., 2018</xref>), industrial insertion tasks (<xref ref-type="bibr" rid="b23">Schoettler et al., 2019</xref>) (different dynamics comes from the differences in parts), and wearable robots (<xref ref-type="bibr" rid="b16">Zhang et al., 2017</xref>) (with users as dynamics).</p><p>We evaluate MULTIPOLAR in a variety of environments ranging from classic control problems to challenging robotics simulations. Our experimental results demonstrate the significant improvement of sample efficiency with the proposed approach, compared to baselines that trained a target policy from scratch or from a single source policy. We also conducted a detailed analysis of our approach and found it works well even when some of the source policies performed poorly in their original environment instance.</p><p>Main contributions: (1) a new transfer RL problem that leverages multiple source policies col- lected under diverse environmental dynamics to train a target policy in another dynamics, and (2) MULTIPOLAR, a simple yet principled and effective solution verified in our extensive experiments.</p></sec><sec><title>PRELIMINARIES</title></sec><sec><title>Reinforcement Learning</title><p>We formulate our problem under the standard RL framework (<xref ref-type="bibr" rid="b27">Sutton &amp; Barto, 1998</xref>), where an agent interacts with its environment modeled by a Markov decision process (MDP). An MDP is represented by the tuple M = (&#961; 0 , &#947;, S, A, R, T ) where &#961; 0 is the initial state distribution and &#947; is a discount factor. At each timestep t, given the current state s t &#8712; S, the agent executes an action a t &#8712; A based on its policy &#960;(a t | s t ; &#952;) that is parameterized by &#952;. The environment returns a reward R(s t , a t ) &#8712; R and transitions to the next state s t+1 based on the state transition distribution T (s t+1 | s t , a t ). In this framework, RL aims to maximize the expected return with respect to the policy parameters &#952;.</p></sec><sec><title>Environment Instances</title><p>In this work, we consider K instances of the same environment that differ only in their state transition dynamics. We model each environment instance by an indexed MDP: M i = (&#961; 0 , &#947;, S, A, R, T i ) where no two state transition distributions T i , T j ; i = j are identical. We also assume that each T i is unknown when training a target policy, i.e., agents cannot access the exact form of T i nor a collection of states sampled from T i .</p></sec><sec><title>Source Policies</title><p>For each of the K environment instances, we are given a deterministic source policy &#181; i : S &#8594; A that only maps states to actions. Each source policy &#181; i can be either parame- terized (e.g., learned from interacting with the environment modeled by M i ) or non-parameterized (e.g., heuristically designed by humans). Either way, we assume no prior knowledge about &#181; i is available for a target agent, such as their representations or original performances, except that they were acquired in M i with an unknown T i .</p></sec><sec><title>Problem Statement</title><p>Given the set of source policies L = {&#181; 1 , . . . , &#181; K }, our goal is to train a new target agent's policy &#960; target (a t | s t ; L, &#952;) in a sample efficient fashion, where the target agent interacts with another environment instance M target = (&#961; 0 , S, A, R, T target ) and T target is not necessarily identical to T i (i = 1 . . . , K).</p></sec><sec><title>MULTI-SOURCE POLICY AGGREGATION</title><p>As shown in <xref ref-type="fig" rid="fig_1">Figure 2</xref>, with the Multi-Source Policy Aggregation (MULTIPOLAR), we formulate a target policy &#960; target using a) the adaptive aggregation of deterministic actions from the set of source policies L, and b) the auxiliary network predicting residuals around the aggregated actions. We first present our method for the continuous action space, and then extend it to the discrete space. Adaptive Aggregation of Source Policies Let us denote by a (i) t = &#181; i (s t ) the action predicted deterministically by source policy &#181; i given the current state s t . For the continuous action space, a (i) t &#8712; R D is a D-dimensional real-valued vector representing D actions performed jointly in each timestep. For the collection of source policies L, we derive the matrix of their deterministic actions:</p><p>The key idea of this work is to aggregate A t adaptively in an RL loop, i.e., to maximize the expected return. This adaptive aggregation gives us a "baseline" action that could introduce a strong inductive bias in the training of a target policy, without knowing source environmental dynamics T i . More specifically, we define the adaptive aggregation function F agg : S &#8594; A that produces the baseline action based on the current state s t as follows: F agg (s t ; L, &#952; agg ) = 1 K 1 K (&#952; agg A t ) , (2) where &#952; agg &#8712; R K&#215;D is a matrix of trainable parameters, is the element-wise multiplication, and 1 K is the all-ones vector of length K. &#952; agg is neither normalized nor regularized, and can scale each action of each policy independently. This means that we do not merely adaptively interpolate action spaces, but more flexibly emphasize informative source actions while suppressing irrelevant ones.</p><p>Predicting Residuals around Aggregated Actions Moreover, we learn auxiliary network F aux : S &#8594; A jointly with F agg , to predict residuals around the aggregated actions. F aux is used to improve the target policy training in two ways. 1) If the aggregated actions from F agg are already useful in Under review as a conference paper at ICLR 2020 the target environment instance, F aux will correct them for a higher expected return. 2) Otherwise, F aux learns the target task while leveraging F agg as a prior to have a guided exploration process. Any network could be used for F aux as long as it is parameterized and fully differentiable. Finally, the MULTIPOLAR function is formulated as: F (s t ; L, &#952; agg , &#952; aux ) = F agg (s t ; L, &#952; agg ) + F aux (s t ; &#952; aux ), (3) where &#952; aux denotes a set of trainable parameters for F aux . Note that the idea of predicting residuals for a source policy has also been presented by <xref ref-type="bibr" rid="b25">Silver et al. (2018)</xref>; <xref ref-type="bibr" rid="b13">Johannink et al. (2019)</xref>; <xref ref-type="bibr" rid="b20">Rana et al. (2019)</xref>. The main difference here is that, while these works just add raw action outputs provided from a single hand-engineered source policy, we adaptively aggregate actions from multiple source policies in order to obtain a more flexible and canonical representation.</p><p>Target Policy Target policy &#960; target can be modeled by reparameterizing the MULTIPOLAR func- tion as a Gaussian distribution, i.e., N (F (s t ; L, &#952; agg , &#952; aux ), &#931;), where &#931; is a covariance matrix estimated based on what the used RL algorithm requires. Since we regard &#181; i &#8712; L as fixed func- tions mapping states to actions, this Gaussian policy &#960; target is differentiable with respect to &#952; agg and &#952; aux , and hence could be trained with any RL algorithm that explicitly updates policy parameters. Unlike <xref ref-type="bibr" rid="b25">Silver et al. (2018)</xref>; <xref ref-type="bibr" rid="b13">Johannink et al. (2019)</xref>; <xref ref-type="bibr" rid="b20">Rana et al. (2019)</xref>, we can formulate the target policy in a principled fashion for actions in a discrete space. Specifically, instead of a D-dimensional real-valued vector, here we have a D-dimensional one-hot vector a (i) t &#8712; {0, 1} D , j (a (i) t ) j = 1 as outputs of &#181; i , where (a (i) t ) j = 1 indicates that the j-th action is to be executed. Following Eqs. (2) and (3), the output of F (s t ; L, &#952; agg , &#952; aux ) can be viewed as D-dimensional un-normalized action scores, from which we can sample a discrete action after normalizing it by the softmax function.</p></sec><sec><title>EXPERIMENTAL EVALUATION</title><p>We aim to empirically demonstrate the sample efficiency of a target policy trained with MULTIPO- LAR (denoted by "MULTIPOLAR policy"). To complete the experiments in a reasonable amount of time, we set the number of source policies to be K = 4 unless mentioned otherwise. Moreover, we investigate the factors that affect the performance of MULTIPOLAR. To ensure fair compar- isons and reproducibility of experiments, we followed the guidelines introduced by <xref ref-type="bibr" rid="b8">Henderson et al. (2018)</xref> and <xref ref-type="bibr" rid="b8">Fran&#231;ois-Lavet et al. (2018)</xref> for conducting and evaluating all of our experiments.</p></sec><sec><title>EXPERIMENTAL SETUP</title></sec><sec><title>Baseline Methods</title><p>To show the benefits of leveraging source policies, we compared our MULTI- POLAR policy to the standard multi-layer perceptron (MLP) trained from scratch, which is typically used in RL literature (<xref ref-type="bibr" rid="b9">Schulman et al., 2017</xref>; <xref ref-type="bibr" rid="b8">Fran&#231;ois-Lavet et al., 2018</xref>). As another baseline, we also used MULTIPOLAR with K = 1, which is an extension of residual policy learning (<xref ref-type="bibr" rid="b25">Silver et al., 2018</xref>; <xref ref-type="bibr" rid="b13">Johannink et al., 2019</xref>; <xref ref-type="bibr" rid="b20">Rana et al., 2019</xref>) (denoted by "RPL") with adaptive residuals as well as the ability to deal with both continuous and discrete action spaces. We stress here that the existing transfer RL or meta RL approaches that train a universal policy network agnostic to the environmental dynamics, such as <xref ref-type="bibr" rid="b9">Frans et al. (2018)</xref>; <xref ref-type="bibr" rid="b3">Chen et al. (2018)</xref>, cannot be used as a baseline since they require a policy to be trained on a distribution of environment instances, which is not possible in our problem setting. Also, other techniques using multiple source policies, such as policy reuse frameworks, are not applicable because their source policies should be collected under the target environmental dynamics.</p></sec><sec><title>Environments</title><p>To show the general effectiveness of the MULTIPOLAR policy, we conducted comparative evaluations of MULTIPOLAR on the following six OpenAI Gym environments: Ro- boschool Hopper, Roboschool Ant, Roboschool InvertedPendulumSwingUp, Acrobot, CartPole, and LunarLander. We chose these six environments because 1) the parameterization of their dynam- ics and kinematics is flexible enough, 2) they cover discrete action space (Acrobot and CartPole) as well as continuous action space, and 3) they are samples of three distinct categories of OpenAI Gym environments, namely Box2d, Classic Control, and Roboschool.</p></sec><sec><title>Experimental Procedure</title><p>For each of the six environments, we first created 100 environment instances by randomly sampling the dynamics and kinematics parameters from a specific range. For example, these parameters in the Hopper environment were link lengths, damping, friction, Under review as a conference paper at ICLR 2020 armature, and link mass 1 Then, for each environment instance, we trained an MLP policy. The trained MLP policies were used in two ways: a) the baseline MLP policy for each environment instance, and b) a pool of 100 source policy candidates from which we sample K of them to train MULTIPOLAR policies and one of them to train RPL policies 2 . Specifically, for each environment instance, we trained three MULTIPOLAR and three RPL policies with distinct sets of source policies selected randomly from the candidate pool. The learning procedure explained above was done three times with fixed different random seeds to reduce variance in results due to stochasticity. As a result, for each of the six environments, we had 100 environment instances &#215; 3 random seeds = 300 experiments for MLP and 100 environment instances &#215; 3 choices of source policies &#215; 3 random seeds = 900 experiments for RPL and MULTIPOLAR. The aim of this large number of experiments is to obtain correct insights into the distribution of performances (<xref ref-type="bibr" rid="b8">Henderson et al., 2018</xref>). Due to the large number of experiments for all the environments, our detailed analysis and ablation study of MULTIPOLAR components were conducted with only Hopper, as its sophisticated second-order dynamics plays a crucial role in agent performance (<xref ref-type="bibr" rid="b3">Chen et al., 2018</xref>).</p></sec><sec><title>Implementation Details</title><p>All the experiments were done using the Stable Baselines (<xref ref-type="bibr" rid="b12">Hill et al., 2018</xref>) implementation of learning algorithms as well as its default hyperparameters and MLP net- work architecture for each environment (see Appendix A.1 for more details). Based on the perfor- mance of learning algorithms reported in the <xref ref-type="bibr" rid="b12">Hill et al. (2018)</xref>, all the policies were trained with Soft Actor-Critic (<xref ref-type="bibr" rid="b10">Haarnoja et al., 2018</xref>) in the LunarLander environment and with Proximal Pol- icy Optimization (<xref ref-type="bibr" rid="b9">Schulman et al., 2017</xref>) in the rest of the environments. For fair comparisons, in all experiments, auxiliary network F aux had an identical architecture to that of the MLP. There- fore, the only difference between MLP and MULTIPOLAR was the aggregation part F agg , which made it possible to evaluate the contribution of transfer learning based on adaptive aggregation of source policies. Also, we avoided any random seed optimization since it has been shown to alter the policies' performance (<xref ref-type="bibr" rid="b8">Henderson et al., 2018</xref>).</p></sec><sec><title>Evaluation Metric</title><p>Following the guidelines of <xref ref-type="bibr" rid="b8">Henderson et al. (2018)</xref>, to measure sampling ef- ficiency of training policies, i.e., how quick the training progresses, we used the average episodic reward over a various number of training samples. Also, to ensure that higher average episodic reward is representative of better performance and to estimate the variation of it, we used the sam- ple bootstrap method (<xref ref-type="bibr" rid="b6">Efron &amp; Tibshirani, 1993</xref>) to estimate statistically relevant 95% confidence bounds of the results of our experiments. Across all the experiments, we used 10K bootstrap itera- tions and the pivotal method. Further details on evaluation method can be found in Appendix A.3.</p></sec><sec><title>RESULTS</title><p>Sample Efficiency of MULTIPOLAR <xref ref-type="fig" rid="fig_2">Figure 3</xref> and <xref ref-type="table" rid="tab_0">Table 1</xref> clearly show that on average, in all the environments, MULTIPOLAR outperformed baseline policies in terms of sample efficiency and sometimes the final episodic reward 3 . For example, in Hopper over 2M training samples, MULTI- POLAR with K = 4 achieved a mean of average episodic reward about three times higher than MLP (i.e., training from scratch) and about twice higher than RPL (i.e., using only a single source policy). It is also noteworthy that MULTIPOLAR with K = 4 had on par or better performance than RPL, which indicates the effectiveness of leveraging multiple source policies 4 . Figure 7 in Appendix, shows the individual average learning curve for each of the instances of Roboschool environments.</p></sec><sec><title>Ablation Study</title><p>To demonstrate the importance of each component of MULTIPOLAR, we eval- uated the following degraded versions: (1) &#952; agg fixed to 1, which just averages the deterministic actions from the source policies without adaptive weights (similar to the residual policy learning Under review as a conference paper at ICLR 2020 methods that used raw action outputs of a source policy), and (2) F aux learned independent of s t , which replaces the state-dependent MLP with an adaptive "placeholder" parameter vector making actions just a linear combination of source policy outputs. As shown in <xref ref-type="table" rid="tab_1">Table 2</xref>, the full version of MULTIPOLAR significantly outperformed both of the degraded versions, suggesting that the adaptive aggregation and predicting residuals are both critical.</p></sec><sec><title>Effect of Source Policy Performances</title><p><xref ref-type="fig" rid="fig_3">Figure 4</xref> illustrates an example of the histogram of fi- nal episodic reward (average rewards of the last 100 training episodes) for the source policy can- didates obtained in the Hopper environment. As shown in the figure, the source policies were diverse in terms of the performance on their original environment instances 5 . In this setup, we investigate the effect of source policies performances on MULTIPOLAR sample efficiency. We created two separate pools of source policies, where one contained only high-performing and the other only low- performing source policies 6 . <xref ref-type="table" rid="tab_2">Table 3</xref> summarizes the results of sampling source policies from these pools (4 high, 2 high &amp; 2 low, and 4 low performances) and compares them to the original MULTIPOLAR (shown as 'Random') also reported in <xref ref-type="table" rid="tab_0">Table 1</xref>. Not surprisingly, MULTIPOLAR performed the best when all the source policies were sampled from the high- performance pool. However, we emphasize that such high- quality policies are not always available in practice, due to the variability of how they are learned or hand-crafted under their own environment instance. Figure 6 in Appendix B.1 illus- trates that MULTIPOLAR can successfully learn to suppress the useless low-performing source policies.</p></sec><sec><title>Effect of Number of Source Policies</title><p>Finally, we show how the number of source policies con- tributes to MULTIPOLAR's sample efficiency in <xref ref-type="table" rid="tab_3">Table 4</xref>. Specifically, we trained MULTIPOLAR policies up to K = 16 to study how the mean of average episodic rewards changes. The monotonic performance improvement over K (for K &#8804; 16), is achieved at the cost of increased training and inference time. In practice, we suggest balancing this speed-performance trade-off by using as many source policies as possible before reaching the inference time limit required by the application.</p></sec><sec><title>DISCUSSION AND RELATED WORK</title><p>Our work is broadly categorized as an instance of transfer RL (<xref ref-type="bibr" rid="b0">Taylor &amp; Stone, 2009</xref>), in which a policy for a target task is trained using information collected from source tasks. In this section, we highlight how our work is different from the existing approaches and also discuss the current limitations as well as future directions.</p></sec><sec><title>Transfer between Different Dynamics</title><p>There has been very limited work on transferring knowl- edge between agents in different environmental dynamics. As introduced briefly in Section 1, some methods require training samples collected from source tasks. These sampled experiences are then used for measuring the similarity between environment instances (Lazaric et al., 2008; <xref ref-type="bibr" rid="b0">Ammar et al., 2014</xref>; <xref ref-type="bibr" rid="b32">Tirinzoni et al., 2018</xref>) or for conditioning a target policy to predict actions (<xref ref-type="bibr" rid="b3">Chen et al., 2018</xref>). Alternative means to quantify the similarity is to use a full specification of MDPs (Song et al., 2016; <xref ref-type="bibr" rid="b26">Wang et al., 2019</xref>) or environmental dynamics <xref ref-type="bibr" rid="b35">Yu et al. (2019)</xref>. In contrast, the proposed MULTI- POLAR allows the knowledge transfer only through the policies acquired from source environment instances, which is beneficial when source and target environments are not always connected to exchange information about their environmental dynamics and training samples.</p></sec><sec><title>Leveraging Multiple Policies</title><p>The idea of utilizing multiple source policies can be found in the literature of policy reuse frameworks (<xref ref-type="bibr" rid="b7">Fern&#225;ndez &amp; Veloso, 2006</xref>; <xref ref-type="bibr" rid="b21">Rosman et al., 2016</xref>; <xref ref-type="bibr" rid="b16">Li &amp; Zhang, 2018</xref>; <xref ref-type="bibr" rid="b0">Zheng et al., 2018</xref>; <xref ref-type="bibr" rid="b16">Li et al., 2019</xref>). The basic motivation behind these works is to provide "nearly-optimal solutions" (<xref ref-type="bibr" rid="b21">Rosman et al., 2016</xref>) for short-duration tasks by reusing one of the source policies, where each source would perform well on environment instances with different rewards (e.g., different goals in maze tasks). In our problem setting, where environmental dynamics behind each source policy are different, reusing a single policy without an adaptation is not the right approach, as described in (<xref ref-type="bibr" rid="b3">Chen et al., 2018</xref>) and also demonstrated in our experiment. Another relevant idea is hierarchical RL (<xref ref-type="bibr" rid="b2">Barto &amp; Mahadevan, 2003</xref>; <xref ref-type="bibr" rid="b14">Kulkarni et al., 2016</xref>; <xref ref-type="bibr" rid="b19">Osa et al., 2019</xref>) that involves a hierarchy of policies (or action-value functions) to enable temporal abstraction. In particular, option frameworks (<xref ref-type="bibr" rid="b27">Sutton et al., 1999</xref>; <xref ref-type="bibr" rid="b1">Bacon et al., 2017</xref>; Mankowitz et al., 2018) make use of a collection of policies as a part of "options". However, they assumed all the policies in the hierarchy to be learned in a single environment instance. Another relevant work along this line of research is (<xref ref-type="bibr" rid="b9">Frans et al., 2018</xref>), which meta-learns a hierarchy of multiple sub-policies by training a master policy over the distribution of tasks. Nevertheless, hierarchical RL approaches are not useful for leveraging multiple source policies each acquired under diverse environmental dynamics.</p><p>Learning Residuals in RL Finally, some recent works adopt residual learning to mitigate the limited performance of hand-engineered policies (<xref ref-type="bibr" rid="b25">Silver et al., 2018</xref>; <xref ref-type="bibr" rid="b13">Johannink et al., 2019</xref>; <xref ref-type="bibr" rid="b20">Rana et al., 2019</xref>). We are interested in a more extended scenario where various source policies with unknown performances are provided instead of a single sub-optimal policy. Also, these approaches focus only on RL problems for robotic tasks in the continuous action space, while our approach could work on both of continuous and discrete action spaces in a broad range of environments.</p></sec><sec><title>Limitations and Future Directions</title><p>Currently, our work has several limitations. First, MULTI- POLAR may not be scalable to a large number of source policies, as its training and testing times will increase almost linearly with the number of source policies. One possible solution for this issue would be pre-screening source policies before starting to train a target agent, for example, by testing each source on the target task and taking them into account in the training phase only when they are found useful. Moreover, our work assumes source and target environment instances to be different only in their state transition distribution. An interesting direction for future work is to involve other types of environmental differences, such as dissimilar rewards and state/action spaces.</p></sec><sec><title>CONCLUSION</title><p>We presented a new problem setting of transfer RL that aimed to train a policy efficiently using a collection of source policies acquired under diverse environmental dynamics. We demonstrated that the proposed MULTIPOLAR is, despite its simplicity, a principled approach with high training sample efficiency on a variety of environments. Our transfer RL approach is advantageous when one does not have access to a distribution of diverse environmental dynamics. Future work will seek to adapt our approach to more challenging domains such as a real-world robotics task. Under review as a conference paper at ICLR 2020</p></sec><sec id="figures"><title>Figures</title><fig id="fig_0"><object-id>fig_0</object-id><label>Figure 1:</label><caption><title>Figure 1:</title><p>Ant Example. A policy of a target agent (right) is learned by utilizing the policies of other source agents with different leg designs (left).</p></caption><graphic /><graphic /><graphic /><graphic /><graphic /></fig><fig id="fig_1"><object-id>fig_1</object-id><label>Figure 2:</label><caption><title>Figure 2:</title><p>Overview of MULTIPOLAR. We formulate a target policy &#960; target with the sum of 1) the adaptive aggregation F agg of deterministic actions from source policies L and 2) the auxiliary network F aux for predicting residuals around F agg .</p></caption><graphic /></fig><fig id="fig_2"><object-id>fig_2</object-id><label>Figure 3:</label><caption><title>Figure 3:</title><p>Average Learning Curves of MLP, RPL, and MULTIPOLAR (K = 4) over all the experiments for each environment. The shaded area represents 1 standard error.</p></caption><graphic /></fig><table-wrap id="tab_0"><label>Table 1:</label><caption><title>Table 1:</title><p>MULTIPOLAR vs. Baselines. Bootstrap mean and 95% confidence bounds of average episodic rewards over various training samples across six environments.</p></caption><table><tbody><tr><td /></tr></tbody></table></table-wrap><table-wrap id="tab_1"><label>Table 2:</label><caption><title>Table 2:</title><p>Results for MULTIPOLAR and its degraded versions in Hopper.</p></caption><table><tbody><tr><td /></tr></tbody></table></table-wrap><table-wrap id="tab_2"><label>Table 3:</label><caption><title>Table 3:</title><p>Results for MULTIPOLAR with different source policy sampling schemes in Hopper.</p></caption><table><tbody><tr><td /></tr></tbody></table></table-wrap><table-wrap id="tab_3"><label>Table 4:</label><caption><title>Table 4:</title><p>Results for MULTIPOLAR with different number of source policies in Hopper.</p></caption><table><tbody><tr><td /></tr></tbody></table></table-wrap><fig id="fig_3"><object-id>fig_3</object-id><label>Figure 4:</label><caption><title>Figure 4:</title><p>Histogram of source pol- icy performances in Hopper.</p></caption><graphic /></fig></sec></body><back><ref-list id="ref-list-1"><ref id="b0"><element-citation publication-type="journal"><article-title>An Automated Measure of MDP Similarity for Transfer in Rein- forcement Learning</article-title><source>Workshops at the AAAI Conference on Artificial Intelligence</source><year>2014</year><person-group person-group-type="author"><name><surname>References Haitham Bou Ammar</surname><given-names>Eric</given-names></name><name><surname>Eaton</surname><given-names>Matthew</given-names></name><name><surname>Taylor</surname><given-names>Constantin</given-names></name><name><surname>Mocanu</surname><given-names>Kurt</given-names></name><name><surname>Driessens</surname><given-names>Gerhard</given-names></name><name><surname>Weiss</surname><given-names>Karl</given-names></name><name><surname>Tuyls</surname><given-names /></name></person-group></element-citation></ref><ref id="b1"><element-citation publication-type="journal"><article-title>The Option-Critic Architecture</article-title><source>AAAI Confer- ence on Artificial Intelligence</source><year>2017</year><person-group person-group-type="author"><name><surname>Bacon</surname><given-names>Pierre-Luc</given-names></name><name><surname>Harb</surname><given-names>Jean</given-names></name><name><surname>Precup</surname><given-names>Doina</given-names></name></person-group></element-citation></ref><ref id="b2"><element-citation publication-type="journal"><article-title>Recent Advances in Hierarchical Reinforcement Learning</article-title><source>Discrete Event Dynamic Systems</source><year>2003</year><volume>13</volume><issue>1-2</issue><fpage>41</fpage><lpage>77</lpage><person-group person-group-type="author"><name><surname>Andrew</surname><given-names>G</given-names></name><name><surname>Barto</surname><given-names>Sridhar</given-names></name><name><surname>Mahadevan</surname><given-names /></name></person-group></element-citation></ref><ref id="b3"><element-citation publication-type="journal"><article-title>Hardware Conditioned Policies for Multi- Robot Transfer Learning</article-title><source>Advances in Neural Information Processing Systems</source><year>2018</year><fpage>9355</fpage><lpage>9366</lpage><person-group person-group-type="author"><name><surname>Chen</surname><given-names>Tao</given-names></name><name><surname>Murali</surname><given-names>Adithyavairavan</given-names></name><name><surname>Gupta</surname><given-names>Abhinav</given-names></name></person-group></element-citation></ref><ref id="b4"><element-citation publication-type="journal"><article-title>Learning to Adapt in Dynamic, Real-World Environments through Meta- Reinforcement Learning</article-title><source>International Conference on Learning Representations</source><year>2019</year><person-group person-group-type="author"><name><surname>Clavera</surname><given-names>Ignasi</given-names></name><name><surname>Nagabandi</surname><given-names>Anusha</given-names></name><name><surname>Liu</surname><given-names>Simin</given-names></name><name><surname>Fearing</surname><given-names>Ronald S</given-names></name><name><surname>Abbeel</surname><given-names>Pieter</given-names></name><name><surname>Levine</surname><given-names>Sergey</given-names></name><name><surname>Finn</surname><given-names>Chelsea</given-names></name></person-group></element-citation></ref><ref id="b5"><element-citation publication-type="journal"><article-title>Learning Mod- ular Neural Network Policies for Multi-Task and Multi-Robot Transfer</article-title><source>IEEE International Conference on Robotics and Automation</source><year>2017</year><fpage>2169</fpage><lpage>2176</lpage><person-group person-group-type="author"><name><surname>Devin</surname><given-names>Coline</given-names></name><name><surname>Gupta</surname><given-names>Abhishek</given-names></name><name><surname>Darrell</surname><given-names>Trevor</given-names></name><name><surname>Abbeel</surname><given-names>Pieter</given-names></name><name><surname>Levine</surname><given-names>Sergey</given-names></name></person-group></element-citation></ref><ref id="b6"><element-citation publication-type="journal"><source>An Introduction to the Bootstrap</source><year>1993</year><person-group person-group-type="author"><name><surname>Efron</surname><given-names>Bradley</given-names></name><name><surname>Tibshirani</surname><given-names>Robert</given-names></name></person-group></element-citation></ref><ref id="b7"><element-citation publication-type="journal"><article-title>Probabilistic Policy Reuse in a Reinforcement Learning Agent</article-title><source>International Joint Conference on Autonomous Agents and Multiagent Systems</source><year>2006</year><fpage>720</fpage><lpage>727</lpage><person-group person-group-type="author"><name><surname>Fern&#225;ndez</surname><given-names>Fernando</given-names></name><name><surname>Veloso</surname><given-names>Manuela</given-names></name></person-group></element-citation></ref><ref id="b8"><element-citation publication-type="journal"><article-title>An Introduction to Deep Reinforcement Learning</article-title><source>Foundations and Trends in Machine Learning</source><year>2018</year><volume>11</volume><issue>3-4</issue><fpage>219</fpage><lpage>354</lpage><person-group person-group-type="author"><name><surname>Fran&#231;ois-Lavet</surname><given-names>Vincent</given-names></name><name><surname>Henderson</surname><given-names>Peter</given-names></name><name><surname>Islam</surname><given-names>Riashat</given-names></name><name><surname>Bellemare</surname><given-names>Marc G</given-names></name><name><surname>Pineau</surname><given-names>Joelle</given-names></name></person-group></element-citation></ref><ref id="b9"><element-citation publication-type="journal"><article-title>Meta learning shared hierarchies</article-title><source>International Conference on Learning Representations</source><year>2018</year><person-group person-group-type="author"><name><surname>Frans</surname><given-names>Kevin</given-names></name><name><surname>Ho</surname><given-names>Jonathan</given-names></name><name><surname>Chen</surname><given-names>Xi</given-names></name><name><surname>Abbeel</surname><given-names>Pieter</given-names></name><name><surname>Schulman</surname><given-names>John</given-names></name></person-group></element-citation></ref><ref id="b10"><element-citation publication-type="journal"><article-title>Soft Actor-Critic: Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor</article-title><source>International Con- ference on Machine Learning</source><year>2018</year><fpage>1856</fpage><lpage>1865</lpage><person-group person-group-type="author"><name><surname>Haarnoja</surname><given-names>Tuomas</given-names></name><name><surname>Zhou</surname><given-names>Aurick</given-names></name><name><surname>Abbeel</surname><given-names>Pieter</given-names></name><name><surname>Levine</surname><given-names>Sergey</given-names></name></person-group></element-citation></ref><ref id="b11"><element-citation publication-type="journal"><article-title>Deep Reinforcement Learning That Matters</article-title><source>AAAI Conference on Artificial Intelligence</source><year>2018</year><fpage>3207</fpage><lpage>3214</lpage><person-group person-group-type="author"><name><surname>Henderson</surname><given-names>Peter</given-names></name><name><surname>Islam</surname><given-names>Riashat</given-names></name><name><surname>Bachman</surname><given-names>Philip</given-names></name><name><surname>Pineau</surname><given-names>Joelle</given-names></name><name><surname>Precup</surname><given-names>Doina</given-names></name><name><surname>Meger</surname><given-names>David</given-names></name></person-group></element-citation></ref><ref id="b12"><element-citation publication-type="journal"><article-title>Stable Baselines</article-title><year>2018</year><person-group person-group-type="author"><name><surname>Hill</surname><given-names>Ashley</given-names></name><name><surname>Raffin</surname><given-names>Antonin</given-names></name><name><surname>Ernestus</surname><given-names>Maximilian</given-names></name><name><surname>Gleave</surname><given-names>Adam</given-names></name><name><surname>Traore</surname><given-names>Rene</given-names></name><name><surname>Dhariwal</surname><given-names>Prafulla</given-names></name><name><surname>Hesse</surname><given-names>Christopher</given-names></name><name><surname>Klimov</surname><given-names>Oleg</given-names></name><name><surname>Nichol</surname><given-names>Alex</given-names></name><name><surname>Plappert</surname><given-names>Matthias</given-names></name><name><surname>Radford</surname><given-names>Alec</given-names></name><name><surname>Schul- Man</surname><given-names>John</given-names></name><name><surname>Sidor</surname><given-names>Szymon</given-names></name><name><surname>Wu</surname><given-names>Yuhuai</given-names></name></person-group></element-citation></ref><ref id="b13"><element-citation publication-type="journal"><article-title>Residual Reinforcement Learning for Robot Control</article-title><source>In International Conference on Robotics and Automation</source><year>2019</year><fpage>6023</fpage><lpage>6029</lpage><person-group person-group-type="author"><name><surname>Johannink</surname><given-names>Tobias</given-names></name><name><surname>Bahl</surname><given-names>Shikhar</given-names></name><name><surname>Nair</surname><given-names>Ashvin</given-names></name><name><surname>Luo</surname><given-names>Jianlan</given-names></name><name><surname>Kumar</surname><given-names>Avinash</given-names></name><name><surname>Loskyll</surname><given-names>Matthias</given-names></name><name><surname>Ojea</surname><given-names>Juan Aparicio</given-names></name><name><surname>Solowjow</surname><given-names>Eugen</given-names></name><name><surname>Levine</surname><given-names>Sergey</given-names></name></person-group></element-citation></ref><ref id="b14"><element-citation publication-type="journal"><article-title>Hierarchical Deep Reinforcement Learning: Integrating Temporal Abstraction and Intrinsic Motivation</article-title><source>Advances in Neural Information Processing Systems</source><year>2016</year><fpage>3675</fpage><lpage>3683</lpage><person-group person-group-type="author"><name><surname>Tejas</surname><given-names>D</given-names></name><name><surname>Kulkarni</surname><given-names>Karthik</given-names></name><name><surname>Narasimhan</surname><given-names>Ardavan</given-names></name><name><surname>Saeedi</surname><given-names>Josh</given-names></name><name><surname>Tenenbaum</surname><given-names /></name></person-group></element-citation></ref><ref id="b15"><element-citation publication-type="journal"><article-title>Transfer of Samples in Batch Rein- forcement Learning</article-title><source>International Conference on Machine Learning</source><year>2008</year><fpage>544</fpage><lpage>551</lpage><person-group person-group-type="author"><name><surname>Lazaric</surname><given-names>Alessandro</given-names></name><name><surname>Restelli</surname><given-names>Marcello</given-names></name><name><surname>Bonarini</surname><given-names>Andrea</given-names></name></person-group></element-citation></ref><ref id="b16"><element-citation publication-type="journal"><article-title>An Optimal Online Method of Selecting Source Policies for Rein- forcement Learning</article-title><source>Under review as a conference paper at ICLR 2020</source><year>2018</year><person-group person-group-type="author"><name><surname>Li</surname><given-names>Siyuan</given-names></name><name><surname>Zhang</surname><given-names>Chongjie</given-names></name></person-group></element-citation></ref><ref id="b17"><element-citation publication-type="journal"><article-title>Context-Aware Policy Reuse</article-title><source>In International Conference on Autonomous Agents and MultiAgent Systems</source><year>2019</year><fpage>989</fpage><lpage>997</lpage><person-group person-group-type="author"><name><surname>Li</surname><given-names>Siyuan</given-names></name><name><surname>Gu</surname><given-names>Fangda</given-names></name><name><surname>Zhu</surname><given-names>Guangxiang</given-names></name><name><surname>Zhang</surname><given-names>Chongjie</given-names></name></person-group></element-citation></ref><ref id="b18"><element-citation publication-type="journal"><article-title>Learn- ing Robust Options</article-title><source>AAAI Conference on Artificial Intelligence</source><year>2018</year><person-group person-group-type="author"><name><surname>Daniel</surname><given-names>J</given-names></name><name><surname>Mankowitz</surname><given-names>Timothy A</given-names></name><name><surname>Mann</surname><given-names>Pierre-Luc</given-names></name><name><surname>Bacon</surname><given-names>Doina</given-names></name><name><surname>Precup</surname><given-names>Shie</given-names></name><name><surname>Mannor</surname><given-names /></name></person-group></element-citation></ref><ref id="b19"><element-citation publication-type="journal"><article-title>Hierarchical Reinforcement Learning via Advantage-Weighted Information Maximization</article-title><source>International Conference on Learning Rep- resentations</source><year>2019</year><person-group person-group-type="author"><name><surname>Osa</surname><given-names>Takayuki</given-names></name><name><surname>Tangkaratt</surname><given-names>Voot</given-names></name><name><surname>Sugiyama</surname><given-names>Masashi</given-names></name></person-group></element-citation></ref><ref id="b20"><element-citation publication-type="journal"><article-title>Residual Reactive Navigation: Combining Classical and Learned Navigation Strategies For Deployment in Unknown Environ- ments</article-title><year>1909</year><person-group person-group-type="author"><name><surname>Rana</surname><given-names>Krishan</given-names></name><name><surname>Talbot</surname><given-names>Ben</given-names></name><name><surname>Milford</surname><given-names>Michael</given-names></name><name><surname>Sunderhauf</surname><given-names>Niko</given-names></name></person-group></element-citation></ref><ref id="b21"><element-citation publication-type="journal"><article-title>Bayesian Policy Reuse</article-title><source>Ma- chine Learning</source><year>2016</year><volume>104</volume><issue>1</issue><fpage>99</fpage><lpage>127</lpage><person-group person-group-type="author"><name><surname>Rosman</surname><given-names>Benjamin</given-names></name><name><surname>Hawasly</surname><given-names>Majd</given-names></name><name><surname>Ramamoorthy</surname><given-names>Subramanian</given-names></name></person-group></element-citation></ref><ref id="b22"><element-citation publication-type="journal"><article-title>Meta Reinforcement Learning with Latent Variable Gaussian Processes</article-title><source>Conference on Uncertainty in Artificial Intelligence</source><year>2018</year><person-group person-group-type="author"><name><surname>Saemundsson</surname><given-names>Steind&#243;r</given-names></name><name><surname>Hofmann</surname><given-names>Katja</given-names></name><name><surname>Peter Deisenroth</surname><given-names>Marc</given-names></name></person-group></element-citation></ref><ref id="b23"><element-citation publication-type="journal"><article-title>Deep reinforcement learning for industrial insertion tasks with visual inputs and natural reward signals</article-title><year>2019</year><person-group person-group-type="author"><name><surname>Schoettler</surname><given-names>Gerrit</given-names></name><name><surname>Nair</surname><given-names>Ashvin</given-names></name><name><surname>Luo</surname><given-names>Jianlan</given-names></name><name><surname>Bahl</surname><given-names>Shikhar</given-names></name><name><surname>Ojea</surname><given-names>Juan Aparicio</given-names></name><name><surname>Solowjow</surname><given-names>Eugen</given-names></name><name><surname>Levine</surname><given-names>Sergey</given-names></name></person-group></element-citation></ref><ref id="b24"><element-citation publication-type="journal"><article-title>Proximal Policy Optimization Algorithms</article-title><source>arXiv preprint arXiv:1707.06347</source><year>2017</year><person-group person-group-type="author"><name><surname>Schulman</surname><given-names>John</given-names></name><name><surname>Wolski</surname><given-names>Filip</given-names></name><name><surname>Dhariwal</surname><given-names>Prafulla</given-names></name><name><surname>Radford</surname><given-names>Alec</given-names></name><name><surname>Klimov</surname><given-names>Oleg</given-names></name></person-group></element-citation></ref><ref id="b25"><element-citation publication-type="journal"><article-title>Residual Policy Learning</article-title><source>arXiv preprint arXiv:1812.06298</source><year>2018</year><person-group person-group-type="author"><name><surname>Silver</surname><given-names>Tom</given-names></name><name><surname>Allen</surname><given-names>Kelsey</given-names></name><name><surname>Tenenbaum</surname><given-names>Josh</given-names></name><name><surname>Kaelbling</surname><given-names>Leslie</given-names></name></person-group></element-citation></ref><ref id="b26"><element-citation publication-type="journal"><article-title>Measuring the Distance Between Finite Markov Decision Processes</article-title><source>International Conference on Autonomous Agents and Multiagent Systems</source><year>2016</year><fpage>468</fpage><lpage>476</lpage><person-group person-group-type="author"><name><surname>Song</surname><given-names>Jinhua</given-names></name><name><surname>Gao</surname><given-names>Yang</given-names></name><name><surname>Wang</surname><given-names>Hao</given-names></name></person-group></element-citation></ref><ref id="b27"><element-citation publication-type="journal"><source>Introduction to Reinforcement Learning</source><year>1998</year><person-group person-group-type="author"><name><surname>Sutton</surname><given-names>Richard S</given-names></name><name><surname>Andrew</surname><given-names>G</given-names></name><name><surname>Barto</surname><given-names /></name></person-group></element-citation></ref><ref id="b28"><element-citation publication-type="journal"><article-title>Between MDPs and Semi-MDPs: A Frame- work for Temporal Abstraction in Reinforcement Learning</article-title><source>Artificial intelligence</source><year>1999</year><volume>112</volume><issue>1-2</issue><fpage>181</fpage><lpage>211</lpage><person-group person-group-type="author"><name><surname>Richard S Sutton</surname><given-names>Doina</given-names></name><name><surname>Precup</surname><given-names>Satinder</given-names></name><name><surname>Singh</surname><given-names /></name></person-group></element-citation></ref><ref id="b29"><element-citation publication-type="journal"><article-title>Sim-to-real: Learning agile locomotion for quadruped robots</article-title><source>Robotics: Science and Systems</source><year>2018</year><person-group person-group-type="author"><name><surname>Tan</surname><given-names>Jie</given-names></name><name><surname>Zhang</surname><given-names>Tingnan</given-names></name><name><surname>Coumans</surname><given-names>Erwin</given-names></name><name><surname>Iscen</surname><given-names>Atil</given-names></name><name><surname>Bai</surname><given-names>Yunfei</given-names></name><name><surname>Hafner</surname><given-names>Danijar</given-names></name><name><surname>Bohez</surname><given-names>Steven</given-names></name><name><surname>Vanhoucke</surname><given-names>Vincent</given-names></name></person-group></element-citation></ref><ref id="b30"><element-citation publication-type="journal"><article-title>GNU Parallel 2018</article-title><source>Ole Tange</source><year>2018</year><person-group person-group-type="author"><name><surname>Tange</surname><given-names>Ole</given-names></name></person-group></element-citation></ref><ref id="b31"><element-citation publication-type="journal"><article-title>Transfer Learning for Reinforcement Learning Domains: A Survey</article-title><source>Journal of Machine Learning Research</source><year>2009</year><volume>10</volume><fpage>1633</fpage><lpage>1685</lpage><person-group person-group-type="author"><name><surname>Taylor</surname><given-names>Matthew E</given-names></name><name><surname>Stone</surname><given-names>Peter</given-names></name></person-group></element-citation></ref><ref id="b32"><element-citation publication-type="journal"><article-title>Importance Weighted Trans- fer of Samples in Reinforcement Learning</article-title><source>International Conference on Machine Learning</source><year>2018</year><fpage>4936</fpage><lpage>4945</lpage><person-group person-group-type="author"><name><surname>Tirinzoni</surname><given-names>Andrea</given-names></name><name><surname>Sessa</surname><given-names>Andrea</given-names></name><name><surname>Pirotta</surname><given-names>Matteo</given-names></name></person-group></element-citation></ref><ref id="b33"><element-citation publication-type="journal"><source>Meta-Learning: A Survey. arXiv preprint arXiv:1810.03548</source><year>2018</year><person-group person-group-type="author"><name><surname>Vanschoren</surname><given-names>Joaquin</given-names></name></person-group></element-citation></ref><ref id="b34"><element-citation publication-type="journal"><article-title>Measuring Structural Similarities in Finite MDPs</article-title><source>In International Joint Conference on Artificial Intelligence</source><year>2019</year><fpage>3684</fpage><lpage>3690</lpage><person-group person-group-type="author"><name><surname>Wang</surname><given-names>Hao</given-names></name><name><surname>Dong</surname><given-names>Shaokang</given-names></name><name><surname>Shao</surname><given-names>Ling</given-names></name></person-group></element-citation></ref><ref id="b35"><element-citation publication-type="journal"><article-title>Policy Transfer with Strategy Optimization</article-title><source>Interna- tional Conference on Learning Representations</source><year>2019</year><person-group person-group-type="author"><name><surname>Yu</surname><given-names>Wenhao</given-names></name><name><surname>Liu</surname><given-names>C Karen</given-names></name><name><surname>Turk</surname><given-names>Greg</given-names></name></person-group></element-citation></ref></ref-list></back></article>