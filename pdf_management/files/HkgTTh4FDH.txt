Title:
```
Published as a conference paper at ICLR 2020 IMPLICIT BIAS OF GRADIENT DESCENT BASED AD- VERSARIAL TRAINING ON SEPARABLE DATA
```
Abstract:
```
Adversarial training is a principled approach for training robust neural networks. Despite of tremendous successes in practice, its theoretical properties still remain largely unexplored. In this paper, we provide new theoretical insights of gradi- ent descent based adversarial training by studying its computational properties, specifically on its implicit bias. We take the binary classification task on linearly separable data as an illustrative example, where the loss asymptotically attains its infimum as the parameter diverges to infinity along certain directions. Specifically, we show that for any fixed iteration T , when the adversarial perturbation during training has proper bounded 2 -norm, the classifier learned by gradient descent based adversarial training converges in direction to the maximum 2 -norm margin classifier at the rate of O(1/ √ T ), significantly faster than the rate O (1/ log T ) of training with clean data. In addition, when the adversarial perturbation during training has bounded q -norm with q ≥ 1, the resulting classifier converges in direction to a maximum mixed-norm margin classifier, which has a natural inter- pretation of robustness, as being the maximum 2 -norm margin classifier under worst-case q -norm perturbation to the data. Our findings provide theoretical back- ups for adversarial training that it indeed promotes robustness against adversarial perturbation.
```

Figures/Tables Captions:
```
Figure 1: GDAT of Linear Classifiers.
Figure 2: GDAT of Neural Network on MNIST Dataset.
```

Main Content:
```

Section Title: INTRODUCTION
  INTRODUCTION Deep neural networks have achieved remarkable success on various tasks, including visual and speech recognitions, with intriguing generalization abilities to unseen data ( Krizhevsky et al., 2012 ;  Hinton et al., 2012 ). One salient feature of deep models is its overparameterization, with the number of parameters several orders of magnitude larger than the training sample size. As a consequence of such overparameterization, it is likely that the empirical loss function, in addition to being non-convex, can have substantial amount of global minimizers ( Choromanska et al., 2015 ), while only a small subset of global minimizers have the desired generalization properties ( Brutzkus et al., 2018 ). Contrary to the worst-case reasoning above, researchers have observed that simple first-order algo- rithm such as Stochastic Gradient Descent (SGD) 1 , performs surprisingly well in practice, even without any explicit regularization terms in the objective function ( Zhang et al., 2017 ). Inspired by classical computational learning theories, one plausible explanation of such a remarkable phe- nomenon is that the training algorithm enjoys some implicit bias. That is, the training algorithm tends to converge to certain kinds of solutions ( Neyshabur et al., 2015b ;c), and SGD converges to low-capacity solutions with the desired generalization property ( Brutzkus et al., 2018 ). Recently, some exciting works have related the implicit bias to specific first-order algorithms ( Wilson et al., Published as a conference paper at ICLR 2020 2017 ), stopping time ( Hoffer et al., 2017 ), and optimization geometry ( Gunasekar et al., 2018a ;  Keskar et al., 2017 ). Some practical suggestions based on these findings have also been proposed to further improve the generalization ability of deep networks ( Neyshabur et al., 2015a ). Despite the aforementioned phenomenal success achieved by deep neural networks, it is observed that adversarially constructed small perturbation to the input can potentially fool the network into making wrong predictions with high confidence ( Szegedy et al., 2014 ;  Goodfellow et al., 2015 ). This issue raises serious concerns about using neural network for some security-sensitive tasks ( Papernot et al., 2017 ). Researchers have devised various mechanisms to generate and defend against adversarial perturbations ( Goodfellow et al., 2015 ;  Moosavi-Dezfooli et al., 2016 ;  Carlini and Wagner, 2017 ;  Athalye et al., 2018 ;  Xie et al., 2018 ;  Papernot et al., 2016 ). However, most of the defense mechanisms are heuristic or ad-hoc, which lack principled theoretical justification ( Carlini and Wagner, 2016 ;  He et al., 2017 ). Inspired by literatures in robust optimization ( Wald, 1939 ;  Ben-Tal et al., 2009 ),  Feige et al. (2015) ;  Madry et al. (2018)  formalize the notion of achieving adversarial robustness (i.e., having small adversarial risk) as solving the following minimax optimization problem min θ∈R d L E adv (θ) = min θ∈R d E (x,y)∼D max δ∈∆ (θ, x + δ, y) , (1) where ∆ is the set that each sample could be contaminated by arbitrary perturbation chosen within this set. As a common practice, adversarial training refers to the finite-sample empirical version of (1) without access to the underlying distribution D that A commonly adopted approach to solving (2) is the the Gradient Descent based Adversarial Training (GDAT) method. At each iteration, GDAT first solves the inner maximization problem (approxi- mately) for adversarial perturbations, and then uses the gradient of the loss function evaluated at the perturbed samples to perform a gradient descent step on the parameter θ. A natural question is then how adversarial training helps the trained model in achieving adversarial robustness. Some recent theoretical results partially answer this question, such as deriving adversarial risk bound ( Athalye et al., 2018 ), relating it to the distributionally robust optimization ( Sinha et al., 2018 ), and characterizing trade-offs between robustness and accuracy via regularization ( Zhang et al., 2019 ). Yet, all existing results neglect the algorithmic effect during the training process in promoting adversarial robustness. Inspired by the significant role of algorithmic bias in the generalization of neural networks, it is natural to ask Does gradient descent based adversarial training enjoy any implicit bias property? If so, does the implicit bias provide insights on how adversarial training promotes robustness? Motivated by these questions, in this paper, we study the algorithmic effect of adversarial training by investigating the implicit bias of GDAT. Due to current technical limits in directly analyzing deep neural networks, we analyze a simpler model, with the key characteristics that the model overfits the training data while being able to generalize well. Specifically, we take the binary classification with linearly separable data as an example. This helps us focus on the effect of implicit bias without dealing with complicated structures of neural networks.

Section Title: Main Contributions
  Main Contributions We summarize our main theoretical findings below. • Our first part of result shows an interesting interplay between adversarial perturbation and implicit bias of the gradient descent (GD). By exploiting this interplay, we show a property of adversarial training that is not known in the literature before: adversarial training accelerates convergence. Specifically, when the perturbation is bounded by 2 -norm, i.e., ∆ = {δ ∈ R d : ||δ|| 2 ≤ c}, with proper choice of c, the gradient descent based adversarial training is directionally convergent that lim t→∞ θ t ||θ t ||2 = u 2 , where u 2 is the maximum 2 -norm margin hyperplane (i.e., standard SVM) of the training data. In addition, when the perturbation level c is set according to T appropriately, the rate of convergence is O(1/ √ T ) 2 , which is exponentially faster than the rate O (1/ log T ) when we use standard clean training, i.e., training with clean data using gradient descent. Based on this, we establish that the convergence of training loss on clean data using GDAT is almost exponentially faster than standard clean training using GD.

Section Title: Published as a conference paper at ICLR 2020
  Published as a conference paper at ICLR 2020 • Our second part of result shows that adversarial training adapts the implicit bias of gradient descent for different adversarial perturbation geometry. Specifically, when the perturbation is bounded by q -norm for q ≥ 1, i.e., ∆ = {δ ∈ R d : ||δ|| q ≤ c}, with proper choice of c, the gradient descent based adversarial training is directionally convergent that lim t→∞ θ t ||θ t ||2 = u 2,q , where u 2,q is the maximum mixed-norm margin hyperplane of the training data. We further reveal natural interpretation of robustness that we obtain the maximum 2 -norm margin classifier under worst-case q -norm perturbation.

Section Title: Notations
  Notations For two vectors x, y ∈ R d , x, y = d j=1 x j y j denotes their Euclidean inner product. For a vector θ ∈ R d , ||θ|| p defined by ||θ|| p p = d j=1 |θ j | p denotes its p-norm for p ∈ [1, ∞), and ||θ|| ∞ = max j∈[d] |θ j |, where [d] = {1, . . . , d}. For any general norm || · ||, we denote its dual norm by ||x|| * = max ||y||≤1 x, y . The sign function is sign(v) = 1 (v≥0) − 1 (v<0) . For a linear subspace L ∈ R d , we denote its orthogonal subspace by L ⊥ .

Section Title: BACKGROUND
  BACKGROUND We consider a binary classification problem using a dataset S = {(x i , y i )} n i=1 ⊂ R d × {−1, +1}. We aim to learn a linear decision boundary f (x) = θ, x and its associated classifier y(x) = sign (f (x)), by solving the empirical risk minimization problem: In what follows, we suppress the explicit presentation of S when the context is clear, and we focus on the exponential loss (r) = exp(−r). We point out that our analysis can be further extended to other smooth loss functions with tight exponential tail such as logistic loss. We assume the dataset S is linearly separable, i.e., there exists u such that min i∈[n] y i x i u > 0. Under this assumption, one notable feature of problem (3) is that there is no finite minimizer, and L(θ) → 0 only if ||θ|| 2 → ∞ along certain directions. In fact, there is a polyhedral cone C, such that for any u ∈ C, we have lim a→∞ L(au) = 0. Several recent results have studied the implicit bias of gradient descent algorithm on separable dataset.  Soudry et al. (2018)  study the implicit bias of the gradient descent algorithm (GD) on (3), and show that lim t→∞ ||θ t || 2 = ∞, while θ t converges in direction to the maximum 2 -norm margin classifier (i.e., the standard SVM).  Ji and Telgarsky (2018)  further study the convergence of risk and parameter without separability condition. ( Ji and Telgarsky, 2019 ) and ( Gunasekar et al., 2018b ) study the implicit bias for training deep linear network and linear convolutional networks, respectively.  Gunasekar et al. (2018a)  also analyze the implicit bias of steepest descent in general norm || · ||, and show that θ t converges in direction to the maximum || · || * -norm margin hyperplane. Throughout this paper, we assume the perturbation set is an q -norm ball with radius c, i.e., ∆ = {δ ∈ R d : ||δ|| q c}. Under the general framework of adversarial training in (2), we aim to minimize the empirical adversarial risk Note that, given any θ, the inner maximization problem in (4) admits a closed form solution. Then the gradient descent based adversarial training (GDAT) algorithm runs iteratively that at the t-th iteration, we first solve the inner maximization problem by deriving the worst adversarial perturbation of each sample. It is not difficult to see that for each sample, the worst perturbation is δ t i = cy i δ * t , where δ * t = argmin δ:||δ||q≤1 δ, θ t . Then, letting each sample's perturbed counterpart be ( x t i , y i ) = (x i + δ t i , y i ), we take gradient of the loss function evaluated at the perturbed samples and perform a gradient descent step, i.e., θ t+1 = θ t − η t ∇ θ L (θ t ; {( x t i , y i )} n i=1 )), where η t > 0 is some prespecified stepsize. We present the outline of GDAT in Algorithm 1.

Section Title: THEORETICAL RESULTS
  THEORETICAL RESULTS In this section, we show that the GDAT algorithm possesses implicit bias, which depends on the perturbation set during training. We provide explicit characterization of the implicit bias, and further conclude that such implicit bias indeed promotes robustness against adversarial perturbation. Since y i ∈ {−1, +1}, when H θ correctly classifies all samples, γ q (θ) measures the minimal q distance between the samples in S and H θ . Given that γ q (θ) is scale- invariant with respect to θ, without loss of generality, we restrict ||θ|| p = 1. We also identify the hyperplane H θ by its normal vector θ. We denote SV(S) as the support vectors of S, i.e., SV(S) = argmin (x,y)∈S u q , yx . By the separability assumption, u q is an optimal hyperplane that correctly classifies all samples with the maximal margin γ q > 0. Next, by the notion of margin defined above, we characterize the landscape of empirical adversarial risk in (4) based on the perturbation level c. Proposition 3.1. Let p, q > 0 satisfy 1/p + 1/q = 1. Given a nonnegative scalar c, where 0 ≤ c < γ q = max ||θ||p≤1 min i∈[n] y i x i θ, problem (4) has infimum 0 but does not admit a finite minimizer. When c > γ q , problem (4) has a unique finite minimizer θ(c), and is equivalent to the standard clean training with explicit p -norm regularization. That is, there exists λ(c) > 0 such that It is not difficult to see that for c < γ q , any perturbed dataset S = {( x i , y i )} n i=1 , with ||x i − x i || q c for all i, is still linearly separable, which directly follows from the definition of γ q above. On the other hand, when c > γ q , by the definition of γ q , there exists some perturbed dataset S = {( x i , y i )} n i=1 , with ||x i − x i || q c for all i, such that S is no longer linearly separable.

Section Title: ADVERSARIAL PERTURBATION WITH BOUNDED 2 -NORM
  ADVERSARIAL PERTURBATION WITH BOUNDED 2 -NORM In this subsection, we analyze both the empirical adversarial risk convergence and the parameter convergence of the case when the perturbation set ∆ in (4) is an 2 -norm ball with radius c.

Section Title: Adversarial Risk Convergence
  Adversarial Risk Convergence We first analyze the convergence of empirical adversarial risk (4) using GDAT. One substantial roadblock of minimizing (4) is its non-smoothness, in the sense that L adv (θ) is not differentiable at the origin, and its Hessian ∇ 2 L adv (θ) explodes around the origin. To address the challenge, our key observation is that, by the next lemma, at each iteration, there exists an acute angle between the update on θ t and the maximum 2 -norm margin hyperplane u 2 . This gives a lower bound on ||θ t || 2 . Lemma 3.1. Take ∆ = {δ ∈ R d : ||δ|| 2 ≤ c} in problem (4). Given c < γ 2 , we have that We highlight that despite its simple proof, Lemma 3.1 and its generalization to q -perturbation is a crucial step for analyzing both adversarial risk and implicit bias. In addition, our techniques here can also be adapted to simplify the proof of Lemma 10 in ( Gunasekar et al., 2018a ), which, in comparison, is more technically involved. Since we initialize GDAT (Alg. 1) using θ 0 = 0, any perturbation inside ∆ will have no effect on the adversarial loss. Hence we take clean samples as adversarial examples at the first iteration of Published as a conference paper at ICLR 2020 GDAT. From Lemma 3.1, we have the following simple corollary showing that our whole solution path {θ t } T t=1 is bounded away from the origin. Corollary 3.1. Let θ 0 = 0 in Algorithm 1 with q = 2, we have: ||θ t || 2 ≥ η 0 γ 2 for all t ≥ 1. By Corollary 3.1, we bypass the non-differentiability issue at the origin and also control the Hessian ∇ 2 L adv (θ) throughout the entire training process. Similar to ( Ji and Telgarsky, 2018 ), in the next theorem, we show that the loss L adv (θ), although not uniformly smooth, is locally L adv (θ)-smooth. Consequently, by the smoothness based analysis of the gradient descent algorithm, we establish the convergence of the empirical adversarial risk. In comparison with the standard clean training using GD ( Ji and Telgarsky, 2018 ), this theorem states that we pay an extra (γ 2 − c) −2 factor in the risk convergence of adversarial training. However, this direct comparison is too pessimistic since we compare the adversarial risk with the standard risk (corresponding to ∆ = {0}). Interestingly, as seen later in Corollary 3.2, we prove that the convergence of standard risk in GDAT is significantly faster than its counterpart in the standard clean training using GD.

Section Title: Parameter Convergence
  Parameter Convergence We then show that if we set the perturbation level c < γ 2 in the GDAT algorithm, GDAT with 2 -norm perturbation possesses the same implicit bias as the standard clean training using GD, i.e., we have lim t→∞ θ t ||θ t ||2 = u 2 . Intuitively, GDAT with 2 -norm perturbation searches for a decision hyperplane that is robust to 2 -norm perturbation. Since the learned decision hyperplane in the standard clean using GD converges to u 2 , which is already the most robust decision hyperplane against 2 -norm perturbation to the data, GDAT retains the implicit bias of standard clean training using GD. Surprisingly, even though both GDAT in the adversarial training and GD in the standard clean training converge in directions to u 2 , their rates of directional convergence are significantly different as shown later. Specifically, letting the perturbation level c depend on the total number of iterations T in the GDAT algorithm, the directional error after T iterations in GDAT algorithm can be significantly smaller than the error of GD in the standard clean training. We first show that the projection of θ t onto the orthogonal subspace of span(u 2 ) is bounded. Lemma 3.2. Define α(S) = min ||ξ||2=1,ξ∈span(u2) ⊥ max (x,y)∈SV(S) ξ, yx , where we assume SV(S) spans R d . Let θ ⊥ be the projection of vector θ onto span(u 2 ) ⊥ . Then there exists a constant K that only depends on α(S) and log n, such that ||θ t ⊥ || 2 ≤ K for any t ≥ 0 in the GDAT algorithm. Note that the same α(S) is defined in ( Ji and Telgarsky, 2019 ) and proved to be positive with probability 1 if the data is sampled from absolutely continuous distribution. We then show in the next lemma that ||θ t || 2 goes to infinity, where we provide a refined analysis to establish the acceleration of the directional convergence in comparison with the standard clean training. Lemma 3.3. Under the same conditions in Theorem 3.1, and let α = α(S) defined in Lemma 3.2. Then for all t ≥ 0, we have Lemma 3.3 provides the key insight to establish the acceleration of directional convergence. Specifi- cally, it allows us to set c depending on the total number of iterations T , so that ||θ T || 2 is sublinear in T , in comparison with being logarithmic in T in standard clean training as in  Ji and Telgarsky (2018) . We are now ready to present the main theorem for parameter convergence. Theorem 3.2 (Speed-up of Parameter Convergence). Under same conditions in Theorem 3.1, and let α = α(S) and K be defined in Lemma 3.2. In GDAT with 2 -norm perturbation, let c and total Published as a conference paper at ICLR 2020 number of iterations T satisfy γ 2 − c = n 1+1/α log T ηT 1/2 , and define θ T = θ T ||θ T ||2 . We have One might argue that the polynomial dependence on sample size n in (7) is too pessimistic, making the GDAT unfavorable in comparison with the standard clean training. We show that this is not an issue by a direct comparision of iteration complexity to achieve ||θ T − u 2 || 2 ≤ for a given precision > 0. Specifically, given > 0, to achieve ||θ T − u 2 || 2 ≤ , GDAT needs O n (1+1/α) −2 number of iterations. In comparison, the standard clean training by GD needs O n exp −1 number of iterations ( Ji and Telgarsky, 2018 ), which has exponential dependence on precision . Finally, by Theorem 3.1 and Lemma 3.3, we show that the empirical clean risk after T iterations of GDAT is almost exponentially smaller than its counterpart in the standard clean training. Corollary 3.2 (Speed-up of Clean Risk Convergence). Under the same conditions in Theorem 3.2, we have L(θ T ) = O exp −µ √ T /log T , where µ is a constant dependent on η, α, n. Note that the empirical clean risk decreases at the rate of O exp(− √ T ) up to a logarithmic factor in the exponent. In comparison, using standard clean training with GD, we only have L(θ T ) = O (1/T ) ( Soudry et al., 2018 ).

Section Title: ADVERSARIAL PERTURBATION WITH BOUNDED q -NORM
  ADVERSARIAL PERTURBATION WITH BOUNDED q -NORM In this subsection, we generalize our results to the case where the perturbation set is some bounded q -norm ball. To facilitate our discussion, we first define a robust version of SVM. Definition 3.2. For a given separable dataset S with q -norm margin γ q and c < γ q , letting 1/p + 1/q = 1, the robust SVM against q -norm perturbation parameterized by c is Remark 3.1 (Maximum Mixed-norm Margin). Note that problem (8) is equivalent to solving for a maximum mixed-norm margin hyperplane. Specifically, by the KKT condition of (8), there exists η(c) > 0, such that (8) is equivalent to the following problem: Now define || · || = || · || 2 + η(c)|| · || p , it is clear that || · || defines a norm which is a mixture of 2 and p norm. Let || · || * be its dual norm. Then we have that the solution to (9) is the maximum || · || * -norm margin hyperplane. Note that the constraint in (8) is equivalent to min ||δi||q≤c y i (x i + δ i ) θ ≥ 1, ∀i = 1, . . . , n. By a simple scaling argument, in the following lemma, we see the robust nature of (8). Lemma 3.4. Under the same notations in Definition 3.2, problem (8) is equivalent to: We denote the (unique) solution to problem (10) as u 2,q (c). In what follows, we surpress explicit presentation of c when the context is clear. The equivalent formulation (10) provides a clear interpretation on the robustness of (10). In particular, the robust SVM against q -norm perturbation parameterized by c is in fact the SVM problem on the the dataset S(c, q), which is generated from S by placing a q -norm ball with radius c around each samples, i.e., S(c, q) = {(x, y) : ∃i ∈ [n], s.t., ||x − x i || p c, y = y i }. In other words, u 2,q is the maximum 2 -norm margin classifier under worst case q -norm perturbation bounded by c. In the remaining part of this section, we first analyze the convergence of the empirical adversarial risk, and then establish the implicit bias of GDAT with q perturbation for q ∈ [1, ∞]. Our analysis for q ∈ {1, ∞} is based on approximation argument. For ease of presentation, we only discuss when q ∈ (1, ∞) in the main text, and defer the discussion for q ∈ {1, ∞} in Appendix D.

Section Title: Adversarial Risk Convergence
  Adversarial Risk Convergence Our analysis is similar to the analysis for GDAT with 2 perturba- tion, where we use similar techniques to address issues such as non-differentiability at the origin and Hessian explosion of L adv (θ) around the origin. Theorem 3.3. Suppose ||x i || 2 ≤ 1 for i = 1, . . . , n, and let 1 p + 1 q = 1. In the GDAT with q -norm perturbation, setting c < γ q and letting We point out here that (6) is a special case of (11). In particular, by the definition of γ 2,q (c), we have that γ 2,2 (c) = γ 2 − c, which recovers bound (6) from (11).

Section Title: Parameter Convergence
  Parameter Convergence We show that if we set c < γ q in the GDAT algorithm with stepsizes specified in Theorem 3.3, with q perturbation, the algorithm still possesses implicit bias property, i.e., θ t still has directional convergence, and the limiting direction depends on the perturbation set ∆. Theorem 3.4 (Implicit Bias of GDAT with q -norm Perturbation). Under the same conditions in Theorem 3.3, define θ t = θ t ||θ t ||2 , then we have: Combining Theorem 3.4 and Lemma 3.4, we conclude that GDAT with q -norm perturbation indeed promotes robustness against q perturbation. Using GDAT with q -norm perturbation will result in a classifier which is the maximum 2 -norm margin classifier under worst case q -norm perturbations to the samples bounded by c. The learned classifier will have q -norm margin at least c. As we increase perturbation level c to γ q , the learned classifier will converge to maximum q -norm margin classifier.

Section Title: NUMERICAL EXPERIMENT
  NUMERICAL EXPERIMENT In this section, we first conduct numerical experiments on linear classifiers to backup our theoretical findings. We further empirically extend our method to neural networks, where our numerical results demonstrate that our theoretical results can be potentially generalized.

Section Title: Linear Classifiers
  Linear Classifiers We investigate the empirical performance of the GDAT algorithm on linear classi- fiers, with training set S = {((−0.5, 1), +1) , ((−0.5, −1), −1) , ((−0.75, −1), −1) , ((2, 1), +1)}. It is straightforward to verify that the maximum 2 -norm margin classifier is u 2 = (0, 1). Considering 2 -norm perturbations, we first run standard clean training with GD, and GDAT with 2 -norm perturbation (c = 0.95γ 2 ), for 2.5 × 10 4 number of iterations. In both GD and GDAT we take constant stepsizes, with η = 1 and η = 0.1, respectively. By Figure 1(a), we see that the convergence rate of adversarial loss using GDAT is similar to the convergence rate of clean loss using GD. However, when we directly compare the clean losses of GDAT and GD, GDAT clearly demonstrates an exponential speed-up in comparison with GD, which is consistent with Corollary 3.2. Additionally, as pointed out by Theorem 3.2, GDAT also enjoys significant speed-up in terms of the directional convergence of θ t to u 2 . We also compare the norm growth ||θ t || 2 , and observe that the norm generated by GDAT grows much faster than the norm generated by GD, which is also in alignment with our discussions in Section 3.1. We further run GDAT with ∞ -norm perturbation (c = 0.5). By Lemma 3.4, we have that u 2,∞ = (0, 1). Note that the Hausdorff distance between q -norm ball and ∞ -norm ball distance goes to zero as q goes to infinity. Thus, we have that (10) for q = 1000 is a close approximation of (10) for q = ∞. We run two versions of GDAT, where one uses q -norm perturbation with q = 1000, and the other uses ∞ -norm perturbation. We run both algorithms with stepsize η = 0.1 for 5.0 × 10 5 number of iterations, and we present the results in Figure 1(b). We find that the two training methods behave similarly. In addition, the empirical directional convergence rates of θ t just differ slightly.

Section Title: Neural Networks
  Neural Networks case on adversarial training of more complicated neural networks. We conduct experiments on neural network with one hidden layer. We take the two classes from MNIST dataset with label "2" and "9" to form our training set S. We also vary the width of the hidden layer in {64×64, 128×128, 256×256}. One major difference from the case of linear classifiers is that we cannot solve the inner maximization problem of (2) exactly as it does not admits a closed-form solution. Instead, we solve the inner problem approximately using projected gradient descent with 20 iterations and stepsize 0.01. We test two versions of GDAT, where one adopts 2 -norm perturbations (c = 2.8), and the other uses ∞ - norm perturbations (c = 0.1). For standard clean training and the outer minimization problem in (2), we use the stochastic gradient descent algorithm with batch size 128 and constant stepsize 10 −5 . We compare the loss and classification accuracy, which are evaluated using the clean training samples, of standard clean training and GDAT. By  Figure 2 , we see that GDAT indeed accelerates the convergence of both loss and classification accuracy on clean training samples. The performance gap is most obvious when the width of the hidden layer is small, and reduces gradually as we increase the width of the hidden layer. We argue that such reduction comes from the fact that as network width increases, the margin on the samples outputted by the hidden layer also increases. As suggested by Theorem 3.2, in this case, a larger perturbation level c should be used. We conduct additional experiments with various perturbation level in Appendix E to empirically verify our argument.

Section Title: DISCUSSIONS
  DISCUSSIONS We investigate the implicit bias of GDAT for linear classifier. There are several plausible natural extensions. For example, we can represent a linear classifier using a deep linear network, which is significantly overparameterized. Some recent results characterize the implicit bias of gradient descent for training deep linear networks ( Ji and Telgarsky, 2019 ) and linear convolutional networks ( Gunasekar et al., 2018b ). Motivated by these results, investigating the implicit bias of GDAT in training deep linear networks worths future investigations. Meanwhile, investigating implicit bias in deep nonlinear networks is a more important and chal- lenging direction: (1) For linear classifiers, adding adversarial perturbations during training can be understood as a form of regularization, which explains the faster convergence in training. Although observed empirically, the potential acceleration of adversarial training is not yet understood in the current literature, to the best of our knowledge. (2) The notion of margin for neural networks still lacks proper definition, which we need to define to facilitate investigations on the effect of adversarial training in promoting robustness. (3) Ultrawide nonlinear networks have been shown to evolve similarly to linear networks using gradient descent ( Ghorbani et al., 2019 ;  Lee et al., 2019 ). We shall further investigate if our results on linear classifiers can be extended to wide nonlinear networks.

Section Title: ACKNOWLEDGEMENTS
  ACKNOWLEDGEMENTS

Section Title: Annex Figures
  Annex Figures   fig_2     O hides logarithmic factor.  

```
