<article article-type="research-article"><front><article-meta><title-group><article-title>Under review as a conference paper at ICLR 2020 UNSUPERVISED LEARNING OF AUTOMOTIVE 3D CRASH SIMULATIONS USING LSTMS</article-title></title-group><abstract><p>Long short-term memory (LSTM) networks allow to exhibit temporal dynamic behavior with feedback connections and seem a natural choice for learning se- quences of 3D meshes. We introduce an approach for dynamic mesh representa- tions as used for numerical simulations of car crashes. To bypass the complication of using 3D meshes, we transform the surface mesh sequences into spectral de- scriptors that efficiently encode the shape. A two branch LSTM based network architecture is chosen to learn the representations and dynamics of the crash dur- ing the simulation. The architecture is based on unsupervised video prediction by an LSTM without any convolutional layer. It uses an encoder LSTM to map an input sequence into a fixed length vector representation. On this representation one decoder LSTM performs the reconstruction of the input sequence, while the other decoder LSTM predicts the future behavior by receiving initial steps of the sequence as seed. The spatio-temporal error behavior of the model is analysed to study how well the model can extrapolate the learned spectral descriptors into the future, that is, how well it has learned to represent the underlying dynamical structural mechanics. Considering that only a few training examples are available, which is the typical case for numerical simulations, the network performs very well.</p></abstract></article-meta></front><body><sec><title>INTRODUCTION</title><p>Data driven virtual product design is nowadays an essential tool in the automotive industry saving time and resources during the development process. For a new car model, numerical crash simu- lations are performed where design parameters are changed to study their effects on physical and functional properties of the car such as firewall intrusion, weight, or cost (<xref ref-type="bibr" rid="b8">Fang et al., 2017</xref>). Since one simulation run takes a couple of hours on a compute cluster, running a large number of sim- ulation is not feasible. Therefore, a system that is able to use a limited dataset and predict new simulations would make the development process faster and more efficient.</p><p>The rise of deep neural networks (DNNs) in recent years encourages further research and industrial usages. Besides manifold research for autonomous driving, it is natural for the automotive industry to seek and evaluate the possible applications of DNNs also in the product design stages. As an example, we investigate car crash tests, in which for example the plate thickness of certain parts strongly influences the bending behavior of structural beams and as a result also the intrusion of the firewall into the passenger compartment. Here, numerical crash simulations for different variations of such thicknesses are used as a dataset for learning. The aim is to design a system based on a DNN architecture that learns the crash behavior and would be able to imitate the crash dynamics.</p><p>Car crash simulations are based on a mathematical model of the plastic deformations and other physical and mechanical effects. They are defined on a computing mesh of currently up to three million points and up to a hundred time steps are stored. Each data instance is a simulation run-of pre-selected parts and/or time steps-that is very high dimensional. Working with this data directly exasperates any machine learning (ML) method, but a transformation of this data presented in <xref ref-type="bibr" rid="b1">Iza- Teran &amp; Garcke (2019)</xref> allows to obtain a new representation that uses only a small number of coefficients to represent the high resolution numerical solutions. The transformed representation is employed here to compress the mesh geometries to feature sets suitable for neural networks, while avoiding to directly handle geometries in the machine learning method. This way, a network Under review as a conference paper at ICLR 2020 designed for video prediction and embedding based on a long short-term memory (LSTM) based architecture (<xref ref-type="bibr" rid="b24">Srivastava et al., 2015</xref>) can be adapted for mesh data. Since LSTM is a recurrent neural network that allows to exhibit temporal dynamic behavior with feedback connections, it is a natural choice for learning the 3D sequences. The aim is that the network learns the observed crash behavior including translation, rotation, or deformation of the parts in the model.</p><p>Since the contribution of this paper is using DNNs for analyzing car crash data, the related works are categorized into a group of publications in which DNNs are extended for 3D graphics and one that concerns the use of ML techniques for analyzing car crash simulations. For the latter, one typically uses different embedding techniques to obtain a low dimensional representation for the intrinsic underlying data space and to cluster simulations with similar characteristics together (<xref ref-type="bibr" rid="b1">Bohn et al., 2013</xref>; <xref ref-type="bibr" rid="b7">Diez, 2018</xref>; <xref ref-type="bibr" rid="b1">Garcke &amp; Iza-Teran, 2015</xref>; <xref ref-type="bibr" rid="b1">Iza-Teran &amp; Garcke, 2019</xref>; <xref ref-type="bibr" rid="b15">Le Guennec et al., 2018</xref>).</p><p>The majority of publications about 3D DNN tried to extend CNN for 3D space and focus on de- scription learning and shape correspondence, also known as geometric deep learning, (<xref ref-type="bibr" rid="b3">Bronstein et al., 2017</xref>; <xref ref-type="bibr" rid="b3">Masci et al., 2015</xref>; <xref ref-type="bibr" rid="b19">Monti et al., 2017</xref>; <xref ref-type="bibr" rid="b3">Boscaini et al., 2015</xref>; 2016; <xref ref-type="bibr" rid="b11">Litany et al., 2017</xref>; <xref ref-type="bibr" rid="b11">Halimi et al., 2018</xref>; <xref ref-type="bibr" rid="b18">Maturana &amp; Scherer, 2015</xref>; Su et al., 2015; <xref ref-type="bibr" rid="b29">Wang et al., 2017</xref>) and some devel- oped CNN filters for unorganized point clouds (<xref ref-type="bibr" rid="b20">Qi et al., 2017a</xref>;b). The very active research is so far very compute resource consuming and there is no extension of ConvLSTM for 3D space to our knowledge, but for prediction one would need an LSTM (or GAN) approach.</p><p>However, a couple of very recent works introduce new feature sets and architectures for mesh em- bedding using autoencoders and LSTM (<xref ref-type="bibr" rid="b26">Tan et al., 2018b</xref>; <xref ref-type="bibr" rid="b22">Qiao et al., 2018</xref>; <xref ref-type="bibr" rid="b26">Tan et al., 2018a</xref>). The feature representation is using local shape deformations obtained by solving an optimization problem at each node and a global optimization for compensating for rotations. They have shown that after training the network, a sequences of 3D shapes as an animation can be generated by do- ing operations in the latent space. The bidirectional LSTM architecture is shown to outperform autoeconders (<xref ref-type="bibr" rid="b26">Tan et al., 2018a</xref>). An LSTM based learning network has also been proposed in <xref ref-type="bibr" rid="b22">Qiao et al. (2018)</xref>, where the obtained feature representation is then taken as the temporal data to be feed into a CNN that takes the features and represents them in a lower dimensional latent space. This information is subsequently feed into the LSTM module.</p></sec><sec><title>MESH DATA AND ITS REPRESENTATION</title><p>Data collection is a bottleneck for deep learning applications. If the training data is not diverse enough, the network would neither be able to properly learn the intrinsic data space nor be able to return reasonable output for before unseen data. We focus on surface mesh data from numerical sim- ulations of car crashes, where in the industrial setting the car model is divided into several physcical car parts.</p><p>As a simple car model we use a Chevrolet C2500 pick-up truck, a model with around 60,000 nodes from the National Crash Analysis Center 1 . The data stems from numerical crash simulation 2 of a frontal crash for random variations of the plate thickness of nine structural components, a setup similar to <xref ref-type="bibr" rid="b1">Bohn et al. (2013)</xref>. The thickness variations result in different deformation behavior. <xref ref-type="fig" rid="fig_0">Figure 1</xref> shows a snapshot of the crash simulation for the truck model.</p><p>The geometries of the car model and parts are available in a regular mesh format and the correspon- dence of vertices between different simulations and over the time of the simulation are known by their node id. Therefore, instead of working with the meshes one can simply work with vertices and treat them like organized point clouds, while being able to recover the mesh and connectivity at any time. Now, the features for training the network are obtained from these point clouds and the network outputs a feature vector that is later post-processed to a point cloud or mesh.</p><p>Instead of working directly with 3D surface meshes, a set of features is extracted for training the network. Such a feature set should be able to represent the dynamics of the crash efficiently. In <xref ref-type="bibr" rid="b1">Iza- Teran &amp; Garcke (2019)</xref> a compact representation for deforming shapes has been presented for the car crash case. The approach is based on the property that the Laplace Beltrami Operator (LBO) on a surface is invariant to isometric deformations. That is, the LBO is the same if a surface mesh is de- Under review as a conference paper at ICLR 2020 formed in such a way that it is neither stretched nor teared apart, which is the case for the considered simulations, which additionally all start from the same geometry. Consequently the eigenvectors, which form an orthogonal basis, do not change under an isometric transformation and can be used to represent mesh functions such as the deformations (as three functions, one each for x, y, z). The representation is obtained by projecting mesh functions onto the common orthogonal basis to com- pute so-called spectral coefficients. It turns out that most of the variations of the deformed shapes are concentrated in a small number of coefficients. Therefore an efficient representation is obtained using few spectral coefficients. As shown by <xref ref-type="bibr" rid="b5">Brezis &amp; G&#243;mez-Castro (2017)</xref> using suitable assump- tions, for the L 2 -approximation of functions controlled in the H 1 -Sobolevnorm the orthonormal basis stemming from the Laplace operator provides an optimal approximation in a certain sense; this result can be extended to the LBO and functions in the Sobolev space H 2,2 . Furthermore, the spectral representation can be understood as a mesh surface analogy to the Fourier decomposition of signals. This representation is introduced here for training LSTMs and allows to bypass the complexity of dealing with large meshes directly.</p><p>To formalise, in B n&#215;m we collect the first m unit eigenvectors of the LBO, where n is the number of points in the considered 3D shape. The spectral coefficients C 3&#215;m at one time step are obtained by C 3&#215;m = R 3&#215;n B n&#215;m , (1) where R 3&#215;n contains the x, y, z coordinates of the n points from the considered 3D shape. Four B matrices are required for four distinct parts in the dataset, which are four distinct geometries. Recovering the 3D shapes from their spectral coefficients is possible by R 3&#215;n = C 3&#215;m B T n&#215;m , (2) since B n&#215;m is orthonormal. R is an approximation of R and by choosing larger m, the approxima- tion error gets smaller. In other words, the more eigenvectors are used in B n&#215;m , the more details are saved in the spectral representation. <xref ref-type="fig" rid="fig_1">Figure 2</xref> visualizes the localization and histogram of average error for reconstructing four selected parts from their spectral coefficients. The averaging is done over 205 &#215; 10 samples for each part independently. Note that the bounding box of the entire data is [2750, 4500] &#215; [&#8722;600, 600] &#215; [250, 650], determined over simulation time. Comparing the his- togram with the tight bounding box's dimensions shows that the reconstruction error is not very high for m = 40, while the part with blue color coding seems to have more overall error. Note that the error is localized mostly toward the front of the parts, this area goes through very large deformations during the crash. Note that with m = 100 the observed maximal observed error would go down from 40 to 20.</p></sec><sec><title>MODEL DESCRIPTION</title><p>For frame prediction in video and to obtain latent representations a number of quite interesting applications in robotics and computer vision have been developed. Different DNN architectures have been investigated very extensively. <xref ref-type="bibr" rid="b24">Srivastava et al. (2015)</xref> was one of the first proposals of an Under review as a conference paper at ICLR 2020 unsupervised LSTM based representation learning method for video. The logic behind the choice is that the same operation must be applied at each step to propagate dynamics to the next step. This implies that the underlying dynamics of the dataset remains the same, i.e. the same dynamics acting on any state, at anytime, should produce the next state (<xref ref-type="bibr" rid="b24">Srivastava et al., 2015</xref>). Others extended this work by adding convolutional operations in LSTM, introducing new recurrent networks, or using model based architectures (<xref ref-type="bibr" rid="b30">Xingjian et al., 2015</xref>; <xref ref-type="bibr" rid="b9">Finn et al., 2016</xref>; <xref ref-type="bibr" rid="b14">Kalchbrenner et al., 2017</xref>; <xref ref-type="bibr" rid="b17">Lotter et al., 2016</xref>; <xref ref-type="bibr" rid="b23">Sharma et al., 2015</xref>).</p><p>Car crash simulation data can basically be considered a sequence of 3D geometries. In analogy to the video processing case, one can see each geometry in the sequence as a video frame. Inspired from the work on video prediction, we use a similar architecture for the case of car crash representation learning. We choose a two branch LSTM architecture with reconstruction and prediction decoders from <xref ref-type="bibr" rid="b24">Srivastava et al. (2015)</xref>. The encoder LSTM maps an input sequence of k time steps into a fixed length vector representation. This representation is decoded using a decoder LSTM to perform the reconstruction of the input sequence of size k. Receiving a few steps l, l &lt; k, from the beginning of the sequence, the other decoder LSTM predicts future sequences of length k &#8722; l, see <xref ref-type="fig" rid="fig_2">Figure 3</xref>, where we here use l = k/2 for simplicity. The architecture is in <xref ref-type="bibr" rid="b24">Srivastava et al. (2015)</xref> shown to have a better performance for predicting future frames compared to other approaches.</p><p>The design of the prediction encoder-decoder LSTM is same as that of the autoencoder LSTM, except that it is like a supervised method in which the decoder LSTM predicts future behavior that comes after the input sequence while the encoders hidden state will capture information about the representation of the input sequence. In comparing to an autoencoder LSTM that receives the entire k time steps of the simulations and reconstruct them again, the prediction encoder-decoder LSTM receives just the first l steps and predicts the remaining time steps. Therefore, the input sequence acts as seed points for generating the rest of the sequence.</p><p>This compositional architecture is supposed to address the shortcomings that one has to confront by the use of an autoencoder or encoder-predictor alone. Namely, on the one hand an autoencoder LSTM suffers from a bias by memorizing the inputs, which is not sufficient for predicting future frames. On the other hand, an encoder-decoder LSTM suffers from the bias to save information mostly about the last few frames since these carry more information for predicting the future frames, but then the representation at the end of the encoder will ignore large parts of the input. In the compositional architecture, the model is trained to also predict all of the input sequence, therefore it cannot just store information about the last few frames (<xref ref-type="bibr" rid="b24">Srivastava et al., 2015</xref>).</p><p>It is worth to mention that the dataset contains the translation, rotation, and deformation of car parts during the crash. Therefore the network would learn the entire degrees of freedom of the crash dynamics and would be able to imitate the crash by the prediction decoder after receiving the initial time steps as seeds.</p></sec><sec><title>EXPERIMENTS AND EVALUATION</title><p>Although the 3D truck model consists of several parts, only the left and right structural beam, which are made of two parts each, are considered for evaluating our approach. These beams are structurally very important parts of the car and typically investigated by engineers, therefore it is justified to concentrate on these parts.</p><p>Overall 205 different simulations were performed. For each simulation, ten snapshots equally dis- tributed in time are selected and the deformation of the two beams are extracted from it. Therefore a data sample consists of the four selected parts during ten time steps. Together, the dataset after the extraction from the crash simulations contains 205 samples and each full sample is a long con- catenated vector of 4 &#215; 3 &#215; 40 &#215; 10 elements. The entire dataset is divided into training and test set of 105 and 100 samples, respectively. Each data point, i.e. each time step of a simulation, is normalized by the l 2 norm of the features at t = 0 beforehand.</p><p>Note that one could consider a different scenario in which each part, out of the four, over all ten time steps is a data sample, that is treat the for beams with individual machine learning models. Since the dynamics of the two beams are coupled during the crash, considering them together can reduce the error for learning the crash dynamics by the network and we thereforce consider this setup.</p><p>We employ the two branch LSTM from <xref ref-type="fig" rid="fig_2">Figure 3</xref>, where the implementation is done with Keras. In our experiments, the encoder component of the network has 1000, the decoder has 1500, and the prediction part has 2000 LSTM units without any convolutional layer, as defining a convolutional layer for 3D graphics is not trivial and existing approaches are resource intensive. Further, we use ReLU as the activation function. Using ADAM with default parameters the entire network is trained together, which includes the encoder, reconstruction, and prediction parts over 100 iterations, until the mean squared error (over all parts) reaches the order of 10 &#8722;7 and becomes stable. The training phase takes about 30 minutes on an Intel i7-7700 CPU@3.60GHz &#215; 8. The achieved minimum during training is a trade-off between reconstruction and prediction loss functions. The prediction part of the network receives the first five time steps as inputs and generates the next five steps of the crash (which are both a vector of 4 &#215; 3 &#215; 40 &#215; 5 elements).</p><p>For each grid point j we compute in the following the least squares error at time t and averaged over the simulations, as well as accumulated over time: E j (t) = 1 s s i=1 S i j (t) &#8722;&#348; i j (t) 2 , AE j = k i=1 E j (t) where s is the number of simulations and S i j (t),&#348; i j (t) are the three-dimensional (one for each direc- tion) original mesh function and its reconstruction (or prediction) for simulation i at time t.</p></sec><sec><title>NETWORK PERFORMANCE AND EXISTING BIFURCATION</title><p><xref ref-type="fig" rid="fig_3">Figure 4</xref> shows the histograms of the reconstruction and prediction error for the training and test datasets of all four parts, only here for the spectral coefficients. The reconstruction error is larger than the prediction error since the reconstruction part recovers the entire k = 10 time steps while the prediction part just generates the last l = 5 time steps. <xref ref-type="bibr" rid="b1">Bohn et al. (2013)</xref> and <xref ref-type="bibr" rid="b1">Iza-Teran &amp; Garcke (2019)</xref> have reported a bifurcation for this car crash dataset. That is, two different bending behavior for the beams arise in the simulation data, which is due to changes in the plate thicknesses. One can cluster the 205 simulations into two groups of 71 and 134 for the two bending behaviors. It gives an unique opportunity to investigate the performance of the proposed system by analyzing and visualizing the error for each branch of bifurcation indi- vidually. Moreover, it is possible to see if and how well the network can recognize this bifurcation and how the spectral coefficients can preserve this.</p><p>The 105 simulations used for training include samples from both branches of the bifurcation, as do the remaining 100 for testing. The distances between ground truth and the outputs of two decoders, after decompression and re-normalization from the spectral coefficient back to 3D shapes, are vi- sualized on a template to present the spatial localization of the average error of each bifurcation branch and its histogram is also shown for a better comparison (all for the testing dataset). <xref ref-type="fig" rid="fig_4">Figure 5</xref> gives the accumulated error over time. Overall, the reconstruction and prediction errors are small in relation to the bounding box of the data, the network based on the spectral coefficients is able to learn the complex structural mechanics.</p><p>It can be seen in first two histograms of <xref ref-type="fig" rid="fig_4">Figure 5</xref> that the errors accumulation for the reconstruction in the two branches is similar, which can also be seen in the first and second row in <xref ref-type="fig" rid="fig_4">Figure 5</xref>. The part color coded red shows somewhat higher error values, while the blue part has more mid-range errors. The third and fourth row in <xref ref-type="fig" rid="fig_4">Figure 5</xref> for the prediction have very different error localization. Considering the localization and histograms of the average error over time for the reconstruction branch of the network, one can observe that the error behavior stays roughly the same over time, i.e. the localization of error and histograms show that the error stay the same.</p><p>Regarding the localization and histogram of the average error over time for the prediction branch of the network, now the error increases in time and the behavior changes at the eighth time step. Here, one can observe that the localization of the error changes and the histograms show that the error increases more strongly for the first branch, see Figure 7 in the Appendix.</p></sec><sec><title>EMBEDDING</title><p>The encoder LSTM weights can be used for visualizing the intrinsic underlying space of the dataset. Therefore, in order to see how well the encoder learned the representation and dynamics of the car crash simulations, the weights are visualized using different markers for the two branches of bifurcation. Due to the encoder having 1000 layers, to which each sample is mapped, one needs to use some visualization techniques for embedding from higher dimensions to a lower dimensional space. We use t-SNE (<xref ref-type="bibr" rid="b14">Van Der Maaten &amp; Hinton, 2008</xref>) and show the result in <xref ref-type="fig" rid="fig_5">Figure 6</xref>. As can be seen, the bifurcation is shown as two well-separated clusters. There are a few points from each Under review as a conference paper at ICLR 2020 reconstruction branch 1 reconstruction branch 2 prediction branch 1 prediction branch 2 bifurcation branch that are misaligned with the rest of their branch members. This might be because of using the spectral coefficients as an approximation which leads to losing some details about the bifurcation, therefore increasing m might rectify the issue. Or a 2D embedding is not sufficient for properly visualization the encoder LSTM weights. Nevertheless, this 2D visualization proves that the network was able to learn the complex dynamics of the crash since the bifurcation is well represented by the encoder weights.</p></sec><sec><title>CONCLUSIONS AND FUTURE WORK</title><p>Video frames prediction has been in the center of attention of researchers for a while, but there has been only very few extensions of these works to the 3D case so far. The problem is addressed here by introducing spectral coefficients to encode functions on the geometry together with a two branch LSTM based architecture without any convolutional layer, which has already proven to be feasible for video embedding and future frames prediction. The employed LBO basis and the resulting spectral coefficients provide a trade-off between accuracy and required computational resources. We encode the 3D shapes by a set of features using the eigenvectors of the LBO.</p><p>For empirical evaluation, a dataset is employed from a set of numerical simulations of a car during crash under different design conditions, i.e. plate thickness variations. The appearance of a bifur- cation during the crash in the dataset, motivates an error analysis done for both groups to see how good the network performs in the presence of a bifurcation. In both branches, the network is able to perform very good predictions, while we observe different error localisations for reconstruction versus prediction. Moreover, the 2D visualization of the reconstruction branch shows the bifurca- tion as two clusters. In any case, from a relatively small number of data, the proposed network using spectral coefficients is able to learn complex dynamical structural mechanical behaviors.</p><p>Future work could go toward scaling the pipeline for learning the crash dynamics of the entire car and larger mesh sizes, which increases the needed computational effort. On the other hand, one might be able to use smaller number of eigenvectors by not simply selecting the first few ones, but those with a large variance in the spectral coefficients of the data set. Furthermore, in practical settings, re-meshing of the parts can take place, here using spectral coefficients can ease this step since one can encode shapes with different vertices number to fixed size feature vectors, as long as the geometry is (approximately) isometric. Still, there is the overall question, if and how a trained network can be evaluated for changed geometries (relevant question for any 3D DNN approach introduced so far) or different crash setups. Moreover, adding design parameters could also improve the accuracy but requires modifications of the networks architecture.</p><p>For practical applications, as each crash simulation requires hours of heavy computation running computational solvers on a large cluster, a system that is able to learn the representation of exper- iments with very few training data and generate the predicted simulation results for new design parameters would save much resources. Moreover, the ultimate goal of research along this direction would be a data driven system that receives very little information about the simulation (like design parameters) and output the crash sequences with minimum error.</p><p>Another application of the current system could be feasibility detectors while running the simulation on the compute cluster. Using the network, one could check if the simulation goes well or if for some reasons it should be terminated. From the current stage of the system, one would be able to generate the parts of the future simulation simply by extrapolating the learned spectral coefficients from a few initial time steps, which are already computed on the cluster, as inputs. If the distance between network predicts and simulation gets very large over the iterations, the simulation can be terminated since it failed the feasibility check.</p><p>Further, related works such as <xref ref-type="bibr" rid="b22">Qiao et al. (2018)</xref> introduce a specific feature set and LSTM autoen- coders, where also graph convolution operation is required. This approach could be applied for car crash data under the assumption that the local optimization can still be applied for large deforma- tions as the ones occurring in our applications. Further, the resulting features are long vectors, which results in 8 hours for learning on a CPU/GPU system for a data set similar in size to ours, where we need 30 minutes. Nevertheless, a comparison of these two approach will be worthwhile future work.</p></sec><sec id="figures"><title>Figures</title><fig id="fig_0"><object-id>fig_0</object-id><label>Figure 1:</label><caption><title>Figure 1:</title><p>A snapshot of the crash simulation of the 3D model used for data collection with a zoom unto the studied longitudinal beams, from Bohn et al. (2013) (left). The four selected parts (beams) after the crash for one selected simulation, later used for illustration (right). The colors of the parts are used to display error behavior per part.</p></caption><graphic /><graphic /><graphic /><graphic /><graphic /><graphic /><graphic /><graphic /><graphic /><graphic /></fig><fig id="fig_1"><object-id>fig_1</object-id><label>Figure 2:</label><caption><title>Figure 2:</title><p>Localization and histogram of errors between original parts and their reconstruction from their spectral representation with m = 40 spectral coefficients. Histogram color coding follows the color convention in Figure 1, right.</p></caption><graphic /><graphic /><graphic /><graphic /><graphic /><graphic /><graphic /><graphic /><graphic /><graphic /><graphic /><graphic /></fig><fig id="fig_2"><object-id>fig_2</object-id><label>Figure 3:</label><caption><title>Figure 3:</title><p>The two branches LSTM autoencoder network architecture. The data sequence of f s are feed into the network in two lengths and a joint encoder is learned. The top decoder is learning the reconstruction, while the bottom decoder is performing future prediction.</p></caption><graphic /></fig><fig id="fig_3"><object-id>fig_3</object-id><label>Figure 4:</label><caption><title>Figure 4:</title><p>Histogram of the reconstruction (blue) and prediction (red) errors for the spectral coeffi- cients by the network, left for the training data, on the right for the testing data.</p></caption><graphic /><graphic /><graphic /><graphic /></fig><fig id="fig_4"><object-id>fig_4</object-id><label>Figure 5:</label><caption><title>Figure 5:</title><p>Localization and histogram of the error accumulation AE. First and third rows show the reconstruction and prediction error, respectively, for the first bifurcation branch. Second and forth rows show the reconstruction and prediction error, respectively, for the second bifurcation branch. The error is shown on a representative final time step of each branch to illustrate the different deformation behaviors. Histogram color coding follows the color convention in Figure 1, right.</p></caption><graphic /><graphic /><graphic /><graphic /><graphic /><graphic /><graphic /><graphic /><graphic /><graphic /><graphic /><graphic /><graphic /><graphic /><graphic /><graphic /><graphic /><graphic /><graphic /><graphic /><graphic /><graphic /><graphic /><graphic /><graphic /><graphic /><graphic /><graphic /><graphic /><graphic /><graphic /><graphic /><graphic /><graphic /><graphic /><graphic /><graphic /><graphic /><graphic /><graphic /><graphic /><graphic /></fig><fig id="fig_5"><object-id>fig_5</object-id><label>Figure 6:</label><caption><title>Figure 6:</title><p>2D embedding of LSTM's reconstruction weights. Two branches of the bifurcation are well separated in the 2D visualization made by t-SNE. First bifurcation branch is marrked by red dots and the second one by blue triangles.</p></caption><graphic /><graphic /></fig></sec></body><back><ref-list id="ref-list-1"><ref id="b0"><element-citation publication-type="journal"><source>Large Scale Kernel Machines</source><year>2007</year><person-group person-group-type="author"><name><surname>Bengio</surname><given-names>Yoshua</given-names></name><name><surname>Lecun</surname><given-names>Yann</given-names></name></person-group></element-citation></ref><ref id="b1"><element-citation publication-type="journal"><article-title>Analysis of car crash simulation data with nonlinear machine learning methods</article-title><source>Procedia Computer Science</source><year>2013</year><volume>18</volume><fpage>621</fpage><lpage>630</lpage><person-group person-group-type="author"><name><surname>Bohn</surname><given-names>Bastian</given-names></name><name><surname>Garcke</surname><given-names>Jochen</given-names></name><name><surname>Iza-Teran</surname><given-names>Rodrigo</given-names></name><name><surname>Paprotny</surname><given-names>Alexander</given-names></name><name><surname>Peherstorfer</surname><given-names>Benjamin</given-names></name><name><surname>Schepsmeier</surname><given-names>Ulf</given-names></name><name><surname>Thole</surname><given-names>Clemens-August</given-names></name></person-group></element-citation></ref><ref id="b2"><element-citation publication-type="journal"><article-title>A sparse grid based method for generative dimensionality reduction of high-dimensional data</article-title><source>Journal of Computational Physics</source><year>2016</year><volume>309</volume><fpage>1</fpage><lpage>17</lpage><person-group person-group-type="author"><name><surname>Bohn</surname><given-names>Bastian</given-names></name><name><surname>Garcke</surname><given-names>Jochen</given-names></name><name><surname>Griebel</surname><given-names>Michael</given-names></name></person-group></element-citation></ref><ref id="b3"><element-citation publication-type="journal"><article-title>Learn- ing class-specific descriptors for deformable shapes using localized spectral convolutional net- works</article-title><source>Computer Graphics Forum</source><year>2015</year><volume>34</volume><issue>5</issue><fpage>13</fpage><lpage>23</lpage><person-group person-group-type="author"><name><surname>Boscaini</surname><given-names>D</given-names></name><name><surname>Masci</surname><given-names>J</given-names></name><name><surname>Melzi</surname><given-names>S</given-names></name><name><surname>Bronstein</surname><given-names>M M</given-names></name><name><surname>Castellani</surname><given-names>U</given-names></name><name><surname>Vandergheynst</surname><given-names>P</given-names></name></person-group></element-citation></ref><ref id="b4"><element-citation publication-type="journal"><article-title>Anisotropic diffusion de- scriptors</article-title><source>Computer Graphics Forum</source><year>2016</year><volume>35</volume><issue>2</issue><fpage>431</fpage><lpage>441</lpage><person-group person-group-type="author"><name><surname>Boscaini</surname><given-names>D</given-names></name><name><surname>Masci</surname><given-names>J</given-names></name><name><surname>Rodol&#224;</surname><given-names>E</given-names></name><name><surname>Bronstein</surname><given-names>M M</given-names></name><name><surname>Cremers</surname><given-names>D</given-names></name></person-group></element-citation></ref><ref id="b5"><element-citation publication-type="journal"><article-title>Rigidity of optimal bases for signal spaces</article-title><source>Comptes Rendus Mathematique</source><year>2017</year><volume>355</volume><issue>7</issue><fpage>780</fpage><lpage>785</lpage><person-group person-group-type="author"><name><surname>Brezis</surname><given-names>Ha&#239;m</given-names></name><name><surname>G&#243;mez-Castro</surname><given-names>David</given-names></name></person-group></element-citation></ref><ref id="b6"><element-citation publication-type="journal"><article-title>Geomet- ric deep learning: Going beyond euclidean data</article-title><source>IEEE Signal Processing Magazine</source><year>2017</year><volume>34</volume><issue>4</issue><fpage>18</fpage><lpage>42</lpage><person-group person-group-type="author"><name><surname>Michael M Bronstein</surname><given-names>Joan</given-names></name><name><surname>Bruna</surname><given-names>Yann</given-names></name><name><surname>Lecun</surname><given-names>Arthur</given-names></name><name><surname>Szlam</surname><given-names>Pierre</given-names></name><name><surname>Vandergheynst</surname><given-names /></name></person-group></element-citation></ref><ref id="b7"><element-citation publication-type="journal"><article-title>Using artificial intelligence to analyze crash simulations</article-title><source>NAFEMS Benchmark</source><year>2018</year><fpage>20</fpage><lpage>23</lpage><person-group person-group-type="author"><name><surname>Diez</surname><given-names>Constantin</given-names></name></person-group></element-citation></ref><ref id="b8"><element-citation publication-type="journal"><article-title>On design optimiza- tion for structural crashworthiness and its state of the art</article-title><source>Structural and Multidisciplinary Optimization</source><year>2017</year><volume>55</volume><issue>3</issue><fpage>1091</fpage><lpage>1119</lpage><person-group person-group-type="author"><name><surname>Fang</surname><given-names>Jianguang</given-names></name><name><surname>Sun</surname><given-names>Guangyong</given-names></name><name><surname>Qiu</surname><given-names>Na</given-names></name><name><surname>Kim</surname><given-names>Nam H</given-names></name><name><surname>Li</surname><given-names>Qing</given-names></name></person-group></element-citation></ref><ref id="b9"><element-citation publication-type="journal"><article-title>Unsupervised learning for physical interaction through video prediction</article-title><source>Advances in neural information processing systems</source><year>2016</year><fpage>64</fpage><lpage>72</lpage><person-group person-group-type="author"><name><surname>Finn</surname><given-names>Chelsea</given-names></name><name><surname>Goodfellow</surname><given-names>Ian</given-names></name><name><surname>Levine</surname><given-names>Sergey</given-names></name></person-group></element-citation></ref><ref id="b10"><element-citation publication-type="journal"><article-title>Machine learning approaches for repositories of numerical simulation results</article-title><source>In 10th European LS-DYNA Conference</source><year>2015</year><person-group person-group-type="author"><name><surname>Garcke</surname><given-names>Jochen</given-names></name><name><surname>Iza-Teran</surname><given-names>Rodrigo</given-names></name></person-group></element-citation></ref><ref id="b11"><element-citation publication-type="journal"><article-title>Self-supervised learning of dense shape correspondence</article-title><source>arXiv preprint arXiv:1812.02415</source><year>2016</year><person-group person-group-type="author"><name><surname>Goodfellow</surname><given-names>Ian</given-names></name><name><surname>Bengio</surname><given-names>Yoshua</given-names></name><name><surname>Courville</surname><given-names>Aaron</given-names></name><name><surname>Halimi</surname><given-names>Oshri</given-names></name><name><surname>Litany</surname><given-names>Or</given-names></name><name><surname>Rodol&#224;</surname><given-names>Emanuele</given-names></name><name><surname>Bronstein</surname><given-names>Alex</given-names></name><name><surname>Kimmel</surname><given-names>Ron</given-names></name></person-group></element-citation></ref><ref id="b12"><element-citation publication-type="journal"><article-title>A fast learning algorithm for deep belief nets</article-title><source>Neural Computation</source><year>2006</year><volume>18</volume><fpage>1527</fpage><lpage>1554</lpage><person-group person-group-type="author"><name><surname>Hinton</surname><given-names>Geoffrey E</given-names></name><name><surname>Osindero</surname><given-names>Simon</given-names></name><name><surname>Teh</surname><given-names>Yee Whye</given-names></name></person-group></element-citation></ref><ref id="b13"><element-citation publication-type="journal"><article-title>A geometrical method for low-dimensional representations of simulations</article-title><source>SIAM/ASA Journal on Uncertainty Quantification</source><year>2019</year><volume>7</volume><issue>2</issue><fpage>472</fpage><lpage>496</lpage><person-group person-group-type="author"><name><surname>Iza-Teran</surname><given-names>Rodrigo</given-names></name><name><surname>Garcke</surname><given-names>Jochen</given-names></name></person-group></element-citation></ref><ref id="b14"><element-citation publication-type="journal"><article-title>Video pixel networks</article-title><year>2017</year><volume>70</volume><fpage>1771</fpage><lpage>1779</lpage><person-group person-group-type="author"><name><surname>Kalchbrenner</surname><given-names>Nal</given-names></name><name><surname>Van Den Oord</surname><given-names>A&#228;ron</given-names></name><name><surname>Simonyan</surname><given-names>Karen</given-names></name><name><surname>Danihelka</surname><given-names>Ivo</given-names></name><name><surname>Vinyals</surname><given-names>Oriol</given-names></name><name><surname>Graves</surname><given-names>Alex</given-names></name><name><surname>Kavukcuoglu</surname><given-names>Koray</given-names></name></person-group></element-citation></ref><ref id="b15"><element-citation publication-type="journal"><article-title>A parametric and non- intrusive reduced order model of car crash simulation</article-title><source>Computer Methods in Applied Mechanics and Engineering</source><year>2018</year><volume>338</volume><fpage>186</fpage><lpage>207</lpage><person-group person-group-type="author"><name><surname>Guennec</surname><given-names>Yves Le</given-names></name><name><surname>Brunet</surname><given-names>J-P</given-names></name><name><surname>Daim</surname><given-names>F-Z</given-names></name><name><surname>Chau</surname><given-names>Ming</given-names></name><name><surname>Tourbier</surname><given-names>Yves</given-names></name></person-group></element-citation></ref><ref id="b16"><element-citation publication-type="journal"><article-title>Deep functional maps: Structured prediction for dense shape correspondence</article-title><source>Proceedings of the IEEE Inter- national Conference on Computer Vision</source><year>2017</year><fpage>5659</fpage><lpage>5667</lpage><person-group person-group-type="author"><name><surname>Litany</surname><given-names>Or</given-names></name><name><surname>Remez</surname><given-names>Tal</given-names></name><name><surname>Rodol&#224;</surname><given-names>Emanuele</given-names></name><name><surname>Bronstein</surname><given-names>Alex</given-names></name><name><surname>Bronstein</surname><given-names>Michael</given-names></name></person-group></element-citation></ref><ref id="b17"><element-citation publication-type="journal"><article-title>Deep predictive coding networks for video pre- diction and unsupervised learning</article-title><source>arXiv preprint arXiv:1605.08104</source><year>2016</year><person-group person-group-type="author"><name><surname>Lotter</surname><given-names>William</given-names></name><name><surname>Kreiman</surname><given-names>Gabriel</given-names></name><name><surname>Cox</surname><given-names>David</given-names></name></person-group></element-citation></ref><ref id="b18"><element-citation publication-type="journal"><article-title>Under review as a conference paper at ICLR 2020 Daniel Maturana and Sebastian Scherer. Voxnet: A 3d convolutional neural network for real-time object recognition</article-title><source>2015 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)</source><year>2015</year><fpage>37</fpage><lpage>45</lpage><person-group person-group-type="author"><name><surname>Masci</surname><given-names>Jonathan</given-names></name><name><surname>Boscaini</surname><given-names>Davide</given-names></name><name><surname>Bronstein</surname><given-names>Michael</given-names></name><name><surname>Vandergheynst</surname><given-names>Pierre</given-names></name></person-group></element-citation></ref><ref id="b19"><element-citation publication-type="journal"><article-title>Geometric deep learning on graphs and manifolds using mixture model CNNs</article-title><source>Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</source><year>2017</year><fpage>5115</fpage><lpage>5124</lpage><person-group person-group-type="author"><name><surname>Monti</surname><given-names>Federico</given-names></name><name><surname>Boscaini</surname><given-names>Davide</given-names></name><name><surname>Masci</surname><given-names>Jonathan</given-names></name><name><surname>Rodola</surname><given-names>Emanuele</given-names></name><name><surname>Svoboda</surname><given-names>Jan</given-names></name><name><surname>Bronstein</surname><given-names>Michael M</given-names></name></person-group></element-citation></ref><ref id="b20"><element-citation publication-type="journal"><article-title>Pointnet: Deep learning on point sets for 3d classification and segmentation</article-title><source>Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</source><year>2017</year><fpage>652</fpage><lpage>660</lpage><person-group person-group-type="author"><name><surname>Charles R Qi</surname><given-names>Hao</given-names></name><name><surname>Su</surname><given-names>Kaichun</given-names></name><name><surname>Mo</surname><given-names>Leonidas J</given-names></name><name><surname>Guibas</surname><given-names /></name></person-group></element-citation></ref><ref id="b21"><element-citation publication-type="journal"><article-title>Pointnet++: Deep hierarchical feature learning on point sets in a metric space</article-title><source>Advances in Neural Information Processing Systems</source><year>2017</year><fpage>5099</fpage><lpage>5108</lpage><person-group person-group-type="author"><name><surname>Charles Ruizhongtai Qi</surname><given-names>Li</given-names></name><name><surname>Yi</surname><given-names>Hao</given-names></name><name><surname>Su</surname><given-names>Leonidas J</given-names></name><name><surname>Guibas</surname><given-names /></name></person-group></element-citation></ref><ref id="b22"><element-citation publication-type="journal"><article-title>Learning bidirectional LSTM networks for synthesizing 3d mesh animation sequences</article-title><source>arXiv preprint arXiv:1810.02042</source><year>2018</year><person-group person-group-type="author"><name><surname>Qiao</surname><given-names>Yi-Ling</given-names></name><name><surname>Gao</surname><given-names>Lin</given-names></name><name><surname>Lai</surname><given-names>Yu-Kun</given-names></name><name><surname>Xia</surname><given-names>Shihong</given-names></name></person-group></element-citation></ref><ref id="b23"><element-citation publication-type="journal"><article-title>Action recognition using visual attention</article-title><source>arXiv preprint arXiv:1511.04119</source><year>2015</year><person-group person-group-type="author"><name><surname>Sharma</surname><given-names>Shikhar</given-names></name><name><surname>Kiros</surname><given-names>Ryan</given-names></name><name><surname>Salakhutdinov</surname><given-names>Ruslan</given-names></name></person-group></element-citation></ref><ref id="b24"><element-citation publication-type="journal"><article-title>Unsupervised learning of video representations using LSTMs</article-title><source>International Conference on Machine Learning</source><year>2015</year><fpage>843</fpage><lpage>852</lpage><person-group person-group-type="author"><name><surname>Srivastava</surname><given-names>Nitish</given-names></name><name><surname>Mansimov</surname><given-names>Elman</given-names></name><name><surname>Salakhudinov</surname><given-names>Ruslan</given-names></name></person-group></element-citation></ref><ref id="b25"><element-citation publication-type="journal"><article-title>Multi-view convo- lutional neural networks for 3d shape recognition</article-title><source>Proceedings of the IEEE international conference on computer vision</source><year>2015</year><fpage>945</fpage><lpage>953</lpage><person-group person-group-type="author"><name><surname>Su</surname><given-names>Hang</given-names></name><name><surname>Maji</surname><given-names>Subhransu</given-names></name><name><surname>Kalogerakis</surname><given-names>Evangelos</given-names></name><name><surname>Learned-Miller</surname><given-names>Erik</given-names></name></person-group></element-citation></ref><ref id="b26"><element-citation publication-type="journal"><article-title>Variational autoencoders for deforming 3d mesh models</article-title><source>Proceedings of the IEEE Conference on Computer Vision and Pattern Recogni- tion</source><year>2018</year><fpage>5841</fpage><lpage>5850</lpage><person-group person-group-type="author"><name><surname>Tan</surname><given-names>Qingyang</given-names></name><name><surname>Gao</surname><given-names>Lin</given-names></name><name><surname>Lai</surname><given-names>Yu-Kun</given-names></name><name><surname>Xia</surname><given-names>Shihong</given-names></name></person-group></element-citation></ref><ref id="b27"><element-citation publication-type="journal"><article-title>Mesh-based autoencoders for localized deformation component analysis</article-title><source>Thirty-Second AAAI Conference on Artificial Intel- ligence</source><year>2018</year><person-group person-group-type="author"><name><surname>Tan</surname><given-names>Qingyang</given-names></name><name><surname>Gao</surname><given-names>Lin</given-names></name><name><surname>Lai</surname><given-names>Yu-Kun</given-names></name><name><surname>Yang</surname><given-names>Jie</given-names></name><name><surname>Xia</surname><given-names>Shihong</given-names></name></person-group></element-citation></ref><ref id="b28"><element-citation publication-type="journal"><article-title>Visualizing high-dimensional data using t-sne</article-title><source>Journal of Machine Learning Research</source><year>2008</year><volume>9</volume><fpage>2579</fpage><lpage>2605</lpage><person-group person-group-type="author"><name><surname>Van Der Maaten</surname><given-names>L J P</given-names></name><name><surname>Hinton</surname><given-names>G</given-names></name></person-group></element-citation></ref><ref id="b29"><element-citation publication-type="journal"><article-title>O-cnn: Octree-based convolutional neural networks for 3d shape analysis</article-title><source>ACM Transactions on Graphics (TOG)</source><year>2017</year><volume>36</volume><issue>4</issue><fpage>72</fpage><lpage>72</lpage><person-group person-group-type="author"><name><surname>Wang</surname><given-names>Peng-Shuai</given-names></name><name><surname>Liu</surname><given-names>Yang</given-names></name><name><surname>Guo</surname><given-names>Yu-Xiao</given-names></name><name><surname>Sun</surname><given-names>Chun-Yu</given-names></name><name><surname>Tong</surname><given-names>Xin</given-names></name></person-group></element-citation></ref><ref id="b30"><element-citation publication-type="journal"><article-title>Convolutional LSTM network: A machine learning approach for precipitation nowcasting</article-title><source>Advances in neural information processing systems</source><year>2015</year><fpage>802</fpage><lpage>810</lpage><person-group person-group-type="author"><name><surname>Shi Xingjian</surname><given-names>Zhourong</given-names></name><name><surname>Chen</surname><given-names>Hao</given-names></name><name><surname>Wang</surname><given-names>Dit-Yan</given-names></name><name><surname>Yeung</surname><given-names>Wai-Kin</given-names></name><name><surname>Wong</surname><given-names>Wang-Chun</given-names></name><name><surname>Woo</surname><given-names /></name></person-group></element-citation></ref></ref-list></back></article>