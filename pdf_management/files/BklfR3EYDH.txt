Title:
```
Under review as a conference paper at ICLR 2020 KEYFRAMING THE FUTURE: DISCOVERING TEMPORAL HIERARCHY WITH KEYFRAME-INPAINTER PREDICTION
```
Abstract:
```
To flexibly and efficiently reason about temporal sequences, abstract representations that compactly represent the important information in the sequence are needed. One way of constructing such representations is by focusing on the important events in a sequence. In this paper, we propose a model that learns both to discover such key events (or keyframes) as well as to represent the sequence in terms of them. We do so using a hierarchical Keyframe-Inpainter (KEYIN) model that first generates keyframes and their temporal placement and then inpaints the sequences between keyframes. We propose a fully differentiable formulation for efficiently learning the keyframe placement. We show that KEYIN finds informative keyframes in several datasets with diverse dynamics. When evaluated on a planning task, KEYIN outperforms other recent proposals for learning hierarchical representations.
```

Figures/Tables Captions:
```
Figure 1: Keyframing the future. Instead of predicting one frame after the other, we propose to represent the sequence with the keyframes that depict the interesting moments of the sequence. The remaining frames can be inpainted given the keyframes.
Figure 2: A probabilistic model for jointly keyframing and inpainting a future sequence. First, a sequence of keyframes K 1:N is gen- erated, as well as corresponding temporal in- dices τ 1:N , defining the structure of the un- derlying sequence. In the second stage, for each pair of keyframes K n and K n+1 , the frames I τ n :τ n+1 −1 are inpainted.
Figure 3: Soft keyframe loss in the relaxed formulation. For each predicted keyframeK n we compute a target imagẽ K n as the sum of the ground truth images weighted with the corresponding distribution over index τ n . Finally, we compute the reconstruction loss between the estimated imagê K n and the soft targetK n .
Figure 4: Sequences generated by KEYIN and a method with constant temporal keyframe offset (Jumpy) on Brownian Motion data. Generation is conditioned on the first five frames. The first half of the sequence is shown. Movement direction changes are marked red in the ground truth sequence and predicted keyframes are marked blue. We see that KEYIN can correctly reconstruct the motion as it selects an informative set of keyframes. The sequence generated by the Jumpy method does not reproduce the direction changes since they cannot be inferred from the selected keyframes.
Figure 5: Example generations by KEYIN on (top) Pushing and (bottom) Gridworld data. The generation is conditioned on a single ground truth frame. Twelve of the 30 predicted frames are shown. We observe that for each transition between pushes and each action of the Gridworld agent our network predicts a keyframe either exactly at the timestep of the event or one timestep apart. Note, although agent position is randomized, objects not visible in the first image can be predicted in Gridworld because the maze is fixed across episodes.
Figure 6: Distribution of trajectories sampled from KEYIN. Each black line denotes one of 100 trajectories of the ma- nipulated object. The obstacle is shown in blue and the initial position in pink. We see that our model covers both modes of the distribution, producing both trajec- tories that go to the right and to the left of the obstacle.
Figure 7: Hierarchical planning on the Pushing dataset. Left: We use the model to produce keyframes that represent the sequence between the current observation image and the goal. A low-level planner based on model predictive control produces the actions, a t , executed to reach each keyframe, until the final goal is reached. Right: Planning performance on a Pushing task. The hierarchy discovered by KEYIN outperforms comparable planning approaches.
Table 1: F1 accuracy score for keyframe discovery on all three datasets. Higher is better.
Table 2: Keyframe discovery for varied number of predicted keyframes. The data has approximately 6 keyframes. Uninterpretable entries are omitted for clarity: see the text for details.
```

Main Content:
```

Section Title: INTRODUCTION
  INTRODUCTION When thinking about the future, humans focus their thoughts on the important things that may happen (When will the plane depart?) without fretting about the minor details that fill each intervening moment (What is the last word I will say to the taxi driver?). Because the vast majority of elements in a temporal sequence contains redundant information, a temporal abstraction can make reasoning and planning both easier and more efficient. How can we build such an abstraction? Consider the example of a lead animator who wants to show what happens in the next scene of a cartoon. Before worrying about every low-level detail, the animator first sketches out the story by keyframing, drawing the moments in time when the important events occur. The scene can then be easily finished by other animators who fill in the rest of the sequence from the story laid out by the keyframes. In this paper, we argue that learning to discover such informative keyframes from raw sequences is an efficient and powerful way to learn to reason about the future. Our goal is to learn such an abstraction for future image prediction. In contrast, much of the work on future image prediction has focused on frame-by-frame synthesis ( Oh et al. (2015) ;  Finn et al. (2016) ). This strategy puts an equal emphasis on each frame, irrespective of the redundant content it may contain or its usefulness for reasoning relative to the other predicted frames. Other recent work has considered predictions that "jump" more than one step into the future, but these approaches either used fixed-offset jumps ( Buesing et al., 2018 ) or used heuristics to select the predicted frames ( Neitz et al., 2018 ;  Jayaraman et al., 2019 ;  Gregor et al., 2019 ). In this work, we propose a method that selects the keyframes that are most informative about the full sequence, so as to allow us to reason about the sequence holistically while only using a small subset of the frames. We do so by ensuring that the full sequence can be recovered from the keyframes with an inpainting strategy, similar to how a supporting animator finishes the story keyframed by the lead. One possible application for a model that discovers informative keyframes is in long-horizon planning. Recently, predictive models have been employed for model-based planning and control ( Ebert et al. (2018) ). However, they reason about every single future time step, limiting their applicability to short horizon tasks. In contrast, we show that a model that reasons about the future using a small set of informative keyframes enables visual predictive planning for horizons much greater than previously possible by using keyframes as subgoals in a hierarchical planning framework. To discover informative frames in raw sequence data, we formulate a hierarchical probabilistic model in which a sequence is represented by a subset of its frames (see  Fig. 1 ). In this two-stage model, a keyframing module represents the keyframes as well as their temporal placement with stochastic latent variables. The images that occur at the timepoints between keyframes are then inferred by an inpainting module. We parametrize this model with a neural network and formulate a variational lower bound on the sequence log-likelihood. Optimizing the resulting objective leads to a model that discovers informative future keyframes that can be easily inpainted to predict the full future sequence. Our contributions are as follows. We formulate a hierarchical approach for the discovery of infor- mative keyframes using joint keyframing and inpainting (KEYIN). We propose a soft objective that allows us to train the model in a fully differentiable way. We first analyze our model on a simple dataset with stochastic dynamics in a controlled setting and show that it can reliably recover the underlying keyframe structure on visual data. We then show that our model discovers hierarchical temporal structure on more complex datasets of demonstrations: an egocentric gridworld environ- ment and a simulated robotic pushing dataset, which is challenging for current approaches to visual planning. We demonstrate that the hierarchy discovered by KEYIN is useful for planning, and that the resulting approach outperforms other proposed hierarchical and non-hierarchical planning schemes on the pushing task. Specifically, we show that keyframes predicted by KEYIN can serve as useful subgoals that can be reached by a low-level planner, enabling long-horizon, hierarchical control.

Section Title: RELATED WORK
  RELATED WORK

Section Title: Hierarchical temporal structure
  Hierarchical temporal structure Hierarchical neural models for efficiently modeling sequences were proposed in  Liu et al. (2015) ;  Buesing et al. (2018) . These approaches were further extended to predict with an adaptive step size so as to leverage the natural hierarchical structure in language data ( Chung et al., 2016 ;  Kádár et al., 2018 ). However, these models rely on autoregressive techniques for text generation and applying them to structured data, such as videos, might be impractical. The video processing community has used keyframe representations as early as 1991 in the MPEG codec ( Gall, 1991 ).  Wu et al. (2018)  adapted this algorithm in the context of neural compression; however, these approaches use constant offsets between keyframes and thus do not fully reflect the temporal structure of the data. Recently, several neural methods were proposed to leverage such temporal structure.  Neitz et al. (2018)  and  Jayaraman et al. (2019)  propose models that find and predict the least uncertain "bottleneck" frames.  Gregor et al. (2019)  construct a representation that can be used to predict any number of frames into the future. In contrast, we propose an approach for hierarchical video representation that discovers the keyframes that best describe a certain sequence. In parallel to our work,  Kipf et al. (2019)  propose a related method for video segmentation via generative modeling.  Kipf et al. (2019)  focus on using the discovered task boundaries for training hierarchical RL agents, while we show that our model can be used to perform efficient hierarchical planning by representing the sequence with only a small set of keyframes. Also concurrently,  Kim et al. (2019)  propose a similar method to KEYIN for learning temporal abstractions. While  Kim et al. (2019)  focuses on learning hierarchical state-space models, we propose a model that operates directly in the observation space and performs joint keyframing and inpainting.

Section Title: Under review as a conference paper at ICLR 2020
  Under review as a conference paper at ICLR 2020

Section Title: Video modeling
  Video modeling Early approaches to probabilistic video modeling include autoregressive models that factorize the distribution by considering pixels sequentially ( Kalchbrenner et al., 2017 ;  Reed et al., 2017 ). To reason about the images in the video holistically, latent variable approaches were developed based on variational inference ( Chung et al., 2015 ;  Rezende et al., 2014 ;  Kingma & Welling, 2014 ), including ( Babaeizadeh et al., 2018 ;  Denton & Fergus, 2018 ;  Lee et al., 2018 ) and large-scale models such as ( Castrejon et al., 2019 ;  Villegas et al., 2019 ).  Kumar et al. (2019)  is a recently proposed approach that uses exact inference based on normalizing flows ( Dinh et al., 2014 ;  Rezende & Mohamed, 2015 ). We build on existing video modeling approaches and show how they can be used to learn temporal abstractions with a novel keyframe-based generative model.

Section Title: Visual planning and model predictive control
  Visual planning and model predictive control We build on recent work that explored applica- tions of learned visual predictive models to planning and control. Several groups ( Oh et al., 2015 ;  Finn et al., 2016 ;  Chiappa et al., 2017 ) have proposed models that predict the consequences of actions taken by an agent given its control output. Recent work ( Byravan et al., 2017 ;  Hafner et al., 2018 ;  Ebert et al., 2018 ) has shown that visual model predictive control based on such models can be applied to a variety of different settings. In this work, we show that the hierarchical representation of a sequence in terms of keyframes improves planning performance in the hierarchical planning setting.

Section Title: KEYFRAMING THE FUTURE
  KEYFRAMING THE FUTURE Our goal is to develop a model that generates se- quences by first predicting key observations and the time steps when they occur and then filling in the remaining observations in between. To achieve this goal, in the following we (i) define a probabilistic model for joint keyframing and inpainting, and (ii) show how a maximum likelihood objective leads to the discovery of keyframe structure.

Section Title: A PROBABILISTIC MODEL FOR JOINT KEYFRAMING AND INPAINTING
  A PROBABILISTIC MODEL FOR JOINT KEYFRAMING AND INPAINTING We first describe a probabilistic model for joint keyframing and inpainting of a sequence I 1:T . The model consists of two parts: the keyframe predictor and the sequence inpainter (see  Fig. 2 ). The keyframe predictor takes in C conditioning frames I co and produces N keyframes K 1:N as well as the corresponding time indices τ 1:N : From each pair of keyframes, the sequence inpainter generates the sequence of frames in between: which completes the generation of the full sequence. The inpainter additionally observes the number of frames it needs to generate τ n+1 − τ n . The temporal spacing of the most informative keyframes is data-dependent: shorter keyframe intervals might be required in cases of rapidly fluctuating motion, while longer intervals can be sufficient for steadier motion. Our model handles this by predicting the keyframe indices τ and inpainting τ n+1 −τ n frames between each pair of keyframes. We parametrize the prediction of τ n in relative terms by predicting offsets δ n : τ n = τ n−1 + δ n .

Section Title: KEYFRAME DISCOVERY
  KEYFRAME DISCOVERY To produce a complex multimodal distribution over K we use a per-keyframe latent variable z with prior distribution p(z) and approximate posterior q(z|I, I co ). 1 We construct a variational lower bound Under review as a conference paper at ICLR 2020 on the likelihood of both I and K as follows 2 : In practice, we use a weight β on the KL-divergence term, as is common in amortized variational inference ( Higgins et al., 2017 ;  Alemi et al., 2018 ;  Denton & Fergus, 2018 ). If a simple model is used for inpainting, most of the representational power of the model has to come from the keyframe predictor. We use a relatively powerful latent variable model for the keyframe predictor and a simpler Gaussian distribution produced with a neural network for inpainting. Because of this structure, the keyframe predictor has to predict keyframes that describe the underlying sequence well enough to allow a simpler inpainting process to maximize the likelihood. We will show that pairing a more flexible keyframe predictor with a simpler inpainter allows our model to discover semantically meaningful keyframes in video data.

Section Title: CONTINUOUS RELAXATION BY LINEAR INTERPOLATION IN TIME
  CONTINUOUS RELAXATION BY LINEAR INTERPOLATION IN TIME Our model can dynamically predict the keyframe placement τ n . However, learning a distribution over the dis- crete variable τ n is challenging due to the expensive evaluation of the ex- pectation over p(τ n |z 1:n , I co ) in the objective in Eq. 3. To be able to eval- uate this term efficiently and in a dif- ferentiable manner while still learn- ing the keyframe placement, we pro- pose a continuous relaxation of the objective. The placement distribution τ n defines a probability for each pre- dicted frame to match to a certain frame in the ground truth sequence. Instead of sampling from this distri- bution to pick a target frame we pro- duce a soft target for each predicted frame by computing the expected tar- get frame, i.e. the weighted sum of all frames in the true sequence, each mul- tiplied with the probability of match- ing to the predicted frame. When the entropy of τ n converges to zero, the continuous relaxation objective is equivalent to the original, discrete objective.

Section Title: Keyframe targets
  Keyframe targets To produce a keyframe target,K n , we linearly interpolate between the ground truth images according to the predicted distribution over the keyframe's temporal placement τ n : K n = t τ n t I t , where τ n t is the probability that the n th keyframe occurs at timestep t. This process is depicted in  Fig. 3 . We parametrize temporal placement prediction in terms of offsets δ with a maximum offset of J. Because of this, the maximum possible length of the predicted sequence is N J. It is desirable for J to be large enough to be able to capture the distribution of keyframes in the data, but this may lead to Under review as a conference paper at ICLR 2020 Ground Truth Predicted, KeyIn (ours) Predicted, Jumpy Segment 1 Segment 2 Segment 3 Keyframe Keyframe the generation of sequences longer than the target N J > T . To correctly compute the value of the relaxed objective in this case, we discard predicted frames at times > T and normalize the placement probability output by the network so that it sums to one over the first T steps. Specifically, for each keyframe we compute this probability as c n : c n = t≤T τ n t . The loss corresponding to the last two terms of Eq. (3) then becomes:

Section Title: Inpainting targets
  Inpainting targets 1:J between each pair of keyframes K n , K n+1 . As in the previous section, the targets for ground truth images are given as an interpolation between generated images weighted by the probability of the predicted frameÎ n j being matched to ground truth frame I t : I t = ( n,j m n j,tÎ n j )/ n,j m n j,t . Here, m n j,t is the probability that the j-th predicted image in segment n has an offset of t from the beginning of the predicted sequence, which can be computed from τ n . To obtain a probability distribution over produced frames, we normalize the result with n,j m n j,t . The full loss for our model is:

Section Title: DEEP VIDEO KEYFRAMING
  DEEP VIDEO KEYFRAMING We show how to instantiate KEYIN with deep neural networks and train it on high-dimensional observations, such as images. We further describe an effective training procedure for KEYIN.

Section Title: ARCHITECTURE
  ARCHITECTURE We use a common encoder-recurrent-decoder architecture ( Denton & Fergus (2018) ;  Hafner et al. (2018) ). Video frames are first processed with a convolutional encoder module to produce image embeddings ι t = CNN enc (I t ). Inferred frame embeddingsι are decoded with a convolutional de- coderÎ n j = CNN dec (ι n j ). The keyframe predictor p(K 1:N , τ 1:N |z 1:N , I co ) is parametrized with a Long Short-Term Memory network ( LSTM, Hochreiter & Schmidhuber (1997) ). To condi- tion the keyframe predictor on past frames, we initialize its state with the final state of another LSTM that processes the conditioning frames. Similarly, we parametrize the sequence inpainter p(I τ n :τ n+1 |K n , K n+1 , τ n+1 − τ n ) with an LSTM. We condition the inpainting on both keyframe embeddings,κ n−1 andκ n , as well as the temporal offset between the two, δ n , by passing these inputs through a multi-layer perceptron that produces the initial state of the inpainting LSTM. We use a Gaussian distribution with identity variance as the output distribution for both the keyframe predictor and the inpainting model and a multinomial distribution for δ n . We parametrize the inference q(z 1:N |I −C+1:T ) with an LSTM with attention over the entire input sequence. The inference distribution is a diagonal covariance Gaussian, and the prior p(z 1:N ) is a unit Gaussian. Further details of the inference procedure are given in Sec. B and Fig. 8 of the Appendix.

Section Title: TRAINING PROCEDURE
  TRAINING PROCEDURE We train our model in two stages. First, we train the sequence inpainter to inpaint between ground truth frames sampled with random offsets, thus learning interpolation strategies for a variety of different inputs. In the second stage, we train the keyframe predictor using the loss from Eq. 5 by feeding the predicted keyframe embeddings to the inpainter. In this stage, the weights of the inpainter are frozen and are only used to backpropagate errors to the rest of the model. We found that this simple two-stage procedure improves optimization of the model. We use L1 reconstruction losses to train the keyframe predictor. We found that this and adding a reconstruction loss on the predicted embeddings of the keyframes, weighted with a factor β κ , improved the ability of the model to produce informative keyframes. Target embeddings are computed using the same soft relaxation used for the target keyframes. More details of the loss computation are given in Sec. E and Algorithm 1 of the Appendix.

Section Title: EXPERIMENTS
  EXPERIMENTS We evaluate the quality of KEYIN's representation for future sequences by addressing the following questions: (i) Can it discover and predict informative keyframes? (ii) Can it model complex data distributions? (iii) Is the discovered hierarchy useful for long-horizon hierarchical planning?

Section Title: Datasets
  Datasets We evaluate our model on three datasets containing structured long-term behavior. The Structured Brownian motion (SBM) dataset consists of binary image sequences of size 32 × 32 pixels in which a ball randomly changes directions after periods of straight movement of six to eight frames. The Gridworld Dataset consists of 20k sequences of an agent traversing a maze with different objects. The agent sequentially navigates to objects and interacts with them following a task sketch.We use the same maze for all episodes and randomize the initial position of the agent and the task sketch. We use 64 × 64 pixel image observations and further increase visual complexity by constraining the field of view to a 5 × 5-cells egocentric window.

Section Title: Under review as a conference paper at ICLR 2020
  Under review as a conference paper at ICLR 2020 The Pushing Dataset consists of 50k sequences of a robot arm pushing a puck towards a goal on the opposite side of a wall. Each sequence consists of six consecutive pushes. We vary start and target position of the puck, as well as the placement of the wall. The demonstrations were generated with the MuJoCo simulator ( Todorov et al., 2012 ) at a resolution of 64 × 64 pixels. For more details on the data generation process, see Sec.D of the Appendix. Further details about the experimental setup are given in Sec. C of the Appendix.

Section Title: KEYFRAME DISCOVERY
  KEYFRAME DISCOVERY To evaluate KEYIN's ability to dis- cover keyframes, we train KEYIN on all three datasets with N = 6, which can be interpreted as selecting the N most informative frames from a se- quence. We show qualitative exam- ples of keyframe discovery for the SBM dataset in  Fig. 4  and for the Grid- world and Pushing datasets in  Fig. 5 . On all datasets the model discovers meaningful keyframes which mark direction changes of the ball, transitions between pushes or interactions with objects, adapting its keyframe prediction patterns to the data. Consequently, the inpainter network is able to produce frames of high visual quality. Misplaced keyframes yield blurry interpolations, as can be seen for the jumpy prediction in  Fig. 4 . This suggests that keyframes found by KEYIN describe the overall sequences better. To show that KEYIN discovers informative keyframes, we compare keyframe predictions against an alternative approach that measures the surprise associated with ob- serving a frame given the previous frames. This ap- proach selects keyframes as the N frames with the largest peaks in "surprise" as measured by the KL-divergence D KL [q(z t |I 1:t )||p(z t )] between the prior and the posterior of a stochastic predictor based on  Denton & Fergus (2018)  (see Sec. F and Algorithm 2 of the Appendix for details). We provide comparisons to alternative formulations of surprise in Appendix Sec. F, Tab. 3. For quantitative analysis, we define approximate ground truth keyframes to be the points of direction change for the SBM dataset, the moments when the robot lifts its arm to transitions between pushes, or when the agent interacts with objects in the gridworld. We report F1 scores that capture both the precision and recall of keyframe discovery. We additionally compare to random keyframe placement, and a learned but static baseline that is the same for all sequences. The evaluation in  Tab. 1  shows that KEYIN discovers better keyframes than alternative methods. The difference is especially large on the more complex Pushing and Gridworld datasets. The surprise-based method does not reason about which frames are most helpful to reconstruct the entire trajectory and thus is unable to discover the correct structure on the more complex datasets. In addition to the F1 scores, we report temporal distance between predicted and annotated keyframes in Appendix, Tab. 4, also indicating that KEYIN is better able to discover the temporal structure in both datasets.

Section Title: KEYFRAME-BASED VIDEO MODELING
  KEYFRAME-BASED VIDEO MODELING Even though the focus of this work is on discovering temporal structure via keyframing and not on improving video prediction quality, we verify that KEYIN can represent complex data distributions in terms of discovered keyframes and attains high diversity and visual quality. We show sample generations from our model on the Pushing and Gridworld datasets on the supplementary website 5 .

Section Title: Under review as a conference paper at ICLR 2020
  Under review as a conference paper at ICLR 2020 We see that KeyIn is able to faithfully model complex distributions of video sequences. We further visualize multiple sampled Pushing sequences from our model conditioned on the same start position in  Fig. 6 , showing that KEYIN is able to cover both modes of the demonstration distribution. We further show that KEYIN compares favorably to prior work on video prediction metrics on sequence modeling in Tab. 5 of the Appendix, and outperforms prior approaches in terms of keyframe modeling in Appendix, Tab. 6.

Section Title: ROBUSTNESS OF KEYFRAME DETECTION
  ROBUSTNESS OF KEYFRAME DETECTION In the previous sections, we showed that when the sequence can indeed be summarized with N keyframes, KEYIN predicts the keyframes that correspond to our notion of salient frames. However, what happens if we train KEYIN to select a larger or a smaller amount of keyframes? To evaluate this, we measure KEYIN recall with extra and precision with fewer available keyframes. We note that high precision is un- achievable in the first case and high recall is un- achievable in the second case, since these prob- lems are misspecified. As these numbers are not informative, we do not report them. In  Tab. 2 , we see that KEYIN is able to find informative keyframes even when N does not exactly match the structure of the data. We further qualitatively show that KEYIN selects a superset or a subset of the original keyframes respectively in Sec. G. This underlines that our method's ability to discover keyframe structure is robust to the choice of the number of predicted keyframes. As a first step towards analyzing the robustness of KEYIN under more realistic conditions we report keyframe discovery when trained and tested on sequences with additive Gaussian noise, a noise characteristic commonly found in real-world camera sensors. We find that KEYIN is still able to discover the temporal structure on both the Pushing and the Gridworld dataset. For qualitative and quantitative results, see Appendix Fig. 11 and Tab. 7.

Section Title: HIERARCHICAL KEYFRAME-BASED PLANNING
  HIERARCHICAL KEYFRAME-BASED PLANNING We have seen that KEYIN can find frames that correspond to an intuitive notion of keyframes. This demonstrates that the keyframes discovered by KEYIN do indeed capture an abstraction that compactly describes the sequence. In light of this, we hypothesize that an informative set of keyframes contains sufficient information about a sequence to effectively follow the trajectory it shows. To test this, we use the inferred keyframes as subgoals for hierarchical planning in the pushing environment. During task execution, we first plan a sequence of keyframes that reaches the target using our learned keyframe predictor. Specifically, we generate keyframe trajectories from our model by sampling latent variables z from the prior and using them to roll out the keyframe prediction model. We optimize for a sequence of latent variables z that results in a keyframe trajectory which reaches the goal using the Cross-Entropy Method ( CEM, Rubinstein & Kroese (2004) ). We then execute the plan by using the keyframes as subgoals for a low-level planner. This planner reaches each subgoal via model predictive control using ground truth dynamics, again employing CEM for optimization of the action trajectory. This planning procedure is illustrated in  Fig. 7  (left). For more details, see Sec. I and Algs. 3 and 4 of the Appendix. We find that KEYIN is able to plan coherent subgoal paths towards the final goal that often lead to successful task execution (executions are shown on the supplementary website 6 ). To quantitatively evaluate the keyframes discovered, we compare to alternative subgoal selection schemes: fixed time offset ( Jumpy, similar to Buesing et al. (2018) ), a method that determines points of peak surprise (Surprise, see Sec. 6.1), and a bottleneck-based subgoal predictor (time-agnostic prediction or TAP,  Jayaraman et al. (2019) ). We additionally compare to an approach that plans directly towards the final goal using the low-level planner (Flat). We evaluate all methods with the shortest path between Under review as a conference paper at ICLR 2020 the target and the actual position of the object after the plan is executed. All compared methods use the same low-level planner as we only want to measure the quality of the predicted subgoals. As shown in  Fig. 7  (right), our method outperforms all prior approaches. TAP shows only a moderate increase in performance over the Flat planner, which we attribute to the fact that it fails to predict good subgoals and often simply predicts the final image as the bottleneck. This is likely due to the relatively large stochasticity of our dataset and the absence of the clear bottlenecks that TAP is designed to find. Our method outperforms the planners that use Jumpy and Surprise subgoals. This further confirms that KEYIN is able to produce keyframes that are informative about the underlying trajectory, such that planning toward these keyframes makes it easier to follow the trajectory.

Section Title: DISCUSSION
  DISCUSSION We presented KEYIN, a method for representing a sequence by its informative keyframes by jointly keyframing and inpainting. KEYIN first generates the keyframes of a sequence and their temporal placement and then produces the full sequence by inpainting between keyframes. We showed that KEYIN discovers informative keyframes on several datasets with stochastic dynamics. Furthermore, by using the keyframes for planning, we showed our method outperforms several other hierarchical planning schemes. Our method opens several avenues for future work. First, an improved training procedure that allows end-to-end training is desirable. Second, more powerful hierarchical planning approaches can be designed using the keyframe representation to scale to long-term real-world tasks. Finally, the proposed keyframing method can be applied to a variety of applications, including video summarization, video understanding, and multi-stage hierarchical video prediction. Under review as a conference paper at ICLR 2020
  We find this occurs most of the time in practice.

```
