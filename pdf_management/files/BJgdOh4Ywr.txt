Title:
```
Under review as a conference paper at ICLR 2020 VISUAL IMITATION WITH REINFORCEMENT LEARN- ING USING RECURRENT SIAMESE NETWORKS
```
Abstract:
```
It would be desirable for a reinforcement learning (RL) based agent to learn be- haviour by merely watching a demonstration. However, defining rewards that fa- cilitate this goal within the RL paradigm remains a challenge. Here we address this problem with Siamese networks, trained to compute distances between observed behaviours and the agent's behaviours. Given a desired motion such Siamese net- works can be used to provide a reward signal to an RL agent via the distance between the desired motion and the agent's motion. We experiment with an RNN- based comparator model that can compute distances in space and time between motion clips while training an RL policy to minimize this distance. Through ex- perimentation, we have had also found that the inclusion of multi-task data and an additional image encoding loss helps enforce the temporal consistency. These two components appear to balance reward for matching a specific instance of a behaviour versus that behaviour in general. Furthermore, we focus here on a par- ticularly challenging form of this problem where only a single demonstration is provided for a given task - the one-shot learning setting. We demonstrate our ap- proach on humanoid agents in both 2D with 10 degrees of freedom (DoF) and 3D with 38 DoF.
```

Figures/Tables Captions:
```
Figure 1: A pair of sequences from the expert (blue) and agent (red). The spatial distance is shown via doted black lines and the temporal distance for d(o e , o a 7 ) is visulized in green. On the right the flow of control for the learning system is shown.
Figure 2: Siamese network structure. The convolutional portion of the network includes 2 convolution layers of 8 filters with size 6 × 6 and stride 2 × 2, 16 filters of size 4 × 4 and stride 2 × 2. The features are then flattened and followed by two dense layers of 256 and 64 units. The majority of the network uses ReLU activations except for the last layer that uses a sigmoid activation. Dropout is used between convolutional layers. The RNN-based model uses a LSTM layer with 128 hidden units, followed by a dense layer of 64 units. The decoder model has the same structure in reverse with deconvolution in place of convolutional layers.
Figure 3: Rasterized frames of the agent's motion after training on humanoid3d walking (row 1,2), running (row 3-5), zombie (row 6,7) and jumping(row 8-10). The multi-coloured agent is a rendering of the imitation video. A video of these results is available here: https://youtu.be/s1KiIrV1YY4 π E . Overlaps in specific areas of the space for similar classes across learned π and expert π E data indicate a well-formed distance metric that does not sperate expert and agent examples. There is also a separation between motion classes in the data, and the cyclic nature of the walking cycle is visible.
Figure 4: Baseline comparisons between our sequence-based method, GAIL and TCN (4a) on the humanoid2d environment. Two additional baseline comparison between VIRL and TCN in 4b. In 4c the benefit of combing spatial and temporal distances is shown. In these plots, the large solid lines are the average performance of a collection of policy training simulations. The dotted lines of the same colour are the specific performance values for each policy training run.
Figure 5: Ablation analysis of VIRL. We find that training RL policies is sensitive to the size and distribution of rewards. A few modifications assist in the siamese network's ability to compute useful distances. Including VAE and AE losses to assist in representation learning. The addition of multi-task training data is also important for learning better policies.
```

Main Content:
```

Section Title: INTRODUCTION
  INTRODUCTION Imitation learning and Reinforcement Learning (RL) often intersect when the goal is to imitate with incomplete information, for example, when imitating from motion capture data (mocap) or video. In this case, the agent needs to search for actions that will result in observations similar to the expert. However, formulating a metric that will provide a reasonable distance between the agent and the expert is difficult. Robots and people plan using types of internal and abstract pose representations that can have reasonable distances; however, typically when animals observe others performing tasks, only visual information is available. Using distances in pose-space is ill-suited for imitation as changing some features can result in drastically different visual appearance. In order to understand how to perform tasks from visual observation a mapping/transformation is used which allows for the minimization of distance in appearance. Even with a method to transform observations to a similar pose space, each person has different capabilities. Because of this, people are motivated to learn transformations in space and time where they can reproduce the behaviour to the best of their own ability. How can we learn a representation similar to this latent space? An essential detail of imitating demonstrations is their sequential and causal nature. There is both an ordering and speed in which a demonstration is performed. Most methods require the agent to learn to imitate the temporal and spatial structure at the same time creating a potentially narrow solution space. When the agent becomes desynchronized with the demonstration, the agent will receive a low reward. Consider the case when a robot has learned to stand when its goal is to walk. Standing is spatially close to the demonstration and actions that help the robot stand, as opposed to falling, should be encouraged. How can such latent goals be encouraged? If we consider a phase-based reward function r = R(s, a, φ) where φ indexes the time in the demon- stration and s and a is the agent state and action. As the demonstration timing φ, often controlled by the environment, and agent diverge, the agent receives less reward, even if it is visiting states that exist elsewhere in the demonstration. The issue of determining if an agent is displaying out- of-phase behaviour can understood as trying to find the φ that would result in the highest reward Under review as a conference paper at ICLR 2020 φ = max φ R(s, a, φ) and the distance φ − φ is an indicator of how far away in time or out-of- phase the agent is. This phase-independent form can be seen as a form of reward shaping. However, this naive description ignores the ordered property of demonstrations. What is needed is a metric that gives reward for behaviour that is in the proper order, independent of phase. This ordering mo- tivates the creation of a recurrent distance metric that is designed to understand the context between two motions. For example, does this motion look like a walk, not, does this motion look precisely like that walk. Our proposed Visual Imitation with Reinforcement Learning (VIRL) method uses Recur- rent Siamese Networks (RSNs) and has similarities to both Inverse Reinforcement Learning (IRL) ( Abbeel & Ng, 2004 ) and Generative Advisarial Imitation Learning (GAIL) ( Ho & Ermon, 2016 ). The process of learning a cost function that understands the space of policies to find an optimal policy given a demonstration is fundamentally IRL. While using positive examples from the expert and negative examples from the policy is similar to the method GAIL uses to train a discriminator to recognize in distribution examples. In this work, we build upon these techniques by constructing a method that can learn policies using noisy visual data without action information. Considering the problem's data sparsity, we include data from other tasks to learn a more robust distance function in the space of visual sequence. We also construct a cost function that takes into account the demonstration ordering as well as pose using a recurrent Siamese network. Our con- tribution consists of proposing and exploring these forms of recurrent Siamese networks as a way to address a critical problem in defining reward structure for imitation learning from the video for deep RL agents and accomplishing this on simulated humanoid robots for the challenging single shot learning setting.

Section Title: RELATED WORK
  RELATED WORK Learning From Demonstration Searching for good distance functions is an active research area ( Abbeel & Ng, 2004 ;  Argall et al., 2009 ). Given some vector of features, the goal is to find an optimal transformation of these features, such in this transformed space, there exists a strong contextual meaning. Previous work has explored the area of state-based distance functions, but most rely on pose based metrics ( Ho & Ermon, 2016 ;  Merel et al., 2017 ) that come from an expert. While there is other work using distance functions, including for example  Sermanet et al. (2017) ;  Finn et al. (2017) ;  Liu et al. (2017) ;  Dwibedi et al. (2018) , few use image based inputs and none con- sider the importance of learning a distance function in time as well as space. In this work, we train recurrent Siamese networks ( Chopra et al., 2005 ) to learn distances between videos.

Section Title: Partially Observable Imitation Without Actions
  Partially Observable Imitation Without Actions For Learning from Demonstration (LfD) prob- lems the goal is to replicate the behaviour of expert π E behaviour. Unlike the typical setting for humans learning to imitate, LfD often assumes the availability of expert action and observation data. Instead, in this work, we focus on the case where only noisy actionless observations of the expert are available. Recent work uses Behavioural Cloning (BC) to learn an inverse dynamics model to estimate the actions used via maximum-likelihood estimation ( Torabi et al., 2018 ). Still, BC often needs many expert examples and tends to suffer from state distribution mismatch issues between the expert policy and student ( Ross et al., 2011 ). Work in ( Merel et al., 2017 ) proposes a system based on GAIL that can learn a policy from a partial observation of the demonstration. In this work, the discriminator's state input is a customized version of the expert's state and does not take into account the demonstration's sequential nature. The work in ( Wang et al., 2017 ) provides a more robust GAIL framework along with a new model to encode motions for few-shot imitation. This model uses an Recurrent Neural Network (RNN) to encode a demonstration but uses expert state and action observations. In our work, the agent is limited to only a partial visual observation as a demonstration. Additional works learn implicit models of distance ( Yu et al., 2018 ;  Pathak et al., 2018 ;  Finn et al., 2017 ;  Sermanet et al., 2017 ), none of these explicitly learn a sequential model considering the demonstration timing. An additional version of GAIL, infoGAIL ( Li et al., 2017 ), included pixel based inputs. Goals can be specified using the latent space from a Variational Auto Encoder (VAE) ( Nair et al., 2018 ). Our work extends this VAE loss using sequence data to train a more temporally consistent latent representation. Recent work ( Peng et al., 2018b ) has a 2D control example of learning from video data. We show results on more complex 3D tasks and additionally model distance in time. In contrast, here we train a recurrent siamese model that can be used to en- Under review as a conference paper at ICLR 2020 able curriculum learning and allow for computing distances even when the agent and demonstration are out of sync.

Section Title: PRELIMINARIES
  PRELIMINARIES In this section, we outline the general RL framework and specific formulations for RL that we rely upon when developing our method in Section 4.

Section Title: Reinforcement Learning
  Reinforcement Learning Using the RL framework formulated with a Markov Dynamic Process (MDP): at every time step t, the world (including the agent) exists in a state s t ∈ S, wherein the agent is able to perform actions a t ∈ A, sampled from a policy π(a t |s t ) which results in a new state s t+1 ∈ S and reward r t according to the transition probability function T (r t , s t+1 |s t , a t ). The policy is optimize to maximize the future discounted reward J(π) = E r0,...,r T T t=0 γ t r t , (1) where T is the max time horizon, and γ is the discount factor, indicating the planning horizon length. Inverse reinforcement learning refers to the problem of extracting a reward function from observed optimal behavior  Ng et al. (2000) . In contrast, in our approach we learn a distance that works across a collection of behaviours. Further, we do not assume the example data to be optimal. See Appendix 7.2 for further discussion of the connections of our work to inverse reinforcement learning. GAIL VIRL is similar to the GAIL framework ( Ho & Ermon, 2016 ) which uses a Generative Advasarial Network (GAN) ( Goodfellow et al., 2014 ), where the discriminator is trained with posi- tive examples from the expert trajectories and negative examples from the policy. The generator is a combination of the environment, policy and current state visitation probability induced by the policy p π (s). In this framework the discriminator provides rewards for the RL policy to optimize, as the probability of a state generated by the policy being in the distribution r t = D(s t , a t |θ φ ). While this framework has been shown to work in practice, this dual optimization is often unstable. In the next section we will outline our method for learning a more stable distance based reward over sequences of images.

Section Title: CONCEPTUAL DISTANCE-BASED REINFORCEMENT LEARNING
  CONCEPTUAL DISTANCE-BASED REINFORCEMENT LEARNING Our approach is aimed at facilitating imitation learning within an underlying RL formulation over partially observed observations o. Unlike the situation in GAIL, we do not rely on having accces to state, s and action, a information - our idea is to minimize a function that determintes the distance between two sequences observations, o, one from the desired example behavior o e , and another from the current agent behavior o a . We can then define the reward used within an underlying RL framework in terms of a distance function D, such that where in our setting here D(o e , o a ,t) models a distance between video clips from time t = 0 tot. A simple formulation of the approach above can be overly restrictive on sequence timing. While these distances can serve as RL rewards, they often provide insufficient signal for the policy to learn a good imitative behaviour, especially when the agent only has partial observations of the expert. We can see an example of this in Figure 1a were starting at t 5 the agent (in red) begins to exhibit behaviour that is similar to the expert (in blue) yet the spatial distance indicates that this state is further away from the desired behaviour than at t 4 . To encourge the agent to match any part of the expert behaviour we propose decomposing the dis- tance into two distances, by adding a type of temporal distance shown in green. To compute a time Under review as a conference paper at ICLR 2020 independant distance we can find the state in the expert sequence that is closest to the agent's current state arg mint ∈T d(o ê t , o a t ) and use it in the following distance measure Using only a single state time-alined may lead to the agent fixating on mataching a single state in the expert demonstration. To avoid this the neighbouring states given sequence timing readjustment are used in the distance computation. This framework allows the agent to be rewarded for exhibiting behaviour that matches any part of the experts demonstration. The better is learns to match parts of the expert demonstration the more reward it is given. The previous spatial distance will then help the agent learn to sync up its timing with the deomonstration. Next we describe how we learn both of these distances.

Section Title: Distance Metric Learning
  Distance Metric Learning Many methods can be used to learn a distance function in state-space. Here we use a Siamese network f (o e , o a ) with a triplet loss over time and task data ( Chopra et al., 2005 ). The triplet loss is used to minimize the distance between two examples that are positive, very similar or from the same class, and maximize the distance between pairs of examples that are known to be unrelated. For more details see supplementary document.

Section Title: Sequence Imitation
  Sequence Imitation The distance metric is formulated in a recurrent style where the distance is computed from the current state and conditioned on all previous states d(o t |o t−1 , . . . , o 0 ). The loss function is a combination of distance Eq. 9 and VAE-based representation learning objectives from Eq. 7 and Eq. 8, detailed in the supplementary material. This combination of sequence- based losses assists in compressing the representation while ensuring intermediate representations are informative. The loss function used to train the distance model on a positive pair of sequences is: Where λ = {0.7, 0.1, 0.1, 0.1}. With a negative pair, the second sequence used in the VAE and AE losses would be the negative sequence. The Siamese loss function remains the same as in Eq. 9 but the overall learning process evolves to use an RNN-based deep networks. A diagram of the full model is shown in  Figure 2 . This model uses a time distributed Long Short-Term Memory (LSTM). A single convolutional network conv a is first used to transform images of the demonstration o a to an encoding vector e a t . After the sequence of images is distributed through conv a there is an encoded sequence < e a 0 , . . . , e a t >, this sequence is fed into the RNN lstm a until a final encoding is produced h a t . This same process is performed for a copy of the RNN lstm a producing h b t for the agent o b . The loss is computed in a similar fashion to ( Mueller & Thyagarajan, 2016 ) using the sequence outputs of images from the agent and another from the demonstration. The reward at each timestep is computed as r t = Under review as a conference paper at ICLR 2020 At the beginning of each episode, the RNN's internal state is reset. The policy and value function have 2 hidden layers with 512 and 256 units, respectively. The use of additional VAE-based image and Auto Encoder (AE)-based sequence decoding losses improve the latent space conditioning and representation.

Section Title: Unsupervised Data labelling
  Unsupervised Data labelling To construct positive and negative pairs for training we make use of time information in a similar fashion to ( Sermanet et al., 2017 ), where observations at similar times in the same sequence are often cor- related and observations at different times will likely have little similarity. We compute pairs by altering one sequence and comparing this modi- fied version to its original. Positive pairs are cre- ated by adding noise to the sequence or altering a few frames of the sequences. Negative pairs are created by shuffling one sequence or reversing it. More details are available in the supplemen- tary material. Imitation data for 24 other tasks are also used to help condition the distance met- ric learning process. These include motion clips for running, backflips, frontflips, dancing, punch- ing, kicking and jumping along with the desired motion. For details on how positive and negative pairs are created from this data, see the supple- mentary document. Importantly the RL environment generates two different state representations for the agent. The first state s t+1 is the internal robot pose. The second state o t+1 is the agent's rendered view, shown in  Figure 2 . The rendered view is used with the distance metric to compute the similarity between the agent and the demonstration. We attempted using the visual features as the state input for the policy as well; this resulted in poor policy quality. Details of the algorithm used to train the distance metric and policy are outlined in the supplementary document Algorithm 1.

Section Title: ANALYSIS AND RESULTS
  ANALYSIS AND RESULTS The simulation environment used in the experiments is similar to the DeepMind Control Suite ( Tassa et al., 2018 ). In this simulated robotics environment, the agent is learning to imitate a given reference motion. The agent's goal is to learn a policy to actuate Proportional Derivative (PD) controllers at 30 fps to mimic the desired motion. The simulation environment provides a hard-coded reward function based on the robot's pose that is used to evaluate the policy quality. The demonstration M the agent is learning to imitate is generated from a clip of mocap data. The mocap data is used to Under review as a conference paper at ICLR 2020 animate a second robot in the simulation. Frames from the simulation are captured and used as video input to train the distance metric. The images captured from the simulation are converted to grey- scale with 64 × 64 pixels. We train the policy on pose data, as link distances and velocities relative to the robot's Centre of Mass (COM). This simulation environment is new and has been created to take motion capture data and produce multi-view video data that can be used for training RL agents or generating data for computer vision tasks. The environment includes challenging and dynamic tasks for humanoid robots. Some example tasks are imitating running, jumping, and walking, shown in  Figure 3  and humanoid2d detailed in the supplementary material.

Section Title: 3D Humanoid Robot Imitation
  3D Humanoid Robot Imitation In these simulated robotics environments the agent is learning to imitate a given reference motion of a walk, run, jump or zombie motion. A single motion demon- stration is provided by the simulation environment as a cyclic motion. During learning, we include additional data from all other tasks for the walking task this would be: walking-dynamic-speed, running, jogging, frontflips, backflips, dancing, jumping, punching and kicking) that are only used to train the distance metric. We also include data from a modified version of the tasks that has a randomly generated speed modifier ω ∈ [0.5, 2.0] walking-dynamic-speed, that warps the demon- stration timing. This additional data is used to provide a richer understanding of distances in space and time to the distance metric. The method is capable of learning policies that produce similar behaviour to the expert across a diverse set of tasks. We show example trajectories from the learned policies in  Figure 3  and in the supplemental Video. It takes 5 − 7 days to train each policy in these results on a 16 core machine with an Nvidia GTX1080 GPU.

Section Title: Algorithm Analysis and Comparison
  Algorithm Analysis and Comparison To evaluate the learning capabilities and improvements of VIRL we compare against two other methods that learn a distance function in state space, GAIL and using a VAE to train an encoding and compute distances between those encodings, similar to ( Nair et al., 2018 ), using the same method as the Siamese network in Figure 4a. We find that the VAE alone does not appear to capture the critical distances between states, possibly due to the decoding transformation complexity. Similarly, the GAIL baseline produces very jerky motion or stands still, both of which are contained in the imitation distribution. Our method that considers the temporal structure of the data learns faster and produces higher value policies. Additionally, we create a multi-modal version of VIRL. Here we replace the bottom conv net with a dense network and learn a distance metric between agent poses and imitation video. The results of these models, along with the default manual reward function provided by the environment, are shown in Figure 4b. The multi-modal version appears to perform about equal to the vision-only modal. In Figure 4b we also compare our method to a non-sequence-based model that is equivalent to Time Contrastive Network (TCN). On average VIRL achieves higher value policies. We find that using the RNN-based distance metric makes the learning process more gradual. We show this learning effect in Figure 4b, where the original manually created reward with flat feedback leads to slow initial learning. In Figure 4c we compare the importance of the spatial ||e a t −e b t || 2 and temporal ||h a t −h b t || 2 represen- tations learned by VIRL. Using the recurrent representation (temporal lstm) alone allows learning to progress quickly but can have difficulty informing the policy of how to best match the desired exam- ple. On the other hand, using only the encoding between single frames (spatial conv) slows learning due to limited reward for out-of-phase behaviour. We achieved the best results by combining the representations from these two models. The assistance of spatial rewards is also seen in Figure 4b, where the manual reward learns the slowest.

Section Title: Ablation
  Ablation We conduct ablation studies in Figure 5a to compare the effects of data augmentation methods, network models and the use of additional data from other tasks. For the more complex humanoid3d control problems the data augmentation methods, including Early Episode Sequence Priority (EESP), increases average policy quality marginally. The use of mutlitask data Figure 8c and the additional representational losses Figure 8a greatly improve the methods ability to learn. More ablation results are available in the supplementary material.

Section Title: Sequence Encoding
  Sequence Encoding Using the learned sequence encoder a collection of motions from different classes are processed to create a TSNE embedding of the encodings ( Maaten & Hinton, 2008 ). In Figure 5c we plot motions both generated from the learned policy π and the expert trajectories Under review as a conference paper at ICLR 2020 In this section, we have described the process followed to create and analyze VIRL. Due to a com- bination of data augmentation techniques, VIRL can imitate given only a single demonstration. We have shown some of the first results to learn imitative policies from video data using a recurrent net- Under review as a conference paper at ICLR 2020 work. Interestingly, the method displays new learning efficiencies that are important to the method success by separating the imitation problem into spatial and temporal aspects. For best results, we found that the inclusion of additional regularizing losses on the recurrent siamese network, along with some multi-task supervision, was key to producing results.

Section Title: DISCUSSION AND CONCLUSION
  DISCUSSION AND CONCLUSION In this work, we have created a new method for learning imitative policies from a single demon- stration. The method uses a Siamese recurrent network to learn a distance function in both space and time. This distance function, trained on noisy partially observed video data, is used as a reward function for training an RL policy. Using data from other motion styles and regularization terms, VIRL produces policies that demonstrate similar behaviour to the demonstration. Learning a distance metric is enigmatic, the distance metric can compute inaccurate distances in areas of the state space it has not yet seen. This inaccuracy could imply that when the agent explores and finds truly new and promising trajectories, the distance metric computes incorrect distances. We attempt to mitigate this effect by including training data from different tasks. We believe VIRL will benefit from a more extensive collection of multi-task data and increased variation of each task. Additionally, if the distance metric confidence is available, this information could be used to reduce variance and overconfidence during policy optimization. It is probable learning a reward function while training adds additional variance to the policy gra- dient. This variance may indicate that the bias of off-policy methods could be preferred over the added variance of on-policy methods used here. We also find it important to have a small learning rate for the distance metric. The low learning rate reduces the reward variance between data collec- tion phases and allows learning a more accurate value function. Another approach may be to use partially observable RL that can learn a better value function model given a changing RNN-based Under review as a conference paper at ICLR 2020 reward function. Training the distance metric could benefit from additional regularization such as constraining the kl-divergence between updates to reduce variance. Learning a sequence-based pol- icy as well, given that the rewards are now not dependent on a single state observation is another area for future research. We compare our method to GAIL, but we found GAIL has limited temporal consistency. This method led to learning jerky and overactive policies. The use of a recurrent discriminator for GAIL may mitigate some of these issues and is left for future work. It is challenging to produce results better than the carefully manually crafted reward functions used by the RL simulation environments that include motion phase information in the observations ( Peng et al., 2018a ; 2017). However, we have shown that our method that can compute distances in space and time has faster initial learning. Potentially, a combination of starting with our method and following with a manually crafted reward function could lead to faster learning of high-quality policies. Still, as environments become increasingly more realistic and grow in complexity, we will need more robust methods to describe the desired behaviour we want from the agent. Training the distance metric is a complicated balancing game. One might expect that the distance metric should be trained early and fast so that it quickly understands the difference between a good and bad demonstration. However, quickly learning confuses the agent, rewards can change, which cause the agent to diverge off toward an unrecoverable policy space. Slower is better, as the distance metric may not be accurate, it may be locally or relatively reasonable, which is enough to learn a good policy. As learning continues, these two optimizations can converge together.

```
