Title:
```
QUANTUM EXPECTATION MAXIMIZATION FOR GAUSSIAN MIX- TURE MODELS
```
Abstract:
```
The Expectation-Maximization (EM) algorithm is a fundamental tool in unsupervised machine learning. It is often used as an efficient way to solve Maximum Likelihood (ML) and Maximum A Posteriori estimation problems, especially for models with latent variables. It is also the algo- rithm of choice to fit mixture models: generative models that represent unlabelled points originating from k different processes, as samples from k multivariate distributions. In this work we define and use a quantum version of EM to fit a Gaussian Mixture Model. Given quantum access to a dataset of n vectors of dimension d, our algorithm has convergence and precision guarantees similar to the classical algorithm, but the runtime is only polylogarithmic in the number of elements in the training set, and is polynomial in other parameters - as the dimension of the feature space, and the number of components in the mixture. We generalize further the algorithm by fitting any mixture model of base distributions in the exponential family. We discuss the performance of the algorithm on datasets that are expected to be classified successfully by those algorithms, arguing that on those cases we can give strong guarantees on the runtime.
```

Figures/Tables Captions:
```
Table 1: We estimate some of the parameters of the VoxForge (Voxforge.org) dataset. The averages for the matrix V are taken over 34 samples, while for Σ is over 170 samples. The accuracy reported in the experiments is measured on 170 samples in the test set, after the threshold on the eigenvalues of Σ. Each model is the result of the best of 3 different initializations of the EM algorithm. The first and the second column are the maximum singular values of all the covariance matrices, and the absolute value of the log-determinant. The column κ * (Σ) consist in the thresholded condition number for the covariance matrices.
```

Main Content:
```

Section Title: INTRODUCTION
  INTRODUCTION Over the last few years, the effort to find real world applications of quantum computers has greatly intensified. Along with chemistry, material sciences, finance, one of the fields where quantum computers are expected to be most beneficial is machine learning. A number of different algorithms have been proposed for quantum machine learning ( Biamonte et al., 2017 ;  Wiebe et al., 2017 ;  Kerenidis & Prakash, 2018 ;  Harrow et al., 2009 ;  Subaşı et al., 2019 ;  Farhi & Neven, 2018 ), both for the supervised and unsupervised setting, and despite the lack of large-scale quantum computers and quantum memory devises, some quantum algorithms have been demonstrated in proof-of-principle experiments ( Li et al., 2015 ;  Otterbach et al., 2017 ;  Jiang et al., 2019 ). Here, we look at Expectation-Maximization (EM), a fundamental algorithm in unsupervised learning, that can be used to fit different mixture models and give maximum likelihood estimates with the so-called latent variable models. Such generative models are one of the most promising approaches for unsupervised problems. The goal of a generative model is to learn a probability distribution that is most likely to have generated the data collected in a training set V ∈ R n×d of n vectors of d features. Fitting the model consists in learning the parameters of a probability distribution p in a certain parameterized family that best describes our vectors v i . We will see that, thanks to this formulation, we can reduce a statistical problem into an optimization problem using maximum likelihood estimation (ML) estimation. The likelihood is the function that we use to measure how good a model is for explaining a given dataset. For a given machine learning model with parameters γ, the likelihood of our data set V is the probability that the data have been generated by the model with parameters γ, assuming each point is independent and identically distributed. We think the likelihood as a function of γ, holding the dataset V fixed. For p(v i |γ) the probability that a point v i comes from model γ, the likelihood is defined as L(γ; V ) := n i=1 p(v i |γ). From this formula, we can see that in order to find the best parameters γ * of our model we need to solve an optimization problem. For numerical and analytical reasons, instead of maximizing the likelihood L, it is common practice to find the best model by maximizing the log-likelihood function (γ; V ) = log L(γ; V ) = n i=1 log p(v i |γ). In this context, we want to find the model that maximizes the log-likelihood: γ * M L := arg max γ n i=1 log p(v i |γ). The procedure to calculate the log-likelihood depends on the specific model under consideration. A possible solution would be to use a gradient based optimization algorithm on . Unfortunately, due to the indented landscape of the function, gradient based techniques often do not perform well. Therefore, it is common to solve the maximum likelihood estimation (or maximum a priori) problem using the Expectation-Maximization (EM) algorithm. EM is an iterative algorithm which is guaranteed to converge to a (local) optimum of the likelihood. This algorithm has a striking variety of applications, and has been successfully used for medical imaging ( Balafar et al., 2010 ), image restoration ( Lagendijk et al., 1990 ), problems in computational biology ( Fan et al., 2010 ), and so on. EM has been proposed in different works by different authors, but has been formalized as we know it only in 1977 ( Dempster et al., 1977 ). For more details, we refer to (Lindsay, 1995;  Bilmes et al., 1998 ). In this work, we introduce Quantum Expectation-Maximization (QEM), a new algorithm for fitting mixture models. We detail its usage in the context of Gaussian Mixture Models, and we extend the result to other distributions in the exponential family. We also generalize the result by showing how to compute the MAP: the Maximum A Posteriori estimate of a mixture model. MAP estimates can be seen as the Bayesian version of maximum likelihood estimation problems. MAP estimates are often preferred over ML estimates, due to a reduced propensity to overfit. Our main result can be stated as: Result (Quantum Expectation-Maximization). (see Theorem 3.9) For a data matrix V ∈ R n×d stored in an appropri- ate QRAM data structure and for parameters δ θ , δ µ > 0 , Quantum Expectation-Maximization (QEM) fits a Maximum Likelihood (or a Maximum A Posteriori) estimate of a Gaussian Mixture Model with k components, in running time per iteration which is dominated by: O d 2 k 4.5 η 3 κ 2 (V )κ 2 (Σ)µ(Σ) δ 3 µ , (1) where Σ is a covariance matrix of a Gaussian distribution, η is a parameter of the dataset related to the maximum norm of the vectors, δ θ , δ µ are error parameters in the QEM algorithm, µ(< √ d) is a factor appearing in quantum linear algebra and κ is the condition number of a matrix. Here we only kept the term in the running time that dominates for the range of parameters of interest. In Theorem 3.9 we explicate the running time of each step of the algorithm. The QEM algorithm runs for a number of iterations until a stopping condition is met (defined by a parameter τ > 0) which implies a convergence to a (local) optimum. Let's have a first high-level comparison of this result with the standard classical algorithms. The runtime of a single iteration in the standard implementation of the EM algorithm is at least O(knd 2 ) ( Pedregosa et al., 2011 ;  Murphy, 2012 ). The advantage of the quantum algorithm is an exponential improvement with respect to the number of elements in the training set, albeit with a worsening on other parameters. It is crucial to find datasets where such a quantum algorithm can offer a speedup. For a reasonable range of parameters ( d = 40, k = 10, η = 10, δ = 0.5, κ(V ) = 25, κ(Σ) = 5, µ(Σ) = 4) which is motivated by some experimental evidence reported in Section 4, datasets where the number of samples in the order of O(10 12 ) might be processed faster on a quantum computer. One should expect that some of the parameters of the quantum algorithm can be improved, especially the dependence on the condition numbers and the errors, which can make enlarge the type of datasets where QEM can offer an advantage. Note that we expect the number of iterations of the quantum algorithm to be proportional to the number of iteration of the classical case. This is to be expected since the convergence rate does not change, and it is corroborated by previous experimental evidence in a similar scenario: the number of iterations needed by q-means algorithm for convergence, is proportional to the number of iterations of the classical k-means algorithm ( Kerenidis et al., 2018 ). Expectation-Maximization is widely used for fitting mixture models in machine learning ( Murphy, 2012 ). Most mix- ture models use a base distribution in the exponential family: Poisson ( Church & Gale, 1995 ), Binomial, Multinomial, log-normal ( Dexter & Tanner, 1972 ), exponential ( Ghitany et al., 1994 ), Dirichlet multinomial ( Yin & Wang, 2014 ), and others. EM is also used to fit mixtures of experts, mixtures of the student T distribution (which does not belong to the exponential family, and can be fitted with EM using ( Liu & Rubin, 1995 )) and for factor analysis, probit regression, and learning Hidden Markov Models ( Murphy, 2012 ).

Section Title: PREVIOUS WORK
  PREVIOUS WORK There is a fair amount of quantum algorithms that have been proposed in the context of unsupervised learning. ( Aïmeur et al., 2013 ;  Lloyd et al., 2013b ;  Otterbach et al., 2017 ). Recently, classical machine learning algorithms were obtained by "dequantizing" quantum machine learning algorithms ( Tang, 2018b ;a; c ;  Gilyén et al., 2018a ;  Chia et al., 2018 ). The runtime of these classical algorithm is poly-logarithmic in the dimensions of the dataset. However, the polynomial dependence on the rank, the error, and the condition number, make these new algorithms impractical on interesting datasets, as shown experimentally by ( Arrazola et al., 2019 ). Fast classical algorithm for GMM exists, albeit assuming only one shared covariance matrix ( Dasgupta, 1999 ), and without a polylogarithmic dependence in the number of elements in the training set. Independently of us, Miyahara, Aihara, and Lechner extended the q-means algorithm ( Kerenidis et al., 2018 ) for Gaussian Mixture Models ( Miyahara et al., 2019 ), using similar techniques. The main difference is that in their work the update step is performed using a hard-clustering approach (as in the k-means algorithm), that is for updating the centroid and the covariance matrices of a cluster j, only the data points for which cluster j is nearest are taken into account. In our work, we use the soft clustering approach (as in the classical EM algorithm), that is for updating the centroid and the covariance matrices of cluster j, all the data points weighted by their responsibility for cluster j are taken into account. Both approaches have merits and can offer advantages ( Kearns et al., 1998 ).

Section Title: EXPECTATION-MAXIMIZATION AND GAUSSIAN MIXTURE MODELS
  EXPECTATION-MAXIMIZATION AND GAUSSIAN MIXTURE MODELS As is common in machine learning literature, we introduce the Expectation-Maximization algorithm by using it to fit Gaussian Mixture Models (GMM). Mixture models are a popular generative model in machine learning. The intuition behind mixture models is to model complicated distributions by using a group of simpler (usually uni-modal) distri- butions. In this setting, the purpose of the learner is to model the data by fitting the joint probability distribution which most likely have generated our samples. In this section we describe GMM: probably the most used mixture model used to solve unsupervised classification problems. In fact, given a sufficiently large number of mixture components, it is possible to approximate any density defined in R d ( Murphy, 2012 ). In unsupervised settings, we are given a training set of unlabeled vectors v 1 · · · v n ∈ R d which we represent as rows of a matrix V ∈ R n×d . Let y i ∈ [k] one of the k possible labels for a point v i . We posit that the joint probability distribution of the data p(v i , y i ) = p(v i |y i )p(y i ), is defined as follow: y i ∼ Multinomial(θ) for θ ∈ R k , and p(v i |y i = j) ∼ N (µ j , Σ j ). The θ j are the mixing weights, i.e. the probabilities that y i = j, and N (µ j , Σ j ) is the Gaussian distribution centered in µ j ∈ R d with covariance matrix Σ j ∈ R d×d . Note that the variables y i are unobserved, and thus are called latent variables. There is a simple interpretation for this model. We assume the data is created by first selecting an index j ∈ [k] by sampling according to Multinomial(θ), and then a vector v i is sampled from N (µ j , Σ j ). Fitting a GMM to a dataset reduces to finding an assignment for the parameters: γ = (θ, µ, Σ) = (θ, µ 1 , · · · , µ k , Σ 1 , · · · , Σ k ) that best maximize the log-likelihood for a given dataset. Note that the algorithm used to fit GMM can return a local minimum which might be different than γ * : the model that represents the global optimum of the likelihood function. We use the letter φ to represent our base distribution, which in this case is the probability density function of a multivariate Gaussian distribution N (µ, Σ). Using this formulation, a GMM is expressed as: p(v) = k j=1 θ j φ(v; µ j , Σ j ) where θ j are the mixing weights of the multinomial distribution such that k j=1 θ j = 1. The probability for an observation v i to be assigned to the component j is given by: r ij = θj φ(vi;µj ,Σj ) k l=1 θ l φ(vi;µ l ,Σ l ) . This value is called responsibility, and corresponds to the posterior probability of the sample i being assigned label j by the current model. As anticipated, to find the best parameters of our gener- ative model, we maximize the log-likelihood of the data. For GMM, the likelihood is given by the following formula ( Ng, 2012 ): (γ; V ) = (θ, µ, Σ; V ) = n i=1 log p(v i ; θ, µ, Σ) =. Alas, it is seldom possible to solve maximum likelihood estimation analytically (i.e. by finding the zeroes of the derivatives of the log-like function, and this is one of those cases. Fortunately, Expectation-Maximization is an iterative algorithm that solves numerically the optimiza- tion problem of ML estimation. To complicate things, the likelihood function for GMM is not convex, and thus we might find some local minima ( Hastie et al., 2009 ). If we were to know the latent variable y i , then the log-likelihood for GMM would be:f (γ; V ) = n i=1 log p(v i | y i ; µ, Σ) + log p(y i ; θ) This formula can be easily maximized with respect to the parameters θ, µ, and Σ. In the Expectation step we calculate the missing variables y i 's, given a guess of the parameters (θ, µ, Σ) of the model. Then, in the Maximization step, we use the estimate of the latent variables obtained in the Expectation step to update the estimate of the parameters. While in the Expectation step we calculate a lower bound on the likelihood, in the Maximization step we maximize it. Since at each iteration the likelihood can only increase, the algorithm is guaranteed to converge, albeit possibly to a local optimum (see ( Hastie et al., 2009 ) for the proof). During the Expectation step all the responsibilities are calculated, while in the Maximization step we update our estimate on the parameters γ t+1 = (θ t+1 , µ t+1 , Σ t+1 ). The stopping criterion for GMM is usually a threshold on the increment of the log-likelihood: if the log-likelihood changes less than a threshold between two iterations, then the algorithm stops. Notice that, since the value of the log-likelihood significantly depends on the amount of data points in the training sets, it is often preferable to adopt a scale-free stopping criterion, which does not depend on the number of samples. For instance, in the toolkit scikit-learn ( Pedregosa et al., 2011 ) the stopping criterion is given by a tolerance on the average increment of the log-probability, which is chosen to be smaller than a certain τ , say 10 −3 . More precisely, the stopping criterion is |E[log p(v i ; γ t )] − E[log p(v i ; γ t+1 )]| < τ which we can estimate as Dataset assumptions in GMM As in q-means ( Kerenidis et al., 2018 ), we have an assumption on the dataset that all elements of the mixture contribute proportionally to the total responsibility: i, e This is equivalent to requiring that clusters share a comparable amount of points in the "well-clusterability" assumption in q-means ( Kerenidis et al., 2018 ). It is also equivalent to assuming that θ j /θ l = Θ(1) ∀j, l ∈ [k]. For convenience, in this work, we also assume that the dataset is normalized such that the shortest vector has norm 1 and define η := max i v i 2 to be the maximum norm squared of a vector in the dataset. This is not a necessary requirement for our dataset, but it will simplify the analysis of our algorithm, allowing us to give strict bounds on the runtime.

Section Title: Preliminaries
  Preliminaries We assume a basic understanding of quantum computing, we recommend Nielsen and Chuang ( Nielsen & Chuang, 2002 ) for an introduction to the subject. A vector state |v for v ∈ R d is defined as |v = 1 v j∈[d] v j |j , where |j represents e j , the j th vector in the standard basis. The dataset is represented by a matrix V ∈ R n×d , i.e. each row is a vector v i ∈ R d for i ∈ [n] that represents a single data point. The cluster centers, called centroids, at time t are stored in the matrix C t ∈ R k×d , such that the j th row c t j for j ∈ [k] represents the centroid of the cluster C t j . We denote as V ≥τ the matrix i=0 σ i u i v T i where σ is the smallest singular value which is greater than τ . With nnz(V ) we mean the number of non-zero elements of the rows of V . When we say κ(V ) we mean the condition number of the matrix V , that is the ratio between the biggest and the smallest (non-zero) singular value. All the tools used in this work, like quantum algorithms for computing distances and linear algebraic operations, are reported in the Supplementary Material section.

Section Title: QUANTUM EXPECTATION-MAXIMIZATION FOR GMM
  QUANTUM EXPECTATION-MAXIMIZATION FOR GMM In this section, we present a quantum Expectation-Maximization algorithm to fit a GMM. The algorithm can also be adapted fit other mixtures models where the probability distributions belong to the exponential family. As the GMM is both intuitive and one of the most widely used mixture models, our results are presented for the GMM case. A robust version of the EM algorithm Similar to the work of ( Kerenidis et al., 2018 ), we define a ∆-robust version of the EM algorithm which we use to fit a GMM. The difference between this formalization and the original EM for GMM is simple. Here we explain the numerical error introduced in the training algorithm. Let γ t = (θ t , µ t , Σ t ) = (θ t , µ t 1 · · · µ t k , Σ t 1 · · · Σ t k ) a model fitted by the standard EM algorithm from γ 0 an initial guess of the parameters, i.e. γ t is the error-free model that standard EM would have returned after t iterations. Starting from the same choice of initial parameters γ 0 , fitting a GMM with the QEM algorithm with Quantum access to the mixture model As in the classical algorithm, we use some subroutines to compute the responsibilities and update our current guess of the parameters. The classical algorithm has clearly two separate steps for Expectation and Maximization. In contrast, the quantum algorithm uses a subroutine to compute the responsibilities inside the step that performs the Maximization, that is the subroutines for computing responsibilities are called multiple times during the quantum Maximization step. During the quantum Maximization step, the algorithm updates the model parameters γ t by creating quantum states corresponding to parameters γ t+1 and then recovering classical estimates for these parameters using quantum tomography or amplitude amplification. In order for this subroutines to be efficient, the values of the GMM are stored in QRAM data structures and are updated following each maximization step. Definition 1 (Quantum access to a GMM). We say that we have quantum access to a GMM if the dataset V ∈ R n×d and model parameters θ j ∈ R, µ j ∈ R d , Σ j ∈ R d×d for all j ∈ [k] are stored in QRAM data structures which allow us to perform in time O(polylog(d)) the following mappings:

Section Title: Algorithm 1 Quantum Expectation Maximization for GMM
  Algorithm 1 Quantum Expectation Maximization for GMM Require: Quantum access to a GMM model, precision parameters δ θ , δ µ , and threshold τ . Ensure: A GMM γ t that maximizes locally the likelihood (γ; V ), up to tolerance τ . 1: Use a heuristic described at the beginning of this section to determine an initial guess for γ 0 = (θ 0 , µ 0 , Σ 0 ), and store these parameters in the QRAM. Quantum initialization strategies exists, and are described in the Appendix.

Section Title: EXPECTATION
  EXPECTATION In this step of the quantum algorithm we are just showing how to compute efficiently the responsibilities as a quantum state. First, we compute the responsibilities in a quantum register, and then we show how to put them as amplitudes of a quantum state. We start by a classical algorithm used to efficiently approximate the log-determinant of the covariance matrices of the data. At each iteration of Quantum Expectation-Maximization we need to compute the determinant of the updated covariance matrices, which is done thanks to Lemma 3.1. We will see from the error analysis that in order to get an estimate of the GMM, we need to call Lemma 3.1 with precision for which the runtime of Lemma 3.1 gets subsumed by the running time of finding the updated covariance matrices through 3.7. Thus, we do not explicitly write the time to compute the determinant from now on in the algorithm and when we say that we update Σ we include an update on the estimate of log(det(Σ)) as well. Lemma 3.1 (Determinant evaluation). There is an algorithm that, given as input a matrix Σ and a parameter 0 < δ < 1, outputs an estimate log(det(Σ)) such that |log(det(Σ)) − log(det(Σ))| ≤ with probability 1 − δ in time: Now we can state the main brick used to compute the responsability: a quantum algorithm for evaluating the exponent of a Gaussian distribution. Lemma 3.2 (Quantum Gaussian Evaluation). Suppose we have stored in the QRAM a matrix V ∈ R n×d , the centroid µ ∈ R d and the covariance matrix Σ ∈ R d×d of a multivariate Gaussian distribution φ(v|µ, Σ), as well as an estimate for log(det(Σ)). Then for 1 > 0, there exists a quantum algorithm that with probability 1 − γ performs the mapping, Using controlled operations it is simple to extend the previous Theorem to work with multiple Gaussians distributions (µ j , Σ j ). That is, we can control on a register |j to do |j |i |0 → |j |i |φ(v i |µ j , Σ j ) . In the next Lemma we will see how to obtain the responsibilities r ij using the previous Theorem and standard quantum circuits for doing arithmetic, controlled rotations, and amplitude amplification. The Lemma is stated in a general way, to be used with any probability distributions that belong to an exponential family. Lemma 3.3 (Calculating responsibilities). Suppose we have quantum access to a GMM with parameters γ t = (θ t , µ t , Σ t ). There are quantum algorithms that can: 1. Perform the mapping |i |j |0 → |i |j |r ij such that |r ij − r ij | ≤ 1 with probability 1 − γ in time: 2. For a given j ∈ [k], construct state |R j such that |R j − 1 √ Zj n i=0 r ij |i < 1 where Z j = n i=0 r 2 ij with high probability in time:

Section Title: MAXIMIZATION
  MAXIMIZATION Now we need to get a new estimate for the parameters of our model. This is the idea: at each iteration we recover the new parameters of the model as quantum states, and then recover it using tomography, amplitude estimation, or sampling. Once the new model has been recovered, we update the QRAM such that we get quantum access to the model γ t+1 . The possibility to estimate θ comes from a call to the unitary we built to compute the responsibilities, and postselection. Lemma 3.4 (Computing θ t+1 ). We assume quantum access to a GMM with parameters γ t and let δ θ > 0 be a precision parameter. There exists an algorithm that estimates θ t+1 ∈ R k such that θ t+1 − θ t+1 ≤ δ θ in time We use quantum linear algebra to transform the uniform superposition of responsibilities of the j-th mixture into the new centroid of the j-th Gaussian. Let R t j ∈ R n be the vector of responsibilities for a Gaussian j at iteration t. The following claim relates the vectors R t j to the centroids µ t+1 j . Claim 3.5. Let R t j ∈ R n be the vector of responsibilities of the points for the Gaussian j at time t, i.e. (R t j ) i = r t ij . The proof is straightforward. Lemma 3.6 (Computing µ t+1 j ). We assume we have quantum access to a GMM with parameters γ t . For a pre- cision parameter δ µ > 0, there is a quantum algorithm that calculates {µ j t+1 } k j=1 such that for all From the ability to calculate responsibility and indexing the centroids, we derive the ability to reconstruct the covari- ance matrix of the Gaussians as well. Again, we use quantum linear algebra subroutines and tomography to recover an approximation of each Σ j . Recall that we have defined the matrix V ∈ R n×d 2 where the i-th row of V is defined as vec[v i v T i ]. For this Lemma, we assume to have the matrix stored in the QRAM. This is a reasonable assumption as the quantum states corresponding to the rows of V can be prepared as |i |0 |0 → |i |v i |v i , using twice the procedure for creating the rows of V . Lemma 3.7 (Computing Σ t+1 j ). We assume we have quantum access to a GMM with parameters γ t . We also have computed estimates µ j t+1 of all centroids such that µ j t+1 − µ t+1 j ≤ δ µ for precision parameter δ µ > 0. Then, there exists a quantum algorithm that outputs estimates for the new covariance matrices {Σ

Section Title: QUANTUM ESTIMATION OF LOG-LIKELIHOOD
  QUANTUM ESTIMATION OF LOG-LIKELIHOOD Now we are going to show how it is possible to get an estimate of the log-likelihood using a quantum procedure and access to a GMM model. A good estimate is crucial, as it is used as stopping criteria for the quantum algorithm as well. Classically, we stop to iterate the EM algorithm when | (γ t ; V ) − (γ t+1 ; V )| < n , or equivalently, we can set a tolerance on the average increase in log probability: Lemma 3.8 (Quantum estimation of likelihood). We assume we have quantum access to a GMM with parameters γ t . For τ > 0, there exists a quantum algorithm that estimates E[p(v i ; γ t )] with absolute error τ in time Putting together all the previous Lemmas, we write the main result of the work. Theorem 3.9 (QEM for GMM). We assume we have quantum access to a GMM with parameters γ t . For parameters δ θ , δ µ , τ > 0, the running time of one iteration of the Quantum Expectation-Maximization (QEM) algorithm is For the range of parameters of interest, the running time is dominated by T Σ . The proof follows directly from the previous lemmas. Note that the cost of the whole algorithm is given by repeating the Estimation and the Maximization steps several times, until the threshold on the log-likelihood is reached. Note also that the expression of the runtime can be simplified from the observation that the cost of performing tomography on the covariance matrices Σ j dominates the cost.

Section Title: EXPERIMENTAL RESULTS
  EXPERIMENTAL RESULTS In this section, we present the results of some experiments on real datasets to bound the condition number and the other parameters of the runtime. Let's discuss the value of κ(Σ), κ(V ), µ(Σ), and µ(V ). We can thresholding the condition number by discarding small singular values of the matrix, as used in quantum linear algebra, might be advantageous. This is indeed done often in classical machine learning models, since discarding the eigenvalues smaller than a certain threshold might even improve upon the metric under consideration (i.e. often the accuracy), and is a form of regularization ( Murphy, 2012 , Section 6.5). This is equivalent to limiting the eccentricity of the Gaussians. We can have a similar consideration on the condition number of the dataset κ(V ). As shown before, the condition number of the matrix V appearing in Lemma 3.2 is κ 2 (V ). Similarly, we can claim that the value of µ(V ) will not increase significantly as we add vectors to the training set. Remember that we have some choice in picking the function µ: in previous experiments we have found that choosing the maximum 1 norm of the rows of V lead to values of µ around 10, and also in this case we expect the samples of a well-clusterable ( Kerenidis et al., 2018 ) dataset to be constant. Also, µ is bounded by the Frobenius norm of V . In case the matrix V can be clustered with high-enough accuracy by k-means, it has been showed that the Frobenius norm of the matrix is proportional to √ k. Given that EM is a more powerful extension of k-means, we can rely on similar observations too. Usually, the number of features d is much more than the number of components in the mixture, i.e. d k, so we expect d 2 to dominate the k 3.5 term in the cost needed to estimate the mixing weights. This makes the runtime of a single iteration proportional to: As we said, the quantum running time saves the factor that depends on the number of samples and introduces a number of other parameters. Using our experimental results we can see that when the number of samples is large enough one can expect the quantum running time to be faster than the classical one. Note as well that one can expect to save some more factors from the quantum running time with a more careful analysis.

Section Title: Experiments
  Experiments In the algorithm, we need to set the parameters δ µ and δ θ to be small enough such that the likelihood is perturbed less than τ /4. We have reasons to believe that on well-clusterable data, the value of these parameters will be large enough, such as not to impact dramatically the runtime. A quantum version of k-means algorithm has already been simulated on real data under similar assumptions ( Kerenidis et al., 2018 ). There, the authors analyzed on the MNIST dataset the performances of q-means, the δ-resistant version of the classical k-means algorithm. The experiment concluded that, for datasets that are expected to be clustered nicely by this kind of clustering algorithms, the value of the parameters δ µ , δ θ did not decrease by increasing the number of samples nor the number of features. We expect similar behaviour in the EM case, namely that for large datasets the impact on the runtime of the errors (δ µ , δ θ ) does not cancel out the exponential gain in the dependence on the number of samples. For instance, in all the experiments of q-means ( Kerenidis et al., 2018 ) on the MNIST dataset the value of δ µ (which in their case was called just δ) has been between 0.2 and 0.5. The value of τ is usually (for instance in scikit-learn ( Pedregosa et al., 2011 ) ) chosen to be 10 −3 . The value of η has always been below 11. We also analyzed some other real-world dataset, which can be fitted well with the EM algorithm ( Reynolds et al., 2000 ; APPML; Voxforge.org) to perform speaker recognition: the task of recognizing a speaker from a voice sample, having access to a training set of recorded voices of all the possible speakers. Details of the measurements are reported in the Supplementary Material section, here we report only the results in  Table 1 . After this, we also experimented the impact of errors on the mixing weights in the accuracy of a ML estimate of a GMM by perturbing the trained model, by adding some random noise. With a value of δ θ = 0.035, δ µ = 0.5 we correctly classified 98.2% utterances. In conclusion, the experimental results suggest that the influence of the extra parameters in the quantum running time (condition thresholds, errors, etc.) is moderate. This allows us to be optimistic that, when quantum computers with quantum access to data become a reality, our algorithm (and improved versions that reduce even more the complexity with respect to these extra parameters) could be useful in analyzing large datasets.

Section Title: ACKNOWLEDGEMENTS
  ACKNOWLEDGEMENTS

```
