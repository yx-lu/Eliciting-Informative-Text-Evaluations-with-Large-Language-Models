<article article-type="research-article"><front><article-meta><title-group><article-title>Published as a conference paper at ICLR 2020 GAT: GENERATIVE ADVERSARIAL TRAINING FOR ADVERSARIAL EXAMPLE DETECTION AND ROBUST CLASSIFICATION</article-title></title-group><contrib-group content-type="author"><contrib contrib-type="person"><name><surname>Yin</surname><given-names>Xuwang</given-names></name></contrib><contrib contrib-type="person"><name><surname>Kolouri</surname><given-names>Soheil</given-names></name></contrib><contrib contrib-type="person"><name><surname>Rohde</surname><given-names>Gustavo K</given-names></name></contrib><contrib contrib-type="person"><xref ref-type="aff" rid="aff0" /></contrib></contrib-group><aff id="aff0"><institution content-type="orgname">Department of Electrical and Computer Engineering University of Virginia</institution></aff><aff id="aff1"><institution content-type="orgname">Information and Systems Sciences Laboratory HRL Laboratories</institution></aff><abstract><p>The vulnerabilities of deep neural networks against adversarial examples have become a significant concern for deploying these models in sensitive domains. Devising a definitive defense against such attacks is proven to be challenging, and the methods relying on detecting adversarial samples are only valid when the attacker is oblivious to the detection mechanism. In this paper we pro- pose a principled adversarial example detection method that can withstand norm- constrained white-box attacks. Inspired by one-versus-the-rest classification, in a K class classification problem, we train K binary classifiers where the i-th bi- nary classifier is used to distinguish between clean data of class i and adver- sarially perturbed samples of other classes. At test time, we first use a trained classifier to get the predicted label (say k) of the input, and then use the k-th binary classifier to determine whether the input is a clean sample (of class k) or an adversarially perturbed example (of other classes). We further devise a generative approach to detecting/classifying adversarial examples by interpreting each binary classifier as an unnormalized density model of the class-conditional data. We provide comprehensive evaluation of the above adversarial example de- tection/classification methods, and demonstrate their competitive performances and compelling properties. Code is available at https://github.com/ xuwangyin/GAT-Generative-Adversarial-Training 1 .</p></abstract></article-meta></front><body><sec><title>INTRODUCTION</title><p>Deep neural networks have become the staple of modern machine learning pipelines, achieving state- of-the-art performance on extremely difficult tasks in various applications such as computer vision (He et al., 2016), speech recognition (Amodei et al., 2016), machine translation (Vaswani et al., 2017), robotics (Levine et al., 2016), and biomedical image analysis (Shen et al., 2017). Despite their outstanding performance, these networks are shown to be vulnerable against various types of adversarial attacks, including evasion attacks (aka, inference or perturbation attacks) (Szegedy et al., 2013; Goodfellow et al., 2014; Carlini &amp; Wagner, 2017b; Su et al., 2019) and poisoning attacks (Liu et al., 2017; Shafahi et al., 2018). These vulnerabilities in deep neural networks hinder their deployment in sensitive domains including, but not limited to, health care, finances, autonomous driving, and defense-related applications and have become a major security concern.</p><p>Due to the mentioned vulnerabilities, there has been a recent surge toward designing defense mech- anisms against adversarial attacks (Gu &amp; Rigazio, 2014; Jin et al., 2015; Papernot et al., 2016b; Published as a conference paper at ICLR 2020 Bastani et al., 2016; Madry et al., 2017; Sinha et al., 2018), which has in turn motivated the de- sign of stronger attacks that defeat the proposed defenses (Goodfellow et al., 2014; Kurakin et al., 2016b;a; Carlini &amp; Wagner, 2017b; Xiao et al., 2018; Athalye et al., 2018; Chen et al., 2018; He et al., 2018). Besides, the proposed defenses have been shown to be limited and often not effective and easy to overcome (Athalye et al., 2018). Alternatively, a large body of work has focused on detection of adversarial examples (Bhagoji et al., 2017; Feinman et al., 2017; Gong et al., 2017; Grosse et al., 2017; Metzen et al., 2017; Hendrycks &amp; Gimpel, 2017; Li &amp; Li, 2017; Xu et al., 2017; Pang et al., 2018; Roth et al., 2019; Bahat et al., 2019; Ma et al., 2018; Zheng &amp; Hong, 2018; Tian et al., 2018). While training robust classifiers focuses on maintaining performance in presence of adversarial examples, adversarial detection only cares for detecting such examples.</p><p>The majority of the current detection mechanisms focus on non-adaptive threats, for which the attacks are not specifically tuned/tailored to bypass the detection mechanism, and the attacker is oblivious to the detection mechanism. In fact, Carlini &amp; Wagner (2017a) and Athalye et al. (2018) showed that the detection methods presented in (Bhagoji et al., 2017; Feinman et al., 2017; Gong et al., 2017; Grosse et al., 2017; Metzen et al., 2017; Hendrycks &amp; Gimpel, 2017; Li &amp; Li, 2017; Ma et al., 2018), are significantly less effective than their claims under adaptive attacks. Overall, current solutions are mostly heuristic approaches that cannot provide performance guarantees. In this paper we propose a detection mechanism that can with- stand adaptive attacks. The idea is to partition the input space into subspaces based on the original classifier's decision bound- ary, and then perform clean/adversarial example classification the subspaces. The binary classifier in each subspace is trained to dis- tinguish in-class samples from adversarially perturbed samples of other classes. At inference time, we first use the original classi- fier to get an input sample's predicted labelk, and then use th&#234; k-th binary classifier to identify whether the input is a clean sample (of classk) or an adversarially perturbed sample (of other classes). <xref ref-type="fig" rid="fig_0">Fig. 1</xref> provides a schematic illustration of the proposed approach.</p><p>Our specific contributions are: (1) We develop a principled adversarial example detection method that can withstand adaptive attacks. Empirically, our best models improve previous state-of-the-art mean L 2 distortion from 3.68 to 5.65 on MNIST dataset, and from 1.1 to 1.5 on CIFAR10 dataset.</p><p>(2) We study powerful and versatile generative classification models derived from our detection framework and demonstrate their competitive performances over discriminative robust classifiers. While discriminative robust classifiers are vulnerable to rubbish examples, inputs that have confident predictions under our models have interpretable features.</p></sec><sec><title>RELATED WORKS</title><p>Adversarial attacks Since the pioneering work of Szegedy et al. (2013), a large body of work has focused on designing algorithms that achieve successful attacks on neural networks (Goodfellow et al., 2014; Moosavi-Dezfooli et al., 2016; Kurakin et al., 2016b; Chen et al., 2018; Papernot et al., 2016a; Carlini &amp; Wagner, 2017b). More recently, iterative projected gradient descent (PGD), has been empirically identified as the most effective approach for performing norm constrained attacks, and the attack reasonably approximates the optimal attack (Madry et al., 2017).</p></sec><sec><title>Adversarial example detection</title><p>The majority of the methods developed for detecting adversarial attacks are based on the following core idea: given a trained K-class classifier, f : R d &#8594; {1...K}, and its corresponding clean training samples, D = {x i &#8712; R d } N i=1 , generate a set of adversarially attacked samples D &#8242; = {x &#8242; j &#8712; R d } M j=1 , and devise a mechanism to discriminate D from D &#8242; . For instance, Gong et al. (2017) use this exact idea and learn a binary classifier to distinguish the clean and adversarially perturbed sets. Similarly, Grosse et al. (2017) append a new "attacked" class to the classifier, f , and re-train a secured network that classifies clean images, x &#8712; D, into the K classes and all attacked images, x &#8242; &#8712; D &#8242; , to the (K + 1)-th class. In contrast to Gong et al. (2017); Grosse et al. (2017), which aim at detecting adversarial examples directly from the image content, Metzen et al. (2017) trained a binary classifier that receives as input the intermediate layer features extracted from the classifier network f , and distinguished D from D &#8242; based on such input features. More Published as a conference paper at ICLR 2020 importantly, Metzen et al. (2017) considered the so-called case of adaptive/dynamic adversary and proposed to harden the detector against such attacks using a similar adversarial training approach as in Goodfellow et al. (2014). Unfortunately, the mentioned detection methods are significantly less effective under an adaptive adversary equipped with a strong attack (Carlini &amp; Wagner, 2017a; Athalye et al., 2018).</p></sec><sec><title>PROPOSED APPROACH TO DETECTING ADVERSARIAL EXAMPLES</title><p>The proposed approach to detecting adversarial examples is based on the following simple idea. Assume there is an input sample x, and it is predicted ask by the classifier f , then x is either a true sample of classk (assuming no misclassification) or an adversarially perturbed sample of other classes. To determine which is the case we can use a binary classifier that is specifically trained to distinguish between clean samples of classk and adversarially perturbed samples of other classes. Becausek can be any one of the K class, we need to train a total of K binary classifier in order to have a complete solution. <xref ref-type="fig" rid="fig_1">Fig. 2</xref> provides a schematic illustration of the above detection idea. We next provide a mathematical justification and more details about how to train the detection model.</p><p>In a K(K &#8805; 2) class classification problem, given a dataset of clean samples</p><p>2 . Consider the following procedure to determine whether a sample x is an adversarial example (i.e., whether it comes from D or D &#8242; ):</p><p>First obtain the estimated class labelk = f (x), then use thek-th binary clas- sifier to predict: if dk(x) &gt;= 1 2 then categorize x as a clean sample, otherwise categorize it as an adversarial example.</p><p>This algorithm can be viewed as a binary classifier, and its accuracy is given by</p><p>Crucially, because the errors of individual binary classifiers are independent, maximizing Eq. (1) is equivalent to optimizing the performances of individual binary classifiers. d k solves the binary clas- sification problem of distinguishing between samples from D f k and samples from D &#8242; f k , and therefore Published as a conference paper at ICLR 2020 can be trained with a binary classification objective: &#952; * k = arg min &#952; k E x&#8764;D &#8242; f k L(d k (x; &#952; k ), 0) + E x&#8764;D f k L(d k (x; &#952; k ), 1) , (2) where L is a loss function that measures the discrepancy between d k 's output and the supplied label (e.g., the negative log likelihood loss). In order to harden d k against adaptive attacks, we follow Madry et al. (2017) and incorporate the adversary into the training objective:</p><p>The equality constraint f (x + &#948;) = k in Eq. (3) complicates the inner maximization. We observe that by dropping this constrain we have the following upper bound of the first loss term:</p><p>Because we are minimizing L(d k (x + &#948;; &#952; k ), 0), we can instead minimizing this upper bound, which gives us the unconstrained objective</p><p>We can further simply this objective by using the fact that when D is used as the training set, f can overfit on D such that D \k = {x i : y i &#824; = k} and D k are respectively good approximations of D f \k and D f k :</p><p>In words, each binary classifier is trained using clean samples of a particular class and adversarial examples (with respect to d k ) created from samples of other class. The inner maximization is solved using the PGD attack (Madry et al., 2017). We use the negative log likelihood loss as L and minimize it using gradient-based optimization methods.</p><p>From a classification point of view, we can reformulate the above detection algorithm as a classifier that has a rejection option: given input x and its prediction labelk = f (x), if dk(x) &lt; T , then x is rejected, otherwise it's classified ask. We will refer to this classification model as an integrated classifier.</p></sec><sec><title>A GENERATIVE APPROACH TO ADVERSARIAL EXAMPLE DETECTION/CLASSIFICATION</title><p>The proposed approach makes use of a trained classifier f to get the predicted label, but f is not strictly necessary: we can use H = {d k } K k=1 in place of f to do classification. We can interpret H as an one-versus-the-rest (OVR) classifier. In a K class classification problem, a OVR classifier consists of K binary classifiers, with each one trained to solve a two-class problem of separating samples in a particular class from samples not in that class. H differs from a traditional OVR classifier in that d k is trained to distinguish between samples in class k and adversarially perturbed samples of other classes, but because the loss on adversarial inputs is an upper bound of the loss on clean samples, the binary classifier should also be able to separate samples of class k from clean samples of other classes. When H is viewed as an OVR classifier, the classification rule is</p><p>We can also interpret H as a generative classifier. Our experiments show that d k has a strong generative property: performing adversarial attacks on d k causes visual features of class k to appear in the attacked data (in some cases, the attacked data become a valid instance of class k). Although Published as a conference paper at ICLR 2020 a similar phenomenon is observed in standard adversarial training (Tsipras et al., 2018; Engstrom et al., 2019; Santurkar et al., 2019), the generative property of our model seems to be much stronger than that of a softmax adversarially robust classier (<xref ref-type="fig" rid="fig_3">Fig. 4</xref>, <xref ref-type="fig" rid="fig_5">Fig. 6</xref>, and <xref ref-type="fig" rid="fig_6">Fig. 7</xref>). These results motivate us to reinterpret d k as an unnormalized density model (i.e., an energy-based model (LeCun et al., 2006)) of the class-k data. This interpretation allows us to obtain the class-conditional probability of an input by: p(x|k) = exp(&#8722;E k (x)) Z k , (7) where E k (x) = &#8722;z d k (x), with z d k being the logit output of d k , and Z k = exp(&#8722;E k (x))dx (8) is an intractable normalizing constant known as the partition function. We can then apply the Bayes classification rule to obtain a generative classifier: H(x) = arg max k p(k|x) = arg max k p(x|k)p(k) p(x) = arg max k z d k (x), (9) where we have assumed all partition functions Z k , k = 1, ..., K and class priors p(k), k = 1, ..., K to be equal. Because we explicitly model p(x, k), we can use this quantity to reject low probability inputs which can be any samples that do not belong to class k. In this work we focus on the scenario where low probability inputs are adversarially perturbed samples of other classes and the rejected samples are considered as adversarial examples. Because d k (x) is computed by applying the logistic sigmoid function to z d k (x), and the logistic sigmoid function is a monotonically increasing function of its argument, the generative classifier (Eq. (9)) is equivalent to the OVR classifier (Eq. (6)).</p><p>In the following sections, we will use integrated detection to refer to the original detection approach where we make use an extra classifier f , and generative detection to refer to this alternative approach where we first use the generative classifier Eq. (9) to get the predicted labelk of an input x, and then use dk to determine whether x is adversarial input.</p></sec><sec><title>EVALUATION METHODOLOGY</title></sec><sec><title>ROBUSTNESS TEST</title><p>We first validate the robustness of individual binary classifiers by following the standard methodol- ogy for robustness testing: we train the binary classifier with PGD attack configured with a particular combination of step-size and number of steps, and then test the binary classifier's performance under PGD attacks configured with different combinations of step-sizes and number of steps. We use AUC (area under the ROC Curve) as the detection performance metric. AUC is an aggregated measure- ment of detection performance across a range of thresholds, and can be interpreted as the probability that the binary classifier assigns a higher score to a random positive sample than to a random negative example. For a given d k , the AUC is computed on the set {(x, 0) :</p></sec><sec><title>ADVERSARIAL EXAMPLE DETECTION PERFORMANCE</title><p>Having validated the robustness of individual binary classifier, we evaluate the overall performance of the proposed approach to detecting adversarial examples. According to the detection algorithm, we first obtain the predicted labelk = f (x), and then use thek-th binary classifier's logit output to predict: if z dk (x) &#8805; T , then x is a clean sample, otherwise it is an adversarially perturbed sample. We use D = {(x i , y i )} N i=1 to denote the test set that contains clean samples, and D &#8242; = {(x i + &#948; i , y i )} N i=1 to denote the corresponding perturbed test set. For a given T , we compute the true positive rate (TPR) on D and false positive rate (FPR) on D &#8242; (here, clean samples are in the positive class). These two metrics are respectively defined as</p><p>Published as a conference paper at ICLR 2020 and</p><p>We observe that for the norm ball constraint we considered in the experiments, not all perturbed samples can cause misclassification on f , so we use f (x) &#824; = y in the FPR definition to constrain that only adversarial inputs that actually cause misclassification can be counted as false positives. Given a clean sample x and its groundtruth label y, we consider three approaches to creating the corresponding adversarial example x &#8242; . Here we will focus on untargeted attacks.</p></sec><sec><title>Classifier attack</title><p>This attack corresponds to the scenario where the adversary is oblivious to the de- tection mechanism. Inspired by the CW attack (Carlini &amp; Wagner, 2017b), the adversarial example x &#8242; is computed by minimizing,</p><p>where z f (x &#8242; ) is the classifier's logit outputs.</p></sec><sec><title>Detector attack</title><p>In this scenario adversarial examples are produced by attacking only the detec- tor. We first construct a detection function H by aggregating the logit outputs of individual binary classifiers:</p><p>The adversarial example x &#8242; is then computed by minimizing</p><p>According to our detection rule, a low value of a binary classifier's logit output indicates the de- tection of an adversarial example, and therefore by minimizing the negative of the logit output we make the adversarial input harder to detect. H can also be used with the CW loss Eq. (12) or the cross-entropy loss, but we find the attack based on Eq. (14) to be most effective.</p></sec><sec><title>Combined attack</title><p>The combined attack is an adaptive method that considers both the classifier and the detector. We consider two loss functions for the combined attack. The first is based on the adaptive attack of Carlini &amp; Wagner (2017a) which has been shown to be effective against existing detection methods. We first construct a new detection function H with Eq. (13) and then use H's largest logit output max k&#824; =y z H (x) k (low value of this quantity indicates detection of an adversarial example) and the classifier logit outputs z f (x) to construct a new classifier g:</p><p>The adversarial example x &#8242; is then computed by minimizing the loss function</p><p>In practice we observe that the optimization of Eq. (16) tends to stuck at the point where max i&#824; =y z f (x &#8242; ) i keeps changing signs while max j&#824; =y z H (x) j staying as a large negative number (which indicates detection). In light of the above issues we derive a more effective attack by com- bining Eq. (12) and Eq. (14):</p><p>In words, if x &#8242; is not yet an adversarial example on f (case 1), optimize it for that goal, otherwise optimize it for evading the detection (case 2).</p><p>We note that the above three attacks are for the original detection approach (i.e., integrated detec- tion). The generative detection approach (Section 3.1) does not make use of f and we use Eq. (14) to create adversarial examples for generative detection.</p></sec><sec><title>ROBUST CLASSIFICATION PERFORMANCE</title><p>In robust classification, the accuracy of a softmax robust classifier is evaluated on the original test test (the standard accuracy) and the adversarially perturbed test set (the robust accuracy). Because the generative classifier comes with the reject option, we use slightly different metrics. On the clean test dataset D = {(x i , y i )} N i=1 , we define the standard accuracy as the fraction of samples that are correctly classified (f (x) = y) and at the same time not rejected (z dk (x) &#8805; T ):</p><p>In the adversarially perturbed test dataset D &#8242; = {(x i +&#948; * i , y i )} N i=1 , we will consider a data sample as properly handled when it is rejected (z dk (x) &lt; T ), regardless of whether it causes misclassification. In this way, only misclassified (f (x) &#824; = y) and unrejected (z dk (x) &#8805; T ) samples are counted as errors:</p><p>To compare different classifiers under the same metrics, we compute the error of a softmax robust classifier g on D &#8242; as</p><p>We respectively use Eq. (17) and Eq. (14) to compute the D &#8242; for the integrated classifier and the generative classifier.</p></sec><sec><title>EXPERIMENTS</title></sec><sec><title>MNIST</title><p>We train four detection models (each consists of ten binary classifiers) by using different combi- nations of p-norm and perturbation limit &#1013; (Table 5). The adversarial examples used for training and validation are computed using PGD attacks of different steps and step sizes (Table 5). At each step of PGD attack we use the Adam optimizer to perform gradient descent, both for L 2 -based and L &#8734; -based attacks. Appendix A.1 provides more training details.</p></sec><sec><title>Robustness results</title><p><xref ref-type="table" rid="tab_0">Table 1</xref> and Table 7 show that d 0 and d 1 are able to withstand PGD attacks configured with different steps and step-size, for both L 2 -based and L &#8734; -based attacks. The binary classifiers also exhibit robustness when the attack uses p-norm or perturbation limit that are different from those used for training the model (Table 8). The models are also robust when the attacks use multiple random restarts (Table 9).</p></sec><sec><title>Detection results</title><p>Fig. 3a shows the performances of integrated detection and generative detection under different attacks. Combined attack with Eq. (17) is the most effective attack against integrated Published as a conference paper at ICLR 2020 detection, and is much more effective than the combined attack with Eq. (16). Overall, genera- tive detection outperforms integrated detection when they are evaluated under their respective most effective attack. It is also interesting to note that when the adversarial examples are created by at- tacking only the classifier (Eq. (12)), integrated detection is able to perfectly detect these adversarial examples (see the red curve that overlaps the y-axis).</p><p>Given that generative detection is the most effective approach among the proposed approaches, we compare it with state-of-the-art detection methods (Carlini &amp; Wagner, 2017a). <xref ref-type="table" rid="tab_1">Table 2</xref> shows that generative detection outperforms the state-of-the-art method by large margins. Appendix B provides details about how the mean L 2 distortions are computed.</p></sec><sec><title>Classification results</title><p>Figure 3b shows the standard and robust classification performances of the proposed classifiers and a state-of-the-art softmax robust classifier (Madry et al., 2017). Our models provide the reject option that allows the user to find a balance between standard accuracy and robust error by adjusting the rejection threshold. We observe that a stronger attack (&#1013; = 0.4) breaks the softmax robust classifier (as indicated by the right red cross), while the generative classifier still exhibits robustness, even though both models are trained with the L &#8734; &#1013; = 0.3 constraint. <xref ref-type="fig" rid="fig_3">Figure 4</xref> shows perturbed samples produced by performing targeted attacks against the generative classifier and softmax robust classifier. The generative classifier's perturbation samples have distin- guishable visible features of the target class, indicating that individual binary classifiers have learned the class conditional distributions, and the perturbations have to change the semantics for a success- ful attack. In contrast, perturbations introduced by attacking the softmax robust classifier are not interpretable, even though they can cause high logit output of the target classes (see Figure 9 for the logit outputs distribution).</p></sec><sec><title>CIFAR10</title><p>On CIFAR10 we train a single detection model that consists of ten binary classifiers using L &#8734; &#1013; = 8 constrain PGD attack of steps 40 and step size 0.5 (note that the scale of &#1013; and step size here is 0-255, as opposed to 0-1 as in the case of MNIST). The softmax robust classifier (Madry et al., 2017) that we compare with is also trained with L &#8734; &#1013; = 8 constraint but with a different step size (Appendix C.2.2 provides a discussion on the effects of step size). Appendix A.2 provides the training details.</p></sec><sec><title>Robustness results</title><p><xref ref-type="table" rid="tab_2">Table 3</xref> shows that d 1 and d 2 can withstand PGD attacks configured with different steps and step-size. In Appendix C.2.1 we report random restart test results, cross-norm and cross-perturbation test results, and robustness test result for L 2 based models.</p></sec><sec><title>Detection results</title><p>Consistent with the MNIST result, <xref ref-type="fig" rid="fig_4">Fig. 5</xref> shows that combined attack with Eq. (17) is the most effective attack against integrated detection, and generative detection simi- larly outperforms integrated detection. <xref ref-type="table" rid="tab_3">Table 4</xref> shows that generative detection outperforms the state-of-the-art adversarial detection method.</p></sec><sec><title>Classification results</title><p>Contrary to MNIST's result, we did not observe a dramatic decrease in the softmax robust classifier's performance when we increase the perturbation limit to &#1013; = 12 (Fig. 5b). Integrated classification can reach the standard accuracy of a regular classifier, but at the cost of significantly increased error on the perturbed set. <xref ref-type="fig" rid="fig_5">Fig. 6</xref> shows some perturbed samples produced by attacking the generative classifier and robust classifier. While these two classifiers have similar errors on the perturbed set, samples produced by attacking the generative classifier have more visible features of the attacked classes, suggesting that the adversary needs to change more semantic to cause the same error. <xref ref-type="fig" rid="fig_6">Fig. 7</xref> and Fig. 11 demonstrate that unrecognizable images are able to cause high logit outputs of the softmax robust classifier. This phenomenon highlights a major defect of the softmax robust classifier: they can be easily fooled by unrecognizable inputs (Nguyen et al., 2015; Goodfellow et al., 2014; Schott et al., 2018). In contrast, samples that cause high logit outputs of the generative classifier all have clear semantic meaning. In Figure 14 we present image synthesis results using L &#8734; &#1013; = 16 constrained detectors. In Appendix D we provide Gaussian noise attack results and a discussion about the interpretability of the generative classification approach.</p></sec><sec><title>CONCLUSION</title><p>We studied the problem of adversarial example detection under the robust optimization frame- work and proposed a novel detection method that can withstand adaptive attacks. Our formula- tion leads to a new generative modeling technique which we called generative adversarial training (GAT). GAT's capability to learn class conditional distributions further gives rise to generative de- tection/classification approaches that show competitive performance and improved interpretability.</p></sec><sec id="figures"><title>Figures</title><fig id="fig_0"><object-id>fig_0</object-id><label>Figure 1:</label><caption><title>Figure 1:</title><p>A conceptual vi- sualization of the proposed adversarial example detection mechanism.</p></caption><graphic /></fig><fig id="fig_1"><object-id>fig_1</object-id><label>Figure 2:</label><caption><title>Figure 2:</title><p>A schematic illustration of the proposed method for determining whether an input sample x (represented by the gray star) is an adversarial example. The first figure shows the case where x is predicted by f as class 1 and then x is identified as an adversarial example by D 1 . The following two figures shows the other two cases where x is respectively predicted as class 2 and class 3 and then D 2 and D 3 is respectively used to predict whether x is an adversarial example.</p></caption><graphic /><graphic /><graphic /><graphic /><graphic /><graphic /><graphic /><graphic /><graphic /><graphic /><graphic /><graphic /><graphic /><graphic /><graphic /><graphic /><graphic /><graphic /><graphic /><graphic /><graphic /><graphic /><graphic /><graphic /><graphic /><graphic /><graphic /><graphic /><graphic /><graphic /><graphic /><graphic /><graphic /><graphic /><graphic /><graphic /><graphic /><graphic /><graphic /><graphic /><graphic /><graphic /><graphic /><graphic /><graphic /><graphic /><graphic /><graphic /></fig><table-wrap id="tab_0"><label>Table 1:</label><caption><title>Table 1:</title><p>AUC scores of the first two binary classifiers (d 1 , d 2 ) tested with different configurations of PGD attacks. In each step of the PGD attack we use the Adam optimizer to perform gradient descent.</p></caption><table><tbody><tr><td /></tr></tbody></table></table-wrap><table-wrap id="tab_1"><label>Table 2:</label><caption><title>Table 2:</title><p>Mean L 2 distortion (higher is better) of perturbed samples when the detection method has 1.0 FPR on the perturbed MNIST test set and 0.95 TPR on the clean MNIST test set.</p></caption><table><tbody><tr><td /></tr></tbody></table></table-wrap><fig id="fig_2"><object-id>fig_2</object-id><label>Figure 3:</label><caption><title>Figure 3:</title><p>(a) Performances of integrated detection and generative detection under L &#8734; &#1013; = 0.3 constrained attacks. (b) Performances of the integrated classifier and generative classifier under L &#8734; &#1013; = 0.3 constrained and L &#8734; &#1013; = 0.4 constrained attacks. The performances of the softmax robust classifier (Madry et al., 2017) (accuracy 0.984, error 0.08 at &#1013; = 0.3, and accuracy 0.984, error 0.941 at &#1013; = 0.4) are marked with red crosses. PGD attack steps 100, step size 0.01.</p></caption><graphic /></fig><fig id="fig_3"><object-id>fig_3</object-id><label>Figure 4:</label><caption><title>Figure 4:</title><p>Clean samples and corresponding perturbed samples produced by performing a targeted attack against the generative classifier and robust classifier (Madry et al., 2017). Targets from top row to bottom row are digit class from 0 to 9. We perform the targeted attack by maximizing the logit output of the targeted class, using L &#8734; &#1013; = 0.4 constrained PGD attack of steps 100 and step size 0.01. Both classifiers are trained with L &#8734; &#1013; = 0.3 constraint.</p></caption><graphic /><graphic /><graphic /></fig><table-wrap id="tab_2"><label>Table 3:</label><caption><title>Table 3:</title><p>AUC scores of the first two CIFAR10 L &#8734; &#1013; = 8 binary classifiers (d 1 , d 2 ) under L &#8734; &#1013; = 8 constrained PGD attacks of different steps and step-size.</p></caption><table><tbody><tr><td /></tr></tbody></table></table-wrap><table-wrap id="tab_3"><label>Table 4:</label><caption><title>Table 4:</title><p>CIFAR10 mean L 2 distortion (higher is better) of perturbed samples when the detection method has 1.0 FPR on perturbed set and 0.95 TPR on the clean set. Appendix B provides details about how the mean L 2 distances are computed.</p></caption><table><tbody><tr><td /></tr></tbody></table></table-wrap><fig id="fig_4"><object-id>fig_4</object-id><label>Figure 5:</label><caption><title>Figure 5:</title><p>(a) Performances of generative detection and integrated detection under L &#8734; &#1013; = 8 attack. (b) Performances of integrated classifier (discussed in Section 4.3) and generative classifier under L &#8734; &#1013; = 8 constrained and L &#8734; &#1013; = 12 constrained attacks. The performances of the robust classi- fier (Madry et al., 2017) (accuracy 0.8735, error 0.5311 at &#1013; = 8, and accuracy 0.8735, error 0.7087 at &#1013; = 12) are annotated. PGD attack step size 2.0, steps 20 for &#1013; = 8, and 30 for &#1013; = 12.</p></caption><graphic /></fig><fig id="fig_5"><object-id>fig_5</object-id><label>Figure 6:</label><caption><title>Figure 6:</title><p>Clean samples and corresponding perturbed samples by performing targeted attack against the generative classifier and robust classifier (Madry et al., 2017). The targeted attack is performed by maximizing the logit output of the targeted class. We use L &#8734; &#1013; = 12 constrained PGD attack of steps 30 and step size 2.0 to produce these samples.</p></caption><graphic /><graphic /><graphic /></fig><fig id="fig_6"><object-id>fig_6</object-id><label>Figure 7:</label><caption><title>Figure 7:</title><p>Images generated from class conditional Gaussian noise by performing targeted attack against the generative classifier and robust classifier. We use PGD attack of steps 60 and step size 0.5 &#215; 255 to perform L 2 &#1013; = 30 &#215; 255 constrained attack (same as Santurkar et al. (2019). The Gaussian noise inputs from which these two plots are generated are the same. Samples not selected.</p></caption><graphic /><graphic /></fig></sec></body><back><sec><p>Tramer et al. (2020).</p></sec></back></article>