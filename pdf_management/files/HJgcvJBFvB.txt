Title:
```
Published as a conference paper at ICLR 2020 NETWORK RANDOMIZATION: A SIMPLE TECHNIQUE FOR GENERALIZATION IN DEEP REINFORCEMENT LEARNING
```
Abstract:
```
Deep reinforcement learning (RL) agents often fail to generalize to unseen envi- ronments (yet semantically similar to trained agents), particularly when they are trained on high-dimensional state spaces, such as images. In this paper, we pro- pose a simple technique to improve a generalization ability of deep RL agents by introducing a randomized (convolutional) neural network that randomly perturbs input observations. It enables trained agents to adapt to new domains by learning robust features invariant across varied and randomized environments. Further- more, we consider an inference method based on the Monte Carlo approxima- tion to reduce the variance induced by this randomization. We demonstrate the superiority of our method across 2D CoinRun, 3D DeepMind Lab exploration and 3D robotics control tasks: it significantly outperforms various regulariza- tion and data augmentation methods for the same purpose. Code is available at github.com/pokaxpoka/netrand.
```

Figures/Tables Captions:
```
Figure 1: (a) Examples of randomized inputs (color values in each channel are normalized for visualization) generated by re-initializing the parameters of a random layer. Examples of seen and unseen environments on (b) CoinRun, (c) DeepMind Lab, and (d) Surreal robotics control.
Figure 2: Samples of dogs vs. cats dataset. The training set consists of bright dogs and dark cats, whereas the test set consists of dark dogs and bright cats.
Figure 3: (a) We collect multiple episodes from various environments by human demonstrators and visualize the hidden representation of trained agents optimized by (b) PPO and (c) PPO + ours constructed by t-SNE, where the colors of points indicate the environments of the corresponding observations. (d) Average success rates for varying number of MC samples.
Figure 4: Visualization of activation maps via Grad-CAM in seen and unseen environments in the small-scale CoinRun. Images are aligned with similar states from various episodes for comparison.
Figure 5: The performances of trained agents in unseen environments under (a) large-scale CoinRun, (b) DeepMind Lab and (c) Surreal robotics control. The solid/dashed lines and shaded regions represent the mean and standard deviation, respectively.
Table 1: The classification accuracy (%) on dogs vs. cats dataset. The results show the mean and standard deviation averaged over three runs and the best result is indicated in bold.
Table 2: Success rate (%) and cycle-consistency (%) after 100M timesteps in small-scale CoinRun. The results show the mean and standard deviation averaged over three runs and the best results are indicated in bold.
Table 3: Comparison with domain randomization. The results show the mean and standard deviation averaged over three runs and the best results are indicated in bold.
```

Main Content:
```

Section Title: INTRODUCTION
  INTRODUCTION Deep reinforcement learning (RL) has been applied to various applications, including board games (e.g., Go ( Silver et al., 2017 ) and Chess ( Silver et al., 2018 )), video games (e.g., Atari games ( Mnih et al., 2015 ) and StarCraft ( Vinyals et al., 2017 )), and complex robotics control tasks ( Tobin et al., 2017 ;  Ren et al., 2019 ). However, it has been evidenced in recent years that deep RL agents of- ten struggle to generalize to new environments, even when semantically similar to trained agents ( Farebrother et al., 2018 ;  Zhang et al., 2018b ;  Gamrian & Goldberg, 2019 ;  Cobbe et al., 2019 ). For example, RL agents that learned a near-optimal policy for training levels in a video game fail to perform accurately in unseen levels ( Cobbe et al., 2019 ), while a human can seamlessly generalize across similar tasks. Namely, RL agents often overfit to training environments, thus the lack of gen- eralization ability makes them unreliable in several applications, such as health care ( Chakraborty & Murphy, 2014 ) and finance ( Deng et al., 2016 ). The generalization of RL agents can be characterized by visual changes ( Cobbe et al., 2019 ;  Gam- rian & Goldberg, 2019 ), different dynamics ( Packer et al., 2018 ), and various structures (Beattie et al., 2016;  Wang et al., 2016 ). In this paper, we focus on the generalization across tasks where the trained agents take various unseen visual patterns at the test time, e.g., different styles of back- grounds, floors, and other objects (see  Figure 1 ). We also found that RL agents completely fail due to small visual changes 1 because it is challenging to learn generalizable representations from high-dimensional input observations, such as images. To improve generalization, several strategies, such as regularization ( Farebrother et al., 2018 ;  Zhang et al., 2018b ;  Cobbe et al., 2019 ) and data augmentation ( Tobin et al., 2017 ;  Ren et al., 2019 ), have been proposed in the literature (see Section 2 for further details). In particular,  Tobin et al. (2017)  showed that training RL agents in various environments generated by randomizing rendering in a simulator improves the generalization performance, leading to a better performance in real environments. This implies that RL agents can learn invariant and robust representations if diverse Published as a conference paper at ICLR 2020 input observations are provided during training. However, their method is limited by requiring a physics simulator, which may not always be available. This motivates our approach of developing a simple and plausible method applicable to training deep RL agents. The main contribution of this paper is to develop a simple randomization technique for improving the generalization ability across tasks with various unseen visual patterns. Our main idea is to utilize random (convolutional) networks to generate randomized inputs (see Figure 1(a)), and train RL agents (or their policy) by feeding them into the networks. Specifically, by re-initializing the parameters of random networks at every iteration, the agents are encouraged to be trained under a broad range of perturbed low-level features, e.g., various textures, colors, or shapes. We discover that the proposed idea guides RL agents to learn generalizable features that are more invariant in unseen environments (see  Figure 3 ) than conventional regularization ( Srivastava et al., 2014 ;  Ioffe & Szegedy, 2015 ) and data augmentation ( Cobbe et al., 2019 ;  Cubuk et al., 2019 ) techniques. Here, we also provide an inference technique based on the Monte Carlo approximation, which stabilizes the performance by reducing the variance incurred from our randomization method at test time. We demonstrate the effectiveness of the proposed method on the 2D CoinRun ( Cobbe et al., 2019 ) game, the 3D DeepMind Lab exploration task (Beattie et al., 2016), and the 3D robotics control task (Fan et al., 2018). For evaluation, the performance of the trained agents is measured in unseen environments with various visual and geometrical patterns (e.g., different styles of backgrounds, ob- jects, and floors), guaranteeing that the trained agents encounter unseen inputs at test time. Note that learning invariant and robust representations against such changes is essential to generalize to unseen environments. In our experiments, the proposed method significantly reduces the generalization gap in unseen environments unlike conventional regularization and data augmentation techniques. For example, compared to the agents learned with the cutout ( DeVries & Taylor, 2017 ) data augmenta- tion methods proposed by  Cobbe et al. (2019) , our method improves the success rates from 39.8% to 58.7% under 2D CoinRun, the total score from 55.4 to 358.2 for 3D DeepMind Lab, and the total score from 31.3 to 356.8 for the Surreal robotics control task. Our results can be influential to study other generalization domains, such as tasks with different dynamics ( Packer et al., 2018 ), as well as solving real-world problems, such as sim-to-real transfer ( Tobin et al., 2017 ).

Section Title: RELATED WORK
  RELATED WORK Generalization in deep RL. Recently, the generalization performance of RL agents has been in- vestigated by splitting training and test environments using random seeds ( Zhang et al., 2018a ) and distinct sets of levels in video games ( Machado et al., 2018 ;  Cobbe et al., 2019 ). Regularization is one of the major directions to improve the generalization ability of deep RL algorithms.  Fare- brother et al. (2018)  and  Cobbe et al. (2019)  showed that regularization methods can improve the generalization performance of RL agents using various game modes of Atari ( Machado et al., 2018 ) and procedurally generated arcade environments called CoinRun, respectively. On the other hand, data augmentation techniques have also been shown to improve generalization.  Tobin et al. (2017)  proposed a domain randomization method to generate simulated inputs by randomizing rendering in the simulator. Motivated by this,  Cobbe et al. (2019)  proposed a data augmentation method by modifying the cutout method ( DeVries & Taylor, 2017 ). Our method can be combined with the prior methods to further improve the generalization performance.

Section Title: Published as a conference paper at ICLR 2020
  Published as a conference paper at ICLR 2020 Random networks for deep RL. Random networks have been utilized in several approaches for different purposes in deep RL.  Burda et al. (2019)  utilized a randomly initialized neural network to define an intrinsic reward for visiting unexplored states in challenging exploration problems. By learning to predict the reward from the random network, the agent can recognize unexplored states.  Osband et al. (2018)  studied a method to improve ensemble-based approaches by adding a randomized network to each ensemble member to improve the uncertainty estimation and efficient exploration in deep RL. Our method is different because we introduce a random network to improve the generalization ability of RL agents.

Section Title: Transfer learning
  Transfer learning Generalization is also closely related to transfer learning ( Parisotto et al., 2016 ;  Rusu et al., 2016a ;b), which is used to improve the performance on a target task by transferring the knowledge from a source task. However, unlike supervised learning, it has been observed that fine- tuning a model pre-trained on the source task for adapting to the target task is not beneficial in deep RL. Therefore,  Gamrian & Goldberg (2019)  proposed a domain transfer method using generative adversarial networks ( Goodfellow et al., 2014 ) and  Farebrother et al. (2018)  utilized regularization techniques to improve the performance of fine-tuning methods.  Higgins et al. (2017)  proposed a multi-stage RL, which learns to extract disentangled representations from the input observation and then trains the agents on the representations. Alternatively, we focus on the zero-shot performance of each agent at test time without further fine-tuning of the agent's parameters.

Section Title: NETWORK RANDOMIZATION TECHNIQUE FOR GENERALIZATION
  NETWORK RANDOMIZATION TECHNIQUE FOR GENERALIZATION We consider a standard reinforcement learning (RL) framework where an agent interacts with an environment in discrete time. Formally, at each timestep t, the agent receives a state s t from the environment 2 and chooses an action a t based on its policy π. The environment returns a reward r t and the agent transitions to the next state s t+1 . The return R t = ∞ k=0 γ k r t+k is the total accumulated rewards from timestep t with a discount factor γ ∈ [0, 1). RL then maximizes the expected return from each state s t .

Section Title: TRAINING AGENTS USING RANDOMIZED INPUT OBSERVATIONS
  TRAINING AGENTS USING RANDOMIZED INPUT OBSERVATIONS We introduce a random network f with its parameters φ initialized with a prior distribution, e.g., Xavier normal distribution ( Glorot & Bengio, 2010 ). Instead of the original input s, we train an agent using a randomized input s = f (s; φ). For example, in the case of policy-based methods, 3 the parameters θ of the policy network π are optimized by minimizing the following policy gradient objective function: L random policy = E (st,at,Rt)∈D − log π (a t |f (s t ; φ) ; θ) R t , (1) where D = {(s t , a t , R t )} is a set of past transitions with cumulative rewards. By re-initializing the parameters φ of the random network per iteration, the agents are trained using varied and random- ized input observations (see Figure 1(a)). Namely, environments are generated with various visual patterns, but with the same semantics by randomizing the networks. Our agents are expected to adapt to new environments by learning invariant representation (see  Figure 3  for supporting experiments). To learn more invariant features, the following feature matching (FM) loss between hidden features from clean and randomized observations is also considered: L random FM = E st∈D ||h (f (s t ; φ); θ) − h (s t ; θ) || 2 , (2) where h(·) denotes the output of the penultimate layer of policy π. The hidden features from clean and randomized inputs are combined to learn more invariant features against the changes in the input observations. 4 Namely, the total loss is: L random = L random policy + βL random FM , (3) where β > 0 is a hyper-parameter. The full procedure is summarized in Algorithm 1 in Appendix M. Details of the random networks. We propose to utilize a single-layer convolutional neural net- work (CNN) as a random network, where its output has the same dimension with the input (see Appendix D for additional experimental results on the various types of random networks). To re- initialize the parameters of the random network, we utilize the following mixture of distributions: P (φ) = αI(φ = I) + (1 − α)N 0; 2 nin+nout , where I is an identity kernel, α ∈ [0, 1] is a positive constant, N denotes the normal distribution, and n in , n out are the number of input and output channels, respectively. Here, clean inputs are used with the probability α because training only randomized inputs can complicate training. The Xavier normal distribution ( Glorot & Bengio, 2010 ) is used for randomization because it maintains the variance of the input s and the randomized input s. We empirically observe that this distribution stabilizes training.

Section Title: Removing visual bias
  Removing visual bias To confirm the desired effects of our method, we conduct an image classi- fication experiment on the dogs and cats database from Kaggle. 5 Following the same setup as  Kim et al. (2019) , we construct datasets with an undesirable bias as follows: the training set consists of bright dogs and dark cats while the test set consists of dark dogs and bright cats (see Appendix H for further details). A classifier is expected to make a decision based on the undesirable bias, (e.g., brightness and color) since CNNs are biased towards texture or color, rather than shape ( Geirhos et al., 2019 ).  Table 1  shows that ResNet-18 ( He et al., 2016 ) does not generalize effectively due to overfitting to an undesirable bias in the training data. To address this issue, several image processing methods ( Cubuk et al., 2019 ), such as grayout (GR), cutout (CO; DeVries & Taylor 2017), inversion (IV), and color jitter (CJ), can be applied (see Appendix C for further details). However, they are not effective in improving the generalization ability, compared to our method. This confirms that our approach makes DNNs capture more desired and meaningful information such as the shape by changing the visual appearance of attributes and entities in images while effectively keeping the semantic information. Prior sophisticated methods ( Ganin et al., 2016 ;  Kim et al., 2019 ) require additional information to eliminate such an undesired bias, while our method does not. 6 Although we mainly focus on RL applications, our idea can also be explorable in this direction.

Section Title: INFERENCE METHODS FOR SMALL VARIANCE
  INFERENCE METHODS FOR SMALL VARIANCE Since the parameter of random networks is drawn from a prior distribution P (φ), our policy is modeled by a stochastic neural network: π(a|s; θ) = E φ π (a|f (s; φ) ; θ) . Based on this inter- pretation, our training procedure (i.e., randomizing the parameters) consists of training stochastic models using the Monte Carlo (MC) approximation (with one sample per iteration). Therefore, at the inference or test time, an action a is taken by approximating the expectations as follows: π (a|s; θ) 1 M M m=1 π a f s; φ (m) ; θ , where φ (m) ∼ P (φ) and M is the number of MC samples. In other words, we generate M random inputs for each observation and then aggregate their decisions. The results show that this estimator improves the performance of the trained agents by approximating the posterior distribution more accurately (see Figure 3(d)).

Section Title: EXPERIMENTS
  EXPERIMENTS In this section, we demonstrate the effectiveness of the proposed method on 2D CoinRun ( Cobbe et al., 2019 ), 3D DeepMind Lab exploration (Beattie et al., 2016), and 3D robotics control task (Fan et al., 2018). To evaluate the generalization ability, we measure the performance of trained agents in unseen environments which consist of different styles of backgrounds, objects, and floors. Due to the space limitation, we provide more detailed experimental setups and results in the Appendix.

Section Title: BASELINES AND IMPLEMENTATION DETAILS
  BASELINES AND IMPLEMENTATION DETAILS For CoinRun and DeepMind Lab experiments, similar to  Cobbe et al. (2019) , we take the CNN architecture used in IMPALA ( Espeholt et al., 2018 ) as the policy network, and the Proximal Policy Optimization (PPO) ( Schulman et al., 2017 ) method to train the agents. 7 At each timestep, agents are given an observation frame of size 64 × 64 as input (resized from the raw observation of size 320 × 240 as in the DeepMind Lab), and the trajectories are collected with the 256-step rollout for training. For Surreal robotics experiments, similar to Fan et al. (2018), the hybrid of CNN and long short-term memory (LSTM) architecture is taken as the policy network, and a distributed version of PPO (i.e., actors collect a massive amount of trajectories, and the centralized learner updates the model parameters using PPO) is used to train the agents. 8 We measure the performance in the unseen environment for every 10M timesteps and report the mean and standard deviation across three runs. Our proposed method, which augments PPO with random networks and feature matching (FM) loss (denoted PPO + ours), is compared with several regularization and data augmentation methods. As regularization methods, we compare dropout (DO;  Srivastava et al. 2014 ), L2 regularization (L2), and batch normalization (BN;  Ioffe & Szegedy 2015 ). For those methods, we use the hyperpa- rameters suggested in  Cobbe et al. (2019) , which are empirically shown to be effective: a dropout probability of 0.1 and a coefficient of 10 −4 for L2 regularization. We also consider various data augmentation methods: a variant of cutout (CO; DeVries & Taylor 2017) proposed in  Cobbe et al. (2019) , grayout (GR), inversion (IV), and color jitter (CJ) by adjusting brightness, contrast, and sat- uration (see Appendix C for more details). As an upper bound, we report the performance of agents trained directly on unseen environments, dented PPO (oracle). For our method, we use β = 0.002 for the weight of the FM loss, α = 0.1 for the probability of skipping the random network, M = 10 for MC approximation, and a single-layer CNN with the kernel size of 3 as a random network.

Section Title: EXPERIMENTS ON COINRUN
  EXPERIMENTS ON COINRUN

Section Title: Task description
  Task description In this task, an agent is located at the leftmost side of the map and the goal is to collect the coin located at the rightmost side of the map within 1,000 timesteps. The agent observes its surrounding environment in the third-person point of view, where the agent is always located at the center of the observation. CoinRun contains an arbitrarily large number of levels which are generated deterministically from a given seed. In each level, the style of background, floor, and obstacles is randomly selected from the available themes (34 backgrounds, 6 grounds, 5 agents, and 9 moving obstacles). Some obstacles and pitfalls are distributed between the agent and the coin, where a collision with them results in the agent's immediate death. We measure the success rates, which correspond to the number of collected coins divided by the number of played levels.

Section Title: Ablation study on small-scale environments
  Ablation study on small-scale environments First, we train agents on one level for 100M timesteps and measure the performance in unseen environments by only changing the style of the background, as shown in Figure 3(a). Note that these visual changes are not significant to the game's dynamics, but the agent should achieve a high success rate if it can generalize accurately. However,  Table 2  shows that all baseline agents fail to generalize to unseen environments, while they achieve a near-optimal performance in the seen environment. This shows that regularization techniques have no significant impact on improving the generalization ability. Even though data augmentation tech- niques, such as cutout (CO) and color jitter (CJ), slightly improve the performance, our proposed method is most effective because it can produce a diverse novelty in attributes and entities. Train- ing with randomized inputs can degrade the training performance, but the high expressive power of DNNs prevents from it. The performance in unseen environments can be further improved by optimizing the FM loss. To verify the effectiveness of MC approximation at test time, we measure the performance in unseen environments by varying the number of MC samples. Figure 3(d) shows the mean and standard deviation across 50 evaluations. The performance and its variance can be improved by increasing the number of MC samples, but the improvement is saturated around ten samples. Thus, we use ten samples for the following experiments.

Section Title: Embedding analysis
  Embedding analysis We analyze whether the hidden representation of trained RL agents exhibits meaningful abstraction in the unseen environments. The features on the penultimate layer of trained agents are visualized and reduced to two dimensions using t-SNE ( Maaten & Hinton, 2008 ).  Figure 3  shows the projection of trajectories taken by human demonstrators in seen and unseen environments (see Figure 17 in Appendix N for further results). Here, trajectories from both seen and unseen environments are aligned on the hidden space of our agents, while the baselines yield scattered and disjointed trajectories. This implies that our method makes RL agents capable of learning the invariant and robust representation. To evaluate the quality of hidden representation quantitatively, the cycle-consistency proposed in  Aytar et al. (2018)  is also measured. Given two trajectories V and U , v i ∈ V first locates its nearest neighbor in the other trajectory u j = arg min u∈U h(v i ) − h(u) 2 , where h(·) denotes the output of the penultimate layer of trained agents. Then, the nearest neighbor of u j in V is located, i.e., v k = arg min v∈V h(v) − h(u j ) 2 , and v i is defined as cycle-consistent if |i − k| ≤ 1, i.e., it can return to the original point. Note that this cycle-consistency implies that two trajectories are accurately aligned in the hidden space. Similar to  Aytar et al. (2018) , we also evaluate the three-way cycle-consistency by measuring whether v i remains cycle-consistent along both paths, V → U → J → V and V → J → U → V , where J is the third trajectory. Using the trajectories shown in Figure 3(a),  Table 2  reports the percentage of input observations in the seen environment (blue curve) that are cycle-consistent with unseen trajectories (red and green curves). Similar to the results shown in Figure 3(c), our method significantly improves the cycle-consistency compared to the vanilla PPO agent.

Section Title: Visual interpretation
  Visual interpretation To verify whether the trained agents can focus on meaningful and high- level information, the activation maps are visualized using Grad-CAM ( Selvaraju et al., 2017 ) by averaging activations channel-wise in the last convolutional layer, weighted by their gradients. As shown in  Figure 4 , both vanilla PPO and our agents make a decision by focusing on essential objects, such as obstacles and coins in the seen environment. However, in the unseen environment, the vanilla PPO agent displays a widely distributed activation map in some cases, while our agent does not. As a quantitative metric, we measure the entropy of normalized activation maps. Specifically, we first normalize activations σ t,h,w ∈ [0, 1], such that it represents a 2D discrete probability distribution at timestep t, i.e., H h=1 W w=1 σ t,h,w = 1. Then, we measure the entropy averaged over the timesteps as follows: − 1 T T t=1 H h=1 W w=1 σ t,h,w log σ t,h,w . Note that the entropy of the activation map quantitatively measures the frequency an agent focuses on salient components in its observation. Results show that our agent produces a low entropy on both seen and unseen environments (i.e., 2.28 and 2.44 for seen and unseen, respectively), whereas the vanilla PPO agent produces a low entropy only in the seen environment (2.77 and 3.54 for seen and unseen, respectively).

Section Title: Results on large-scale experiments
  Results on large-scale experiments Similar to  Cobbe et al. (2019) , the generalization ability by training agents is evaluated on a fixed set of 500 levels of CoinRun. To explicitly separate seen and unseen environments, half of the available themes are utilized (i.e., style of backgrounds, floors, agents, and moving obstacles) for training, and the performances on 1,000 different levels consisting of unseen themes are measured. 9 As shown in Figure 5(a), our method outperforms all baseline methods by a large margin. In particular, the success rates are improved from 39.8% to 58.7% compared to the PPO with cutout (CO) augmentation proposed in  Cobbe et al. (2019) , showing that our agent learns generalizable representations given a limited number of seen environments.

Section Title: EXPERIMENTS ON DEEPMIND LAB AND SURREAL ROBOTICS CONTROL
  EXPERIMENTS ON DEEPMIND LAB AND SURREAL ROBOTICS CONTROL

Section Title: Results on DeepMind Lab
  Results on DeepMind Lab We also demonstrate the effectiveness of our proposed method on DeepMind Lab (Beattie et al., 2016), which is a 3D game environment in the first-person point of view with rich visual inputs. The task is designed based on the standard exploration task, where a goal object is placed in one of the rooms in a 3D maze. In this task, agents aim to collect as many goal objects as possible within 90 seconds to maximize their rewards. Once the agent collects the goal object, it receives ten points and is relocated to a random place. Similar to the small-scale CoinRun experiment, agents are trained to collect the goal object in a fixed map layout and tested in unseen environments with only changing the style of the walls and floors. We report the mean and standard deviation of the average scores across ten different map layouts, which are randomly selected. Additional details are provided in Appendix G. Note that a simple strategy of exploring the map actively and recognizing the goal object achieves high scores because the maze size is small in this experiment. Even though the baseline agents achieve high scores by learning this simple strategy in the seen environment (see Figure 6(c) in Appendix A for learning curves), Figure 5(b) shows that they fail to adapt to the unseen environ- ments. However, the agent trained by our proposed method achieves high scores in both seen and unseen environments. These results show that our method can learn generalizable representations from high-dimensional and complex input observations (i.e., 3D environment).

Section Title: Results on Surreal robotics control
  Results on Surreal robotics control We evaluate our method in the Block Lifting task using the Surreal distributed RL framework (Fan et al., 2018): the Sawyer robot receives a reward if it succeeds to lift a block randomly placed on a table. We train agents on a single environment and test on five unseen environments with various styles of tables and blocks (see Appendix I for further details). Figure 5(c) shows that our method achieves a significant performance gain compared to all baselines in unseen environments while maintaining its performance in the seen environment (see Figure 13 in Appendix I), implying that our method can maintain essential properties, such as structural spatial features of the input observation.

Section Title: Comparison with domain randomization
  Comparison with domain randomization To further verify the effectiveness of our method, the vanilla PPO agents are trained by increasing the number of seen environments generated by ran- domizing rendering in a simulator, while our agent is still trained in a single environment (see Ap- pendices G and I for further details).  Table 3  shows that the performance of baseline agents can be improved with domain randomization ( Tobin et al., 2017 ). However, our method still outperforms the baseline methods trained with more diverse environments than ours, implying that our method is more effective in learning generalizable representations than simply increasing the (finite) number of seen environments.

Section Title: CONCLUSION
  CONCLUSION In this paper, we explore generalization in RL where the agent is required to generalize to new envi- ronments in unseen visual patterns, but semantically similar. To improve the generalization ability, we propose to randomize the first layer of CNN to perturb low-level features, e.g., various textures, colors, or shapes. Our method encourages agents to learn invariant and robust representations by producing diverse visual input observations. Such invariant features could be useful for several other related topics, like an adversarial defense in RL (see Appendix B for further discussions), sim-to-real transfer ( Tobin et al., 2017 ;  Ren et al., 2019 ), transfer learning ( Parisotto et al., 2016 ;  Rusu et al., 2016a ;b), and online adaptation ( Nagabandi et al., 2019 ). We provide the more detailed discussions on an extension to the dynamics generalization and failure cases of our method in Appendix J and K, respectively.

```
