Title:
```
Under review as a conference paper at ICLR 2020 DEEP INTERACTION PROCESSES FOR TIME-EVOLVING GRAPHS
```
Abstract:
```
Time-evolving graphs are ubiquitous such as online transactions on an e-commerce platform and user interactions on social networks. While neural approaches have been proposed for graph modeling, most of them focus on static graphs. In this paper we present a principled deep neural approach that models continuous time- evolving graphs at multiple time resolutions based on a temporal point process framework. To model the dependency between latent dynamic representations of each node, we define a mixture of temporal cascades in which a node's neural representation depends on not only this node's previous representations but also the previous representations of related nodes that have interacted with this node. We generalize LSTM on this temporal cascade mixture and introduce novel time gates to model time intervals between interactions. Furthermore, we introduce a selection mechanism that gives important nodes large influence in both k−depth subgraphs of nodes in an interaction. To capture temporal dependency at multiple time-resolutions, we stack our neural representations in several layers and fuse them based on attention. Meanwhile, our method can process unseen nodes and their interactions. Experimental results on interaction prediction and classification tasks - including a real-world financial application - illustrate the effectiveness of the time gate, the selection and fusion mechanisms of our approach, as well as its superior performance over the alternative approaches.
```

Figures/Tables Captions:
```
Figure 1: An illustrative example. Figure (a) shows an illegal cash-out event. It can be revealed by high-frequency transactions with multiple merchants. However, if we merge the transaction data into a static graph, we cannot distinguish it from the static graph generated from normal online shopping activities. Thus, learning from such a static graph will fail to detect the cash-out event.
Figure 2: A toy example of interactions, the corresponding temporal dependency graph and computa- tion of nodes' dynamic embedding using DIP unit on its 3-depth temporal dependency subgraph.
Figure 3: An illustrative example. Enhanced dynamic representation with Fusion and Selection.
Figure 4: Mean rank results. As low mean rank indicates that the ground-truth item is ranked accurately, we can observe that DIP always outperforms baselines.
Figure 5: Ablation study results of the interaction prediction task. Disabling any one of the three component leads to a performance drop.
Figure 6: Ablation study results of the interaction classification task. All of the three component contributes a lot to improve the final classification precision.
Table 1: Dataset Statistics for interaction prediction
Table 2: Interaction classification results
```

Main Content:
```

Section Title: INTRODUCTION
  INTRODUCTION Representation learning over graph data has become a core machine learning task with a wide range of applications including e-commerce, finance, social networks, and bioinformatics. Various neural graph representations such as ( Perozzi et al., 2014 ;  Grover & Leskovec, 2016 ;  Wang et al., 2016 ;  Kipf & Welling, 2017 ;  Defferrard et al., 2016 ;  Scarselli et al., 2009 ;  Ying et al., 2018 ;  Hamilton et al., 2017b ;  Monti et al., 2017 ;  Den Berg et al., 2017 ) have been proposed to learn from static graph data and successfully used for downstream tasks (e.g., classification). Graph data, however, are often dynamic in practice; nodes and interactions between them can grow and shrink. A straightforward approach to handle dynamic graphs is to compress them into one or several static graphs. The drawbacks of this approach are multifold; we not only blur temporal structural information but also miss time information that can be critical for real-world applications. An illustrative example is given in  figure 1 . To handle continuous time-evolving graph, we can approximate a by a sequence of snapshot graphs, each of which includes all interactions that occur during a user-specified discrete-time interval, as shown in ( Goyal et al., 2018 ;  Leskovec et al., 2007 ;  Zhou et al., 2018 ;  Sankar et al., 2019 ). This treatment reduces time resolution and it is tricky to specify the appropriate aggregation granularity. To avoid these problems,  Nguyen et al. (2018)  proposed continuous-time dynamic networks (CTDNE) that generalize deep walk methods to learn time-dependent network embedding. As a transductive method, CTDNE cannot handle the growth of new nodes.  Dai et al. (2016)  applied temporal point processes to model time-evolving graphs and, as a nonparametric Bayesian approach, their approach can naturally cope with the growth of new nodes and interactions. They used recurrent neural networks (RNNs) to define an intensity function in temporal point processes. These RNN models are shallow and one-step unrolled, making it easy to compute but relatively limited in modeling power.  Trivedi et al. (2019)  extended this approach by modeling two-time scale and adopting temporal-attention mechanism. In this paper we present a powerful deep neural approach that models continuous time-evolving graphs at multiple time resolutions based on a temporal point process framework. We name the new approach deep interaction processes (DIPs). To model the dependency between latent dynamic representations of each node, we define a mixture of temporal cascades in which a node's neural representation depends on not only this node's previous representations but also the previous representations of related nodes that have interacted with this node. We generalize LSTM on this temporal cascade mixture and introduce novel time gates to model time intervals between interactions. Furthermore, We introduce a selection mechanism that gives important nodes large influence in both k−depth subgraphs of nodes in an interaction. To obtain representations from fine-to-coarse time-resolutions, we stack our neural representations in several layers and fuse them based on attention. Based on the temporal point process framework, our approach can naturally handle growth of graph nodes and interactions, making it inductive. The rest of the paper is organized as follows. In Section 2 we give background on temporal point processes and in Section 3 we present the new DIP approach. In Section 4 we discuss related works. In Section 5 we report experimental results on multiple interaction prediction and classification tasks including an important real-world anti-fraud financial application, demonstrating superior performance of the new approach over the alternatives.

Section Title: TEMPORAL POINT PROCESSES
  TEMPORAL POINT PROCESSES We first describe temporal point processes (a class of nonparametric Bayesian models) that our approach is based on. Specifically, a temporal point process is a stochastic process that generates a sequence of discrete events localized at times {t i } N i=1 in any given observed time window [0, T ], where N is the number of events. An important way to characterize temporal point processes is via the conditional intensity function λ (t|H t ) -the stochastic model for the next event time t given all historical events before time t, denoted as H t = {t i |t i < t}. Formally, within a small time window [t, t + dt), λ (t|H t ) dt is the probability for the occurrence for a new event given the H t : λ (t|H t ) dt = P { event in [t, t + dt)|H t }. From the survival analysis theory( Aalen et al., 2008 ), given the times of the past events {t 1 , t 2 , . . . , t i }, the conditional density that an event occurs at t i+1 is given as follows:p t i+1 |H ti+1 = λ t i+1 |H ti+1 exp − ti+1 ti λ (t|H t ) dt ,where the exponential part in the above equation means the conditional probability that no event happens during [t i , t i+1 ). The functional forms of the conditional intensity function λ (t|H t ) can represent certain forms of dependencies of the historical events. For instance, for Poisson processes( Kingman, 2005 ) we set λ to be constant - making the assumption that the process is stationary and the temporal events in history are independent of each other. For classical Hawkes processes( Hawkes, 1971 ), the intensity function λ is often set to be a sum of multiple exponential functions, assuming that the mutual excitation among events is positive, additive over the past events, and exponentially decaying with time.  Mei & Eisner (2017a)  removed these limiting assumptions using LSTM to learn λ from data.

Section Title: DEEP INTERACTION PROCESSES
  DEEP INTERACTION PROCESSES In this section, we present a new neural non-parametric Bayesian approach over continuous-time evolving graphs. First, we present a temporal dependency graph that is a mixture of the temporal cascades, to model interdependence between graph nodes (as well as latent node representations).

Section Title: Under review as a conference paper at ICLR 2020
  Under review as a conference paper at ICLR 2020 Then we present a novel deep model to learn dynamic node representations in the temporal dependency graph. This model naturally generalizes a chain-structured LSTM to a temporal Graph-structured LSTM equipped with time gates to handle interaction with irregular time intervals. Furthermore, we propose the FUSION and SELECTION methods to enhance the dynamic interactive nodes representation. Given the dynamic node representations, we define deep interaction processes that model potential interactions between any two nodes over time and we finally layout the maximum likelihood estimation method to optimize it. A toy example is shown in  Figure 2  to introduce the corresponding concepts and the whole procedure of computing enhanced dynamic representation is given in  Figure 3 .

Section Title: TEMPORAL DEPENDENCY GRAPH
  TEMPORAL DEPENDENCY GRAPH Consider a collection of people-movie interactions at different time points (e.g., David saw the movie The Skull at t 6 .) as shown in figure 2(a). The people and movies form a temporal dependency graph in which each person or movie is a node and interactions happen over time (in figure 2(b)). After one interaction occurs, we update the neural representation of the two nodes linked to this interaction; e.g., right after time t 6 , we update the representations for David and movie The Skull. The new neural representation of David depends on both his current and previous interactions - as a result, depending on the representations of the two nodes associated with the previous interaction. This naturally forms a dependency cascade. Similarly we can obtain a dependency cascade for Lucy's representations. Because of the common movies David and Lucy saw, their dependency cascades overlap and form a cascade mixture. Formally, we denote a dynamic interaction or link at time t by l u,v,t where u and v are two nodes associated with this interaction. We denote the node u at time t by u(t) and the two nodes associated with u's precedent interaction at time t − as u 1 (t) and u 2 (t). For later usage, Under review as a conference paper at ICLR 2020 we denote the k−depth temporal dependency subgraph for u(t) as subgraph(u(t), k) as shown in figure 2(b).

Section Title: DIP NEURAL UNIT AND DYNAMIC REPRESENTATION
  DIP NEURAL UNIT AND DYNAMIC REPRESENTATION Now we present a novel neural unit to update dynamic latent representations of nodes over the temporal dependency graph which is illustrated in figure 2(c) and figure 2(d). First, let us denote node u's features or embedding (i.e., a static representation jointly learned from data) at time t by x u(t) and denote features of interaction l by x l . The interaction feature can be empty if the interaction contains only the temporal information. The concatenation of x u(t) and x l is denoted byx u(t) . Let ∆ (u,t) = t − t − be the time interval between two consecutive interactions involving u at time t and t − . Our neural unit, i.e, DIP unit, generalizes a chain-structured LSTM unit to depict the temporal dependency on graph data. The DIP unit has an update gate s, an input gate z, an output gate o and two forget gates f overx u(t) , dynamic representation of u i (t), i.e, h u i (t) and cell states c u i (t) (i = 1, 2) as shown in figure 2(c). Additionally, we introduce time gates g to capture the impacts of irregular time interval ∆ (u,t) . Specifically, h u(t) and c u(t) are updated as follows: where σ, tanh and represent the sigmoid function, the hyperbolic tangent function, and the Hadamard product (pointwise multiplication), respectively, and parameters in the unit including the recurrent weights R zi , R oi , R si , R fi and R gi , the projection matrices W z , W o , W s , W fi and W gi , the bias vectors b z , b o , b s , b fi and b gi and the time weight matrix M gi are learned from data. For convenience, we use DIP-UNIT (·) to summarize the above equations, then the dynamic representation of node u(t) is given as follows: where Θ represent all the parameters. Considering the computational cost in practice, when obtaining nodes' dynamic representation given an happened interaction, we only utilize history information in the subgraph(u(t), k) as illustrated in figure 2(d), which is similar to a chain-structured LSTM training unfolded with the max k steps. The k is a hyper-parameter.

Section Title: ENHANCED DYNAMIC REPRESENTATION
  ENHANCED DYNAMIC REPRESENTATION

Section Title: STACKING AND FUSION
  STACKING AND FUSION To model nonlinear dependency relationships at different temporal resolutions, we stack L layers of DIP-UNIT together. The output of the j-th layer is computed recursively as follows: To train deeper dynamic neural networks easily, we employ the residual connection as the following form: skip(h j−1 u(t) , h j u(t) ) = W skip h j−1 u(t) +h j u(t) where W skip is a weight matrix. Motivated by ELMo ( Peters et al., 2018 ), we fuse all internal dynamic representations from all the layers to achieve rich dynamic representations. The fusion is a weighted summation of all layers defined as follows: h u(t) = γ L j=0 α j h j u(t) , where α j are softmax-normalized weights and γ is a scaling parameter. They both are learned parameters.

Section Title: SELECTION
  SELECTION Given an interaction l u,v,t , it is reasonable to assume that not all the historical interactive nodes have the equal importance for formalizing this interaction. Thus we use a two-phase gating mechanism to select relevant nodes to learn dynamic representations and cell states of the current node. Specifically, a co-attention mechanism is first used to measure relevance of historical time-evolving patterns between subgraph(u(t), k) and subgraph(v(t), k), subgraph(v(t), k), m and n are the numbers of nodes in the two corresponding subgraphs, W Q ∈ R d×d are the weight parameters. The Q j is a co-attention affinity matrix which captures the relevance information in subgraph(u(t),k) and subgraph(v(t),k). The co-dependent global embedding p j u , p j v are obtained by the following equations. where the weights w p , and w h are shared by all the stacked layers. Using these gates, we enhance the dynamic node representations as follows: Similarly, we can compute (h j v(t) , c j v(t) ) for v(t) based on the selection mechanism.

Section Title: CONDITIONAL INTENSITY FUNCTION
  CONDITIONAL INTENSITY FUNCTION We model the dynamic interactions as a multi-dimensional temporal point process. Specifically, we de- fine the conditional intensity function of the temporal point process at the dimension indexed by (u, v), given its graph-structured history H u,v t where

Section Title: MAXIMUM LIKELIHOOD PARAMETER ESTIMATION
  MAXIMUM LIKELIHOOD PARAMETER ESTIMATION

Section Title: INTERACTION PREDICTION
  INTERACTION PREDICTION Given a set of interactions as I = {(u i , v i , t i )} i=N i=1 observed in a time window [0, T], we can learn the model by minimizing the negative joint log-likelihood of I as follows: L 1 = − i log P ui,vi t i |H ui,vi ti where P ui,vi (t i |H ui,vi ti ) represents the probability of formalizing an interaction between u i and v i at time t i given the dependant history of non-chain structures H ui,vi ti . Based on the intensity definition, we have L 1 = − i log λ ui,vi t i |H ui,vi ti + T 0 Λ(t)dt, where Λ(t) = u,v λ u,v (t). Since the survival part does not have an analytic solution, we apply Monte Carlo to do numerical integrations. We follow the negative sampling approaches used by  Dai et al. (2016)  and  Trivedi et al. (2019)  to accelerate the survival term calculation.

Section Title: INTERACTION CLASSIFICATION
  INTERACTION CLASSIFICATION An interaction sequence with markers is denoted as I = {(u i , v i , t i , y i )} i=N i=1 , where y i is a marker at time t i and usually is a discrete variable. In practice, the markers have different meanings in distinct scenes. A marker can be treated as a magnitude in modeling earthquakes and aftershocks. For financial transactions, a marker can be used to label whether a transaction is a fraudulent trading or not. The joint conditional density of an interaction(u i , v i , t i ) with marker y i is given as P ui,vi t i , y i |Ĥ ui,vi ti . By applying the Bayesian rule , the joint conditional density can be written as: P ui,vi t i , y i |Ĥ ui,vi ti = P ui,vi (t i |Ĥ ui,vi ti )P y i |t i ,Ĥ ui,vi ti ,where P ui,vi (t i |Ĥ ui,vi ti ) has the same meaning as given in subsection 3.5.1, while P (y i |t i ,Ĥ ui,vi ti ) means the distribution of y i given the interaction happened at t i with interaction historyĤ ui,vi ti . It should be noted that the historyĤ ui,vi ti contains the information of history markers and one can design a marker-specific intensity function like  Mei & Eisner (2017b) . For simplicity, in our marked temporal point processes, we assume that P ui,vi t i , y i |Ĥ ui,vi ti is independent of historical markers. We model P y i |t i ,Ĥ ui,vi ti as P (y i |h ui,vi,ti ) = exp (V yi h ui,vi,ti ) yi exp (V yi h ui,vi,ti ) (9) where h ui,vi,ti is the concatenation of h u(ti) and h v(ti) which can be regarded as a dynamic repre- sentation for an interaction between u and v at t i , V yi is the weight parameters for the y i th class. Then the overall cost function is L 2 = L 1 + L cross−entropy , where L cross−entropy is a cross-entropy loss over marks:

Section Title: RELATED WORK
  RELATED WORK Inspired by the Skip-gram ( Mikolov et al., 2013 ) for word embedding, a series of node embedding methods based on the random walks on graphs have been proposed( Perozzi et al., 2014 ;  Tang et al., 2015 ;  Grover & Leskovec, 2016 ;  Wang et al., 2016 ; 2017). GCN and its variants ( Bruna et al., 2013 ;  Hamilton et al., 2017a ;  Kipf & Welling, 2017 ) are a recent class of algorithms which extend convolutions from spatial domains to graph-structured domains. Meanwhile they can efficiently generate node embeddings for previously unseen data. All models above are designed for static graphs. The intuitive and popular approaches for modeling dynamic graphs are based on a sequence for graph snapshots( Goyal et al., 2018 ;  Zhou et al., 2018 ;  Seo et al., 2018 ;  Yu et al., 2018 ), but it can be difficult to specify the appropriate aggregation granularity.  Nguyen et al. (2018)  adds a temporal constraint on random walk sampling, but it can't model the rich temporal information explicitly. Temporal point processes (TPPs) are an another alternative to model dynamics( Daley & Vere-Jones, 2007 ). Several dynamic graph modeling methods based on the TPPs ( Dai et al., 2016 ;  Trivedi et al., 2019 ) have been proposed. Our method DIP differs from these TPP-based methods by the extension of the LSTM model over temporal dependency graphs, the multiple time resolution modeling via stacking, fusing and the selection mechanism. A recently proposed method  Kumar et al. (2019)  Under review as a conference paper at ICLR 2020 models interactions directly by predicting the next interaction embedding. More detailed related work are included in Appendix.C.

Section Title: EXPERIMENTS
  EXPERIMENTS We evaluate the proposed DIP model for interaction prediction and interaction classification on several real-world datasets.

Section Title: BASELINES AND EVALUATION METRICS
  BASELINES AND EVALUATION METRICS GraphSage( Hamilton et al., 2017a ) is an inductive graph neural network framework consisting of three different aggregators which are GCN, Mean and LSTM aggregators respectively. We report the best results among these three aggregators noted as Graphsage*. What's more, for comparing with GAT( Veličković et al., 2017 ) we also implement a graph attention aggregator based on GraphSage. CTDNE( Nguyen et al., 2018 ) is a newly-proposed temporal network embedding method and also a tranductive method like DeepWalk( Perozzi et al., 2014 ). It incorporates temporal order constraint when sampling walks from time-continuous graphs. DynGEM( Goyal et al., 2018 ) takes a sequence of static graph snapshots as inputs to learn node embeddings by a deep auto-encoder network. DeepCoevolve ( Dai et al., 2016 ) models dynamic interaction sequences with two co-evolution recurrent neural networks. Hidden embeddings are learned for interactive nodes after each interaction. DyREP ( Trivedi et al., 2019 ) uses a two-time scale deep temporal point process model to capture dynamics of graphs. JODIE ( Kumar et al., 2019 ) models interaction processes in a novel way by predicting the next interaction embedding directly instead of modeling the intensity function. For interaction prediction, we report Mean Rank results. To evaluate the effectiveness of top-n, we also report performances on hit@1 and hit@5. As for interaction classification, we employ KS ( Kolmogorov, 1933 ) score as well as AUC(Area under the ROC Curve) score.

Section Title: EXPERIMENTAL SETTING
  EXPERIMENTAL SETTING We conduct all the experiments with a hyper-parameter grid search strategy. For all methods, we search the dimension of embedding from {16, 32, 64, 128} and the learning rate from {0.01, 0.001, 0.0005, 0.0001, 0.00001}. For our DIP model, we go through {1, 2, 3, 4} for k and {1, 2, 3} for L. For Graphsage, the maximum number of 1-hop and 2-hop neighbor nodes are set to 25 and 20 respectively. All the models are trained for at most 50 epochs with an early-stop operation if the performance does not improve for 5 epochs. For Graphsage, DynGEM ,DeepCoevolve and JODIE, we use the open source codes provided by the authors. We implement the CTNDE and GAT based on the Graphsage framework, and implement DyREP based on the pytorch implementation of DeepCoevolve. After the best configuration is found, we repeat the full experiments 5 times and report the mean results and standard deviation.

Section Title: INTERACTION PREDICTION
  INTERACTION PREDICTION

Section Title: DATASETS
  DATASETS CollegeMsg( Leskovec & Krevl, 2014 ) consists of sending message interactions on an online social network at the University of California, Irvine during 193 days. Ubuntu( Leskovec & Krevl, 2014 ) is a temporal interaction dataset extracted from the stack exchange website. An interaction between two users means one answered another's questions or replied to his/her posts. Amazon( McAuley et al., 2015 ) is composed of commodity rating data from amazon users. We use the Clothing subset of this dataset. MathOverflow( Leskovec & Krevl, 2014 ) is comprised of interactions of commenting an existing answer on the Math Overflow website.  Table 1  shows the detailed dataset statistics. In this table, Repetition is the rate of repeated interaction in datasets. For each dataset,we first sort these interactions by occurrence time and split them to be training/validation/test sets. The cold-start participants which only exist in validation set or test set are removed.

Section Title: RESULTS AND ANALYSIS
  RESULTS AND ANALYSIS   Figure 4  summarizes the Mean Rank performances of all methods. On the whole, our DIP method consistently beats all baselines by 65.84%, 41.64%, 10.69% and 43.99% over the four diverse datasets. The CTDNE method performs worst across all the datasets since the generated embedding is static and can't be updated and evolved across validation and test datasets. Meanwhile we can see that there is no consistent winner among baselines and all the methods perform relatively better on the CollegeMsg, Ubuntu and MathOverflow datasets than do on the Amazon-Clothing dataset. It is also noteworthy that the static methods GAT and GraphSage* perform competitive with dynamic baselines on these three datasets. These phenomenons above could be explained that the CollegeMsg, Ubuntu and MathOverflow have lots of repetitive interactions (at least 75% for CollegeMsg and MathOverflow on test data, 68% for ubuntu on test data as shown in  Table 1 ) and repetitive information makes recurring interaction predicted easily. The two static methods perform worse than the other dynamic methods(but JODIE ) on the Amazon-Clothing because no repetitive information can be reused and nodes representation can't be updated across validation and test like CTDNE. As for comparison with dynamic methods, our work performs better than the Dyrep, DeepCoevolve and JODIE which use the similar mutually-recursive RNNs to capture and update co-evolution states sequentially. We argue that the vallina RNNs can't capture long-term dependency history well and have optimization problems which may lead to worse performances than ours. Moreover they treat past history information equally while our selection mechanism can select more relevant information for updating dynamic representation. The performances of all methods on hit@1 and hit@5 are given in Appendix.B. The effects of k and L are also investigated in Appendix.B.

Section Title: INTERACTION CLASSIFICATION
  INTERACTION CLASSIFICATION

Section Title: DATASETS
  DATASETS We conduct this task on an industrial dataset: Huabei Trade Data. This dataset consists of about 150,000 transaction records processed by Huabei during August 2018. Each transaction is initiated with three parties: the buyer, the seller and transaction details such as merchant category and transaction amount. Around 15% of the transaction are fraudulent and is labeled by a complicated Ex-Post method. For each interaction event, there are 11 context features including information about buyer types, seller types, purchased items' categories and trading platform. We use the first 10 days data as training set, the following 10 days data as validation set, the rest as test set. Note that, in this scenario, there are users who only appear in validation/testing dataset. Thus, the transductive method CTDNE is not applicable on this task. Meanwhile, since the dynamic baseline methods are all unsupervised, we only report GCN and our marked DIP methods Alternatively, we employ the XGBoost ( Chen & Guestrin, 2016 ) as an additional baseline which is a popular baseline method in the cash-out detection task( Hu et al., 2019 ). The detailed data statistics are given in Appendix.B.1.

Section Title: RESULTS AND ANALYSIS
  RESULTS AND ANALYSIS   Table 2  compares the results. Obviously, our model outperforms all the baseline methods with large margins. The Xgboost method which only utilize interaction context feature can't model any co-evolution information in time-evolving interactions, thus it performs worst. The GCN-related methods perform worse than our DIP since the static construction of interaction graph can blur temporal structural information and miss time-interval information. A toy example in  figure.1  can explain why static graph methods fail in this task.

Section Title: ABLATION STUDY
  ABLATION STUDY As we described in Section 3, the DIP model consists of three important components: First, it uses a Time Gate in the DIP neural unit to explicitly model the temporal information. Second, the selection mechanism enables our model to select more important historical information for interactions. Third, the Fusion of multi-layer DIP-UNIT's hidden state vector helps to extract high level feature. We investigate the contribution of each component by disabling each of them one by one, and compare the corresponding result to the full model.  figure 5  and  figure 6  give the detailed ablation results. FullModel in the two figures means all the three components are enabled. • No Time Gate: In this configuration, the time gate in DIP-UNIT is disabled. This leads to a significant drop of the Mean Rank performance. It provides a strong evidence for the effectiveness of the time gate. • No Selection: In this configuration the selection mechanism is disabled. Accordingly, all the historical node representations contribute equally, thus again leading to a performance drop. • No Fusion: In this variant, we directly use the hidden state vector of the last layer. Again, the per- formance degrades significantly. This demonstrates that a fusion of different layers' representations gives richer information than the last layer only.

Section Title: CONCLUSIONS
  CONCLUSIONS In this paper, we have proposed a deep multidimensional point process approach, DIP, to learn dynamic graph representations. We generalize LSTM over temporal dependency graphs and model Under review as a conference paper at ICLR 2020 multiple time resolutions via stacking, selection and fusion. Experimental results show the effective- ness of the components of our neural unit and the superior performance on several datasets.

```
