Title:
```
None
```
Abstract:
```
How many training data are needed to learn a supervised task? It is often observed that the generalization error decreases as n −β where n is the number of training examples and β an exponent that depends on both data and algorithm. In this work we measure β when applying kernel methods to real datasets. For MNIST we find β ≈ 0.4 and for CIFAR10 β ≈ 0.1. Remarkably, β is the same for regression and classification tasks, and for Gaussian or Laplace kernels. To rationalize the existence of non-trivial exponents that can be independent of the specific kernel used, we introduce the Teacher-Student framework for kernels. In this scheme, a Teacher generates data according to a Gaussian random field, and a Student learns them via kernel regression. With a simplifying assumption - namely that the data are sampled from a regular lattice - we derive analytically β for translation invariant kernels, using previous results from the kriging literature. Provided that the Student is not too sensitive to high frequencies, β depends only on the training data and their dimension. We confirm numerically that these predictions hold when the training points are sampled at random on a hypersphere. Overall, our results quantify how smooth Gaussian data should be to avoid the curse of dimensionality, and indicate that for kernel learning the relevant dimension of the data should be defined in terms of how the distance between nearest data points depends on n. With this definition one obtains reasonable effective smoothness estimates for MNIST and CIFAR10.
```

Figures/Tables Captions:
```
Figure 1: Learning curves for regression on MNIST and CIFAR10 (a-b); and for classification on MNIST and CIFAR10 (c-d). Curves are averaged over 400 runs. A power law is plotted to estimate the asymptotic behavior at large n: the exponent is fitted on the last decade on the average of the two curves, since it does not seem to depend significantly on the specific kernel or on the task. In each setting we use both a Gaussian kernel K(x) ∝ exp(−||x|| 2 /(2σ 2 )) and a Laplace one K(x) ∝ exp(−||x||/σ), with σ = 1000.
Figure 2: Results for the Teacher-Student kernel regression problem, where the Student is always a Laplace kernel. Data points are sampled uniformly at random on a d-dimensional hypersphere. (a-b) Mean-square error versus the size of the training dataset, for Gaussian and Laplace Teachers and for multiple spatial dimensions. Dotted lines are the fitted power laws - we fit starting from n = 700. (c-d) Fitted exponent −β = log E MSE/ log n against the spatial dimension, for several dataset sizes. We fit from n = 0 to a varying n (written in the legends). The thick black lines are the theoretical predictions.
Figure 3: Average distance from one point to its nearest neighbor as a function of the dataset size n. (a) For random points on d-dimensional hypersphere, δ min ∼ n − 1 /d . Colored solid curves are found numerically, dashed lines are the theoretical asymptotic prediction and the gray lines are numerical fit (we fitted only starting from n ≈ 6000 to reduce finite size effects, and the fit have been rescaled to match the data at n = 10). The larger d, the stronger the preasymptotic effects (a larger n is needed to observe the predicted scaling). (b) Comparison between random data on 15- and 35-dimensional hyperspheres and the MNIST, CIFAR10 datasets. According to this definition of effective dimension, MNIST live on a 15-dimensional manifold and CIFAR10 on a 35-dimensional one. Data have been rescaled along the y-axis for ease of comparison.
```

Main Content:
```

Section Title: INTRODUCTION
  INTRODUCTION In supervised learning machines learn from a finite collection of n training data, and their generaliza- tion error is then evaluated on unseen data drawn from the same distribution. How many data are needed to learn a task is characterized by the learning curve relating generalization error to n. In various cases, the generalization error decays as a power law n −β , with an exponent β that depends on both the data and the algorithm. In ( Hestness et al., 2017 ) β is reported for state-of-the-art (SOTA) deep neural networks for various tasks: in for neural-machine translation β ≈ 0.3-0.36 (for fixed model size) or β ≈ 0.13 (for best-fit models at any n); language modeling shows β ≈ 0.06-0.09; in speech recognition β ≈ 0.3; SOTA models for image classification (on ImageNet) have exponents β ≈ 0.3-0.5. Currently there is no available theory of deep learning to rationalize these observations. Recently it was shown that for a proper initialization of the weights, deep learning in the infinite-width limit ( Jacot et al., 2018 ) converges to kernel learning. Moreover, it is nowadays part of the lore that there exist kernels whose performance is nearly comparable to deep networks (Bruna and Mallat, 2013; Arora et al., 2019), at least for some tasks. It is thus of great interest to understand the learning curves of kernels. For regression, if the target function being learned is simply assumed to be Lips- chitz, then the best guarantee is β = 1 /d ( Luxburg and Bousquet, 2004 ;  Bach, 2017 ) where d is the data dimension. Thus for large d, β is very small: learning is completely inefficient, a phenomenon referred to as the curse of dimensionality. As a result, various works on kernel regression make the much stronger assumption that the training points are sampled from a target function that belongs to the reproducing kernel Hilbert space (RKHS) of the kernel (see for example ( Smola et al., 1998 )). With this assumption β does not depend on d (for instance in ( Rudi and Rosasco, 2017 ) β = 1/2 is guaranteed). Yet, RKHS is a very strong assumption which requires the smoothness of the target Under review as a conference paper at ICLR 2020 function to increase with d ( Bach, 2017 ) (see more on this point below), which may not be realistic in large dimensions. In this work we compute β empirically for kernel methods applied on MNIST and CIFAR10 datasets. We find β MNIST ≈ 0.4 and β CIFAR10 ≈ 0.1 respectively. Quite remarkably, we observe essentially the same exponents for regression and classification tasks, using either a Gaussian or a Laplace kernel. Thus the exponents are not as small as 1 /d (d = 784 for MNIST, d = 3072 for CIFAR10), but neither are they 1 /2 as one would expect under the RKHS assumption. These facts call for frameworks in which assumptions on the smoothness of the data can be intermediary between Lipschitz and RKHS. Here we propose such a framework for regression, in which the target function is assumed to be a Gaussian random field of zero mean with translation-invariant isotropic covariance K T (x). The data can equivalently be thought as being synthesized by a "Teacher" kernel K T (x). Learning is performed with a "Student" kernel K S (x) that minimizes the mean-square error. In general K T (x) = K S (x). In this set-up learning is very similar to a technique referred to as kriging, or Gaussian process regression, originally developed in the geostatistics community ( Matheron, 1963 ;  Stein, 1999b ). To quantify learning, we first perform numerical experiments for data points distributed uniformly at random on a hypersphere of varying dimension d, focusing on a Laplace kernel for the Student, and considering a Laplace or Gaussian kernel for the Teacher. We observe that in both cases β(d) is a decreasing function. To derive β(d) we consider the simplified situation where the Gaussian random field is sampled at training points lying on a regular lattice. Building on the kriging literature ( Stein, 1999b ), we show that β is controlled by the high-frequency scaling of both the Teacher and Student kernels: assuming that the Fourier transforms of the kernels decay asK Importantly (i) Eq. (1) leads to a prediction for β(d) that accurately matches our numerical study for random training data points, leading to the conjecture that Eq. (1) holds in that case as well. We offer the following interpretation: ultimately, kernel methods are performing a local interpolation whose quality depends on the distance δ(n) between adjacent data points. δ(n) is asymptotically similar for random data or data sitting on a lattice. (ii) If the kernel K S is not too sensitive to high-frequencies, then learning is optimal as far as scaling is concerned and β = (α T − d)/d. We will argue that the smoothness index s ≡ [(α T − d)/2] characterizes the number of derivatives of the target function that are continuous. We thus recover the curse of dimensionality: s needs to be of order d to have non-vanishing β in large dimensions. Point (ii) leads to an apparent paradox: β is significant for MNIST and CIFAR10, for which d is a priori very large, leading to a smoothness value s in the hundreds in both cases, which appears unrealistic. The paradox is resolved by considering that real datasets actually live on lower-dimensional manifolds. As far as kernel learning is concerned, our findings support that the correct definition of dimension should be based on how the nearest-neighbors distance δ(n) scales with n: δ(n) ∼ n − 1 /d eff . Direct measurements of δ(n) support that MNIST and CIFAR10 live on manifolds of lower dimensions d eff MNIST ≈ 15 and d eff CIFAR10 ≈ 35. Considering the effective dimensions that we find, the observed values for β would be obtained for Gaussian fields of smoothness s MNIST ≈ 3 and s CIFAR10 ≈ 1, values that appear intuitively more reasonable. More generally this analogy with Gaussian fields allows one to associate a smoothness index s to any dataset once β and d eff are measured, which may turn out to be a useful characterization of data complexity in the future.

Section Title: RELATED WORKS
  RELATED WORKS Our set-up of Teacher-Student learning with kernels is also referred to as kriging, or Gaussian process regression, and it was originally developed in the geostatistics community ( Matheron, 1963 ). In Section 5 we present a theorem that allows one to know the rate at which the test error decreases as we increase the number of training points, assumed to lie on a high-dimensional regular lattice. Similar results have been previously derived in the kriging literature ( Stein, 1999b ) when sampling occurs on the regular lattice with the exception of the origin, where the inference is made. Here we propose an alternative derivation that some readers might find simpler. We also study a slightly different problem: instead of computing the test error when the inference is carried on at the origin, Under review as a conference paper at ICLR 2020 we compute the average error for a test point that lie at an arbitrary point, sampled uniformly at random and not necessarily on the lattice. In what follows we show, via extensive numerical simulations, that such predictions are accurate even when the training points do not lie on a regular lattice, but are taken at random on a hypersphere. An exact proof of our result in such a general setting is difficult and cannot be found even in the kriging literature. To our knowledge the results that get closer to the point are those discussed in ( Stein, 1999a ), where the author studies one-dimensional processes where the training data are not necessarily evenly spaced. In this work the effective dimension of the data plays an import role, as it controls how the distance between nearest neighbors scales with the dataset size. Of course, there exists a vast literature ( Grassberger and Procaccia, 1983 ;  Costa and Hero, 2004 ;  Hein and Audibert, 2005 ;  Levina and Bickel, 2005 ;  Rozza et al., 2012 ;  Facco et al., 2017 ;  Allegra et al., 2019 ) devoted to the study of effective dimensions, where other definitions are analyzed. The effective dimensions that we find are compatible with those obtained with more refined methods.

Section Title: LEARNING CURVE FOR KERNEL METHODS APPLIED TO REAL DATA
  LEARNING CURVE FOR KERNEL METHODS APPLIED TO REAL DATA In what follows we apply kernel methods to the MNIST and CIFAR10 datasets, each consisting of a set of images (x µ ) n µ=1 . We simplify the problem by considering only two classes whose label Z(x µ ) = ±1 correspond to odd and even numbers for MNIST, and to two groups of 5 classes in CIFAR10. The goal is to infer the value of the labelẐ S (x) of an image x that does not belong to the dataset. The S subscript reminds us that inference is performed using a positive definite kernel K S . We perform inference in both a regression and a classification setting. The following algorithms and associated results can be found in ( Scholkopf and Smola, 2001 ). Regression. Learning corresponds to minimizing a mean-square error: For algorithms seeking solutions of the formẐ S (x) = µ a µ K S (x µ , x) ≡ a · k S (x) by minimizing the man-square loss over the vector a, one obtains: Z S (x) = k S (x) · K −1 S Z, (3) where the vector Z contains all the labels in the training set, Z ≡ (Z(x µ )) n µ=1 , and K S,µν ≡ K S (x µ , x ν ) is the Gram matrix. The Gram matrix is always invertible if the kernel K S is positive definite. The generalization error is then evaluated as the expected mean-square error on unseen data, estimated by averaging over a test set composed of n test unseen data points:

Section Title: Classification
  Classification We perform kernel classification via the algorithm soft-margin SVM. The details can be found in Appendix A. After learning from the training data with a student kernel K S , performance is evaluated via the generalization error. It is estimated as the fraction of correctly predicted labels for data points belonging to a test set with n test elements. In  Fig. 1  we present the learning curves for (binary) MNIST and CIFAR10, for regression and classification. Learning is performed both with a Gaussian kernel K(x) ∝ exp(−||x|| 2 /(2σ 2 )) and a Laplace one K(x) ∝ exp(−||x||/σ). Remarkably, the power laws in the two tasks are essentially identical (although the estimated exponent appears to be slightly larger, in absolute value, for classification). Moreover, the two kernels display a very similar behavior, compatible with the same exponent: about −0.4 for MNIST and −0.1 for CIFAR10. The presented data are for σ = 1000; in Appendix B we show that the same behaviour is observed for different values.

Section Title: GENERALIZATION SCALING IN KERNEL TEACHER-STUDENT PROBLEMS
  GENERALIZATION SCALING IN KERNEL TEACHER-STUDENT PROBLEMS We study β in a simplified setting where the data is assumed to follow a Gaussian distribution with known covariance. It falls into the class of teacher-Student problems, which are characterized by a machine (the Teacher) that generates the data, and another machine (the Student) that tries to learn from them. The Teacher-Student paradigm has been broadly used to study supervised learning ( Saad and Solla, 1995 ;  Monasson and Zecchina, 1995 ;  Opper and Saad, 2001 ;  Engel and Van den Broeck, 2001 ;  Zdeborová and Krzakala, 2016 ;  Barbier et al., 2019 ;  Gabrié et al., 2018 ;  Aubin et al., 2018 ;  Franz et al., 2018 ). He we restrict our attention to kernel methods: we assume that a target function is distributed according to a Gaussian random field Z ∼ N (0, K T ) - the Teacher - characterized by a translation-invariant isotropic covariance function K T (x, x ) = K T (||x − x ||), and that the training dataset consists the finite set of n observations Z = (Z(x µ )) n µ=1 . This is equivalent to saying that the vector of training points follows a centered Gaussian distribution with a covariance matrix that depends on K T and on the location of the points (x µ ) n µ=1 : Once the Teacher has generated the dataset, the rest follows as in the kernel regression described in the previous section. We use another translation-invariant isotropic kernel K S (x, x ) - the Student - to infer the value of the field at another point,Ẑ S (x), with a regression task, i.e. minimizing the mean-square error in Eq. (2). The solution is therefore given again by Eq. (3).  Fig. 2 (a-b)  shows the mean-square error obtained numerically. In the examples the Student is always taken to be a Laplace kernel, and the Teacher is either a Laplace kernel or a Gaussian kernel. The Under review as a conference paper at ICLR 2020 points (x µ ) n µ=1 are taken uniformly at random on the unit d-dimensional hypersphere for several dimensions d and for several dataset sizes n. We take σ S = σ T = d as we observed that with this choice smaller datasets were enough to approach a limiting curve - in Appendix C we show the plots for the case σ S = σ T = 10, which appears to converge to the same limit curve with increasing n, but at a smaller pace. The figure shows that when n is large enough, the mean-square error behaves as a power law (dashed lines) with an exponent that depends on the spatial dimension of the data, as well as on the kernels. The fitted exponents are plotted in  Fig. 2 (c-d)  as a function of the spatial dimension d for different dataset sizes n. In the next section we will discuss the theoretical prediction, that in the figure is plotted a thick black line. The figure shows that as the dataset gets bigger, the asymptotic exponent tends to our prediction. In Appendix D we present the learning curves of Gaussian Students with both a Laplace and a Gaussian kernel. When both kernels are Gaussian the test error decays exponentially fast, a result that matches our theoretical prediction. In Appendix E we also provide further numerical results for the case where the Teacher kernel is a Matérn kernel (as defined therein).

Section Title: ANALYTIC ASYMPTOTICS FOR THE KERNEL TEACHER-STUDENT PROBLEM ON A LATTICE
  ANALYTIC ASYMPTOTICS FOR THE KERNEL TEACHER-STUDENT PROBLEM ON A LATTICE In this section we compute analytically the exponent that describe the asymptotic decay of the generalization error when the number n of training data increases. In order to derive the result we assume that both the Teacher Gaussian random field lives on a bounded hypercube, x ∈ V ≡ [0, L] d , where L is a constant and d is the spatial dimension. The fields and the kernels can then be thought of (a) (b) (c) (d) Under review as a conference paper at ICLR 2020 as L-periodic along each dimension. Furthermore, to make the problem tractable we assume that the points (x µ ) n µ=1 live on a regular lattice, covering all the hypercube V. Therefore, the linear spacing between neighboring points is δ = Ln − 1 /d . This is of course a different setting than the one used in the numerical simulations presented in the previous section, yet our results below support that these differences do not matter. Generalization error is then evaluated via the typical mean-square error E MSE = E Z(x) −Ẑ S (x) 2 , (6) where the expectation is taken over both the Teacher process and the point x at which we estimate the field, assumed to be uniformly distributed in the hypercube V. In Appendix F we prove the following: Theorem 1. LetK T (w) = c T ||w|| −α T + o (||w|| −α T ) andK S (w) = c S ||w|| −α S + o (||w|| −α S ) as ||w|| → ∞, whereK T (w) andK S (w) are the Fourier transforms of the kernels K T (x), K S (x) respectively, assumed to be positive definite. We assumeK T (w) andK S (w) has a finite limit as ||w|| → 0 and that K(0) < ∞. Then, Moreover, in the case of a Gaussian kernel the result holds valid if we take the corresponding exponent to be α = ∞. Apart from the specific value of the exponent in Eq. (7), Theorem 1 implies that if the Student kernel decays fast enough in the frequency domain, then β depends only on the data through the behaviour of the Teacher kernel at high frequencies. One then recovers β = (α T − d)/d, also found for the Bayes-optimal setting where the Student is identical to the Teacher. Consider the predictions of Theorem 1 in the cases presented in  Fig. 2 (a-b)  of Gaussian and Laplace kernels. If both kernels are Laplace kernels then α T = α S = d + 1 and E MSE ∼ n − 1 /d , which scales very slowly with the dataset size in large dimensions. If the Teacher is a Gaussian kernel (α T = ∞) and the Student is a Laplace kernel then β = 2(1 + 1/d), leading to β → 2 as d → ∞. In  Fig. 2 (c-d)  we compare these predictions with the exponents extracted from  Fig. 2 (a-b) . We plot log E MSE/ log n ≡ −β, against the dimension d of the data, varying the dataset size n. The exponents extracted numerically tend to our analytical predictions when n is large enough. Notice that, although the theory and the experiments do not assume the same distribution for the sampling points (x µ ) n µ=1 , this does not seem to yield any difference in the asymptotic behavior of the generalization error, leading to the conjecture that our predictions are exact even when the training set is random, and does not correspond to a lattice. The conjecture can be proven in one dimension following results of the kriging literature ( Stein, 1999a ), but generalization to higher d is a much harder problem. Intuitively, for kernel learning performs an expansion, whose quality is governed by the target function smoothness and the typical distance δ min between a point and its nearest neighbors in the training set. Both for random points or on a lattice, one has δ min ∼ n − 1 /d when n is large enough, thus both situations lead to the same β. Theorem 1 underlines that kernel methods are subjected to the curse of dimensionality. Indeed for appropriate students, one obtains β = (α T − d)/d. Let us define the smoothness index s ≡ [(α T − d)/2] = βd/2, which must be O(d) to avoid β → 0 for large d. The two Lemmas below, derived in Appendix, indicate that the target function is s time differentiable (in a mean-square sense). Thus learning with kernels in very large dimension can only occur if the target function is O(d) times differentiable, a condition that appears very restrictive in large d. • derivatives of Z(x) are a Gaussian random fields; If we approximate the high-dimensional MNIST and CIFAR10 datasets with Gaussian random fields, to obtain the curves shown in  Fig. 1  and to find the values the we report for β these fields would have to be hundreds of times differentiable, which seems unrealistic. A possible resolution of this paradox lies in the fact that the data live in a much smaller manifold than the number of pixels of these pictures would suggest. As argued above, a key determinant of kernel performance is the typical distance δ min between a point in the training set and its nearest neighbor. We define the effective dimension d eff accordingly from the asymptotic relationship between δ min and n: For random points on a d-dimensional hypersphere δ min displays fluctuations and the scaling is valid only on average and only asymptotically, that is for n larger than some characteristic scale n (d) that depends on the spatial dimension. In  Fig. 3 (a)  we show how the typical δ min scales with the dataset size n for random points on hyperspheres of dimension d = 15 and d = 35. Notice that while for d = 15 the asymptotic regime is reached when n 10 4 , for d = 35 a larger dataset is needed, with n > 10 5 points (that is about the maximum size of the dataset that we can use to apply kernel methods in our simulations, due to memory constraints). One can naturally wonder whether real data are also subjected to a scaling relation like in Eq. (8), from which an effective dimension can be defined. Consider for instance the MNIST dataset, and sample from it a subset of n pictures. For each Under review as a conference paper at ICLR 2020 point we could compute the distance from its nearest neighbor and average such quantities. As the number of data points increases, we expect that this measure characterizes the geometry of the local manifold where the data live in, since nearest neighbors are going to be closer and closer. In  Fig. 3 (b)  we present how δ min scales with n for the MNIST and CIFAR10 datasets. Both display a power-law decay, but the exponent is not compatible with 1 /d with d the spatial dimension, namely d = 784 for MNIST and d = 3072 for CIFAR10. MNIST actually seems to scale pretty much like random data on a hypersphere with d = 15, and CIFAR10 scales approximately as random data on a hypersphere with d = 35. For this reason, the effective dimensions of these datasets are consistent with: Obviously, the intrinsic dimension of the data could vary in data space, as has been reported for MNIST ( Costa and Hero, 2004 ;  Hein and Audibert, 2005 ;  Rozza et al., 2012 ;  Facco et al., 2017 ). In this qualitative discussion we neglect such subtle effects. Interestingly, our effective dimensions leads to reasonable values for the effective smoothness: In particular we find s ≈ 3 for MNIST and s ≈ 1 for CIFAR10.

Section Title: CONCLUSION
  CONCLUSION In this work we have shown for CIFAR10 and MNIST respectively that kernel regression and classification display a power-law decay in the learning curves, quite remarkably with essentially the same exponent β, found to be larger for MNIST. These exponents are much larger than β = 1/d expected for Lipschitz target functions and smaller than β = 1/2 expected for RKHS target functions. This observation led us to introduce a framework in which data are modeled as Gaussian random fields of varying smoothness, in which intermediary values of β are obtained. It is important to note the high degree of smoothness underlying the RKHS hypothesis. Consider realizations Z(x) of a Teacher Gaussian process with covariance K T and assume that they lie in the RKHS of the Student kernel K S , namely If the Teacher and Student kernels decay in the frequency domain with exponents α T and α S respectively, convergence requires α T > α S + d, and K S (0) ∝ dwK S (w) < ∞ (true for many commonly used kernels) implies α S > d. Then using Lemma 1 and Lemma 2 we can conclude that the realizations Z(x) must be at least d /2 -times mean-square differentiable to be RKHS. From this perspective, the RKHS assumption appears to be very strong, and thus may not provide an accurate description of various empirical learning curves. Our assumption that data are generated by Gaussian random processes is milder, and may thus have broader applications. Yet, we view this approximation as a first step on which to build on, to later include other effects such as noise in the data and deviations from Gaussianity.

```
