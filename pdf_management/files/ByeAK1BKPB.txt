Title:
```
None
```
Abstract:
```
The leading approaches to tensor completion and link prediction are based on the canonical polyadic (CP) decomposition of tensors. While these approaches were originally motivated by low rank approximations, the best performances are usu- ally obtained for ranks as high as permitted by computation constraints. For large scale factorization problems where the factor dimensions have to be kept small, the performances of these approaches tend to drop drastically. The other main tensor factorization model, Tucker decomposition, is more flexible than CP for fixed factor dimensions, so we expect Tucker-based approaches to yield better per- formance under strong constraints on the number of parameters. However, as we show in this paper through experiments on standard benchmarks of link prediction in knowledge bases, ComplEx, (Trouillon et al., 2016), a variant of CP, achieves similar performances to recent approaches based on Tucker decomposition on all operating points in terms of number of parameters. In a control experiment, we show that one problem in the practical application of Tucker decomposition to large-scale tensor completion comes from the adaptive optimization algorithms based on diagonal rescaling, such as Adagrad. We present a new algorithm for a constrained version of Tucker which implicitly applies Adagrad to a CP-based model with an additional projection of the embeddings onto a fixed lower dimen- sional subspace. The resulting Tucker-style extension of ComplEx obtains similar best performances as ComplEx, with substantial gains on some datasets under constraints on the number of parameters.
```

Figures/Tables Captions:
```
Figure 1: Results of the control experiment of Section 4.
Figure 2: The two algorithms used in the training of PComplEx
Figure 3: MRR as a function of #floats / entities (see Appendix 9.11) on four knowledge bases. We plot the convex envelope of various operating points we tested, varying D for several values of d. For some datasets (WN18, WN18RR), Adam (Kingma & Ba, 2014) is beneficial, in which case we use the implicit adaptation of Adam detailed in Appendix 9.7.
```

Main Content:
```

Section Title: INTRODUCTION
  INTRODUCTION The problems of representation learning and link prediction in multi-relational data can be formu- lated as a binary tensor completion problem, where the tensor is obtained by stacking the adjacency matrices of every relations between entities. This tensor can then be intrepreted as a "knowledge base", and contains triples (subject, predicate, object) representing facts about the world. Link pre- diction in knowledge bases aims at automatically discovering missing facts ( Bordes et al., 2011 ;  Nickel et al., 2011 ;  Bordes et al., 2013 ;  Nickel et al., 2016a ;  Nguyen, 2017 ). State of the art methods use the canonical polyadic (CP) decomposition of tensors ( Hitchcock, 1927 ) or variants of it ( Trouillon et al., 2016 ;  Kazemi & Poole, 2018 ;  Lacroix et al., 2018 ). While initially motivated by low-rank assumptions on the underlying ground-truth tensor, the best performances are obtained by setting the rank as high as permitted by computational constraints, using tensor norms for regularization ( Lacroix et al., 2018 ). However, for large scale data where computational or memory constraints require ranks to be low ( Lerer et al., 2019 ), performances drop drastically. Tucker decomposition is another multilinear model which allows richer interactions between entities and predicate vectors. A special case of Tucker decomposition is RESCAL ( Nickel et al., 2011 ), in which the relations are represented by matrices and entities factors are shared for subjects and ob- jects. However, an evaluation of this model in  Nickel et al. (2016b)  shows that RESCAL lags behind other methods on several benchmarks of interest. Recent work have obtained more competitive re- sults with similar models ( Balažević et al., 2019b ;  Wang et al., 2019 ), using different regularizers or deep learning heuristics such as dropout and label smoothing. Despite these recent efforts, learning Tucker decompositions remains mostly unresolved.  Wang et al. (2019)  does not achieve state of the art results on standard benchmarks, and we show (see  Figure 3 ) that the performances reported by Under review as a conference paper at ICLR 2020  Balažević et al. (2019b)  are actually matched by ComplEx ( Trouillon et al., 2016 ;  Lacroix et al., 2018 ) optimized with Adam, which has less hyperparameters. In this work, we overcome some of the difficulties associated with learning a Tucker model for knowledge base completion.  Balažević et al. (2019b)  use deep-learning mechanisms such as batch normalization ( Ioffe & Szegedy, 2015 ), dropout ( Srivastava et al., 2014 ) or learning-rate annealing to address both regularization and optimization issues. Our approach is different: We factorize the core tensor of the Tucker decomposition with CP to obtain a formulation which is closer to CP and better understand what difficulties appear. This yields a simple approach, which has a single regularization hyperparameter to tune for a fixed model specification. The main novelty of our approach is a more careful application of adaptive gradient techniques. State-of-the-art methods for tensor completion use optimization algorithms with adaptive diagonal rescaling such as Adagrad ( Duchi et al., 2011 ) or Adam ( Kingma & Ba, 2014 ). Through control experiments in which our model is equivalent to CP up to a fixed rotation of the embeddings, we show that one of the difficulties in training Tucker-style decompositions can be attributed to the lack of invariance to rotation of the diagonal rescaling. Focusing on Adagrad, we propose a different update rule that is equivalent to implicitely applying Adagrad to a CP model with a projection of the embedding to a lower dimensional subspace. Combining the Tucker formulation and the implicit Adagrad update, we obtain performances that match state-of-the-art methods on the standard benchmarks and achieve significanly better results for small embedding sizes on several datasets. Compared to the best current algorithm for Tucker decomposition of  Balažević et al. (2019b) , our approach has less hyperparameters, and we effec- tively report better performances than the implementation of  ComplEx of Lacroix et al. (2018)  in the regime of small embedding dimension. We discuss the related work in the next section. In Section 3, we present a variant of the Tucker decomposition which allows to interpolate between Tucker and CP. The extreme case of this vari- ant, which is equivalent to CP up to a fixed rotation of the embedding, serves as control model to highlight the deficiency of the diagonal rescaling of Adagrad for Tucker-style decompositions in experiments reported in Section 4 . We present the modified version of Adagrad in Section 5 and present experimental results on standard benchmarks of knowledge base completion in Section 7.

Section Title: LINK PREDICTION IN KNOWLEDGE BASES
  LINK PREDICTION IN KNOWLEDGE BASES Notation Tensors and matrices are denoted by uppercase letters. For a matrix U , u i is the vector corresponding to the i-th row of U . The tensor product is written ⊗ and the Hadamard product (i.e., elementwise product) is written .

Section Title: LEARNING SETUP
  LEARNING SETUP A knowledge base consists of a set S of triples (subject, predicate, object) that represent (true) known facts. The goal of link prediction is to recover facts that are true but not in the database. The data is represented as a tensorX ∈ {0, 1} N ×L×N for N the number of entities and L the number of predicates. Given a training set of triples, the goal is to provide a ranking of entities for queries of the type (subject, predicate, ?) and (?, predicate, object). Following  Lacroix et al. (2018) , we use the cross-entropy as a surrogate of the ranking loss. As proposed by  Lacroix et al. (2018)  and  Kazemi & Poole (2018) , we include reciprocal predicates: for each predicate P in the original dataset, and given an item o, each query of the form (?, P , o) is reformulated as a query (o, P −1 , ?), where o is now the subject of P −1 . This doubles the effective number of predicates but reduces the problem to queries of the type (subject, predicate, ?) only. For a given triple (i, j, k) ∈ S, the training loss function for a tensor X is then Under review as a conference paper at ICLR 2020 For a tensor decomposition model X(θ) parameterized by θ, the parametersθ are found by minimiz- ing the regularized empirical risk with regularizer Λ: This work studies specific models for X(θ), inspired by CP and Tucker decomposition. We discuss the related work on tensor decompositions and link prediction in knowledge bases below.

Section Title: RELATED WORK
  RELATED WORK

Section Title: CANONICAL DECOMPOSITION AND ITS VARIANTS
  CANONICAL DECOMPOSITION AND ITS VARIANTS The canonical polyadic (CP) decomposition of a tensor X is defined entrywise by The smallest value of d for which this decomposition exists is the rank of X. Each element X i,j,k is thus represented as a multi-linear product of the 3 embeddings in R d associated respectively to the ith subject, the jth predictate and the kth object. CP currently achieves near state-of-the-art performances on standard benchmarks of knowledge base completion ( Kazemi & Poole, 2018 ;  Lacroix et al., 2018 ). Nonetheless, the best reported results are with the ComplEx model ( Trouillon et al., 2016 ), which learns complex-valued embeddings and sets the embeddings of the objects to be the complex conjugate of the embeddings of subjects, i.e., w k =ū k . Prior to ComplEx, Dismult was proposed ( Yang et al., 2014 ) as a variant of CP with w k = u k . While this model obtained good performances (Kadlec et al., 2017), it can only model symmetric relations and does not perform as well as ComplEx. CP-based models are optimized with vanilla Adam or Adagrad and a single regularization parameter ( Trouillon et al., 2016 ; Kadlec et al., 2017;  Lacroix et al., 2018 ) and do not require additional heuristics for training.

Section Title: TUCKER DECOMPOSITION AND ITS VARIANTS
  TUCKER DECOMPOSITION AND ITS VARIANTS Given a tensor X of size N ×L×N , the Tucker decomposition of X is defined entrywise by The triple (d 1 , d 2 , d 3 ) are the rank parameters of the decomposition. We also use a multilinear product notation X = [[C; U, V, W ]], where U, V, W are the matrices whose rows are respectively u j , v k , w l and C the three dimensional d 1 × d 2 × d 3 core tensor. Note that the CP decomposition is a Tucker decomposition in which d 1 = d 2 = d 3 = d and C is the identity, which we write [[U, V, W ]]. With a non-trivial core tensor, Tucker decomposition is thus more flexible than CP for fixed embedding size. In knowledge base applications, we typically have d ≤ L N , so the vast majority of the model parameters are in the embedding matrices of the entities U and W . When constraints on the number of model parameters arise (e.g., memory constraints), Tucker models appear as natural candidates to increase the expressivity of the decomposition compared to CP with limited impact on the total number of parameters. While many variants of the Tucker decomposition have been proposed in the literature on tensor factorization (see e.g.,  Kolda & Bader, 2009 ), the first approach based on Tucker for link prediction in knowledge bases is RESCAL ( Nickel et al., 2011 ). RESCAL uses a special form of Tucker decomposition in which the object and subject embeddings are shared, i.e., U = W , and it does not compress the relation matrices. In the multilinear product notation above, a RESCAL model is thus written as X = [[C; U, I, U ]]. Despite some success on a few smaller datasets, RESCAL performances drop on larger datasets ( Nickel et al., 2016b ). This decrease in performances has been attributed either to improper regularization ( Nickel et al., 2011 ) or optimization issues ( Xue et al., 2018 ).  Balažević et al. (2019b)  revisits Tucker decomposition in the context of large-scale knowledge bases and resolves some of the optimization and regularization issues using learning rate Under review as a conference paper at ICLR 2020 annealing, batch-normalization and dropout. It comes at the price of more hyperparameters to tune for each dataset (label smoothing, three different dropouts and a learning rate decay), and as we discuss in our experiments, the results they report are not better than ComplEx for the same number of parameters. Two methods were previously proposed to interpolate between the expressivity of RESCAL and CP.  Xue et al. (2018)  expands the HolE model ( Nickel et al., 2016b ) (and thus the ComplEx model ( Hayashi & Shimbo, 2017 )) based on cross-correlation of embeddings to close the gap in expressivity with the Tucker decomposition for a fixed embedding size.  Jenatton et al. (2012)  express the relation matrices in RESCAL as low-rank combination of a family of matrices. We describe the link between these approaches and ours in Appendix 9.4. None of these approach however studied the effect of their formulation on optimization, and reported results inferior to ours.

Section Title: OTHER APPROACHES
  OTHER APPROACHES (Graph) neural networks for link prediction Several methods have introduced models that go beyond the form of Tucker and canonical decompositions. ConvE ( Dettmers et al., 2018 ) uses a convolution on a 2D tiling of the subject and relation embeddings as input to a 2-layer neural net that produces a new embedding for the pair, then compares to the object embedding. Graph neural networks ( Scarselli et al., 2009 ;  Niepert et al., 2016 ;  Li et al., 2016 ; Bruna et al., 2014) have recently gained popularity and have been applied to link prediction in knowledge bases by  Schlichtkrull et al. (2018) . This model uses a graph convolutional architecture to generate a variant of CP.

Section Title: Poincaré embeddings
  Poincaré embeddings Poincaré embeddings have been proposed as an alternative to usual tensor decomposition approaches to learn smaller embeddings when the relations are hierarchical ( Nickel & Kiela, 2017 ). The method has recently been extended to link prediction in relational data with very good performance trade-offs for small dimensional embeddings on the benchmark using WordNet ( Balažević et al., 2019a ), which contains relationships such as hypernyms and hyponyms which are purely hierarchical. However, such good results do not extend to other benchmarks.

Section Title: INTERPOLATING BETWEEN CP AND TUCKER
  INTERPOLATING BETWEEN CP AND TUCKER In order to better understand the underlying difficulties in learning (variants of) Tucker decompo- sitions compared to CP, our analysis starts from a Tucker model in which the core tensor is itself decomposed with CP. Given a N × L × N tensor, a fixed d and assuming a (d, d, d) Tucker de- composition to simplify notation, a Tucker model where the core tensor is itself decomposed with a rank-D CP can be written as (details are given in Appendix 9.3): X ijk = u i ⊗ v j ⊗ w k , C = P 1 u i , P 2 v j , P 3 w k or equivalently X = [[UP 1 , V P 2 , W P 3 ]], where P 1 , P 2 , P 3 are all D × d matrices. Since most knowledge bases have much fewer predicates than entities (L N ), the dimension of the predictate factors has little impact on the overall number of model parameters. So in the remainder of the paper, we always consider P 2 = I. Learning matrices U, V, W, P 1 , P 3 of this decomposition simultaneously leads to the following model, which we call CP-Tucker (CPT): The CPT model is similar to a CP model except that the embedding matrices U and W have an additional low-rank constraint (d instead of D). We say that the model interpolates between CP and Tucker because for D = d it is equivalent to CP (as long as P 1 and P 3 are full rank), whereas for D = d 2 we recover a full Tucker model because the matrices P 1 and P 3 can be chosen such that P 1 u i , v j , P 3 w k = u i Mat(v j )v T j , where Mat is the operator that maps a d 2 vector to a d×d matrix (see Appendix 9.5). CPT is similar to CANDELINC ( Carroll et al., 1980 ), except that in CANDELINC the factors U , V and W are fixed and used to compress the data in order to efficiently learn the P i . Closer to CPT,  Bro & Andersson (1998)  first learn a Tucker3 decomposition of X before applying CANDELINC using the learned factors. These methods are only applicable to least-square estimation, and for tensors of smaller scale than knowledge bases. (a) Performances on FB15K-237 in the control exper- iments for D = d, i.e., when PCP is a reparameteriza- tion of CP. We observe that PCP and CPT with vanilla Adagrad, which are variants of Tucker, underperform compared to CP. As expected in this case D = d, our modification of the Adagrad update leads to the same performances for PCP as for CP. (b) Adagrad coefficients for subject/object embedding matrices in the control experiments (D = d) for CP and PCP, averaged by columns (i.e., embedding di- mension) and sorted by values. Adagrad coefficients decay exponentially for CP, but the values are similar across most dimensions in PCP: the fixed unitary trans- form in PCP removes the benefit of Adagrad. Fixed projection matrices: The Projected Canonical Polyadic (PCP) Decomposition In order to clarify the difficulty that arise when learning a CPT model compared to a CP model, we study a simpler model in which the matrices P 1 and P 3 are not learned but rather fixed during training and taken as random matrices with orthonormal columns. We call the resulting model the Projected Canonical Polyadic (PCP) decomposition, since P 1 , P 3 project the embeddings of dimension d into a higher dimension D: When D = d the matrices P i are then fixed unitary transformations. The PCP (or CPT) model in this case D = d is then equivalent to a CP model, up to a fixed invertible transformation of the embeddings. The capacity of the model grows beyond that of CP as D increases up to d 2 .

Section Title: MOTIVATION: OPTIMIZATION ISSUES WITH CPT AND PCP
  MOTIVATION: OPTIMIZATION ISSUES WITH CPT AND PCP As discussed in the related works, previous results suggest that Tucker models are more difficult to train than CP models. The goal of this section is to isolate an issue faced with CPT/PCP models when trained with vanilla adaptive gradient methods such as Adagrad or Adam.

Section Title: CONTROL EXPERIMENT: UNITARY P 1 AND P 3 IN PCP
  CONTROL EXPERIMENT: UNITARY P 1 AND P 3 IN PCP When D = d in PCP, the model becomes equivalent to CP. Indeed, the matrices P 1 and P 3 are unitary (P 1 P 1 = P 3 P 3 = I) and so [[(UP 1 )P 1 , V, (W P 3 )P 3 ]] = [[U, V, W ]]. There is no practical interest in considering this degenerate case of PCP, we only use it in the following toy experiment to exhibit one of the difficulties encountered when training PCP. We perform a simple control experiment in which we take one of the standard benchmarks of link prediction in knowledge bases, called FB15K-237, and train a CP model for different values of the rank D and a PCP model with D = d with vanilla Adagrad. The full experimental protocol, including hyperparameter tuning, is similar to our main experiments and is described in Section 7.2. Figure 1a plots the performances in terms of the standard metric mean reciprocal rank (higher is better) as a function of D of CP (blue curve) and PCP (red curve, called PCP (Adagrad)). We observe that CP obtains significantly better performances than CPT for larger embedding dimen- sion D. Since in this toy experiment CP and PCP can represent exactly the same tensors and have equivalent regularizations, the only difference between the algorithms that can explain the difference in performances is in how the optimization is carried out, namely the diagonal rescaling performed by Adagrad: Adagrad adapts the learning rate on a per-parameter basis, depending on previous and Under review as a conference paper at ICLR 2020 current gradients, and is therefore not invariant by the addition of the matrices P 1 and P 2 even if these are unitary (we provide the formal justification in the next section). This is shown experimen- tally in Figure 1b where we plot the average Adagrad coefficients for each embedding dimensions (i.e., adagrad coefficients of subject/object embedding matrices averaged by column). The addition of the random P 1 and P 2 flattens the Adagrad weights, which in turn removes all the benefit of the adaptive rescaling of the algorithm. For reference, we also tried to directly learn all parameters including P 1 and P 3 (i.e., learn a CPT model) with vanilla Adagrad. The performances obtained are also lower than those of CP, as shown in Figure 1a (orange curve).

Section Title: A ROTATION INVARIANT ADAGRAD: ADA imp
  A ROTATION INVARIANT ADAGRAD: ADA imp In this section, we study the optimization problem in more details, and more precisely the effect of the diagonal rescaling performed by Adagrad. As a remainder, given a sequence of stochastic gradients g (t) of L and denoting G (t) = I + t τ =1 g (τ ) g (τ ) , the (practical) AdaGrad update is: where Diag(G) is the diagonal matrix obtained by extracting the diagonal elements of G.

Section Title: TWO EQUIVALENT PARAMETRIZATIONS OF PCP
  TWO EQUIVALENT PARAMETRIZATIONS OF PCP The following decomposition is equivalent to PCP, but its embeddings are expressed in R D : Note that with u i = P 1 u i and w k = P 3 w k , we go from PCP full to PCP. The practical differences between PCP and PCP full are that PCP full learn embeddings in the high-dimensional space, main- taining the low-rank structure of the overall entity embeddings through the orthogonal projections P 1 P T 1 and P 3 P T 3 . The practical interest of PCP full is not in terms of modeling but rather from the optimization perspective with Adagrad because it has a structure that is closer to that of CP. Indeed, for d = D, P 1 and P 3 disappear in PCP full , so that optimizing a PCP full model with Adagrad is equivalent to optimizing a CP model with Adagrad. This property suggests an alternative to the vanilla PCP + Adagrad algorithm, which we call implicit Adagrad: Implicit Adagrad: Ada imp The approach we propose is to effectively optimize PCP full with Ada- grad. However, when d < D, which is the interesting case for PCP, we notice that we do not need to maintain embeddings in R D . Our approach, called Ada imp , computes the gradients and Adagrad coefficients with respect to u i , w k ∈ R D , but the full dimension factor matrices U and W are never explicitly stored in memory. Rather, we store u i = P 1 u i and w k = P 3 w k ∈ R d , which is all that is required for any model computation in PCP full since P 1 and P 3 are fixed. Overall, the effective model parameters are exactly the same as in PCP, and we call this approach PCP +Ada imp . An Ada imp update is described in Algorithm 4. While PCP +Adagrad and PCP +Ada imp work with the same number of model parameters, the fundamental difference is the computation of Adagrad coefficients. Since Ada imp effectively applies Adagrad to PCP full , we need to maintain the Adagrad coefficients in R D even when d < D: the overall update is first computed in R D and projected back to R d after the application of Adagrad rescaling. In constrat, in vanilla PCP +Adagrad, the gradient is projected to R d before Adagrad rescaling.

Section Title: IMPLICIT OPTIMIZATION OF PCP FULL : ADA imp
  IMPLICIT OPTIMIZATION OF PCP FULL : ADA imp In this section, we discuss more formally the Ada imp updates, and how they compare to PCP +Ada- grad. In the following,Ũ,W are in R N ×d , whereas U, V and W are in R N ×D . Using d ≤ D, P 1 , P 3 ∈ R D×d , and we use the notation Π 1 = P 1 P 1 and Π 3 = P 3 P 3 .

Section Title: Under review as a conference paper at ICLR 2020
  Under review as a conference paper at ICLR 2020 The empirical risk L can be expressed as a function of three matrices M (1) , M (2) and M (3) corre- sponding to factors of a CP decomposition. We focus on the following problems : We focus on a step at time (t) on vectorsũ i and u i . We assume that at this time t, the tensor iterates are the same, that isŨ = UP 1 (resp.W = W P 1 ) so that [[ŨP 1 , V,W P 3 ]] = [[U Π 1 , V, W Π 3 ]]. In this case, the gradient ∇ M (1) i L is the same in both optimization problems, we denote it by g (t) i . Let G (t) i = I d + t τ =1 g (τ ) i g (τ ) i . The updates for (PCP) are: Note that due to the presence of P 1 inside the Diag operator, the update (3) is not rotation invariant. Moreover, for random P 1 , the matrix P 1 G (t) i P 1 will be far from diagonal with high probability, making its diagonal meaningless. This is visualized in Figure 1b. Similar updates can be derived for (PCP full ): As a sanity check, clearly, for d = D and Π 1 = I, the update (4) is equivalent to the Adagrad update for the CP model. In the general case d ≤ D, in order to avoid storing U ∈ R N ×D , we apply these updates implicitly with Ada imp , by storing u (t) i = P 1 u (t) i in R d . Let us compare the updates : Going back to our control experiment, we note on Figure 1a that PCP +Ada imp matches the perfor- mances of CP+Adagrad for all D, indicating that we fixed this optimization issue.

Section Title: ALTERNATIVES TO ADA imp
  ALTERNATIVES TO ADA imp Another solution would be to use Adagrad projected on the column space of Π, but we show in Appendix 9.1 that even with the diagonal approximation, this is impractical. Note that the version of Adagrad which uses the full matrix G (t) is rotation invariant (see Appendix 9.2 for details), but it cannot be used at the scale of our problems. It could be argued that the strength of the AdaGrad algorithm in our context mostly comes from its adaptation to the different frequencies of updates of each embedding. In fact, this is one of the examples chosen in  Duchi et al. (2011)  to display the advantages of AdaGrad compared to stochastic gradient descent. A version of AdaGrad that would only keep one coefficient per embedding (we call this version Ada row ) would be invariant to unitary transforms by design and would adapt to the various update frequencies of different embeddings. In fact, this version of AdaGrad is used to save memory in  Lerer et al. (2019) . We test this algorithm in Appendix 9.8. The difference in performances shows that this adaptation is not sufficient to recover the performances of the finer diagonal AdaGrad in our setting. The claim that the approximations made in Ada imp are indeed better is further backed by the experiments in the next section .

Section Title: COMPLEXITY
  COMPLEXITY The time complexity of our Ada imp update for a batch of size B is O(D · d · B) which is similar, up to constants, to the complexity of updates for the AdaGrad algorithm. We do not notice any runtime differences between our algorithm applied in dimensions (d, D) and a CP decomposition of dimension D (see Section 7). The runtime for large enough D is dominated by the matrix product (O(D 2 · B)) required to compute the cross-entropy in Equation (1).

Section Title: PROJECTED COMPLEX
  PROJECTED COMPLEX As the state-of-the-art variant of CP is ComplEx ( Trouillon et al., 2016 ;  Lacroix et al., 2018 ), we propose the following alternative to PCP base on ComplEx with Ada imp in practice. Given the ComplEx decomposition X = Re([U, V, U ]), a low-rank decomposition of the entity factor U as PŨ leads to the model PComplEx we use in the experiments of Section 7: PComplEx is similar to ComplEx but with interactions described by full matrices of rank D that share a same basis. We learn this decomposition with Algorithms 1 and 4.

Section Title: EXPERIMENTS
  EXPERIMENTS In this section, we compare ComplEx optimized with AdaGrad and PComplEx optimized with Ada imp . We optimize the regularized empirical risk of Equation (2).  Following Lacroix et al. (2018) , we regularize ComplEx with the weighted nuclear-3 norm, which is equivalent to regulariz- ing u i 3 3 + u j 3 3 + w k 3 3 for each training example (i, j, k). For PComplEx based models, we regularize P u i 3 3 + v j 3 3 + P u k 3 3 by analogy. We conduct all experiments on a Quadro GP 100 GPU. The code for PComplEx and Ada imp is available in the supplementary materials, experiments on ComplEx use the code 1 from ( Lacroix et al., 2018 ). We include results from TuckER ( Balažević et al., 2019b ), DRT and SRT which are the two models considered in  Wang et al. (2019) , ConvE ( Dettmers et al., 2018 ), HolEx ( Xue et al., 2018 ), LFM ( Jenatton et al., 2012 ) and MurP ( Balažević et al., 2019a ) without re-implementation on our parts. All the parameters for the experiments in this section are reported in Appendix 9.11.

Section Title: DATASETS
  DATASETS WN18 ( Bordes et al., 2013 ) is taken from the Wordnet database which contains words and relations between them. WN18RR ( Dettmers et al., 2018 ) is a filtering of this dataset which removes train/test leakage. YAGO3-10 ( Dettmers et al., 2018 ) is taken from the eponymous knowledge base. Finally, SVO ( Jenatton et al., 2012 ) contains observations of Subject, Verb, Object triples. All statistics for these datasets can be found in Appendix 9.13. Experiments on the FB15K and FB15K-237 datasets are deferred to Appendix 9.10.

Section Title: RESULTS
  RESULTS We report the filtered Mean Reciprocal Rank ( Nickel et al., 2016b ) on  Figure 3 . For SVO, we report the only figure available in previous work which is the filtered hits at 5% ( Jenatton et al., 2012 ). These measures are detailed in Appendix 9.11. Only the grid-search parameters were given for LFM, so we were not able to obtain a precise number of parameters for the number they report. On WN18, SVO and YAGO3-10 we observe sizable performance gains for low embedding sizes : up to 0.14 MRR points on WN18, 0.05 MRR points on YAGO and 0.03 H@5% points on SVO. The TuckER ( Balažević et al., 2019b ) model performs similarly to PComplEx and ComplEx except on FB15K and WN18 where it underperforms (see Appendix 9.10). We expect this discrepancy to come from a less extensive grid-search rather than any intrinsic differences in the models that are both based on the Tucker decomposition. The consistency on all operating points of our method with ComplEx shows an advantage of our method, which enjoys the same learning rate robustness as AdaGrad, and does not require choosing a learning-rate decay, leading to easier experiments with only the regularization strength to tune. The MurP model ( Balažević et al., 2019a ) provides good performances for low embedding sizes on WN18RR, but underperforms on FB15K-237 (see Appendix 9.10). All other models fail to match the performances of ComplEx and PComplEx with equivalent number of parameters. Variance of performances in PComplEx due to random choice of P is similar to the variance of ComplEx. We present experiments on the WN18 dataset for 5 different seeds in Appendix 9.9.

Section Title: CONCLUSION
  CONCLUSION By observing that the core tensor of the Tucker decomposition can itself be decomposed, we obtain new models that are reminiscent of the canonical decomposition with low-rank factors. We provide experimental evidence that a naive application of AdaGrad on this decomposition fails, due to in- dividual coordinates losing their meaning. We propose a new algorithm, Ada imp , which fixes this issue. Our model, when optimized with Ada imp , provides better performances than ComplEx in the low-rank regime, and matches its performances in the other regimes. Under review as a conference paper at ICLR 2020

```
