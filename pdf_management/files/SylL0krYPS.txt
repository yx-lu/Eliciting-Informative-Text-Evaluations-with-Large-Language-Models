Title:
```
Published as a conference paper at ICLR 2020 TOWARD EVALUATING ROBUSTNESS OF DEEP REIN- FORCEMENT LEARNING WITH CONTINUOUS CONTROL
```
Abstract:
```
Deep reinforcement learning has achieved great success in many previously diffi- cult reinforcement learning tasks, yet recent studies show that deep RL agents are also unavoidably susceptible to adversarial perturbations, similar to deep neural networks in classification tasks. Prior works mostly focus on model-free adver- sarial attacks and agents with discrete actions. In this work, we study the problem of continuous control agents in deep RL with adversarial attacks and propose the first two-step algorithm based on learned model dynamics. Extensive experiments on various MuJoCo domains (Cartpole, Fish, Walker, Humanoid) demonstrate that our proposed framework is much more effective and efficient than model-free at- tacks baselines in degrading agent performance as well as driving agents to unsafe states.
```

Figures/Tables Captions:
```
Figure 1: Two commonly-used threat models.
Figure 2: Video frames of best attacks in each baseline among 10 runs for the Walker.walk example. Only our proposed attack can constantly make the Walker fall down (since we are minimizing its head height to be zero).
Table 1: Compare three model-free attack baselines (rand-U, rand-B, flip) and our algorithm (Ours) in 4 different domains and tasks. We report the following statistics over 10 different runs: mean, standard deviation, averaged ratio, and best attack (number of times having smallest loss over 10 different runs). Results show that our attack outperforms all the model-free attack baselines for the observation manipulation threat model by a large margin for all the statistics. Our proposed attack is also superior on the action manipulation threat model and win over most of the evaluation metrics.
Table 2: Compare three attack baselines (rand-U, rand-B, flip) and our algorithm (Ours) in three different domains and tasks. Performance statistics of 10 different runs are reported.
Table 3: Ablation study on the planning length T . Compare 3 attack baselines (rand-U, rand-B, flip) and our algorithm (Ours) and report performance statistics of 10 different runs.
Table 4: Less frequency attack. Report statistics of 10 different runs with different initial states in the walker domain with task stand.
```

Main Content:
```

Section Title: INTRODUCTION
  INTRODUCTION Deep reinforcement learning (RL) has revolutionized the fields of AI and machine learning over the last decade. The introduction of deep learning has achieved unprecedented success in solving many problems that were intractable in the field of RL, such as playing Atari games from pixels and performing robotic control tasks (Mnih et al., 2015; Lillicrap et al., 2015; Tassa et al., 2018). Un- fortunately, similar to the case of deep neural network classifiers with adversarial examples, recent studies show that deep RL agents are also vulnerable to adversarial attacks. A commonly-used threat model allows the adversary to manipulate the agent's observations at every time step, where the goal of the adversary is to decrease the agent's total accumulated reward. As a pioneering work in this field, (Huang et al., 2017) show that by leveraging the FGSM attack on each time frame, an agent's average reward can be significantly decreased with small input adversarial perturbations in five Atari games. (Lin et al., 2017) further improve the efficiency of the attack in (Huang et al., 2017) by leveraging heuristics of detecting a good time to attack and luring agents to bad states with sample-based Monte-Carlo planning on a trained generative video prediction model. Since the agents have discrete actions in Atari games (Huang et al., 2017; Lin et al., 2017), the prob- lem of attacking Atari agents often reduces to the problem of finding adversarial examples on image classifiers, also pointed out in (Huang et al., 2017), where the adversaries intend to craft the input perturbations that would drive agent's new action to deviate from its nominal action. However, for agents with continuous actions, the above strategies can not be directly applied. Recently, (Uesato et al., 2018) studied the problem of adversarial testing for continuous control domains in a similar but slightly different setting. Their goal was to efficiently and effectively find catastrophic failure given a trained agent and to predict its failure probability. The key to success in (Uesato et al., 2018) is the availability of agent training history. However, such information may not always be accessible to the users, analysts, and adversaries. Besides, although it may not be surprising that adversarial attacks exist for the deep RL agents as adversarial attacks have been shown to be possible for neural network models in various supervised Published as a conference paper at ICLR 2020 learning tasks. However, the vulnerability of RL agents can not be easily discovered by existing baselines which are model-free and build upon random searches and heuristics - this is also verified by our extensive experiments on various domains (e.g. walker, humanoid, cartpole, and fish), where the agents still achieve close to their original best rewards even with baseline attacks at every time step. Hence it is important and necessary to have a systematic methodology to design non-trivial ad- versarial attacks, which can efficiently and effectively discover the vulnerabilities of deep RL agents - this is indeed the motivation of this work. This paper takes a first step toward this direction by proposing the first sample-efficient model-based adversarial attack. Specifically, we study the robustness of deep RL agents in a more challenging set- ting where the agent has continuous actions and its training history is not available. We consider the threat models where the adversary is allowed to manipulate an agent's observations or actions with small perturbations, and we propose a two-step algorithmic framework to find efficient adversarial attacks based on learned dynamics models. Experimental results show that our proposed model- based attack can successfully degrade agent performance and is also more effective and efficient than model-free attacks baselines. The contributions of this paper are the following: • To the best of our knowledge, we propose the first model-based attack on deep RL agents with continuous actions. Our proposed attack algorithm is a general two-step algorithm and can be directly applied to the two commonly-used threat models (observation manipulation and action manipulation). • We study the efficiency and effectiveness of our proposed model-based attack with model- free attack baselines based on random searches and heuristics. We show that our model- based attack can degrade agent performance in numerous MuJoCo domains by up to 4× in terms of total reward and up to 4.6× in terms of distance to unsafe states (smaller means stronger attacks) compared to the model-free baselines. • Our proposed model-based attack also outperform all the baselines by a large margin in a weaker adversary setting where the adversary cannot attack at every time step. In addition, ablation study on the effect of planning length in our proposed technique suggests that our method can still be effective even when the learned dynamics model is not very accurate.

Section Title: BACKGROUND
  BACKGROUND

Section Title: Adversarial attacks in reinforcement learning
  Adversarial attacks in reinforcement learning Compared to the rich literature of adversarial examples in image classifications (Szegedy et al., 2013) and other applications (including natural language processing (Jia & Liang, 2017), speech (Carlini & Wagner, 2018), etc), there is relatively little prior work studying adversarial examples in deep RL. One of the first several works in this field are (Huang et al., 2017) and (Lin et al., 2017), where both works focus on deep RL agent in Atari games with pixels-based inputs and discrete actions. In addition, both works assume the agent to be attacked has accurate policy and the problem of finding adversarial perturbation of visual input reduces to the same problem of finding adversarial examples on image classifiers. Hence, (Huang et al., 2017) applied FGSM (Goodfellow et al., 2015) to find adversarial perturbations and (Lin et al., 2017) further improved the efficiency of the attack by heuristics of observing a good timing to attack - when there is a large gap in agents action preference between most-likely and least- likely action. In a similar direction, (Uesato et al., 2018) study the problem of adversarial testing by leveraging rejection sampling and the agent training histories. With the availability of training histories, (Uesato et al., 2018) successfully uncover bad initial states with much fewer samples compared to conventional Monte-Carlo sampling techniques. Recent work by (Gleave et al., 2019) consider an alternative setting where the agent is attacked by another agent (known as adversarial policy), which is different from the two threat models considered in this paper. Finally, besides adversarial attacks in deep RL, a recent work (Wang et al., 2019) study verification of deep RL agent under attacks, which is beyond the scope of this paper.

Section Title: Learning dynamics models
  Learning dynamics models Model-based RL methods first acquire a predictive model of the environment dynamics, and then use that model to make decisions (Atkeson & Santamaria, 1997). These model-based methods tend to be more sample efficient than their model-free counterparts, Published as a conference paper at ICLR 2020 (a) Attack observations of agent. (b) Attack actions of agent. and the learned dynamics models can be useful across different tasks. Various works have focused on the most effective ways to learn and utilize dynamics models for planning in RL (Kurutach et al., 2018; Chua et al., 2018; Chiappa et al., 2017; Fu et al., 2016).

Section Title: PROPOSED FRAMEWORK
  PROPOSED FRAMEWORK In this section, we first describe the problem setup and the two threat models considered in this paper. Next, we present an algorithmic framework to rigorously design adversarial attacks on deep RL agents with continuous actions.

Section Title: PROBLEM SETUP AND FORMULATION
  PROBLEM SETUP AND FORMULATION Let s i ∈ R N and a i ∈ R M be the observation vector and action vector at time step i, and let π : R N → R M be the deterministic policy (agent). Let f : R N × R M → R N be the dynamics model of the system (environment) which takes current state-action pair (s i , a i ) as inputs and outputs the next state s i+1 . We are now in the role of an adversary, and as an adversary, our goal is to drive the agent to the (un-safe) target states s target within the budget constraints. We can formulate this goal into two optimization problems, as we will illustrate shortly below. Within this formalism, we can consider two threat models: Threat model (i): Observation manipulation. For the threat model of observation manipulation, an adversary is allowed to manipulate the observation s i that the agent perceived within an budget: ∆s i ∞ ≤ , L s ≤ s i + ∆s i ≤ U s , (1) where ∆s i ∈ R N is the crafted perturbation and U s ∈ R N , L s ∈ R N are the observation limits. Threat model (ii): Action manipulation. For the threat model of action manipulation, an adver- sary can craft ∆a i ∈ R M such that ∆a i ∞ ≤ , L a ≤ a i + ∆a i ≤ U a , (2) where U a ∈ R M , L a ∈ R M are the limits of agent's actions.

Section Title: Our formulations
  Our formulations Given an initial state s 0 and a pre-trained policy π, our (adversary) objective is to minimize the total distance of each state s i to the pre-defined target state s target up to the unrolled (planning) steps T . This can be written as the following optimization problems in Equations 3 and 4 for the Threat model (i) and (ii) respectively: A common choice of d(x, y) is the squared 2 distance x − y 2 2 and f is the learned dynamics model of the system, and T is the unrolled (planning) length using the dynamics models.

Section Title: OUR ALGORITHM
  OUR ALGORITHM In this section, we propose a two-step algorithm to solve Equations 3 and 4. The core of our pro- posal consists of two important steps: learn a dynamics model f of the environment and deploy optimization technique to solve Equations 3 and 4. We first discuss the details of each factor, and then present the full algorithm by the end of this section. Step 1: learn a good dynamics model f . Ideally, if f is the exact (perfect) dynamics model of the environment and assuming we have an optimization oracle to solve Equations 3 and 4, then the solutions are indeed the optimal adversarial perturbations that give the minimal total loss with -budget constraints. Thus, learning a good dynamics model can conceptually help on developing a strong attack. Depending on the environment, different forms of f can be applied. For example, if the environment of concerned is close to a linear system, then we could let f (s, a) = As+Ba, where A and B are unknown matrices to be learned from the sample trajectories (s i , a i , s i+1 ) pairs. For a more complex environment, we could decide if we still want to use a simple linear model (the next state prediction may be far deviate from the true next state and thus the learned dynamical model is less useful) or instead switch to a non-linear model, e.g. neural networks, which usually has better prediction power but may require more training samples. For either case, the model parameters A, B or neural network parameters can be learned via standard supervised learning with the sample trajectories pairs (s i , a i , s i+1 ). Step 2: solve Equations 3 and 4. Once we learned a dynamical model f , the next immediate task is to solve Equation 3 and 4 to compute the adversarial perturbations of observations/actions. When the planning (unrolled) length T > 1, Equation 3 usually can not be directly solved by off-the- shelf convex optimization toolbox since the deel RL policy π is usually a non-linear and non-convex neural network. Fortunately, we can incorporate the two equality constraints of Equation 3 into the objective and with the remaining -budget constraint (Equation 1), Equation 3 can be solved via projected gradient descent (PGD) 1 . Similarly, Equation 4 can be solved via PGD to get ∆a i . We note that, similar to the n-step model predictive control, our algorithm could use a much larger planning (unrolled) length T when solving Equations 3 and 4 and then only apply the first n (≤ T ) adversarial perturbations on the agent over n time steps. Besides, with the PGD framework, f is not limited to feed-forward neural networks. Our proposed attack is summarized in Algorithm 2 for Step 1, and Algorithm 3 for Step 2.

Section Title: EXPERIMENTS
  EXPERIMENTS In this section, we conduct experiments on standard reinforcement learning environment for con- tinuous control (Tassa et al., 2018). We demonstrate results on 4 different environments in Mu- Published as a conference paper at ICLR 2020

Section Title: Algorithm 2 learn dynamics
  Algorithm 2 learn dynamics 1: Input: pre-trained policy π, MaxSampleSize n s , environment env, trainable parameters W 2: Output: learned dynamical model f (s, a; W ) Algorithm 3 model based attack 1: Input: pre-trained policy π, learned dynamical model f (s, a; W ), threat model, maximum perturbation magnitude , unroll length T , apply perturbation length n (≤ T ) 2: Output: a sequence of perturbation δ 1 , . . . , δ n 3: if threat model is observation manipulation (Eq. 1) then 4: Solve Eq. 3 with parameters (π, f, , T ) via PGD to get δ 1 , . . . , δ T 5: else if threat model is action manipulation (Eq. 2) then 6: Solve Eq. 4 with parameters (π, f, , T ) via PGD to get δ 1 , . . . , δ T 7: end if 8: Return δ 1 , . . . , δ n JoCo Tassa et al. (2018) and corresponding tasks: Cartpole-balance/swingup, Fish-upright, Walker- stand/walk and Humanoid-stand/walk. For the deep RL agent, we train a state-of-the-art D4PG agent (Barth-Maron et al., 2018) with default Gaussian noise N (0, 0.3I) on the action and the score of the agents without attacks is summarized in Appendix A.3. The organization is as follows: we first evaluate the effectiveness of our proposed model-based attack and three model-free baselines in terms of both loss and reward. Next, we conduct ablation study on the key parameter of our algorithm the planning length T , evaluate our algorithm on a weaker attack setting and also discuss the efficiency of our proposed attack in terms of sample complexity.

Section Title: Evaluations
  Evaluations We conduct experiments for 10 different runs, where the environment is reset to different initial states in different runs. For each run, we attack the agent for one episode with 1000 time steps (the default time intervals is usually 10 ms) and we compute the total loss and total return reward. The total loss calculates the total distance of current state to the unsafe states and the total return reward measures the true accumulative reward from the environment based on agent's action. Hence, the attack algorithm is stronger if the total return reward and the total loss are smaller.

Section Title: Baselines
  Baselines We compare our algorithm with the following model-free attack baselines with random searches and heuristics: • rand-U: generate m randomly perturbed trajectories from Uniform distribution with inter- val [− , ] and return the trajectory with the smallest loss (or reward), • rand-B: generate m randomly perturbed trajectories from Bernoulli distribution with prob- ability 1/2 and interval [− , ], and return the trajectory with the smallest loss (or reward), • flip: generate perturbations by flipping agent's observations/actions within the budget in ∞ norm. For rand-U and rand-B, they are similar to Monte-Carlo sampling methods, where we generate m sample trajectories from random noises and report the loss/reward of the best trajectory (with minimum loss or reward among all the trajectories). We set m = 1000 throughout the experiments. More details see Appendix A.2. Our algorithm. A 4-layer feed-forward neural network with 1000 hidden neurons per layer is trained as the dynamics model f respectively for the domains of Cartpole, Fish, Walker and Hu- manoid. We use standard 2 loss (without regularization) to learn a dynamics model f . Instead of using recurrent neural network to represent f , we found that the 1-step prediction for dynamics with the 4-layer feed-forward network is already good for the MuJoCo domains we are studying. Specif- ically, for the Cartpole and Fish, we found that 1000 episodes (1e6 training points) are sufficient Published as a conference paper at ICLR 2020 to train a good dynamics model (the mean square error for both training and test losses are at the order of 10 −5 for Cartpole and 10 −2 for Fish), while for the more complicated domain like Walker and Humanoid, more training points (5e6) are required to achieve a low test MSE error (at the order of 10 −1 and 10 0 for Walker and Humanoid respectively). Consequently, we use larger planning (unrolled) length for Cartpole and Fish (e.g. T = 10, 20), while a smaller T (e.g. 3 or 5) is used for Walker and Humanoid. Meanwhile, we focus on applying projected gradient descent (PGD) to solve Equation 3 and 4. We use Adam as the optimizer with optimization steps equal to 30 and we report the best result for each run from a combination of 6 learning rates, 2 unroll length {T 1 , T 2 } and n steps of applying PGD solution with n ≤ T i .

Section Title: RESULTS
  RESULTS For observation manipulation, we report the results on Walker, Humanoid and Cartpole domains with tasks (stand, walk, balance, swingup) respectively. The unsafe states s target for Walker and Humanoid are set to be zero head height, targeting the situation of falling down. For Cartpole, the unsafe states are set to have 180 • pole angle, corresponding to the cartpole not swinging up and nor balanced. For the Fish domain, the unsafe states for the upright task target the pose of swimming fish to be not upright, e.g. zero projection on the z-axis. The full results of both two threat models on observation manipulation and action manipulation are shown in Table 1a, b and c, d respectively. Since the loss is defined as the distance to the target (unsafe) state, the lower the loss, the stronger the attack. It is clear that our proposed attack achieves much lower loss in Table 1a & c than the other three model-free baselines, and the averaged ratio is also listed in 1b & d. Notably, over the 10 runs, our proposed attack always outperforms baselines for the threat model of observation perturbation and the Cartpole domain for the threat model of action perturbation, while still superior to the baselines despite losing two times to the flip baseline on the Fish domain. To have a better sense on the numbers, we give some quick examples below. For instance, as shown in Table 1a and b, we show that the average total loss of walker head height is almost unaffected for the three baselines - if the walker successfully stand or walk, its head height usually has to be greater than 1.2 at every time step, which is 1440 for one episode - while our attack can successfully lower the walker head height by achieving an average of total loss of 258(468), which is roughly 0.51(0.68) per time step for the stand (walk) task. Similarly, for the humanoid results, a successful humanoid usually has head height greater than 1.4, equivalently a total loss of 1960 for one episode, and Table 1a shows that the d4pg agent is robust to the perturbations generated from the three model- free baselines while being vulnerable to our proposed attack. Indeed, as shown in  Figure 2 , the Published as a conference paper at ICLR 2020

Section Title: DISCUSSION
  DISCUSSION Evaluating on the total reward. Often times, the reward function is a complicated function and its exact definition is often unavailable. Learning the reward function is also an active research field, which is not in the coverage of this paper. Nevertheless, as long as we have some knowledge of unsafe states (which is often the case in practice), then we can define unsafe states that are related to low reward and thus performing attacks based on unsafe states (i.e. minimizing the total loss of distance to unsafe states) would naturally translate to decreasing the total reward of agent. As demonstrated in  Table 2 , the results have the same trend of the total loss result in  Table 1 , where our proposed attack significantly outperforms all the other three baselines. In particular, our method can lower the average total reward up to 4.96× compared to the baselines result, while the baseline results are close to the perfect total reward of 1000.

Section Title: Evaluating the effect of planning length
  Evaluating the effect of planning length To investigate model effect over time, we perform ab- lation studies on the planning/unroll length T of our proposed model-based attack in three examples: (I) cartpole.balance (II) walker.walk and (III) walker.stand. (I) Cartpole balance. Our learned models are very accurate (test MSE error on the order of 10 −6 ). We observed that the prediction error of our learned model compared to the true model (the MuJoCo simulator) is around 10% for 100 steps. Hence, we can choose T to be very large (e.g. 20-100) and our experiments show that the result of T = 100 is slightly better, see Appendix A.4. (II) Walker walk. This task is much more complicated than (I), and our learned model is less accurate (test MSE is 0.447). For 10 steps, the prediction error of our learned model compared to the true model is already more than 100%, and hence using a small T for planning would be more reasonable. Table 3a shows that T = 1 indeed gives the best attack results (decreases the loss by 3.2× and decreases the reward by 3.6× compared to the best baseline (randB)) and the attack becomes less powerful as T increases. Nevertheless, even with T = 10, our proposed technique still outperforms the best baseline (randB) by 1.4× both in the total loss and total reward. (III) Walker stand. The learned model is slightly more accurate than the (II) (test MSE is 0.089) in this task. Interestingly, Table 3b show that with the more accurate walker.stand model (compared to the walker.walk model), T = 10 gives the best avg total loss& reward , which are 13.4× and 4.9× smaller than the best baseline rand-B. Note that even with T = 1, the worst choice among all our reported T , the result is still 3.5× and 2.9× better than the best baselines, demonstrating the effectiveness of our proposed approach. The main takeaway from these experiments is that when the model is accurate, we can use larger T in our proposed attack; while when the model is less accurate, smaller T is more effective (as in the Walker.walk example). However, even under the most unfavorable hyperparameters, our proposed attack still outperforms all the baselines by a large margin.

Section Title: Evaluating on the effectiveness of attack
  Evaluating on the effectiveness of attack We study the setting where attackers are less powerful - they can only attack every 2 time steps instead of every transition.  Table 4  shows that our proposed attack is indeed much stronger than the baselines even when the attackers power is limited to attack every 2 time steps: (1) compared to the best results among three baselines, our attack gives 1.53× smaller avg total loss (2) the mean reward of all the baselines is close to perfect reward, while our attacks can achieve 1.43× smaller average total reward compared to the best baseline.

Section Title: Evaluating on the efficiency of attack
  Evaluating on the efficiency of attack We also study the efficiency of the attack in terms of sample complexity, i.e. how many episodes do we need to perform an effective attack? Here we adopt the convention in control suite (Tassa et al., 2018) where one episode corresponds to 1000 time steps (samples) and learn the neural network dynamical model f with different number of episodes. Figure 3 in Appendix A.1 plots the total head height loss of the walker (task stand) for 3 base- lines and our method with dynamical model f trained with three different number of samples: {5e5, 1e6, 5e6}, or equivalently {500, 1000, 5000} episodes. We note that the sweep of hyper pa- rameters is the same for all the three models, and the only difference is the number of training samples. The results show that for the baselines rand-U and flip, the total losses are roughly at the order of 1400-1500, while a stronger baseline rand-B still has total losses of 900-1200. However, if we solve Eq. equation 3 with f trained by 5e5 or 1e6 samples, the total losses can be decreased to the order of 400-700 and are already winning over the three baselines by a significant margin. Same as our expectation, if we use more samples (e.g. 5e6, which is 5-10 times more), to learn a more accurate dynamics model, then it is beneficial to our attack method - the total losses can be further decreased by more than 2× and are at the order of 50-250 over 10 different runs. See Appendix A.1 for more details. Here we also give a comparison between our model-based attack to existing works (Uesato et al., 2018; Gleave et al., 2019) on the sample complexity. In (Uesato et al., 2018), 3e5 episodes of training data is used to learn the adversarial value function, which is roughly 1000× more data than even our strongest adversary (with 5e3 episodes). Similarly, (Gleave et al., 2019) use roughly 2e4 episodes to train an adversary via deep RL, which is roughly 4× more data than ours 2 .

Section Title: CONCLUSIONS AND FUTURE WORKS
  CONCLUSIONS AND FUTURE WORKS In this paper, we study the problem of adversarial attacks in deep RL with continuous control for two commonly-used threat models. We proposed the first model-based attack algorithm and showed that our formulation can be easily solved by off-the-shelf gradient-based solvers. Extensive experiments on 4 MuJoCo domains show that our proposed algorithm outperforms all model-free based attack baselines by a large margin. We hope our discovery of the vulnerability of deep RL agent can bring more safety awareness to researchers when they design algorithms to train deep RL agents. There are several interesting future directions can be investigated based on this work, including learning reward functions to facilitate a more effective attack, extending our current approach to develop effective black-box attacks, and incorporating our proposed attack algorithm to adversarial training of the deep RL agents. In particular, we think there are three important challenges that need to be addressed to study adversarial training of RL agents along with our proposed attacks: (1) The adversary and model need to be jointly updated. How do we balance these two updates, and make sure the adversary is well-trained at each point in training? (2) How to avoid cycles in the training process due to the agent overfitting to the current adversary? (3) How to ensure the adversary doesn't overly prevent exploration/balance unperturbed vs. robust performance?
  1 Alternatively, standard optimal control methods such as Linear Quadratic Regulator (LQR) and iterative Linear Quadratic Regulator (i-LQR) can also be applied to solve Equations 3 and 4 approximately.

```
