Title:
```
Published as a conference paper at ICLR 2020 AUTOMATED CURRICULA THROUGH SETTER-SOLVER INTERACTIONS
```
Abstract:
```
Reinforcement learning algorithms use correlations between policies and rewards to improve agent performance. But in dynamic or sparsely rewarding environ- ments these correlations are often too small, or rewarding events are too infre- quent to make learning feasible. Human education instead relies on curricula-the breakdown of tasks into simpler, static challenges with dense rewards-to build up to complex behaviors. While curricula are also useful for artificial agents, hand- crafting them is time consuming. This has lead researchers to explore automatic curriculum generation. Here we explore automatic curriculum generation in rich, dynamic environments. Using a setter-solver paradigm we show the importance of considering goal validity, goal feasibility, and goal coverage to construct use- ful curricula. We demonstrate the success of our approach in rich but sparsely rewarding 2D and 3D environments, where an agent is tasked to achieve a sin- gle goal selected from a set of possible goals that varies between episodes, and identify challenges for future work. Finally, we demonstrate the value of a novel technique that guides agents towards a desired goal distribution. Altogether, these results represent a substantial step towards applying automatic task curricula to learn complex, otherwise unlearnable goals, and to our knowledge are the first to demonstrate automated curriculum generation for goal-conditioned agents in environments where the possible goals vary between episodes.
```

Figures/Tables Captions:
```
Figure 1: Training schematic (see also appendix B.2). The setter and judge only receive condi- tioning observations of the environment (blue) when the environment varies across episodes. The desirability loss (red) can be used when the distribution of desired tasks is known a priori.
Figure 2: Our environments. For the 2D grid-world task, the solver (white square) can pick-up objects (bi-colored squares). The object it is currently carrying is displayed in the upper left.
Figure 3: All three of our setter losses (V: validity, F: feasibility, C: coverage) are necessary (pink) in complex environments, but various subsets (green) suffice in simpler environments. (Curves plotted are rewards on random evaluation tasks averaged across three runs per condition ± standard deviation across runs; performance is averaged over the last 1000 trials for (a-b), and 5000 for (c). We plot performance as a % of the best agent performance.) (a) Color-pair finding, recolored 3D. (b) Grid-world alchemy.
Figure 4: In varying environments, setters that condition on environment observations outperform unconditioned setters, but unconditioned setters are still much better than random goals. (Curves plotted are evaluation rewards averaged across three runs per condition ± standard deviation across runs; performance is averaged over the last 15000 trials for (a), and 1000 for (b).)
Figure 5: Targeting a desired goal distribution, when one is known. (Averages across three runs per condition ± std. dev. across runs. Performance is averaged over the last 5000 and 1000 trials for (b) and (c), respectively.)
Figure 6: Comparison to the Goal GAN (Florensa et al., 2017), the closest approach to our own. (Curves plotted are averages across three runs per condition ± standard deviation across runs; per- formance is averaged over the last 1000 trials at each step for (b), and the last 5000 for (a,c).)
```

Main Content:
```

Section Title: INTRODUCTION
  INTRODUCTION Reinforcement learning (RL) algorithms use correlations between policies and environmental re- wards to reinforce and improve agent performance. But such correlation-based learning may strug- gle in dynamic environments with constantly changing settings or goals, because policies that cor- relate with rewards in one episode may fail to correlate with rewards in a subsequent episode. Correlation-based learning may also struggle in sparsely rewarding environments since by defini- tion there are fewer rewards, and hence fewer instances when policy-reward correlations can be measured and learned from. In the most problematic tasks, agents may fail to begin learning at all. While RL has been used to achieve expert-level performance in some sparsely rewarding games ( Sil- ver et al., 2016 ;  OpenAI, 2018 ;  Vinyals et al., 2019 ), success has often required carefully engineered curricula to bootstrap learning, such as learning from millions of expert games or hand-crafted shap- ing rewards. In some cases self-play between agents as they improve can serve as a powerful au- tomatic curriculum for achieving expert or superhuman performance ( Silver et al., 2018 ;  Vinyals et al., 2019 ). But self-play is only possible in symmetric two-player games. Otherwise humans must hand-design a curriculum for the agents, which requires domain knowledge and is time-consuming, especially as tasks and environments grow in complexity. It would be preferable to have an algorithm that could automatically generate a curriculum for agents as they learn.

Section Title: Published as a conference paper at ICLR 2020
  Published as a conference paper at ICLR 2020 Several automatic-curriculum generating algorithms have been proposed, including some that help agents explore and learn about their environments (e.g.  Gregor et al., 2016b ;  Eysenbach et al., 2018 ), and some that attempt to gradually increase goal difficulty (e.g.  Florensa et al., 2017 ). Most of these approaches have been tested only on simple tasks in simple environments, and often assume that either the environment is fixed from one episode to the next or that the agent's goal is fixed and unchanging. Ideally, curricula would apply to complex, varying environments and would support goal-conditioning to handle changing tasks. Surprise- or difficulty-based exploration may sometimes discover desired agent behaviors ( Gregor et al., 2016b ;  Burda et al., 2018 ;  Haber et al., 2018 ). This approach may not always be practical, though, since many difficult, but otherwise irrelevant tasks might "distract" exploration objectives. For example, training a self-driving car to successfully do flips might be challenging and novel, but it would not be particularly beneficial. Human curricula efficiently lead learners towards a desired competency, rather than along arbitrary dimensions of difficulty. Analogously, it would be useful for algorithms to leverage knowledge of the desired goal distribution to develop more targeted curricula. This paper take several steps toward automatic, targeted curriculum generation by proposing an al- gorithm for training a goal-conditioned agent in dynamic task settings with sparse rewards. The approach trains a "setter" model to generate goals for a "solver" agent by optimizing three setter ob- jectives: discovering the subset of expressible goals that are valid, or achievable by an expert solver (goal validity), encouraging exploration in the space of goals (goal coverage), and maintaining goal feasibility given the agent's current skill (goal feasibility). We also propose an extension for tar- geting a distribution of desired tasks (if one is known) using a Wasserstein discriminator (Arjovsky et al., 2017). We demonstrate our approach in a rich 3D environment and a grid-world wherein observation statistics and possible goals vary between episodes, and show that it substantially out- performs baselines, lesions and prior approaches.

Section Title: RELATED WORK
  RELATED WORK Uniform sampling of sub-tasks Perhaps the simplest curriculum is training uniformly over sub- tasks of varying difficulty. For example,  Agostinelli et al. (2019)  trained a Rubik's cube solver on problems sampled uniformly between 1 and K moves from the solved state. This curriculum lever- ages the fact that some sub-tasks can be solved before others, and that learning of these sub-tasks bootstraps learning of harder sub-tasks, and ultimately the task as a whole. However, in complex set- tings uniform training may not suffice, either because easier sub-tasks do not exist, they are still too hard to learn, or they do not help learning of harder sub-tasks. When uniform sampling is ineffec- tive, hand-engineered curricula may work ( Elman, 1993 ;  Bengio et al., 2009 ;  Zaremba & Sutskever, 2014 ;  Graves et al., 2016 ). Their effectiveness has led to research on automated ways to derive curricula ( Graves et al., 2017 ). Here we outline a number of such approaches in the RL setting.

Section Title: Exploration
  Exploration Some work leverages exploration to encourage state diversity ( Gregor et al., 2016b ), state-transition surprise ( Burda et al., 2018 ;  Haber et al., 2018 ), or distinguishable skills ( Eysenbach et al., 2018 ). These exploration-based methods are usually validated in relatively simple, unchanging environments, and have not been tested as pre-training for goal-conditioned RL tasks. A few studies have considered varying environments; e.g.  Wang et al. (2019)  considered evolving environments together with paired agents. However, because each agent is paired to a single environment, the method results in agents that are specialized to single, unchanging environments with fixed goals.

Section Title: Optimal task selection
  Optimal task selection Other approaches include selecting tasks on which learning is progressing (or regressing) the fastest ( Baranes & Oudeyer, 2013 ). However, it can be prohibitively expensive to determine goal regions and track progress within them, especially as task spaces grow larger and more complex. Some approaches work for a set of pre-specified tasks ( Narvekar & Stone, 2019 ), but they require human effort to hand-select tasks from this set. Again, these approaches have also generally been demonstrated in simple, fixed environments.

Section Title: Agent-agent interactions
  Agent-agent interactions Agent interactions can also generate effective curricula. For example, in symmetric two-player (or two-team) zero-sum games agents jointly improve and thus are forced to face stronger and stronger opponents. This natural curriculum may work on tasks where random play can achieve rewards with reasonable frequency ( Silver et al., 2018 ). But in other cases, hand- engineered auxiliary tasks may be used to avoid the difficult initial problem of learning from sparse Published as a conference paper at ICLR 2020 rewards, such as imitation learning on data from experts ( Silver et al., 2016 ;  Vinyals et al., 2019 ). Or, dense shaping rewards may be needed ( OpenAI, 2018 ;  Jaderberg et al., 2019 ). Furthermore, this type of curriculum has not been tested in goal-conditioned environments - while the environment might vary because of opponent play, or on a different map, the ultimate goal of winning is fixed. More fundamentally, while this type of curriculum works well for two-player zero-sum games, it is less clear how it can be used to train a single agent on a non-competitive, goal-conditioned task. Asymmetric agent-agent interactions, for example when one agent tries to repeat or undo another's actions ( Sukhbaatar et al., 2017 ), can also be useful. However, this requires the desired task distribu- tion to be close to the distribution generated by these reversing/repeating tasks. In goal-conditioned settings, guaranteeing this is likely as difficult as the original learning problem.

Section Title: Goal conditioning
  Goal conditioning In the goal-conditioned setting, hindsight experience replay ( Andrychowicz et al., 2017 ) has agents retrospectively imagine that they were trying to achieve the state they actually ended up in. While this is an active curriculum for starting learning, it does not necessarily encourage goal-space exploration, nor does it provide a framework for generating novel goals.  Nair et al. (2018)  used a generative model of state space to sample "imagined" goals, rewarding the agent based on similarity to the generative model's latent space.  Florensa et al. (2017)  used a GAN to generate goals of intermediate difficulty for the agent, which resulted in goals that gradually expanded to fill the goal space. This work is closely related to part of our proposal, and we use it as an important benchmark. Critically, this approach has not been tested in environments which vary substantially from episode to episode, particularly ones where the valid goals change from episode to episode. This is an important distinction because training generative models with non-trivial conditioning can be challenging. In particular, while conditioning directly on an informative latent variable can work well, for example when trying to generate images from a given class ( Mirza & Osindero, 2014 ;  Brock et al., 2018 ), even this problem is not completely solved ( Ravuri & Vinyals, 2019 ). Adding the challenge of trying to discover latent variables with which to condition and performing even a simple manipulation of them makes things much more difficult ( Rezende & Viola, 2018 ) (c.f. the difficulty of learning hierarchies of latent variables ( Sønderby et al., 2016 ;  Maaløe et al., 2019 )). This means that if the valid goals are not trivially observable from the environment, it may be difficult for the goal-setter to discover the goal structure via a generative loss alone. In section 4.2, we demonstrate this particular failure mode, along with some successes.

Section Title: Summary
  Summary A variety of automated curriculum generation approaches for RL have demonstrated some success, but the challenge of curriculum generation in the more complex settings remains open. This is because these approaches have not demonstrated success in tasks with the complexity reflective of difficult real-world tasks; in particular, no approach can handle goal-conditioned tasks in dynamic environments, wherein the set of possible goals varies from one episode to the next, and the set of possible goals might be tiny compared to the set of expressible goals.

Section Title: METHOD
  METHOD Our model consists of three main components: A solver - the goal-conditioned agent we are train- ing. A setter (S) - A generative model we are using to generate a curriculum of goals for the agent. A judge (J) - A discriminative model that predicts the feasibility of a goal for the agent at present. See appendix B for architectural details. See  fig. 1  for training schematics (see also Appendix B.2). The solver agent trains on setter- generated goals using a distributed learning setup to compute policy gradients ( Espeholt et al., 2018 ). For setter training, three concepts are important: goal validity, goal feasibility and goal coverage. We say a goal is valid if there exists a solver agent policy which has a non-zero probability of achiev- ing this goal. This concept is independent of the current policy of the solver. By contrast, feasibility captures whether the goal is achievable by the solver at present. Specifically, we say a goal has feasi- bility f ∈ [0, 1] if the probability that the solver will achieve the goal is f . The set of feasible goals will therefore evolve as the solver learns. The judge is a learned model of feasibility, trained via supervised learning on the solver's results. Finally, goal coverage indicates the variability (entropy) of the goals generated by the setter.

Section Title: REWARD AND LOSSES FOR THE SOLVER
  REWARD AND LOSSES FOR THE SOLVER Our solver is a goal conditioned RL agent. At the beginning of every episode it receives a goal g sampled by the setter, and a single reward R g at the end of the episode. The reward R g is 1 if the solver achieved the goal, or 0 if it did not after a fixed maximum amount of time. The solver could be trained by any RL algorithm. We chose to adopt the training setup and losses from  Espeholt et al. (2018) . The solver consists of a policy π and a baseline function V π which are trained using the V-trace policy gradient actor-critic algorithm with an entropy regularizer (see  Espeholt et al. (2018)  for details).

Section Title: LOSS FOR THE JUDGE
  LOSS FOR THE JUDGE The Judge J is trained as a binary classifier to predict the reward, 0 or 1. Given a goal g (see section 3.3), J(g) are logits such that σ(J(g)) = p(R g = 1|g), where σ is the sigmoid function, R g are returns obtained by the solver when trying to achieve those goals, and p(R g = 1|g) is the probability assigned by the judge that the agent will have a return of 1 when given goal g. We use a cross-entropy loss with the input distribution defined by the setter, and labels are obtained by testing the solver on these goals:

Section Title: LOSSES FOR THE SETTER
  LOSSES FOR THE SETTER Our setter takes as input a desired goal feasibility f ∈ (0, 1). In particular, we can sample a goal g = S(z, f ) for some sample z from a Gaussian prior N (0, 1) and a desired feasibility f , or we can map backwards from a goal g to a latent z = S −1 (g, f ), for which we can then compute the probability under the prior. Both directions are used in training. With these features in mind, we define three losses for the setter that reflect the concepts of goal validity, feasibility, and coverage:

Section Title: Validity
  Validity A generative loss that increases the likelihood of the setter generating goals which the solver has achieved. This is analogous to the hindsight of  Andrychowicz et al. (2017) , but from the setter perspective rather than the solver. Specifically: Where g is sampled from goals that the solver achieved, regardless of what it was tasked with on that episode, ξ is a small amount of noise to avoid overfitting 1 , and p(.) denotes the probability of Published as a conference paper at ICLR 2020 sampling that latent under a fixed gaussian prior for the latent of S. This loss may not cover all valid goals, but it is a good estimate available without any other source of knowledge.

Section Title: Feasibility
  Feasibility A loss that encourages the setter to choose goals which match the judge's feasibility estimates for the solver at present. Specifically: This loss uniformly samples a desired feasibility f (to train the setter to provide goals at a range of difficulties), then attempts to make the setter produce goals that the judge rates as matching that desired feasibility. Note although gradients pass through the judge, its parameters are not updated. Coverage: A loss that encourages the setter to pick more diverse goals. This helps the setter to cover the space of possible goals, and to avoid collapse. Specifically: This loss maximises the average of the conditional entropy of the setter. Since the density of f is constant, adding a term log(p(f )) in the above formula only changes the loss by a constant, and shows that our loss is equivalent to maximising the entropy of the joint distribution (S(z, f ), f ). The setter is trained to minimize the total loss L setter = L val. +L feas. +L cov. . Note that the sum L feas. + L cov. can be interpreted as a KL-divergence between an energy model and the setter's distribution. Specifically, for a fixed feasibility f , define an energy function on the space of goals by E f (g) = (J(g) − σ −1 (f )) 2 . Let p f (g) = e −E f (g) /Z be the density of the distribution defined by this energy, where Z is a normalizing constant. Then the sum of the feasibility and coverage losses is, up to a constant, the average over f ∈ [0, 1] of the divergence KL(p f ||p(S(g, f ))). We also demonstrate two important extensions to our framework which are critical in more compli- cated environments:

Section Title: Variable environments and conditioned setters
  Variable environments and conditioned setters While prior work has often focused on fixed en- vironments, such as the same maze each episode, we would like to train agents in variable worlds where the possible goals vary from one episode to the next. For this to be possible, our setter and judge must condition on an environmental observation. However, learning these conditional gener- ative models can be challenging if the valid goals are not trivially observable (see the related work section above). We demonstrate the success of our approach in these environments, and advantages with a conditioned setter and judge.

Section Title: Desired goal distributions
  Desired goal distributions In complex task spaces, the goals we want agents to accomplish will likely lie in a small region within the space of all possible goals. Thus it may not be efficient to uni- formly expand difficulty. We propose an additional loss for optimizing the setter towards a desired goal distribution, when such a distribution is known. Specifically, we propose training a Wasserstein discriminator (Arjovsky et al., 2017) to discriminate setter-generated goals from goals sampled from the desired goal distribution. The Wasserstein discriminator has the beneficial property that it can give useful gradients even when the distributions are non-overlapping, which is critical in this set- ting, since the easy goals the setter generates initially may not have any overlap with the target goal distribution. Specifically, the desirability discriminator loss is: L disc. = E g∈desired goal distribution [D(g)] − E z∈N (0,1),f ∈Unif(0,1) [D(S(z, f ))] and the setter is trained with the loss: Where β des. is a hyperparameter. While targeting the desired distribution can be helpful, it is usually not sufficient on its own - the desired tasks may be infeasible at first, so the other setter losses are needed to develop a feasible curriculum. The desirability loss just tries to aim this curriculum in the right direction.

Section Title: ENVIRONMENTS
  ENVIRONMENTS We work in two environments, which are briefly described below (see appendix C for further details). In each, the solver receives a goal as input during each episode, which it must attempt to achieve. 3D color finding: A semi-realistic 3D environment built in Unity (http://unity3d.com), consisting of a room containing colored objects and furniture (fig. 2a). The agent can move and look around, and can pick up, manipulate, and drop objects. This results in a complex 46-dimensional action space. Objects and furniture are randomly placed around the room at the beginning of each episode. The agent receives a color (or pair of colors) as a goal, and is rewarded if a patch (or two adjacent patches) in the center of its view contain average colors close to this goal. Both of these tasks sometimes require complex behavior 2 . For example, the agent might have to pick up an object of a yellow color, move it to an object of a blue color and look in between to obtain a green that was not otherwise present in the room. Our agents trained within our framework do indeed exhibit these behaviors. For our extensions, we also used a version of this environment in which the walls, ceiling, and floor of the room, as well as all objects, are procedurally recolored into one of two randomly chosen colors each episode (fig. 2b). This makes the achievable colors in each episode lie in a small subset of color space that overlaps little, if at all, with the achievable colors in other episodes. Grid-world alchemy: A 2D grid world environment, containing a variety of two-colored objects (fig. 2c). The colors of the objects are randomly sampled each episode. The solver can move around the grid, and can walk over an object to pick it up. It cannot put down an object once it has picked it up. If it is already carrying another object, the two objects will systematically combine to make a new object (specifically, the colors are combined by a component-wise max). The solver receives a goal object as input, and is rewarded if it produces a similar object. Because of the combinatorics of the possible object combinations, the irreversibility of picking an object up, and the difficulty of inferring the result of combining two objects, this environment is challenging for both the setter and the solver. Both have the challenging task of learning what is achievable in any particular episode, since each episode contains colors never seen before.

Section Title: Evaluation
  Evaluation In each experiment we evaluate on a fixed test distribution of tasks, regardless of what setter is used for training, in order to have a fair comparison between conditions. In both environments, the space of valid tasks (that could be done by an expert) occupies a small volume in the space of tasks expressible by the setter. In the colour-finding tasks, we do not even know which goals are valid, because of color averaging, shadows, etc. We therefore test on the full set of expressible goals (most of which are invalid), but report performance as a % of best observed scores.

Section Title: EXPERIMENTS 3
  EXPERIMENTS 3

Section Title: COMPLEX ENVIRONMENTS REQUIRE ALL THREE LOSSES
  COMPLEX ENVIRONMENTS REQUIRE ALL THREE LOSSES

Section Title: Complex environments require all three losses
  Complex environments require all three losses First, we demonstrate that it is necessary to consider all of goal validity, feasibility, and coverage in complex environments ( fig. 3 ). In the alchemy environment the validity and coverage losses are necessary, while the feasibility loss is not necessary, but does improve consistency (fig. 3a). In the 3D single-color-finding task, various subsets of the losses suffice for learning the task (fig. 3b). However, when the agent must find color pairs, fewer goals are possible and achieving a goal more often requires difficult manipulation of objects. Removing any of the setter losses results in substantially worse performance (fig. 3c). See Appendix B.3 for further analysis of the losses, and supplemental fig. 9 for a visualization of the generated curriculum on a simple location-finding task.

Section Title: ENVIRONMENTS THAT VARY REQUIRE OBSERVATION CONDITIONING
  ENVIRONMENTS THAT VARY REQUIRE OBSERVATION CONDITIONING

Section Title: Environments that vary require observation conditioning
  Environments that vary require observation conditioning While much prior work in automated curriculum generation focused on varying goals within a fixed environment, we would like RL systems to perform well on varied tasks in varied environments. For this, they will need to experience a diversity of environments during training, creating the unique challenge of generating curricula that take into account both the current environment and the current abilities of the agent. To address this we implemented a randomly colored version of our color-finding environment, and the grid-world alchemy task. In both, the set of possible goals changes each episode. We compare a version of our algorithm in which the setter and judge condition on an environmental observation before generating (or evaluating) a task to the basic unconditioned version used in the previous experiments, as well as a random baseline ( fig. 4 ). Solvers trained by the basic version of our Published as a conference paper at ICLR 2020 Untargeted T a rg e te d Desired (a) Motivation. (b) 3D color finding. (c) Grid-world alchemy. model still outperform those trained with randomly generated goals. However, the version of our model which conditions on an observation results in better solver performance. To the best of our knowledge, these are the first results demonstrating the success of any automated curriculum approach for goal-conditioned RL in a varying environment. There are a few points worth noting about our results in the alchemy environment. First, the uncon- ditioned setter had a tendency to not produce stable solver performance. Solver performance would generally degrade after reaching a maximum, while the conditioned setter was able to more steadily maintain solver performance. This was observed across a variety of hyperparameter settings, and merits further investigation. Second, even our conditioned setters are not leading the agents to per- fect performance on this task. However, in grid-world alchemy, the conditioned setter teaches the solver to reach performance close to that of a solver trained by an oracle which samples from the true distribution of possible tasks (fig. 4b). This suggests the limitation is not our setter algorithm, but rather the limitations of the solver agent, for example, the fact that it lacks features like planning ( Racanière et al., 2017 ) or relational inductive biases ( Zambaldi et al., 2019 ) that have proven useful for similar tasks. In more complex settings the setter may also need auxiliary supervision or stronger inductive bi- ases to overcome the challenges of learning conditional generative models. Indeed, we found that conditioning on a compressed representation (closer to the latent variables) in the recolored color- finding environment gave better results than conditioning on raw observations (see Fig. 10 in the Appendix). Furthermore, in more complex versions of the alchemy environment (for example, in- troducing more objects with more colors), even our conditioned setter algorithm could not learn to reliably generate feasible goals from raw observations. These results again highlight the challenges of learning conditional generative models when conditioning requires extracting latent variables and performing complex relational reasoning. This will be an important area for future work. Despite this caveat, the success of our setter-solver approach in varied environments represents an important step towards generating curricula in environments closer to the richness of the real world.

Section Title: TARGETING A DESIRED GOAL DISTRIBUTION IS MORE EFFICIENT
  TARGETING A DESIRED GOAL DISTRIBUTION IS MORE EFFICIENT

Section Title: Targeting a desired goal distribution is more efficient
  Targeting a desired goal distribution is more efficient In complex task environments discovering desired behaviors through difficulty-based exploration may not be feasible. There may be many ways a task can be difficult, most of which are irrelevant to what we would ultimately like the agent to achieve. By targeting the desired goal distribution with our desired-goal loss, the setter can push the solver toward mastering the desired tasks more efficiently (fig. 5a). In reality, the path will not be perfectly direct, as the setter trades off feasibility, possibility, and coverage with targeting the desired tasks. However, it will generally be more efficient than untargeted setting, or training on only the desired tasks (if they are difficult). We first explore this in the 3D color-finding environment. We target a distribution of pairs of 12 bright colors. These pairs are rarely achieved by a random policy, so discovering them is difficult without a setter. Training on only the desired distribution thus results in no learning. The untargeted (a) Location finding in 3D room. (b) Color finding in 3D room. (c) Color pair finding in 3D room. setter-solver setup does eventually learn these tasks. However, with targeting it discovers them much more rapidly (fig. 5b), and has a lasting advantage over the untargeted version (see supp. fig. 7). In the alchemy environment, the story is somewhat different (fig. 5c). We chose the desired distri- bution to be the most difficult tasks in the environment, consisting of combining half the objects in the room. However, because the setter has the difficult challenge of learning the conditional genera- tive distribution (which is built in to the desired distribution), we find that learning from the desired distribution (if available) results in earlier learning. This is in contrast to the 3D color finding en- vironment, where the desired distribution alone resulted in no learning. This again highlights the complexity of learning to generate goals when the valid goal distribution is conditional in complex, non-linear ways on the environment state. However, once the setter figures out the task structure, it is more easily able to train the solver, and so it surpasses desired distribution training to reach asymptotic mastery sooner. Furthermore, the fact that the desired tasks are somewhat feasible early in learning means that the targeted setter has less of an advantage over the regular setter.

Section Title: COMPARISON TO PRIOR WORK
  COMPARISON TO PRIOR WORK

Section Title: Comparison to prior work
  Comparison to prior work We compare to the Goal GAN ( Florensa et al., 2017 ), which is the closest to our approach. Our notion of goal feasibility is related to their binary partitioning of goals into those that are of interme- diate difficulty, and those that are not. However, our continuous notion of feasibility has advantages: it allows uniformly sampling feasibility, can be estimated from one run per goal, and may be easier to learn. Furthermore, while their discriminator may implicitly encourage increasing coverage and identifying valid goals, training GANs can be difficult, and our explicit losses may be more effective. We implemented an asynchronous version of the algorithm outlined in  Florensa et al. (2017) , which continuously trains the GAN and the agent, rather than iterating between training each. This allowed us to equalize computational resources between approaches, and apply their approach to the same distributed solver agent we used, in order to have as fair a comparison as possible. See appendix D for details. We first demonstrate that our implementation of their approach achieves similar per- formance to our method on a simple (x, y) location-finding task like that used in their paper, but implemented in our more complex 3D environment (fig. 6a), and learnt from pixels rather than state. However, we show that on our more complex color-finding tasks their approach is not as suc- cessful as ours (fig. 6b-c, and supp. fig. 8). Furthermore, maintaining and sampling from a large memory buffer, and running the agents on each goal multiple times to get a label of whether it was intermediate difficulty were very costly, and their approach thus required more memory and wall- clock time than ours for an equivalent amount of agent steps. In addition, the instabilities introduced by the adversarial training resulted in less consistent results from their approach even on the simple location finding task. Overall, our results suggest that our approach is more stable and is more effective on complex tasks. Furthermore, as noted above, Florensa et al. did not attempt the challenge of curriculum generation in environments that vary (which is why we did not compare to their algorithm in the alchemy environment), while we have also demonstrated success in that setting.

Section Title: DISCUSSION
  DISCUSSION In this paper we outlined a strategy for automated curriculum generation for goal-conditioned RL agents in complex environments. The curriculum is generated by training a setter to propose goals for a solver agent. The setter is trained to choose goals based on their validity, feasibility and cov- erage, and we demonstrated that all three of these components are necessary in a complex environ- ment. Furthermore, we showed that this approach substantially outperforms a prior approach and baselines on complex tasks, including 3D environments with rich visual experiences, interactions with objects, and complex control (a nearly 50-dimensional action space). These results represent a substantial step towards automated curriculum generation in rich environments. We also highlighted the necessity of employing curriculum generation in environments that vary from episode to episode. To address this challenge, we demonstrated that by providing an environ- mental observation to the setter and judge, our algorithm can learn to generate reasonable curricula in variable environments. This approach outperformed a lesioned version without the environmen- tal observation and other baselines, and nearly reached the performance of an oracle curriculum based on the true task distribution (where available). To our knowledge, these are the first results to demonstrate automated curriculum generation for goal-conditioned agents in environments where the possible goals vary from episode to episode. This is an important step towards developing auto- mated curricula in environments with complexity closer to the real world. However, our work also highlights challenges when the environment varies. Learning a conditional generative model for the setter in combinatorially complex environments like our alchemy setting can be challenging. From only a generative loss, it is difficult for the model to learn how to extract the appropriate latent variables from an observation and manipulate them appropriately. Training setters in rich environments may require auxiliary information about the structure of the world, or breakthroughs in conditional generative modelling. This is an important direction for future work. Finally, we pointed out the challenge of efficiently achieving competence on desired goals which are distributed in a small region of goal space. We demonstrated a loss that can help to solve this problem by targeting the setter toward the desired goal distribution. Overall, we showed the success of our setter-solver approach in rich environments, and extensions that allowed it to work in complex tasks with varying environments and guide the solver efficiently towards mastering desired goals. Although work remains to be done, we believe that the strategies we have outlined here will be a useful starting point for automatically devising curricula for RL agents in the increasingly complex tasks we desire them to solve.

Section Title: ACKNOWLEDGEMENTS
  ACKNOWLEDGEMENTS

```
