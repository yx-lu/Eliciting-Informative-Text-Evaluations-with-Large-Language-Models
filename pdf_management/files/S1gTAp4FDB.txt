Title:
```
Under review as a conference paper at ICLR 2020 NEURAL-GUIDED SYMBOLIC REGRESSION WITH ASYMPTOTIC CONSTRAINTS
```
Abstract:
```
Symbolic regression is a type of discrete optimization problem that involves search- ing expressions that fit given data points. In many cases, other mathematical constraints about the unknown expression not only provide more information be- yond just values at some inputs, but also effectively constrain the search space. We identify the asymptotic constraints of leading polynomial powers as the func- tion approaches 0 and ∞ as useful constraints and create a system to use them for symbolic regression. The first part of the system is a conditional production rule generating neural network which preferentially generates production rules to construct expressions with the desired leading powers, producing novel expressions outside the training domain. The second part, which we call Neural-Guided Monte Carlo Tree Search, uses the network during a search to find an expression that conforms to a set of data points and desired leading powers. Lastly, we provide an extensive experimental validation on thousands of target expressions showing the efficacy of our system compared to exiting methods for finding unknown functions outside of the training set.
```

Figures/Tables Captions:
```
Figure 1: Overview. (a) Exemplary expression. (b) Leading powers of 1/(x + 1) at 0 and ∞. (c) Parse tree of 1/(x + 1). (d) Production rule sequence, the preorder traversal of production rules in the parse tree. (e) Architecture of the model to predict the next production rule from the partial sequence conditioned on desired leading powers. (f) Using (e) to guide MCTS.
Figure 2: Extrapolation errors of symbolic re- gression methods in holdout set M [f ] = 5. Each expression is a point, where log 10 ∆g ext.
Figure 3: Plot of force field expressions found by each method. Grey area is the region to compute interpolation error ∆g int. and light blue area is the region to compute extrapolation error ∆g ext. .
Figure 4: Visualizing metrics for conditional production rule generating NN and LHNC (8) on each condition within |P x→0 [f ]| ≤ 9 and |P x→∞ [f ]| ≤ 9. Conditions with M [f ] ≤ 4 are inside the red boundary and points with 0 value are left blank.
Table 1: Results of symbolic regression methods. Search expressions in holdout sets M [f ] ≤ 4, M [f ] = 5 and M [f ] = 6 with data points on D train and / or leading powers P x→0 [f ] and P x→∞ [f ]. The options are marked by on ( √ ), off (×) and not available (-). If the RMSEs of the best found expression g(x) in interpolation and extrapolation are both smaller than 10 −9 and ∆P [g] = 0, it is solved. If g(x) is non-terminal or ∞, it is invalid. Hard includes expressions in the holdout set which are not solved by any of the six methods. The medians of ∆g train , ∆g int. , ∆g ext. and the median absolute errors of leading powers ∆P [g] for hard expressions are reported.
Table 2: Results of force field expressions found by each method.
Table 3: Metrics for conditional production rule generating NN and baseline models.
Table 4: Examples of top-3 probable expression completions for different desired conditions.
```

Main Content:
```

Section Title: INTRODUCTION
  INTRODUCTION The long standing problem of symbolic regression tries to search expressions in large space that fit given data points ( Koza & Koza, 1992 ;  Schmidt & Lipson, 2009 ). These mathematical expressions are much more like discovered mathematical laws that have been an essential part of the natural sciences for centuries. Since the size of the search space increases exponentially with the length of expressions, current search methods can only scale to find expressions of limited length. Moreover, current symbolic regression techniques fail to exploit a key value of mathematical expressions that has traditionally been well used by natural scientists. Symbolically derivable properties such as bounds, limits, and derivatives can provide significant guidance to finding an appropriate expression. In this work, we consider one such property corresponding to the behavior of the unknown function as it approaches 0 and ∞. Many expressions have a defined leading polynomial power in these limits. For examples when x → ∞, 2x 2 + 5x has a leading power of 2 (because the expression behaves like x 2 ) and 1/x 2 + 1/x has a leading power of −1. We call these properties "asymptotic constraints" because this kind of property is known a priori for some physical systems before the detailed law is derived. For example, most materials have a heat capacity proportional to T 3 at low temperatures T and the gravitational field of planets (at distance r) should behave as 1/r as r → ∞. Asymptotic constraints not only provide more information about the expression, leading to better extrapolation, but also constrain the search in the desired semantic subspace, making the search more tractable in much larger space. These constraints can not be simply incorporated using syntactic restrictions over the grammar of expressions. We present a system to effectively use asymptotic constraints for symbolic regression, which has two main parts. The first is a conditional production rule generating neural network (NN) of the desired polynomial leading powers that generates production rules to construct novel expressions (both syntactically and semantically) and, more surprisingly, generalize to leading powers not in the training set. The second part is a Neural- Guided Monte Carlo Tree Search (NG-MCTS) that uses this NN to probabilistically guide the search at every step to find expressions that fit a set of data points. Finally, we provide an extensive empirical evaluation of the system compared to several strong baseline techniques. We examine both the NG-MCTS and conditional production rule generating NN Under review as a conference paper at ICLR 2020 alone. In sharp contrast to almost all previous symbolic regression work, we evaluate our technique on thousands of target expressions and show that NG-MCTS can successfully find the target expressions in a much larger fraction of cases (71%) than other methods (23%) with search space sizes of more than 10 50 expressions. In summary, this paper makes the following key contributions: 1) We identify asymptotic constraints as important additional information for symbolic regression tasks. 2) We develop a conditional production rule generating NN to learn a distribution over (syntactically-valid) production rules conditioned on the asymptotic constraints. 3) We develop the NG-MCTS algorithm that uses the conditional production rule generating NN to efficiently guide the MCTS in large space. 4) We extensively evaluate our production rule generating NN to demonstrate generalization for leading powers, and show that the NG-MCTS algorithm significantly outperforms previous techniques on thousands of tasks.

Section Title: PROBLEM DEFINITION
  PROBLEM DEFINITION In order to demonstrate how prior knowledge can be incorporated into symbolic regression, we construct a symbolic space using a context-free grammar G: This expression space covers a rich family of rational expressions, and the size of the space can be further parameterized by a bound on the maximum sequence length. For an expression f (x), the leading power at x 0 is defined as P x→x0 [f ] = p s.t. lim x→x0 f (x)/x p = non-zero constant. In this paper, we consider the leading powers at x 0 ∈ {0, ∞} as additional specification. Let S(G, k) denote the space of all expressions in the Grammar G with a maximum sequence length k. Conventional symbolic regression searches for a desired expression f (x) in the space of expressions S(G, k) that conforms to a set of data points {(x, f (x)) | x ∈ D train }, i.e. find a g(x) ∈ S(G, k) : φ(g(x), D train ), where φ denotes the acceptance criterion, usually root mean square error (RMSE). With the additional specification of leading powers c (0) and c (∞) at 0 and ∞, the problem becomes: find

Section Title: CONDITIONAL PRODUCTION RULE GENERATING NEURAL NETWORK
  CONDITIONAL PRODUCTION RULE GENERATING NEURAL NETWORK It is difficult to directly incorporate the asymptotic constraints as syntactic restrictions over the grammar. We, therefore, develop a neural architecture that learns to generate production rule in the grammar conditioned on the given asymptotic constraints. Under review as a conference paper at ICLR 2020 Figure 1(a) and (c) show an example of how an expression is parsed as a parse tree by the grammar defined in Eq. (1). The parse tree in Figure 1(c) can be serialized into a production rule sequence r 1 , . . . , r L by a preorder traversal (Figure 1(d)), where L denotes the length of the production rule sequence. Figure 1(b) shows the leading powers of the exemplary expression in Figure 1(a). The conditional distribution of an expression is parameterized as a sequential model We build a NN (as shown in Figure 1(e)) to predict the next production rule r t+1 from a partial sequence r 1 , . . . , r t and conditions c (0) , c (∞) . During training, each expression in the training set is first parsed as a production rule sequence. Then a partial sequence of length t ∈ {1, . . . , L − 1} is sampled randomly as the input and the (t + 1)-th production rule is selected as the output (see blue and orange text in Figure 1(d)). Each production rule of the partial sequence is represented as an embedding vector of size 10. The conditions are concatenated with each embedding vector. This sequence of embedding vectors are fed into a bidirectional Gated Recurrent Units (GRU) ( Cho et al., 2014 ) with 1000 units. A softmax layer is applied to the final output of GRU to obtain the raw probability distribution over the next production rules in Eq. (1). Note that not all the production rules are grammatically valid as the next production rule for a given partial sequence. The partial sequence is equivalent to a partial parse tree. The next production rule expands the leftmost non-terminal symbol in the partial parse tree. For the partial sequence colored in blue in Figure 1(d), the next production rule expands non-terminal symbol T , which constrains the next production rule to only those with left-hand-side symbol T . We use a stack to keep track of non-terminal symbols in a partial sequence as described in GVAE ( Kusner et al., 2017 ). A mask of valid production rules is computed from the input partial sequence. This mask is applied to the raw probability distribution and the result is normalized to 1 as the output probability distribution. The training loss is calculated as the cross entropy between the output probability distribution and the next target production rule. It is trained from partial sequences sampled from expressions in the training set using validation loss for early stopping.

Section Title: NEURAL-GUIDED MONTE CARLO TREE SEARCH
  NEURAL-GUIDED MONTE CARLO TREE SEARCH We now briefly describe the NG-MCTS algorithm that uses the conditional production rule generating NN to guide the symbolic regression search. The discrepancy between the best found expression g(x) and the desired f (x) is evaluated on data points and leading powers. The error on data points is measured by RMSE ∆g {·} = x∈D {·} (f (x) − g(x)) 2 /|D {·} | on training points D train : {1.2, 1.6, 2.0, 2.4, 2.8}, points in interpolation region D interpolation : {1.4, 1.8, 2.2, 2.6} and points in extrapolation region D extrapolation : {5, 6, 7, 8, 9}. The error on leading powers is measured by sum of absolute errors at 0 and ∞, ∆P [g] = |P x→0 [f ] − P x→0 [g]| + |P x→∞ [f ] − P x→∞ [g]|. The default choice of objective function for symbolic regression algorithms is ∆g train alone. With additional leading powers constraint, the objective function can be defined as ∆g train + ∆P [g], which minimizes both the RMSE on the training points and the absolute difference of the leading powers. Most symbolic regression algorithms are based on EA ( Schmidt & Lipson, 2009 ), where it is nontrivial to incorporate our conditional production rule generating NN to guide the generation strategy in a step-by-step manner, as the mutation and cross-over operators perform transformations on fully completed expressions. However, it is possible to incorporate a probability distribution over expressions in many heuristic search algorithms such as Monte Carlo Tree Search (MCTS). MCTS is a heuristic search algorithm that has been shown to perform exceedingly well in problems with large combinatorial space, such as mastering the game of Go ( Silver et al., 2016 ) and planning chemical syntheses ( Segler et al., 2018 ). In MCTS for symbolic regression, a partial parse tree sequence r 1 , . . . , r t can be defined as a state s t and the next production rule is a set of actions {a}. In the selection step, we use a variant of the PUCT algorithm ( Silver et al., 2016 ;  Rosin, 2011 ) for exploration. For MCTS, the prior probability distribution p(a i |s t ) is uniform among all valid actions. We develop NG-MCTS by incorporating the conditional production rule generating NN into MCTS for symbolic regression. Figure 1(f) presents a visual overview of NG-MCTS. In particular, the prior Under review as a conference paper at ICLR 2020

Section Title: Dataset and Search Space
  Dataset and Search Space We denote the leading power constraint as a pair of integers (P x→0 [f ], P x→∞ [f ]) and define the complexity of a condition by M [f ] = |P x→0 [f ]| + |P x→∞ [f ]|. Obviously, expressions with M [f ] = 4 are more complicated to construct than those with M [f ] = 0. We create a dataset balanced on each condition, as described in Appendix A. The conditional produc- tion rule generating NN is trained on 28837 expressions and validated on 4095 expressions, both with M [f ] ≤ 4. The training expressions are sampled sparsely, which are only 10 −23 of the expressions within 31 production rules. Symbolic regression tasks are evaluated on 4250 expressions in holdout sets with M [f ] ≤ 4, = 5, = 6. The challenges are: 1) conditions M [f ] = 5, 6 do not exist in training set; 2) the search spaces of M [f ] = 5, 6 are 10 7 and 10 11 times larger than the training set.

Section Title: EVALUATION OF SYMBOLIC REGRESSION TASKS
  EVALUATION OF SYMBOLIC REGRESSION TASKS We now present the evaluation of our NG-MCTS method, where each step in the search is guided by the conditional production rule generating NN. Recent developments of symbolic regression methods ( Kusner et al., 2017 ;  Sahoo et al., 2018 ) compare methods on only a few expressions, which may cause the performance to depend on random seed for initialization and delicate tuning. To mitigate this, we apply different symbolic regression methods to search for expressions in holdout sets with thousands of expressions and compare their results in  Table 1 . Additional comparison on a subset of holdout sets is reported in Appendix H. obtained by NG-MCTS is on x-axis and those obtained by other three methods are on the y-axis. We first discuss the results for holdout set M [f ] ≤ 4. Conventional symbolic regression only fits on the data points D train . EA solves 12.83% expressions, while MCTS only solves 0.54% expressions. This suggests that compared to EA, MCTS is not efficient in searching a large space with limited number of simulations. The median errors ∆P [g] are both 3 for hard expressions (expressions unsolved by all methods), which are large as maximum M [f ] for this set is 4. In order to examine the effect of leading powers, we use leading powers alone in MCTS (PW-ONLY). The median of ∆P [g] for hard expressions is reduced to 1 but the medians of ∆g int. and ∆g ext. are significantly higher. We then add leading powers to the objective function together with data points. MCTS + PW does not have a notable difference to MCTS. However, EA + PW improves solved expressions to 23.37% and ∆P [g] of hard expressions is 0. This indicates adding leading power constraints in the objective function is helpful for symbolic regression. Most importantly, we observe step-wise guidance of NN conditioned on leading powers can lead to even more significant improvements compared to adding them in the objective function. NG-MCTS solves 71.22% expressions in the holdout set, three times over the best EA + PW. Note that both MCTS + PW and EA + PW have access to the same input information. Although EA has the lowest medians of ∆g train and ∆g int. , NG-MCTS is only slightly worse. On the other hand, NG-MCTS outperforms on ∆g ext. and ∆P [g], which indicates that step-wise guidance of leading powers helps to generalize better in extrapolation than all other methods. We also apply the aforementioned methods to search expressions in holdout sets M [f ] = 5 and M [f ] = 6. The percentage of solved expressions decreases as M [f ] increases as larger M [f ] requires learning expressions with more complex syntactic structure. The median of ∆P [g] also increases with larger M [f ] for the other methods, but the value for NG-MCTS is always zero. This demonstrates that our NN model is able to successfully guide the NG-MCTS even for leading powers not appearing in the training set. Due to the restriction on the computational time of Bayesian optimization for GVAE, we evaluate GVAE + PW on a subset of holdout sets (Appendix H). GVAE+ PW fails in holdout sets M [f ] = 5, 6. Overall, NG-MCTS still significantly outperforms other methods in solved percentage and extrapolation.  Figure 2  compares ∆g ext. for each expression among different methods in holdout set M [f ] = 5. The upper right cluster in each plot represents expressions unsolved by both methods. Most of the plotted points are above the 1:1 line (dashed), which shows that NG-MCTS outperforms the others for most unsolved expressions in extrapolation. Examples of expressions solved by NG-MCTS but unsolved by EA + PW and vice versa are presented in Appendix J. We also perform similar experiments with Gaussian noise on D train and NG-MCTS still outperforms all other methods ( Appendix K).

Section Title: CASE STUDY: FORCE FIELD POTENTIAL
  CASE STUDY: FORCE FIELD POTENTIAL Molecular dynamics simulations ( Alder & Wainwright, 1959 ) study the dynamic evolution of physical systems, with extensive applications in physics, quantum chemistry, biology and material science. The interaction of atoms or coarse-grained particles ( Kmiecik et al., 2016 ) is described by potential energy function called force field, which is derived from experiments or computations of quantum mechanics algorithms. Typically, researchers know the interactions in short and long ranges, which are examples Under review as a conference paper at ICLR 2020 Coulomb interaction, uniform electric field and harmonic interaction. Assuming the true potential is unknown, the goal is to discover this expression. As a physical potential, besides values at D train : {1.2, 1.6, 2.0, 2.4, 2.8}, researchers also know the short (x → 0) and long range (x → ∞) behaviors as leading powers.  Table 2  shows the expressions found by NG-MCTS, GVAE, GVAE + PW, EA and EA + PW, which are plotted in  Figure 3 . NG-MCTS can find the desired expression. The second best method is GVAE + PW, differing by a constant of 1 from the true expression.

Section Title: EVALUATION OF CONDITIONAL PRODUCTION RULE GENERATING NN
  EVALUATION OF CONDITIONAL PRODUCTION RULE GENERATING NN NG-MCTS significantly outperforms other methods on searching expressions in large space. To better understand the effective guidance from NN, we demonstrate its ability to generate syntactically and semantically novel expressions given desired conditions. In order to examine the NN alone, we directly sample from the model by Eq. (2) instead of using MCTS. The model predicts the probability distribution over the next production rules from the starting rule r 1 : O → S and desired condition (c (0) , c (∞) ). The next production rule is sampled from distribution p θ (r 2 |r 1 , c (0) , c (∞) ) and then appended to r 1 . Then r 3 is sampled from p θ (r 3 |r 1 , r 2 , c (0) , c (∞) ) and appended to [r 1 , r 2 ]. This procedure is repeated until [r 1 , . . . , r L ] form a parse tree where all the leaf nodes are terminal, or the length of generated sequence reaches the prespecified limit, which is set to 100 for our experiments.

Section Title: Baseline Models
  Baseline Models We compare NN with a number of baseline models that provide a probability distribution over the next production rules. All these distributions are masked by the valid production rules computed from the partial sequence before sampling. For each desired condition within |P x→0 [f ]| ≤ 9 and |P x→∞ [f ]| ≤ 9, k = 100 expressions are generated. We consider the following baseline models: i) Neural Network No Condition (NNNC): same setup as NN except no conditioning on leading powers, ii) Random: uniform distribution over valid next production rules, iii) Full History (FH): using full partial sequence and conditions to sample next production rule from its empirical distribution, iv) Full History No Condition (FHNC), v) Limited History (LH) (l): sampling next production rule from its empirical distribution given only the last l rules in the partial sequence, and vi) Limited History No Condition (LHNC) (l). Note that the aforementioned empirical distributions are derived from the training set {f }. For limited history models, if l exceeds the length of the partial sequence, we instead take the full partial sequence. More details about the baseline models can be found in Appendix B.

Section Title: Metrics
  Metrics We propose four metrics to evaluate the performance. For each condition (c (0) , c (∞) ), k expressions {g i } are generated from model p θ (f |c (0) , c (∞) ). i) Success Rate: proportion of generated expressions with leading powers (P x→0 [g i ], P x→∞ [g i ]) that match the desired condition, ii) Mean L1-distance: the mean L1-distance between (P x→0 [g i ], P x→∞ [g i ]) and (c (0) , c (∞) ), iii) Syntactic Novelty Rate: proportion of generated expressions that satisfy the condition and are syntactically novel (no syntactic duplicate of generated expression in the training set), and iv) Semantic Novelty Rate: proportion of expressions satisfying the condition and are semantically novel (the expression obtained after normalizing the generated expression does not exist in the training set). For example, expressions x + 1 and (1) + x are syntactically different, but semantically duplicate. We use simplify function in SymPy ( Meurer et al., 2017 ) to normalize expressions. To avoid inflating the rates of syntactic and semantic novelty, we only count the number of unique syntactic and semantic novelties in terms of their expressions and simplified expressions, respectively.

Section Title: Quantitative Evaluation
  Quantitative Evaluation   Table 3  compares the model performance of baseline and NN models measured by different metrics. We define M [f ] ≤ 4 as in-sample condition region and M [f ] > 4 as out-of-sample condition region. In both regions, the generalization ability of the model is reflected by the number of syntactic and semantic novelties it generates, not just the number of successes. For Under review as a conference paper at ICLR 2020 example, FH behaves as a look-up table based method (i.e., sampling from the training set) so it has 100% success rate in in-sample condition region. However, it is not able to generate any novel expressions. NN has the best performance on the syntactic and semantic novelty rates in both the in- sample (35% and 2.7%) and out-of-sample (1416 and 1084) condition regions by a significant margin. This indicates the generalization ability of the model to generate unseen expressions matching a desired condition. It is worth pointing out that NNNC performs much worse than NN, which indicates that NN is not only learning the distribution of expressions in the dataset, but instead is also learning a conditional distribution to map leading powers to the corresponding expression distributions. Furthermore, the L1-distance measures the deviation from the desired condition when not matching exactly. NN has the least mean L1-distance in the out-of-sample condition region. This suggests that for the unmatched expressions, NN prefers expressions with leading powers closer to the desired condition than all other models. NN outperforms the other models not only on the metrics aggregated over all conditions, but also for individual conditions.  Figure 4  shows the metrics for NN and LHNC (8) on each condition. NN performs better in the in-sample region (inside the red boundary) and also generalizes to more conditions in the out-of-sample region (outside the red boundary).

Section Title: Qualitative Evaluation
  Qualitative Evaluation To better comprehend the learned NN model and its generative behavior, we also perform a task of expression completion given a structure template of the form 1/ − and a variety of desired conditions in  Table 4 . For each condition, 1000 expressions are generated by NN and the probability of each syntactically unique expression is computed from its occurrence. We first start with c (0) = 0, c (∞) = 1. The completed expression g(x) is required to be a nonzero constant as x → 0 and g(x) → x as x → ∞. The top three probabilities are close since this task is relatively easy to complete. We then repeat the task on c (0) = −1, c (∞) = 1 and c (0) = −2, c (∞) = 2, which Under review as a conference paper at ICLR 2020 are still in the in-sample condition region and the model can still complete expressions that match the desired conditions. We also show examples of c (0) = −3, c (∞) = 2, which is in the out-of-sample condition region. Note that to match condition c (∞) = −3, more complicated completion such as 1/(x * (x * x)) has to be constructed by the model. Even in this challenging case, the model can still generate some expressions that match the desired condition. We also show examples of the syntactic novelties learned by model in Appendix L. Symbolic Regression:  Schmidt & Lipson (2009)  present a symbolic regression technique to learn natural laws from experimental data. The symbolic space is defined by operators +, −, *, /, sin, cos, constants, and variables. An expression is repre- sented as a graph, where intermediate nodes rep- resent operators and leaves represent coefficients and variables. The EA varies the structures to search new expressions using a score that accounts for both accuracy and the complexity of the ex- pression. This approach has been further used to get empirical expressions in electronic engineer- ing ( Ceperic et al., 2014 ), water resources ( Klotz et al., 2017 ), and social science ( Truscott & Korns, 2014 ). GVAE ( Kusner et al., 2017 ) was recently proposed to learn a generative model of structured arithmetic expressions and molecules, where the latent representation captures the underlying structure. This model was further shown to improve a Bayesian optimization based method for symbolic regression. Similar to these approaches, most other approaches search for expressions from scratch using only data points ( Schmidt & Lipson, 2009 ;  Ramachandran et al., 2017 ;  Ouyang et al., 2018 ) without other symbolic constraints about the desired expression.  Abu-Mostafa (1994)  suggests incorporating prior knowledge of a similar form to our property constraints, but actually implements those priors by adding additional data points and terms in the loss function.

Section Title: Neural Program Synthesis
  Neural Program Synthesis Program synthesis is the task of learning programs in a domain-specific language (DSL) that satisfy a given specification ( Gulwani et al., 2017 ). It is closely related to symbolic regression, where the DSL can be considered as a grammar defining the space of expressions. Some recent works use neural networks for learning programs ( Devlin et al., 2017 ;  Balog et al., 2016 ;  Parisotto et al., 2017 ;  Vijayakumar et al., 2018 ). RobustFill ( Devlin et al., 2017 ) trains an encoder- decoder model that learns to decode programs as a sequence of tokens given a set of input-output examples. For more complex DSL grammars such as Karel that consists of nested control-flow, an additional grammar mask is used to ensure syntactic validity of the decoded programs ( Bunel et al., 2018 ). However, these approaches only use examples as specification. Our technique can be useful to guide search for programs where the program space is defined using grammars such as SyGuS with additional semantic constraints other than only examples ( Alur et al., 2013 ;  Si et al., 2018 ).

Section Title: CONCLUSION
  CONCLUSION We present a two-step framework of first learning a neural network of the relation between syntactic structure and leading powers and then using that to guide MCTS for efficient search. This framework is evaluated in the context of symbolic regression and focused on the leading power properties on thousands of desired expressions, a much larger evaluation set than benchmarks considered in existing literature. We plan to further extend the applicability of this framework to cover other symbolically derivable properties of expressions. Similar modeling ideas could be equally useful in general program synthesis settings, where other properties such as the desired time complexity or maximum control flow nesting could be used as constraints. Under review as a conference paper at ICLR 2020
  1 Model implemented in TensorFlow ( Abadi et al., 2016 ) and available in submitted materials.

```
