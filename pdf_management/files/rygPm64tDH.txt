Title:
```
None
```
Abstract:
```
Two important topics in deep learning both involve incorporating humans into the modeling process: Model priors transfer information from humans to a model by regularizing the model's parameters; Model attributions transfer information from a model to humans by explaining the model's behavior. Previous work has taken important steps to connect these topics through gradient regularization, but we find that these methods do not successfully align a model's behavior with human intuition. We develop an efficient and theoretically grounded feature attribution method, expected gradients, and a novel framework, attribution priors, to enforce prior expectations about a model's behavior during training. We demonstrate that attribution priors are broadly applicable in three different domains: image data, gene expression data, and health care data. Our experiments show that models trained with attribution priors are more intuitive and achieve better generalization performance than both equivalent baselines and existing methods to regularize model behavior.
```

Figures/Tables Captions:
```
Figure 1: Left: Expected gradients attributions (from 100 samples) on CIFAR10 for both the baseline model and the model trained with an attribution prior, for five randomly selected images classified correctly by both the baseline and the regularized model. Training with an attribution prior generates visually smoother attribution maps in all cases. Notably, these smoothed attributions also appear more localized towards the object of interest. Right: Training with an attribution prior induces robustness to Gaussian noise, achieving more than double the accuracy of the baseline at high noise levels. This robustness isn't achievable by choosing gradients as the attribution function.
Figure 2: Left: A neural network trained with our graph attribution prior (bold) attains the best test performance, while a neural network trained with the same graph penalty on the gradients (italics, adapted from (Ross et al., 2017b)) does not perform significantly better than a standard neural network. Right: A neural network trained with our graph attribution prior has far more significantly captured biological pathways than a standard neural network, and also captures more AML-relevant pathways.
Figure 3: Left: A sparse attribution prior enables more accurate test predictions (top) and sparser models (bottom) across 100 small subsampled datasets (100 training and 100 validation samples each). Top right: Across the full range of tuned parameters, the sparse attribution prior achieves greater sparsity and a smooth sparsity-performance tradeoff. Bottom right: A sparse attribution prior concentrates a larger fraction of global feature importance in the top few features.
Table 1: Benchmark results on synthetic data with correlated features. Larger numbers are better for all metrics. For metric names (K = Keep, R = Remove), (P = Positive, N = Negative, A = Absolute), (M = Mean masking, R = Resample masking, and I = Impute masking) (see Appendix for details).
```

Main Content:
```

Section Title: INTRODUCTION
  INTRODUCTION Recent work on interpreting machine learning models has focused on feature attribution methods. Given an input feature, a model, and a prediction on a particular sample, such methods assign a number to the input feature that represents how important the input feature was for making the prediction. Previous literature about such methods has focused on the axioms they should satisfy ( Lundberg and Lee, 2017 ;  Sundararajan et al., 2017 ;  Štrumbelj and Kononenko, 2014 ;  Datta et al., 2016 ), and how attribution methods can give us insight into model behavior ( Lundberg et al., 2018a ;b;  Sayres et al., 2019 ;  Zech et al., 2018 ). These methods can be an effective way of revealing problems in a model or a dataset. For example, a model may place too much importance on undesirable features, rely on many features when sparsity is desired, or be sensitive to high frequency noise. In such cases, we often have a prior belief about how a model should treat input features, but for neural networks it can be difficult to mathematically encode this prior in terms of the original model parameters.  Ross et al. (2017b)  introduce the idea of regularizing explanations to train models that better agree with domain knowledge. Given a binary variable indicating whether each feature should or should not be important for predicting on each sample in the dataset, their method penalizes the gradients of unimportant features. However, two drawbacks limit the method's applicability to real-world problems. First, gradients don't satisfy the theoretical guarantees that modern feature attribution methods do ( Sundararajan et al., 2017 ). Second, it is often difficult to specify which features should be important in a binary manner. More recent work has stressed that incorporating intuitive, human priors will be necessary for developing robust and interpretable models ( Ilyas et al., 2019 ). Still, it remains challenging to encode meaningful, human priors like "have smoother attribution maps" or "treat this group of features similarly" by penalizing the gradients or parameters of a model. In this work, we propose an expanded framework for encoding abstract priors, called attribution priors, in which we directly regularize differentiable functions of a model's axiomatic feature attributions during training. This framework, which can be seen as a generalization of gradient-based regularization ( LeCun et al., 2010 ;  Ross et al., 2017b ;  Yu et al., 2018 ;  Jakubovitz and Giryes, 2018 ;  Roth et al., 2018 ), can be used to encode meaningful domain knowledge more effectively than existing methods. Furthermore, we introduce a novel feature attribution method - expected gradients - which extends integrated gradients ( Sundararajan et al., 2017 ), is naturally suited to being regularized Under review as a conference paper at ICLR 2020 under an attribution prior, and avoids hyperparameter choices required by previous methods. Using attribution priors, we build improved deep models for three different prediction tasks. On images, we use our framework to train a deep model that is more interpretable and generalizes better to noisy data by encouraging the model to have piecewise smooth attribution maps over pixels. On gene expression data, we show how to both reduce prediction error and better capture biological signal by encouraging similarity among gene expression features using a graph prior. Finally, on a patient mortality prediction task, we develop a sparser model and improve performance when learning from limited training data by encouraging a skewed distribution of the feature attributions.

Section Title: ATTRIBUTION PRIORS
  ATTRIBUTION PRIORS In this section, we formally define an attribution prior, and give three example priors for different data types. Let X ∈ R n×p denote a dataset with labels y ∈ R o , where n is the number of samples, p is the number of features, and o is the number of outputs. In standard deep learning we aim to find optimal parameters θ by minimizing loss, subject to a regularization term Ω (θ) on the parameters: For some model parameters θ, let Φ(θ, X) be a feature attribution method, which is a function of θ and the data X. Let φ i be the feature importance of feature i in sample . We formally define an attribution prior as a scalar-valued penalty function of the feature attributions, Ω(Φ(θ, X)), which represents a log-transformed prior probability distribution over possible attributions. θ = argmin θ L(θ; X, y) + λΩ(Φ(θ, X)), where λ is the regularization strength. We note that the attribution prior function Ω is agnostic to the attribution method Φ. While in Section 3 we propose a feature attribution method for attribution priors, other attribution methods can be used. This includes existing methods like integrated gradients or simply the gradients themselves. In the latter case, we can see the method proposed in  Ross et al. (2017b)  as a specific instance of an attribution prior: θ = argmin θ L(θ; X, y) + λ ||A ∂L ∂X || 2 F where the attribution method Φ(θ, X) is the gradients of the model, represented by the matrix ∂L ∂X whose , ith entry is the gradient of the loss at the th sample with respect to the ith feature. A is a binary matrix indicating which features should be penalized in which samples. Often, however, we do not know which features are important in advance. Instead, we can define different attribution priors for different tasks depending on the data and our domain knowledge. To demonstrate how attribution priors can capture human intuition in a variety of domains, in the following sections we first define and then apply three different priors for three different data types.

Section Title: PIXEL ATTRIBUTION PRIOR FOR IMAGE CLASSIFICATION
  PIXEL ATTRIBUTION PRIOR FOR IMAGE CLASSIFICATION Prior work on interpreting image models has focused on creating pixel attribution maps, which assign a value to each pixel indicating how important that pixel was for a model's prediction ( Selvaraju et al., 2017 ;  Sundararajan et al., 2017 ). These attribution maps can be noisy and often highlight seemingly unimportant pixels in the background. Such attributions can be difficult to understand, and may indicate the model is vulnerable to adversarial attacks ( Ross and Doshi-Velez, 2018 ). Although we may desire a model with smoother attributions, existing methods only post-process attribution maps and do not change model behavior ( Smilkov et al., 2017 ;  Selvaraju et al., 2017 ;  Fong and Vedaldi, 2017 ). Such techniques may not be faithful to the original model ( Ilyas et al., 2019 ). In this section, we describe how to apply our framework to train image models with naturally smoother attributions. To regularize pixel-level attributions, we use the following intuition: neighboring pixels should have a similar impact on an image model's output. To encode this intuition, we apply a total variation loss on pixel-level attributions as follows: Under review as a conference paper at ICLR 2020 where φ i,j is the attribution for the i, j-th pixel in the -th training image. Including the λ scale factor, this penalty is equivalent to placing a Laplace(0, λ −1 ) prior on the differences between adjacent pixel attributions. For further details, see  Bardsley (2012)  and the Appendix.

Section Title: GRAPH ATTRIBUTION PRIOR FOR GENE EXPRESSION DATA
  GRAPH ATTRIBUTION PRIOR FOR GENE EXPRESSION DATA In the image domain, our attribution prior took the form of a penalty encouraging smoothness over adjacent pixels. In other domains, we may have prior information about specific relationships between features that can be encoded as an arbitrary graph (such as social networks, knowledge graphs, or protein-protein interactions). For example, prior work in bioinformatics has shown that protein- protein interaction networks contain valuable information that can be used to improve performance on biological prediction tasks ( Cheng et al., 2014 ). These networks can be represented as a weighted, undirected graph. Formally, say we have a weighted adjacency matrix W ∈ R p×p + for an undirected graph, where the entries encode our prior belief about the pairwise similarity of the importances between two features. For a biological network, W i,j encodes either the probability or strength of interaction between the i-th and j-th genes (or proteins). We can encourage similarity along graph edges by penalizing the squared Euclidean distance between each pair of feature attributions in proportion to how similar we believe them to be. Using the graph Laplacian (L G = D − W ), where D is the diagonal degree matrix of the weighted graph this becomes: In this case, we choose to penalize global rather than local feature attributions. So we defineφ i to be the importance of feature i across all samples in our data set, where this global attribution is calculated as the average magnitude of the feature attribution across all samples:φ i = 1 n n =1 |φ i |. Overall, Ω graph is equivalent to placing a Normal(0, λ −1 ) prior on the differences between attributions for features that are adjacent in the graph. See  Bardsley (2012)  and the Appendix for details.

Section Title: SPARSITY ATTRIBUTION PRIOR FOR FEATURE SELECTION
  SPARSITY ATTRIBUTION PRIOR FOR FEATURE SELECTION Feature selection and sparsity are popular ways to alleviate the curse of dimensionality, facilitate interpretability, and improve generalization by building models that use a small number of input features. A straightforward way to build a sparse deep model is to apply an L1 penalty to the first layer (and possibly subsequent layers) of the network. Similarly, the sparse group lasso (SGL) penalizes all weights connected to a given feature ( Feng and Simon, 2017 ;  Scardapane et al., 2017 ), while  Ross et al. (2017a)  penalize the gradients of each feature in the model. These approaches suffer from two problems: First, a feature with small gradients or first-layer weights may still strongly affect the model's output ( Shrikumar et al., 2017 ). A feature whose attribution value (e.g., integrated or expected gradient) is zero, is much less likely to have any effect on predictions. Second, successfully minimizing the L1 or SGL penalty is not necessarily the best way to create a sparse model. A model that puts weight w on 1 feature is penalized more than one that puts weight w 2p on each of p features. Prior work on sparse linear regression has shown that the Gini coefficient G of the weights, proportional to 0.5 minus the area under the CDF of sorted values, avoids such problems and corresponds more directly to a sparse model ( Hurley and Rickard, 2009 ;  Zonoobi et al., 2011 ). We extend this analysis to deep models by noting that the Gini coefficient can be written differentiably and using it to develop an attribution penalty based on the global feature attributionsφ i : This is similar to the total variation penalty Ω image , but normalized and with a flipped sign to encourage differences. The corresponding attribution prior is maximized when global attributions are zero for all but one feature, and minimized when attributions are uniform across features.

Section Title: EXPECTED GRADIENTS
  EXPECTED GRADIENTS Here we propose a feature attribution method called expected gradients and describe why it is a natural choice for attribution priors. Expected gradients is an extension of integrated gradients ( Sundararajan et al., 2017 ) with fewer hyperparameter choices. Like several other attribution methods, integrated gradients aims to explain the difference between a model's current prediction and the prediction that the model would make when given a baseline input. This baseline input is meant to represent some uninformative reference input, which represents not knowing the value of the input features. Although choosing such an input is necessary for several feature attribution methods ( Sundararajan et al., 2017 ;  Shrikumar et al., 2017 ;  Binder et al., 2016 ), the choice is often made arbitrarily. For example, in image tasks, the image of all zeros is often chosen as a baseline, but doing so implies that black pixels will not be highlighted as important by existing feature attribution methods. In many domains, it is not clear how to choose a baseline that correctly represents a lack of information. Our method avoids an arbitrary choice of baseline by modeling not knowing the value of a feature by integrating over a dataset. For a model f , the integrated gradients value for feature i is defined as: where x is the target input and x is baseline input. To avoid specifying x , we define the expected gradients value for feature i as: where D is the underlying data distribution. Since expected gradients is also a diagonal path method, it satisfies the same axioms as integrated gradients ( Friedman, 2004 ). Directly integrating over the training distribution is intractable; so we instead reformulate the integrals as expectations: This expectation-based formulation lends itself to a natural sampling based approximation method: draw samples of x from the training dataset and α from U (0, 1), compute the value inside the expectation for each sample, and average over samples.

Section Title: Training with expected gradients
  Training with expected gradients If we let the attribution function Φ in our attribution prior Ω(Φ(θ, X)) be expected gradients, a good approximation during training appears to require com- puting an expensive Monte Carlo estimate with hundreds of extra gradient calls every training step. Ordinarily, this would make training with such attributions intractable. However, most deep learning models today are trained using some variant of batch gradient descent, in which the gradient of a loss function is approximated over many training steps using mini-batches of data. We can use a batch training procedure to approximate expected gradients over the training procedure as well. During training, we let k be the number of samples we draw to compute expected gradients for each mini-batch of data. Remarkably, we find that as small as k = 1 suffices to regularize the explanations because of the averaging effect of the expectation formulation over many training samples. This choice of k leads to every sample in the training set being used as a reference over the course of an epoch with only one additional gradient call per training step. This results in far more reference samples than the 100-200 we found necessary for reliable individual attributions (see Appendix).

Section Title: EXPERIMENTS
  EXPERIMENTS We first evaluate expected gradients by comparing it with other feature attribution methods on 18 benchmarks introduced in  Lundberg et al. (2019)  ( Table 1 ). These benchmark metrics aim to evaluate how well each attribution method finds the most important features for a given dataset and model. For all metrics, a larger number corresponds to a better feature attribution method. Expected gradients significantly outperforms the next best feature attribution method (p = 7.2 × 10 −5 , one-tailed Binomial test). We provide more details and also additional benchmarks in the Appendix.

Section Title: A PIXEL ATTRIBUTION PRIOR IMPROVES ROBUSTNESS TO IMAGE NOISE
  A PIXEL ATTRIBUTION PRIOR IMPROVES ROBUSTNESS TO IMAGE NOISE We apply our Ω pixel attribution prior to the CIFAR-10 dataset ( Krizhevsky et al., 2009 ). We train a VGG16 network from scratch ( Simonyan and Zisserman, 2014 ), and optimize hyperparameters for the baseline model without an attribution prior. To choose λ, we search over values in [10 −20 , 10 −1 ], and choose the λ that minimizes the attribution prior penalty and achieves a test accuracy within 10% of the baseline model.  Figure 1  displays expected gradients attribution maps for both the baseline and the model regularized with an attribution prior on 5 randomly selected test images. In all examples, the attribution prior results in a model with visually smoother attributions. Remarkably, smoother attributions also often better highlight the structure of the target object in the image in many instances. Recent work in understanding image classifiers has suggested that they are brittle to small domain shifts: small changes in the underlying distribution of the training and test set can result in significant drops in test accuracy ( Recht et al., 2019 ). To simulate a domain shift, we apply Gaussian noise to images in the test set and re-evaluate the performance of the regularized model and the baseline model. As an adaptation of  Ross et al. (2017b) , we also compare to regularizing the total variation of gradients with the same criteria for choosing λ. For each method, we train 5 models with different random initializations. In  Figure 1 , we plot the mean and standard deviation of test accuracy as a function of standard deviation of added Gaussian noise. The figure shows that our regularized model is more robust to noise than both the baseline and the gradient-based model. Although our method provides both robustness and more intuitive saliency maps, this comes at the cost of reduced test set accuracy (0.93 ± 0.002 for the baseline vs. 0.85 ± 0.003 for pixel attribution prior model). The trade-off between robustness and accuracy that we observe is in line with previous work that suggests image classifiers trained solely to maximize test accuracy rely on features that are brittle and difficult to interpret ( Ilyas et al., 2019 ;  Tsipras et al., 2018 ;  Zhang et al., 2019 ). Despite this trade-off, we find that at a stricter hyperparameter cutoff for λ - within 1% test accuracy of the baseline, rather than 10% - our methods are still able to achieve modest but significant robustness relative to the baseline. For results at different hyperparameter thresholds, as well as more details on our training procedure and additional experiments on MNIST, see the Appendix.

Section Title: A GRAPH ATTRIBUTION PRIOR IMPROVES ANTI-CANCER DRUG RESPONSE PREDICTION
  A GRAPH ATTRIBUTION PRIOR IMPROVES ANTI-CANCER DRUG RESPONSE PREDICTION Incorporating the Ω graph attribution prior not only leads to a model with more reasonable attributions, but also improves predictive performance by allowing us to incorporate prior biological knowledge into the training process. We downloaded publicly available gene expression and drug response data for patients with acute myeloid leukemia (AML, a type of blood cancer) and tried to predict patients' drug response from their gene expression ( Tyner et al., 2018 ). For this regression task, an input sample was a patient's gene expression profile plus a one-hot encoded vector indicating which drug was tested in that patient, while the label we tried to predict was drug response (measured by IC50 - the concentration of the drug required to kill half of the patient's tumor cells). To define the graph Under review as a conference paper at ICLR 2020 used by our prior we downloaded the tissue-specific gene interaction graph for the tissue most closely related to AML in the HumanBase database ( Greene et al., 2015 ). We find that a two-layer neural network trained with our graph attribution prior (Ω graph ) significantly outperforms all other methods in terms of test set performance as measured by R 2 ( Figure 2 ). Unsurprisingly, when we replace the biological graph from HumanBase with a randomized graph, we find that the test performance is no better than the performance of a neural network trained without any attribution prior. Extending the method proposed in  Ross et al. (2017b)  by applying our novel graph prior as a penalty on the model's gradients, rather than a penalty on the axiomatically correct expected gradient feature attribution, does not perform statistically significantly better than a baseline neural network. We also observe significantly improved test performance when using the prior graph information to regularize a linear LASSO model. Finally, we note that our graph attribution prior neural network significantly outperforms a recent method for utilizing graph information in deep neural networks, graph convolutional neural networks ( Kipf and Welling, 2016 ). To see if our model's attributions match biological intuition we conducted Gene Set Enrichment Analysis (a modified Kolmogorov-Smirnov test) to see if our top genes, as ranked by mean absolute feature attribution, were enriched for membership in any pathways (see the Appendix for more details, including the top pathways for each model) ( Subramanian et al., 2005 ). We see that the neural network with the tissue-specific graph attribution prior captures significantly more biologically- relevant pathways (increased number of significant pathways after FDR correction) than a neural network without attribution priors (See  Figure 2 ) ( Benjamini and Hochberg, 1995 ). Furthermore, the pathways used by our model more closely match with biological expert knowledge - pathways included prognostically useful AML gene expression profiles, as well as important AML-related transcription factors (see  Figure 2  and Appendix) ( Liu et al., 2017 ;  Valk et al., 2004 ).

Section Title: A SPARSITY PRIOR IMPROVES PERFORMANCE WITH LIMITED TRAINING DATA
  A SPARSITY PRIOR IMPROVES PERFORMANCE WITH LIMITED TRAINING DATA Here, we show that the Ω sparse attribution prior can build sparser models that perform significantly better in settings with limited training data. We use a publicly available healthcare mortality prediction dataset of 13,000 patients ( Miller, 1973 ), where the 36 features (119 after one-hot encoding) represent Under review as a conference paper at ICLR 2020 medical data such as a patient's age, vital signs, and laboratory measurements. The binary outcome is survival after 10 years. Sparse models in this setting may enable accurate models to be trained with very few labeled patient samples or reduce cost by accurately risk-stratifying patients using few lab tests. We subsample the training and validation sets to each contain only 100 patients, and run each experiment 100 times with a new random subsample to average out variance. We build 3-layer binary classifier neural networks regularized using L1, sparse group lasso (SGL) and sparse attribution prior penalties to predict patient survival, as well as an L1 penalty on gradients adapted for global sparsity from  Ross et al. (2017b; a). The regularization strength was tuned from 10 −10 to 10 3 using the validation set for all methods, and the best model for each run was chosen using validation performance over 100 models trained with the chosen parameters (see Appendix). The sparse attribution prior enables more accurate test predictions ( Figure 3 ) and sparser models when little training data is available, with p < 10 −3 by Wilcoxon signed-rank test for all comparisons. We also plot the average cumulative importance of sorted features and find that the sparse attribution prior is much more effective at concentrating importance in the top few features ( Figure 3 ). In particular, L1 penalizing the model's gradients as in  Ross et al. (2017a)  improves neither sparsity nor performance. A Gini gradient penalty slightly improves performance and sparsity but does not match the sparse attribution prior. Finally, we plot the average sparsity of the models (Gini coefficient) against their validation ROC-AUC across the full range of regularization strengths ( Figure 3 ). The sparse attribution prior attains higher performance and sparsity than other models. Details and results for L2 penalties, dropout, and other attribution priors are in the Appendix.

Section Title: RELATED WORK
  RELATED WORK There have been many previous attribution methods proposed for deep learning models ( Lundberg and Lee, 2017 ;  Binder et al., 2016 ;  Shrikumar et al., 2017 ;  Sundararajan et al., 2017 ). We chose to extend integrated gradients because it is easy to differentiate and comes with theoretical guarantees. Training with gradient penalties has also been discussed by existing literature.  Drucker and Le Cun (1992)  introduced the idea of regularizing the magnitude of model gradients in order to improve generalization performance on digit classification. Since then, gradient regularization has been used extensively as an adversarial defense mechanism in order to minimize changes to network outputs over small perturbations of the input ( Jakubovitz and Giryes, 2018 ;  Yu et al., 2018 ;  Roth et al., 2018 ).  Ross and Doshi-Velez (2018)  make a connection between gradient-based training for adversarial purposes and network interpretability.  Ilyas et al. (2019)  formally describe how the phenomena of Under review as a conference paper at ICLR 2020 adversarial examples may arise due to features that are predictive yet non-intuitive, and stress the need to incorporate human intuition into the training process. There is very little previous work on actually incorporating feature attribution methods into training.  Sen et al. (2018)  formally describe the problem of classifiers having unexpected behavior on inputs not seen in the training distribution, like those generated by asking whether a prediction would change if a particular feature value changed. They describe an active learning algorithm that updates a model based on points generated from a counter-factual distribution. Their work differs from ours in that they use feature attributions to generate counter-factual examples, but do not directly penalize the attributions themselves.  Ross et al. (2017b)  introduce the idea of training models to have correct explanations, not just good performance. Their method can be seen as a specific instance of our framework, in which the attribution function is gradients and the penalty function is minimizing the gradients of features known to be unimportant for each sample. Our work is more general in two ways. First, we instantiate three different penalty functions that encode human intuition without needing to know which features are unimportant in advance. Second, we propose a novel feature attribution method that can be regularized efficiently using a sampling procedure, and show that doing so provides better generalization performance than regularizing gradients with the same penalty.

Section Title: DISCUSSION
  DISCUSSION The immense popularity of deep learning has driven its application in many domains with diverse, complicated prior knowledge. While it is in principle possible to hand-design network architectures to encode this knowledge, we propose a simpler approach. Using attribution priors, any knowledge that can be encoded as a differentiable function of feature attributions can be used to encourage a model to act in a particular way in a particular domain. We also introduce expected gradients, a feature attribution method that is theoretically justified and removes the choice of a single reference value that many existing feature attribution methods require. We further demonstrate that expected gradients naturally integrates with attribution priors via sampling during SGD. The combination allows us to improve model performance by encoding prior knowledge across several different domains. It leads to smoother and more interpretable image models, biological predictive models that incorporate graph-based prior knowledge, and sparser health care models that can perform better in data-scarce scenarios. Attribution priors provide a broadly applicable framework for encoding domain knowledge, and we believe they will be valuable across a wide array of domains in the future.

```
