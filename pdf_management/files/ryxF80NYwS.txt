Title:
```
Under review as a conference paper at ICLR 2020 NEURAL CLUSTERING PROCESSES
```
Abstract:
```
Mixture models, a basic building block in countless statistical models, involve latent random variables over discrete spaces, and existing posterior inference methods can be inaccurate and/or very slow. In this work we introduce a novel deep learning architecture for efficient amortized Bayesian inference over mixture models. While previous approaches to amortized clustering assumed a fixed or maximum number of mixture components and only amortized over the continuous parameters of each mixture component, our method amortizes over the local discrete labels of all the data points, and performs inference over an unbounded number of mixture components. The latter property makes our method natural for the challenging case of nonparametric Bayesian models, where the number of mixture components grows with the dataset. Our approach exploits the exchangeability of the generative models and is based on mapping distributed, permutation-invariant representations of discrete arrangements into varying-size multinomial conditional probabilities. The resulting algorithm parallelizes easily, yields iid samples from the approximate posteriors along with a normalized probability estimate of each sample (a quantity generally unavailable using Markov Chain Monte Carlo) and can easily be applied to both conjugate and non-conjugate models, as training only requires samples from the generative model. We also present an extension of the method to models of random communities (such as infinite relational or stochastic block models). As a scientific application, we present a novel approach to neural spike sorting for high-density multielectrode arrays.
```

Figures/Tables Captions:
```
Figure 1: Encoding cluster labels. The col- ored points have fixed labels c 1:n−1 , forming K = 3 clusters. The four possible labels for the circled point give four encoding vectors G k , while the vector U encodes the 3 gray unlabeled points (Best seen in color).
Figure 2: Architecture of the Neural Clustering Process. The full model is composed by the deep networks h, g, u, f . Left: After assigning the cluster labels c 1:n−1 , each possible discrete value k for c n gives a different symmetry-invariant encoding of x 1:n into the vector G k , using the functions h and g. The remaining, yet-unassigned points x n+1:N are encoded by u and summed into the vector U . Right: Each pair G k , U is mapped by f into a real number (logit), which in turn is mapped into the multinomial distribution q θ (c n |c 1:n−1 , x) via a variable-input softmax.
Figure 3: Mixture of 2D Gaussians: Given the observations in the leftmost panel, we show samples from the NCP posterior. Note that less-reasonable samples are assigned lower probability by the NCP. The dotted ellipses indicate assignments which differ from the first, highest-probability sample. Our GPU implementation gives thousands of such samples in a fraction of a second. (Best seen in color.)
Figure 4: NCP trained on MNIST clusters. The top row shows 20 images from the MNIST test set. The five rows below show five samples of c 1:20 from the NCP posterior. Note that each sample captures some ambiguity suggested by the form of particular digits.
Figure 5: Quantitative Evaluations. Upper left: Two 2D clusters of 50 points each (k = 0, 1) and a line over possible locations of a 101st last point. Upper right: Assuming the 2D model from (10), the posterior p(c 101 |c 1:100 , x) can be computed exactly, and we compare it to the NCP estimate as a function of the horizontal coordinate of x 101 , as this point moves over the gray line on the upper left panel. Geweke's Tests. Lower left: The curves compare the exact mean (± one std.) of the number of clusters K for different N 's from the CRP prior (with α = 0.7), with sampled estimates using equation (11). Lower right: Similar comparison for the full histogram of K for N = 30 points.
Figure 6: Community Detection with Neural Block Processes. The model is a single-type Infinite Relational Model (Kemp et al., 2006; Xu et al., 2006), with a CRP prior with α = 0.7. The entries in each block are Bernoulli samples, with a block parameter sampled from a Beta(0.2, 0.2) prior. From left to right: (i) the original block structure, sampled from the generative model, (ii) the observed random permutation of rows and columns, (iii) four samples from the NBP posterior, along with their estimated probabilities. Each sample from the posterior here corresponds to a plausible partition.
Figure 7: Clustering multi-channel spike waveforms using NCP. Each row is an electrode channel. Spikes with the same color belong to the same cluster. (Scale bar: 5× standard deviation (SD)).
Figure 8: Spike sorting on real data. 2000 spikes from real data were clustered by NCP (top-left) and vGMFM (top-mid). Each column shows the spikes assigned to one cluster (overlaying traces and their average). Each row is one electrode channel. Top-right: t-SNE visualization of the spike clusters. Bottom-left: Example pairs of matched RFs recovered by NCP (red boxes) and Kilosort (blue boxes). Blank indicates no matched counterpart. Bottom-right: Venn diagram of recovered RFs.
Figure 9: Spike sorting on hybrid data. Top: NCP, Kilo- sort, vGMFM recovered 13, 8, and 6 of the 20 injected ground-truth templates. Bottom: Peak-to-peak (PTP) size and firing rate of each injected template. (Smaller tem- plates with lower firing rates are more challenging.)
Figure 10: Clustering ambiguous small spikes. In both examples, multiple plausi- ble clustering results of small spikes were produced by sampling from the NCP pos- terior. (scale bar = 5× SD)
Table 1: Comparing amortized approaches to Gaussian mixtures. We compare NCP with Pro- gram Compilation (Le et al., 2016) and Set Transformer (Lee et al., 2018), two previous approaches to amortized mixtures of Gaussians. Note however that NCP can be applied to any mixture model.
```

Main Content:
```

Section Title: INTRODUCTION
  INTRODUCTION Mixture models (or equivalently, probabilistic clustering models) are a staple of statistical modelling in which a discrete latent variable is introduced for each observation, indicating its mixture component identity. Popular inference methods in these models fall into two main classes. When exploring the full posterior is crucial (e.g. there is irreducible uncertainty about the latent structure or many separate local optima exist), the method of choice is Markov Chain Monte Carlo (MCMC) (Neal, 2000; Jain & Neal, 2004). This method is asymptotically accurate but time-consuming, with convergence that is difficult to assess. Models whose likelihood and prior are non-conjugate are particularly challenging, since in general in these cases the model parameters cannot be marginalized and must be kept as part of the state of the Markov chain. Alternatively, variational methods (Blei & Jordan, 2004; Kurihara et al., 2007; Hughes et al., 2015) are typically much faster but do not come with accuracy guarantees. As an alternative to MCMC and variational approaches, in recent years there has been steady progress on amortized inference methods, and such is the spirit of this work. Concretely, we propose a novel technique to perform amortized approximate posterior inference over discrete latent variables in mixture models. The basic idea is to use neural networks to express posteriors in the form of multinomial distributions (with varying support) in terms of fixed-dimensional, distributed representations that respect the permutation symmetries imposed by the discrete variables. A major advantage of our architecture, compared to previous approaches to amortized clustering, is its ability to handle an arbitrary number of clusters. This makes the method a natural choice for nonparametric Bayesian models, such as Dirichlet process mixture models (DPMM), and their extensions, where the number of components, a measure of the model complexity, is inferred as a posterior random variable; see (Rodriguez & Mueller, 2013) for a recent overview. Moreover, the method can be applied to both conjugate and non-conjugate models.

Section Title: Under review as a conference paper at ICLR 2020
  Under review as a conference paper at ICLR 2020 The term 'amortization' refers to the process of investing computational resources to train a model that is later used for very fast posterior inference (Gershman & Goodman, 2014). Concretely, the amortized approach learns a parametrized function q θ (z|x) that approximates p(z|x) for any x; learning the model parameters θ may be computationally challenging, but once θ is in hand then evaluating q θ (z|x) for new data x is fast. The amortized inference literature can be coarsely divided into two approaches. On one side, the variational autoencoder approach (Kingma & Welling, 2013), with roots in the wake-sleep algorithm (Hinton et al., 1995), learns q θ (z|x) along with the generative model p φ (x|z). While p(z) is usually a known simple distribution, for discrete latent variables backpropagation cannnot be performed through them, and special approaches have been developed for those cases (Mnih & Rezende, 2016; Jang et al., 2016; Maddison et al., 2016). Our work corresponds to the alternative case: a generative model p(x, z) is postulated , and posterior inference is the main focus of the learning phase. Amortized methods in this case usually involve a degree of specialization to the particular generative model of interest. Examples include methods developed for Bayesian networks (Stuhlmüller et al., 2013), sequential Monte Carlo (Paige & Wood, 2016), probabilistic programming (Ritchie et al., 2016; Le et al., 2016), neural decoding (Parthasarathy et al., 2017) and particle tracking (Sun & Paninski, 2018). Our work is specialized to the case where the latent variables are discrete and their range is not fixed beforehand. In the approach we present, after training the neural architecture using labeled samples from a particular generative model, we can obtain independent, parallelizable, approximate posterior samples of the discrete variables for any new set of observations of arbitrary size, with no need for expensive MCMC steps. These samples can be used (i) to compute approximate expectations, (ii) as high quality importance samples, or (iii) as independent Metropolis-Hastings proposals. In Section 2 we study amortized mixtures and in Section 3 we review related works. In Section 4 we discuss quantitative evaluations of the new method. In Section 5 we present an extension of the method to random community graph models. We close in Section 6 with a neuroscientific application of this method to spike sorting for high-density multielectrode probes.

Section Title: AMORTIZING MIXTURE MODELS
  AMORTIZING MIXTURE MODELS We start by presenting mixture models from the perspective of probabilistic models for cluster- ing (McLachlan & Basford, 1988). The latter introduce random variables c i denoting the cluster number to which the data point x i is assigned, and assume a generating process of the form Here α 1 , α 2 are hyperparameters. The number of clusters K is a random variable, indicating the number of distinct values among the sampled c i 's, and µ k denotes a parameter vector controlling the distribution of the k-th cluster (e.g., µ k could include both the mean and covariance of a Gaussian mixture component). We assume that the priors p(c 1:N |α 1 ) and p(µ 1:K |α 2 ) are exchangeable, p(c 1 , . . . , c N |α 1 ) = p(c σ1 , . . . , c σ N |α 1 ) , where {σ i } is an arbitrary permutation of the indices, and similarly for p(µ 1:K |α 2 ). Our interest in this work is in cases where K can take any value K ≤ N , such as the Chinese Restaurant Process (CRP), or its Pitman-Yor generalization. Of course, our methods will also work for models with K < B with fixed B, such as Mixtures of Finite Mixtures (Miller & Harrison, 2018). Given N data points x = {x i }, we would like to draw independent samples from the posterior Note that p(c 1 = 1|x) = 1, since the first data point is always assigned to the first cluster. While we might also be interested in the hidden variables α 1 , α 2 , µ k , the reason to focus on the discrete Under review as a conference paper at ICLR 2020 variables c i 's is that given samples from them, it is generally relatively easy to obtain posterior samples from p(α 1 |c 1:N ) and p(µ k , α 2 |x, c 1:N ). We would like to model all the factors in (1) in a unified way, with a generic factor given by Here we assumed that there are K unique values in c 1:n−1 , and therefore c n can take K + 1 values, corresponding to x n joining any of the K existing clusters, or forming its own new cluster. We are interested in approximating (2): where q θ is parameterized by a flexible model such as a neural network that takes as inputs (c 1:n−1 , x), then extracts features and combines them nonlinearly to output a probability distribution on c n . Critically, we will design the network to enforce the highly symmetric structure of the lhs of (3). To make this symmetric structure more transparent, and in light of the expression (2), let us consider the joint distribution of the assignments of the first n data points, A neural representation of this quantity should respect the permutation symmetries imposed on the x i 's by the values of c 1:n . Therefore, our first task is to build permutation-invariant representations of the observations x. The general problem of constructing such invariant encodings was discussed recently in (Zaheer et al., 2017); to adapt this approach to our context, we consider three distinct permutation symmetries: • Permutations within a cluster: (4) is invari- ant under permutations of x i 's in the same clus- ter. For each of the K clusters that have been sampled so far, we define the encoding H k = i:ci=k h(x i ) h : R dx → R d h (5) which is clearly invariant under permutations of x i 's in the same cluster. In general h is an encoding function we learn from data, unless p(x|µ) belongs to an exponential family and the prior p(c 1:N ) is constant, as shown in Ap- pendix A. • Permutations between clusters: (4) is invari- ant under permutations of the cluster labels. In terms of the within-cluster invariants H k , this symmetry can be captured by • Permutations of the unassigned data points: (4) is also invariant under permutations of the N − n unassigned data points. This can be captured by Note that G and U provide fixed-dimensional, symmetry-invariant representations of the assigned and non-assigned data points, respectively, for any values of N and K. Encodings of this form were shown in (Zaheer et al., 2017) to lead to arbitrarily accurate approximations of symmetric functions.

Section Title: THE VARIABLE-INPUT SOFTMAX
  THE VARIABLE-INPUT SOFTMAX After assigning values to c 1:n−1 , each of the K + 1 possible values for c n corresponds to h(x n ) appearing in one particular H k in (5), and yields a separate vector G k in (6). See  Figure 1  for an example. In terms of the G k 's and U , we propose to model (2) as where we have introduced a new real-valued function f . In other words, each value of c n corresponds to a different channel through which the encoding h(x n ) flows to the logit value f , as shown in  Figure 2 . Note that k = K + 1 corresponds to c n forming its own new cluster with H k = h(x n ). The softmax (8) differs from its usual form in, e.g., classification networks, where a fixed number of categories receive their logit values f from the fixed-size final layer of an MLP. In our case, the discrete identity of each logit is determined by the neural path that the input h(x n ) takes to G, thus allowing a flexible number of categories. In eq. (8), θ denotes the parameters in the functions h, g, u and f , which we represent with neural networks. By storing and updating G and U for successive values of n, the computational cost of a full i.i.d. sample of c 1:N is O(N K), the same as a single Gibbs sweep. See Algorithm 1 for details; we term this approach the Neural Clustering Process (NCP). It is relatively easy to run hundreds of copies of Algorithm 1 in parallel on a GPU, with each copy yielding a different set of samples c 1:N . 1

Section Title: THE OBJECTIVE FUNCTION
  THE OBJECTIVE FUNCTION In order to learn the parameters θ of the neural networks, we use stochastic gradient descent to minimize the expected KL divergence, Samples from p(c 1:N , x) are obtained from the generative model, irrespective of the model being conjugate. If we can take an unlimited number of samples from the generative model, we can potentially train a neural network to approximate p(c n |c 1:n−1 , x) arbitrarily accurately. Note that the gradient here acts only on the variable-input softmax term q θ , not p(c, x), so there is no problem of backpropagating through discrete variables (Jang et al., 2016; Maddison et al., 2016).

Section Title: TWO EXAMPLES
  TWO EXAMPLES

Section Title: Clustering in 2D Gaussian models
  Clustering in 2D Gaussian models where CRP stands for the Chinese Restaurant Process, with concentration parameter α, σ µ = 10, and σ = 1.  Figure 3  shows that the NCP captures the posterior uncertainty inherent in clustering this data. Note that since the generative model is an analytical known distribution, there is no distinction here between training and test sets.

Section Title: Clustering of MNIST digits
  Clustering of MNIST digits where CRP 10 is a Chinese Restaurant Process truncated to up to 10 clusters, and d x = 28 × 28. Training was performed by sampling x i from the MNIST training set.  Figure 4  shows posterior samples for a set of digits from the MNIST test set, illustrating how the estimated model correctly captures the shape ambiguity of some of the digits. Note that in this case the generative model has no analytical expression (and therefore is non-conjugate), but this presents no problem; a set of labelled samples is all we need for training. See Appendix F for details of all the network architectures used.

Section Title: RELATED WORKS
  RELATED WORKS Permutation-invariant neural architectures have been explored recently in (Ravanbakhsh et al., 2017; Korshunova et al., 2018; Lee et al., 2018; Bloem-Reddy & Teh, 2019; Wagstaff et al., 2019). The representation of a set via a sum (or mean) of encoding vectors was also used in (Guttenberg et al., 2016; Ravanbakhsh et al., 2016; Edwards & Storkey, 2017; Zaheer et al., 2017; Garnelo et al., 2018a;b). Most works on neural network-based clustering focus on learning features as inputs to traditional clustering algorithms, as reviewed in (Du, 2010; Aljalbout et al., 2018; Min et al., 2018). The works closest to ours are (Le et al., 2016) and (Lee et al., 2018). Both present techniques for amortized inference of mixtures of Gaussians, so it is instructive to compare them in detail to our approach. The work (Le et al., 2016) studies amortized inference of a variable number of latent variables generated during the trace of a general sequential probabilistic program. For the case of a mixture of 2D Gaussians with a latent random number of components, a 2D histogram image of binned observations is fed to a convolutional network whose output enters into a recurrent neural network with a fixed-sized softmax output layer to estimate the number of clusters. The network also outputs the means and covariances of each cluster. The work (Lee et al., 2018) presents Set Transformer, an attention-based architecture that improves the simple sum-based set encoding that we used above. In their 2D Gaussian clustering application, the number of components is fixed beforehand, and inference is made only on the cluster parameters. These approaches have several limitations compared to ours. First, the number of clusters is upper bounded by the size of the softmax layer (Le et al., 2016) or fixed (Lee et al., 2018). Second, the models perform inference on the continuous parameters µ k , but not on the discrete labels of each data point. Finally, in (Le et al., 2016), the use of a convnet on a 2D histogram to determine the number of clusters does not scale to higher dimensional data due to the curse of dimensionality. In  Table 1  we summarize the comparison between the three approaches.

Section Title: EXPECTATIONS, EVALUATIONS AND DIAGNOSTICS
  EXPECTATIONS, EVALUATIONS AND DIAGNOSTICS Samples from the NCP can be used to compute approximate expectations. If the interest is in asymptotically exactness, the samples can be used as self-normalized importance samples, E[f (c)] = M i=1 f (c (i) )w i / M i=1 w i where w i = p(x, c (i) )/q θ (c (i) |x). Alternatively, the samples can be used as proposals in Metropolized independent sampling (Liu, 1996). Of course, in both cases the variance of the estimated expectations will be lower when the NCP posterior is closer to the true posterior. The examples presented in Sec. 2.3 provide strong qualitative evidence that our approximations to the true posterior distributions in these models are capturing the uncertainty inherent in the observed data. But we would like to go further and ask quantitatively how well our approximations match the exact posterior. Unfortunately, for sample sizes much larger than N = O(10) it is impossible to compute the exact posterior in these models. Nonetheless, there are several quantitative metrics we can examine to check the accuracy of the model output.

Section Title: Global symmetry from exchangeability
  Global symmetry from exchangeability Our results relied on p(c 1:N |α 1 ) being exchangeable, which in turn implies exchangeability of the joint posterior (1). But this is not explicit in the rhs of (1), where a particular order is chosen for the expansion. If our model learns the conditional probabilities correctly, this symmetry should be (approximately) satisfied, and this can be monitored during training, as we show in Appendix C.

Section Title: Estimated vs. Analytical Probabilities
  Estimated vs. Analytical Probabilities Some conditional probabilities can be computed analyt- ically and compared with the estimates output by the network; in the example shown in  Figure 5 , upper-right, the estimated probabilities are in close agreement with their exact values.

Section Title: Geweke's Tests
  Geweke's Tests where p(x) is the marginal from the generative model.  Figure 5  shows such a comparison for the 2D Gaussian DPMM from Section 2.3, showing excellent agreement. NCP vs. MCMC: NCP has some advantages over MCMC approaches. First, it gives a probability estimate for each sample, in general unavailable in MCMC. Secondly, NCP enjoys higher efficiency, due to parallelization of iid samples. For example, in the Gaussian 2D example in eq.(10), in the time a collapsed Gibbs sampler produces one (correlated) sample, our GPU-based method produces more than 100 iid approximate samples. Finally, NCP does not need a burn-in period.

Section Title: NCP vs. Variational Inference
  NCP vs. Variational Inference In Section 6, we compare NCP with a variational approach on clustering neural spikes. For 2000 spikes, the variational approach returned one clustering estimate in 0.76 secs., but does not properly handle the uncertainty about the number of clusters. NCP produced 150 clustering configurations in 10 secs., efficiently capturing clustering uncertainty. In addition, the variational approach requires a preprocessing step that projects the samples to lower dimensions, whereas NCP directly consumes the high-dimensional data by learning an encoder function h.

Section Title: COMMUNITIES
  COMMUNITIES As an extension, we consider now a similar prior as above over cluster labels, but the observation model is more challenging: where k 1 , k 2 = 1 . . . K. Here p(c 1:n |α) can be any exchangeable prior, and the binary observations x i,j represent edges in a graph of N vertices. We focus on the symmetric graph case here, so φ k1,k2 = φ k2,k1 and x i,j ≡ x j,i . We use a Beta model for p(φ|β), but other choices are possible. These models include stochastic block models (Holland et al., 1983; Nowicki & Snijders, 2001) and the single-type Infinite Relational Model (Kemp et al., 2006; Xu et al., 2006; Schmidt & Morup, 2013). Neural architectures for communities in graphs have been studied in (Chen et al., 2019) as a classification problem for every node over a fixed predetermined number of clusters. We could proceed similarly to the clustering case, considering N particles, each given by a row of the adjacency matrix x i = (x i,1 . . . x i,N ). But we should be careful when encoding these particles. When values of c 1:n are assigned, a generic encoding h(x i ) would ignore the permutation symmetries present among the components of x i , i.e., the columns of x i,j , as a result of the c 1:n assignments (the same three permutation symmetries discussed above for clustering models). Moreover, a fixed encoding h(x i ) cannot accommodate the arbitrary length N of x i . In Appendix B we present an invariant encoding that respects all these requirements. We call our approach Neural Block Process (NBP). See  Figure 6  for an example.

Section Title: APPLICATION: SPIKE SORTING WITH NCP
  APPLICATION: SPIKE SORTING WITH NCP Large-scale neural population recordings using multi-electrode arrays (MEA) are crucial for under- standing neural circuit dynamics. Each MEA electrode reads the signals from many neurons, and each neuron is recorded by multiple nearby electrodes. As a key analysis step, spike sorting converts the raw signal into a set of spike trains belonging to individual neurons (Pachitariu et al., 2016; Chung et al., 2017; Jun et al., 2017; Lee et al., 2017; Chaure et al., 2018; Carlson & Carin, 2019). At the core of many spike sorting pipelines is a clustering algorithm that groups the detected spikes into clusters, each representing a putative neuron ( Figure 7 ). However, clustering spikes can be challenging: (1) Spike waveforms form highly non-Gaussian clusters in spatial and temporal dimensions, and it is unclear what are the optimal features for clustering. (2) It is unknown a priori how many clusters there are. (3) Existing methods do not perform well on spikes with low signal-to-noise ratios (SNR) due to increased clustering uncertainty, and fully-Bayesian approaches proposed to handle this uncertainty (Wood & Black, 2008; Carlson et al., 2013) do not scale to large datasets. To address these challenges, we propose a novel approach to spike clustering using NCP. We consider the spike waveforms as generated from a Mixture of Finite Mixtures (MFM) distribution (Miller & Harrison, 2018), which can be effectively modeled by NCP. (1) Rather than selecting arbitrary features for clustering, the spike waveforms are encoded with a convolutional neural network (ConvNet), which is learned end-to-end jointly with the NCP network to ensure optimal feature encoding. (2) Using a variable-input softmax function, NCP is able to perform inference on cluster labels without assuming a fixed or maximum number of clusters. (3) NCP allows for efficient probablistic clustering by GPU-parallelized posterior sampling, which is particularly useful for handling the clustering uncertainty of ambiguous small spikes. (4) The computational cost of NCP training can be highly amortized, since neuroscientists often sort spikes form many statistically similar datasets. We trained NCP for spike clustering using synthetic spikes from a simple yet effective generative model that mimics the distribution of real spikes, and evaluated the spike sorting performance on labeled synthetic data, unlabeled real data and hybrid test data by comparing NCP against two other methods: (1) vGMFM, variational inference on Gaussian MFM (Hughes & Sudderth, 2013). (2) Kilosort, a state-of-the-art spike sorting pipeline described in Pachitariu et al. (2016). In Appendix D, we describe the dataset, neural architecture, and the training/inference pipeline of NCP spike sorting.

Section Title: Synthetic Data
  Synthetic Data We run NCP and vGMFM on 20 sets of synthetic test data each with 500, 1000, and 2000 spikes. As the ground-truth cluster labels are known, we compared the clustering quality using Adjusted Mutual Information (AMI) (Vinh et al., 2010). The AMI of NCP is on average 11% higher than vGMFM (Figure 13), showing better performance of NCP on synthetic data.

Section Title: Real Data
  Real Data We run NCP, vGMFM and Kilosort on a retina recording with white noise stimulus as described in Appendix D, and extracted the averaged spike template of each cluster (i.e. putative neuron). Example clustering results in  Figure 8  (top) shows that NCP produces clean clusters with visually more distinct spike waveforms compared to vGMFM. As real data do not come with ground- truth cluster labels, we compared the spike templates extracted from NCP and Kilosort using retinal Under review as a conference paper at ICLR 2020 receptive field (RF), which is computed for each cluster as the mean of the stimulus present at each spike. A clearly demarcated RF provides encouraging evidence that the spike template corresponds to a real neuron. Side-by-side comparisons of matched RF pairs are shown in  Figure 8  (bottom-left) and Figure 14. Overall, NCP found 103 templates with clear RFs, among which 48 were not found by Kilosort. Kilosort found 72 and 17 of them were not found by NCP ( Figure 8  bottom-right), showing that NCP performs at least as well as Kilosort, and finds many additional templates with clear RFs. Hybrid Data. We compared NCP against vGMFM and Kilosort on a hybrid recording with partial ground truth as in Pachitariu et al. (2016). Spikes from 20 ground-truth templates were inserted into a real recording to test the spike sorting performance on realistic recordings with complex background noise and colliding spikes. As shown in  Figure 9 , NCP recovered 13 of the 20 injected ground-truth templates, outperforming both Kilosort and vGMFM, which recovered 8 and 6, respectively.

Section Title: Probabilistic clustering of ambiguous small spikes
  Probabilistic clustering of ambiguous small spikes Sorting small spikes has been challenging due to the low SNR and increased uncertainty of cluster assignment. By efficient GPU-parallelized posterior sampling of cluster labels, NCP is able to handle the clustering uncertainty by producing multiple plausible clustering configurations.  Figure 10  shows examples where NCP separates spike clusters with amplitude as low as 3-4× the standard deviation of the noise into plausible units that are not mere scaled version of each other but have distinct shapes on different channels. Overall, our results show that using NCP for spike sorting provides high clustering quality, matches or outperforms a state-of-the-art method, and handles clustering uncertainty by efficient posterior sampling, demonstrating substantial promise for incorporating NCP into production-scale pipelines.

```
