<article article-type="research-article"><front><article-meta><title-group><article-title>Published as a conference paper at ICLR 2020 INFLUENCE-BASED MULTI-AGENT EXPLORATION</article-title></title-group><contrib-group content-type="author"><contrib contrib-type="person"><name><surname>Wang</surname><given-names>Tonghan</given-names></name></contrib><contrib contrib-type="person"><name><surname>Wang</surname><given-names>Jianhao</given-names></name></contrib><contrib contrib-type="person"><name><surname>Wu</surname><given-names>Yi</given-names></name></contrib><contrib contrib-type="person"><name><surname>Zhang</surname><given-names>Chongjie</given-names></name></contrib><contrib contrib-type="person"><xref ref-type="aff" rid="aff0" /></contrib></contrib-group><aff id="aff0"><institution content-type="orgname">Institute for Interdisciplinary Information Sciences Tsinghua University Beijing</institution><country>China</country></aff><abstract><p>Intrinsically motivated reinforcement learning aims to address the exploration challenge for sparse-reward tasks. However, the study of exploration methods in transition-dependent multi-agent settings is largely absent from the literature. We aim to take a step towards solving this problem. We present two exploration methods: exploration via information-theoretic influence (EITI) and exploration via decision-theoretic influence (EDTI), by exploiting the role of interaction in coordinated behaviors of agents. EITI uses mutual information to capture the interdependence between the transition dynamics of agents. EDTI uses a novel intrinsic reward, called Value of Interaction (VoI), to characterize and quantify the influence of one agent's behavior on expected returns of other agents. By optimiz- ing EITI or EDTI objective as a regularizer, agents are encouraged to coordinate their exploration and learn policies to optimize the team performance. We show how to optimize these regularizers so that they can be easily integrated with pol- icy gradient reinforcement learning. The resulting update rule draws a connection between coordinated exploration and intrinsic reward distribution. Finally, we empirically demonstrate the significant strength of our methods in a variety of multi-agent scenarios.</p></abstract></article-meta></front><body><sec><title>INTRODUCTION</title><p>Reinforcement learning algorithms aim to learn a policy that maximizes the accumulative reward from an environment. Many advances of deep reinforcement learning rely on a dense shaped reward function, such as distance to the goal (<xref ref-type="bibr" rid="b0">Mirowski et al., 2016</xref>; <xref ref-type="bibr" rid="b0">Wu et al., 2018</xref>), scores in games (<xref ref-type="bibr" rid="b0">Mnih et al., 2015</xref>) or expert-designed rewards (<xref ref-type="bibr" rid="b0">Wu &amp; Tian, 2016</xref>; <xref ref-type="bibr" rid="b0">OpenAI, 2018</xref>), but they tend to strug- gle in many real-world scenarios with sparse rewards (<xref ref-type="bibr" rid="b0">Burda et al., 2019</xref>). Therefore, many recent works propose to introduce additional intrinsic incentives to boost exploration, including pseudo- counts (<xref ref-type="bibr" rid="b0">Bellemare et al., 2016</xref>; <xref ref-type="bibr" rid="b0">Tang et al., 2017</xref>; <xref ref-type="bibr" rid="b0">Ostrovski et al., 2017</xref>), model-learning improve- ments (<xref ref-type="bibr" rid="b0">Burda et al., 2019</xref>; <xref ref-type="bibr" rid="b0">Pathak et al., 2017</xref>; <xref ref-type="bibr" rid="b0">Burda et al., 2018</xref>), and information gain (<xref ref-type="bibr" rid="b0">Florensa et al., 2017</xref>; <xref ref-type="bibr" rid="b0">Gupta et al., 2018</xref>; <xref ref-type="bibr" rid="b0">Hyoungseok Kim, 2019</xref>). These works result in significant progress in many challenging tasks such as Montezuma Revenge (<xref ref-type="bibr" rid="b0">Burda et al., 2018</xref>), robotic manipula- tion (<xref ref-type="bibr" rid="b0">Pathak et al., 2018</xref>; <xref ref-type="bibr" rid="b0">Riedmiller et al., 2018</xref>), and Super Mario games (<xref ref-type="bibr" rid="b0">Burda et al., 2019</xref>; <xref ref-type="bibr" rid="b0">Pathak et al., 2017</xref>).</p><p>Notably, most of the existing breakthroughs on sparse-reward environments have been focusing on single-agent scenarios and leave the exploration problem largely unstudied for multi-agent settings - it is common in real-world applications that multiple agents are required to solve a task in a coordi- nated fashion (<xref ref-type="bibr" rid="b0">Cao et al., 2012</xref>; <xref ref-type="bibr" rid="b0">Now&#233; et al., 2012</xref>; <xref ref-type="bibr" rid="b0">Zhang &amp; Lesser, 2011</xref>). This problem has recently attracted attention and several exploration strategies have been proposed for transition-independent cooperative multi-agent settings (<xref ref-type="bibr" rid="b0">Dimakopoulou &amp; Van Roy, 2018</xref>; <xref ref-type="bibr" rid="b0">Dimakopoulou et al., 2018</xref>; <xref ref-type="bibr" rid="b0">Bar- giacchi et al., 2018</xref>; <xref ref-type="bibr" rid="b0">Iqbal &amp; Sha, 2019b</xref>). Nevertheless, how to explore effectively in more general scenarios with complex reward and transition dependency among cooperative agents remains an open research problem.</p></sec><sec><title>Published as a conference paper at ICLR 2020</title><p>This paper aims to take a step towards this goal. Our basic idea is to coordinate agents' exploration by taking into account their interactions during their learning processes. Configurations where in- teraction happens (interaction points) lie at critical junctions in the state-action space, through these critical configurations can transit to potentially important under-explored regions. To exploit this idea, we propose exploration strategies where agents start with decentralized exploration driven by their individual curiosity, and are also encouraged to visit interaction points to influence the ex- ploration processes of other agents and help them get more extrinsic and intrinsic rewards. Based on how to quantify influence among agents, we propose two exploration methods. Exploration via information-theoretic influence (EITI) uses mutual information (MI) to capture the interdependence between the transition dynamics of agents. Exploration via decision-theoretic influence (EDTI) goes further and uses a novel measure called value of interaction (VoI) to disentangle the effect of one agent's state-action pair on the expected (intrinsic) value of other agents. By optimizing MI or VoI as a regularizer to the value function, agents are encouraged to explore state-action pairs where they can exert influences on other agents for learning sophisticated multi-agent cooperation strategies.</p><p>To efficiently optimize MI and VoI, we propose augmented policy gradient formulations so that the gradients can be estimated purely from trajectories. The resulting update rule draws a connection between coordinated exploration and the distribution of individual intrinsic rewards among team members, which further explains why our methods are able to facilitate multi-agent exploration.</p><p>We demonstrate the effectiveness of our methods on a variety of sparse-reward cooperative multi- agent tasks. Empirical results show that both EITI and EDTI allow for the discovery of influential states and EDTI further filter out interactions that have no effects on the performance. Our results also imply that these influential states are implicitly discovered as subgoals in search space that guide and coordinate exploration. The video of experiments is available at https://sites. google.com/view/influence-based-mae/.</p></sec><sec><title>SETTINGS</title><p>In our work, we consider a fully cooperative multi-agent task that can be modelled by a factored multi-agent MDP G = N, S, A, T, r, h, n , where N &#8801; {1, 2, ..., n} is the finite set of agents, S &#8801; &#215; i&#8712;N S i is the finite set of joint states and S i is the state set of agent i. At each timestep, each agent selects an action a i &#8712; A i at state s, forming a joint action a &#8712; A &#8801; &#215; i&#8712;N A i , resulting in a shared extrinsic reward r(s, a) for each agent and the next state s according to the transition function T (s |s, a).</p><p>The objective of the task is that each agent learns a policy &#960; i (a i |s i ), jointly maximizing team performance. The joint policy &#960;= &#960; 1 , . . . , &#960; n induces an action-value function, Q ext,&#960; (s, a)= E &#964; [ h t=0 r t |s 0 =s, a 0 =a, &#960;], and a value function V ext,&#960; (s)= max a Q ext,&#960; (s, a), where &#964; is the episode trajectory and h is the horizon.</p><p>We adopt a centralized training and decentralized execution paradigm, which has been widely used in multi-agent deep reinforcement learning (<xref ref-type="bibr" rid="b0">Foerster et al., 2016</xref>; <xref ref-type="bibr" rid="b0">Lowe et al., 2017</xref>; <xref ref-type="bibr" rid="b0">Foerster et al., 2018</xref>; <xref ref-type="bibr" rid="b0">Rashid et al., 2018</xref>). During training, agents are granted access to the states, actions, (intrinsic) rewards, and value functions of other agents, while decentralized execution only requires individual states.</p></sec><sec><title>INFLUENCE-BASED COORDINATED MULTI-AGENT EXPLORATION</title><p>Efficient exploration is critical for reinforcement learning, particularly in sparse-reward tasks. In- trinsic motivation (<xref ref-type="bibr" rid="b0">Oudeyer &amp; Kaplan, 2009</xref>) is a crucial mechanism for behaviour learning since it provides the driver of exploration. Therefore, to trade off exploration and exploitation, it is com- mon for an RL agent to maximize an objective of the expected extrinsic reward augmented by the expected intrinsic reward. Curiosity is one of the extensively-studied intrinsic rewards to encourage an agent to explore according to its uncertainty about the environment, which can be measured by model prediction error (<xref ref-type="bibr" rid="b0">Burda et al., 2019</xref>; <xref ref-type="bibr" rid="b0">Pathak et al., 2017</xref>; <xref ref-type="bibr" rid="b0">Burda et al., 2018</xref>) or state visitation count (<xref ref-type="bibr" rid="b0">Bellemare et al., 2016</xref>; <xref ref-type="bibr" rid="b0">Tang et al., 2017</xref>; <xref ref-type="bibr" rid="b0">Ostrovski et al., 2017</xref>).</p><p>While such an intrinsic motivation as curiosity drives effective individual exploration, it is often not sufficient enough for learning in collaborative multi-agent settings, because it does not take Published as a conference paper at ICLR 2020 into account agent interactions. To encourage interactions, we propose an influence value aims to quantify one agent's influence on the exploration processes of other agents. Maximizing this value will encourage agents to visit interaction points more often through which the agent team can reach configurations that are rarely visited by decentralized exploration. In next sections, we will provide two ways to formulate the influence value with such properties, leading to two exploration strategies. Thus, for each agent i, our overall optimization objective is: J &#952;i [&#960; i |&#960; &#8722;i , p 0 ] &#8801; V ext,&#960; (s 0 ) + V int,&#960; i (s 0 ) + &#946; &#183; I &#960; &#8722;i|i , (1) where p 0 (s 0 ) is the initial state distribution, &#960; &#8722;i is the joint policy excluding that of agent i, and V int,&#960; i (s) is the intrinsic value function of agent i, I &#960; &#8722;i|i is the influence value, &#946; &gt; 0 is a weighting term. In this paper, we use the following notations:</p><p>where u i (s i , a i ) is a curiosity-derived intrinsic reward,r i (s, a) is a sum of intrinsic and extrinsic rewards, V &#960; i (s) and Q &#960; i (s, a) here contain both intrinsic and extrinsic rewards.</p></sec><sec><title>EXPLORATION VIA INFORMATION-THEORETIC INFLUENCE</title><p>One critical problem in our learning framework presented above is to define the influence value I. For simplicity, we start with a two-agent case. The first method we propose is to use mutual information between agents' trajectories to measure one agent's influence on other agents' learning processes. Such mutual information can be defined as information gain of one agent's state transition given the other's state and action. Without loss of generality, we define it from the perspective of agent 1:</p><p>where s = (s 1 , s 2 ) is the joint state, a = (a 1 , a 2 ) is the joint action, and S i and A i are the random variables of state and action of agent i subject to the distribution induced by the joint policy &#960;. So we define I &#960; 2|1 as M I &#960; 2|1 (S 2 ; S 1 , A 1 |S 2 , A 2 ) that captures transition interactions between agents. Optimizing this objective encourages agent 1 to visited critical points where it can influence the transition probability of agent 2. We call such an exploration method exploration via information- theoretic influence (EITI).</p><p>Optimizing M I &#960; 2|1 with respect to the policy parameters &#952; 1 of agent 1 is a little bit challenging, because it is an expectation with respect to a distribution that depends on &#952; 1 . The gradient consists of two terms:</p><p>While the second term is an expectation over the trajectory and can be shown to be zero (see Ap- pendix B.1), it is unwieldy to deal with the first term because it requires the gradient of the stationary distribution, which depends on the policies and the dynamics of the environment. Fortunately, the gradient can still be estimated purely from sampled trajectories by drawing inspiration from the proof of the policy gradient theorem (<xref ref-type="bibr" rid="b0">Sutton et al., 2000</xref>).</p><p>The resulting policy gradient update is:</p><p>Published as a conference paper at ICLR 2020 whereV &#960; 1 (s t ) is an augmented value function ofR t</p><p>The third term, which we call EITI reward, is 0 when the agents are transition-independent, i.e., when p(s t+1 2 |s t 1 , s t 2 , a t 1 , a t 2 ) = p(s t+1 2 |s t 2 , a t 2 ), and is positive when s t 1 , a t 1 increase the probability of agent 2 translating to s t+1 2 . Therefore, the EITI reward is an intrinsic motivation that encourages agent 1 to visit more frequently the state-action pairs where it can influence the trajectory of agent 2. The estimation of p(s t+1 2 |s t 1 , s t 2 , a t 1 , a t 2 ) and p(s t+1 2 |s t 2 , a t 2 ) are discussed in Appendix C. We assume that agents know the states and actions of other agents, but this information is only available during centralized training. When execution, agents only have access to their local observations.</p></sec><sec><title>EXPLORATION VIA DECISION-THEORETIC INFLUENCE</title><p>Mutual information characterizes the influence of one agent's trajectory on that of the other and captures interactions between the transition functions of the agents. However, it does not provide the value of these interactions to identify interactions related to more internal and external rewards (r). To address this issue, we propose exploration via decision-theoretic influence (EDTI) based on a decision-theoretic measure of I, called Value of Interaction (VoI), which disentangles both transition and reward influences. VoI is defined as the expected difference between the action-value function of one agent (e.g., agent 2) and its counterfactual action-value function without considering the state and action of the other agent (e.g., agent 1): V oI &#960; 2|1 (S 2 ; S 1 , A 1 |S 2 , A 2 ) = s,a,s 2 &#8712;(S,A,S2) p &#960; (s, a, s 2 ) Q &#960; 2 (s, a, s 2 ) &#8722; Q &#960;,* 2|1 (s 2 , a 2 , s 2 ) , (9) where Q &#960; 2 (s, a, s 2 ) is the expected rewards (including intrinsic rewards) of agent 2 defined as:</p><p>and the counterfactual action-value function Q &#960;,* 2 (also includes intrinsic and extrinsic rewards) can be obtained by marginalizing out the state and action of agent 1:</p><p>Note that the definition of VoI is analogous to that of MI and the difference lies in that log p(&#183;) measures the amount of information while Q measures the action value. Although VoI can be ob- tained by learning Q &#960; 2 (s, a) and Q &#960; 2 (s 2 , a 2 ) and calculating the difference, we propose to explicitly marginalize out s * 1 and a * 1 utilizing the estimated model transition probability p &#960; (s 2 |s 2 , a 2 ) and p(s 2 |s, a) to get a more accurate value estimate (<xref ref-type="bibr" rid="b0">Feinberg et al., 2018</xref>). The performance of these two formulations are compared in the experiments.</p><p>Value functions Q and V used in VoI contains both expected external rewards and internal rewards, which will not only encourage coordinated exploration by the influence between intrinsic rewards but also filter out meaningless interactions which can not lead to extrinsic reward after intrinsic reward diminishes. To facilitate the optimization of VoI, we rewrite it as an expectation over state- action trajectories.</p><p>wherer &#960; 2 (s 2 , a 2 ) is the counterfactual immediate reward. The detailed proof is deferred to Appendix B.2. From this definition, we can intuitively see how VoI reflects the value of interactions.r 2 (s, a)&#8722; r &#960; 2 (s 2 , a 2 ) and 1 &#8722; p &#960; (s 2 |s 2 , a 2 )/p(s 2 |s, a) measure the influence of agent 1 on the immediate reward and the transition function of agent 2, and V &#960; 2 (s ) serves as a scale factor in terms of future value. Only when agent 1 and agent 2 are both transition- and reward-independent, i.e., when p &#960; (s 2 |s 2 , a 2 ) = p(s 2 |s, a) and r &#960; 2 (s 2 , a 2 ) = r 2 (s, a) will VoI equal to 0. In particular, maximizing Published as a conference paper at ICLR 2020 VoI with respect to policy parameters &#952; 1 will lead agent 1 to meaningful interaction points, where V &#960; 2 (s ) is high and s 1 , a 1 can increase the probability that s is reached. In this learning framework, agents initially explore the environment individually driven by its own curiosity, during which process they will discover potentially valuable interaction points where they can influence the transition function and (intrinsic) rewarding structure of each other. VoI highlights these points and encourages agents to visit these configurations more frequently. As intrinsic re- ward diminishes, VoI can gradually distinguish those interaction points which are necessary to get extrinsic rewards.</p></sec><sec><title>POLICY OPTIMIZATION WITH VOI</title><p>We want to optimize J &#952;i with respect to the policy parameters &#952; i , where the most cumbrous term is &#8711; &#952;i V oI &#8722;i|i . For brevity, we can consider a two-agent case, e.g., optimizing V oI 2|1 with respect to the policy parameters &#952; 1 . Directly computing the gradient &#8711; &#952;1 V oI 2|1 is not stable, because V oI 2|1 contains policy-dependent functionsr &#960; 2 (s 2 , a 2 ), p &#960; (s 2 |s 2 , a 2 ), and V &#960; 2 (s ) (see Eq. 12). To stabilize training , we use target functions to approximate these policy-dependent functions, which is a commonly used technique in deep RL (<xref ref-type="bibr" rid="b0">Mnih et al., 2015</xref>). With this approximation, we denote</p><p>where r &#8722; 2 , p &#8722; , and V &#8722; 2 are corresponding target functions. As these target functions are only period- ically updated during the learning, their gradients over &#952; 1 can be approximately ignored. Therefore, from Eq. 12, we have</p><p>Similar to the calculation of &#8711; &#952;i M I, we get the gradient at every step (see Appendix B.3 for proof):</p><p>whereV &#960; 1 (s t ) is an augmented value function regressed towardsR t</p></sec><sec><title>DISCUSSIONS</title><p>Scale to Large Settings: For cases with more than two agents, the VoI of agent i on other agents can be defined similarly to Eq. 9, which is annotated with V oI &#960; &#8722;i|i (S &#8722;i ; S i , A i |S &#8722;i , A &#8722;i ), where S &#8722;i and A &#8722;i are the state and action sets of all agents other than agent i. In practice, agents in- teraction can often be decomposed to pairwise interaction so V oI &#960; &#8722;i|i (S &#8722;i ; S i , A i |S &#8722;i , A &#8722;i ) is well approximated by the sum of values of pairwise value of interaction:</p><p>Relationship between EITI and EDTI: EITI and EDTI gradient updates are obtained by information- and decision-theoretical influence respectively. Therefore, it is nontrivial to derive that part of the EDTI reward is a lower bound of the EITI reward:</p></sec><sec><title>Comparing EDTI to Centralized Methods</title><p>Different from a centralized method which directly includes value functions of other agents in the optimization objective, (i.e., by setting total reward r i = r + u i + &#946;(u &#8722;i + &#947;V &#8722;i ), which is called plusV henceforth), the EDTI reward for agent i disentangles its contributions to values of another agents using a counterfactual formulation. This difference is important for quantifying influence because the value of another agent does not just contain the contributions from agent i, but also those of itself and third-party agents. Therefore, EDTI is a kind of intrinsic reward assignment. Our experiments in the next section will compare the performance of plusV against our methods, which verify the importance of the intrinsic reward assignment.</p></sec><sec><title>EXPERIMENTAL RESULTS</title><p>Our experiments aim to answer the following questions: (1) Can EITI and EDTI rewards capture interaction points? If they can, how do these points change throughout exploration? (2) Can exploit- ing these interaction points facilitate exploration and learning performance? (3) Can EDTI filter out interaction points that are not related to environmental rewards? (4) What if only reward influence between agents are disentangled? We evaluate our approach on a set of multi-agent tasks with sparse rewards based on a discrete version of multi-agent particle world environment (<xref ref-type="bibr" rid="b0">Lowe et al., 2017</xref>). PPO (<xref ref-type="bibr" rid="b0">Schulman et al., 2017</xref>) is used as the underlying algorithm. For evaluation, all experiments are carried out with 5 different random seeds and results are shown with 95% confidence interval. Demonstrative videos 1 are available online.</p></sec><sec><title>Baselines</title><p>We compare our methods with various baselines shown in <xref ref-type="table" rid="tab_0">Table 1</xref>. In particular, we carry out the following ablation studies: i) r influence disentangles immediate reward influence between agents, (derivation of the associated augmented reward can be found in Appendix B.4. Reward influ- ence in long term is not considered because it inevitably involves transition interactions) ii) PlusV as described in Sec. 3.3. iii) Shared critic uses decentralized PPO agents with shared centralized value function and thus is a cooperative version of MADDPG (<xref ref-type="bibr" rid="b0">Lowe et al., 2017</xref>) augmented with intrin- sic reward of curiosity. iv) Q-Q is similar to EDTI but without explicit counterfactual formulation, as described in Sec. 3.2. We also note that EITI is an ablation of EDTI which considers transition interactions. PlusV, shared critic, Q-Q, and cen control have access to global or other agents' value functions during training. When execution, all the methods except cen control only require local state.</p></sec><sec><title>DIDACTIC EXAMPLES</title><p>We present two didactic examples of multi-agent cooperation tasks with sparse reward to explain how EITI and EDTI work. The first didactic example consists of a 30 &#215; 30 maze with two rooms and a door with two switches (<xref ref-type="fig" rid="fig_0">Fig. 1</xref> left). In the optimal strategy, one agent should first step on switch 1 to help the other agent pass the door, and then the agent that has already reached the right half should further go to switch 2 to bring the remaining agent in. There are two pairs of interaction points in this task: (switch 1, door) and (switch 2, door), i.e., transition probability of the agent near door is determined by whether another agent is on one of the switch. Fig. 1-right and <xref ref-type="fig" rid="fig_1">Fig. 2</xref>-top show the learning curves of our methods and all the baselines, among which EITI, EDTI, r influence, Multi, and centralized control can learn the winning strategy and ours learn much more efficiently. <xref ref-type="fig" rid="fig_1">Fig. 2</xref>-bottom gives a possible explanation why our methods work. EITI and EDTI rewards successfully highlight the interaction points (before 100 and 2100 updates, respectively). Agents are encouraged to explore these configurations more frequently and thus have better chance to learn the goal strategy. EDTI reward considers the value function of the other agent, so it converges slower than the EITI reward. In contrast, directly adding the other agent's intrinsic rewards and value functions is noisy (see "plusV reward") and confuses the agent because these contain the effect of the other agent's exploration. As for centralized control, global curiosity encourages agents to try all possible configurations, so it can find environmental rewards in most tasks. However, visiting all configurations without bias renders it inefficient - external rewards begin to dominate the behaviors of agents after 7000 updates even with the help of centralized learning algorithm. Our methods use the same information as centralized exploration but take advantages of agents' interactions to accelerate exploration.</p><p>In order to evaluate whether EDTI can help filter out noisy interaction points and accelerate explo- ration, we conduct experiments in a second didactic task (see <xref ref-type="fig" rid="fig_0">Fig. 1</xref> middle). It is also a navigation task in a 25 &#215; 25 maze where agents are rewarded for being in a goal room. However, in this exper- iment, we consider a case where there are four rooms and the upper right one is attached to reward. This task contains 6 pairs of interaction points (switch 1 with each of the doors, each switch with the door of the same room), but only two of them are related to external rewards, i.e., (switch 1, door 1) and (switch 2, door 1). As <xref ref-type="fig" rid="fig_2">Fig. 3</xref>-right shows, EITI agents treat three doors equally even after 7400 updates (see <xref ref-type="fig" rid="fig_2">Fig. 3</xref> right, 7400 updates, top row). In comparison, although EDTI reward suffers from noise in the beginning, it clearly highlight two pairs of valuable interaction points (see <xref ref-type="fig" rid="fig_2">Fig. 3</xref> right, 7400 updates, bottom row) as intrinsic reward diminishes. This can explain why EDTI outperforms EITI (<xref ref-type="fig" rid="fig_2">Fig. 3</xref> left).</p></sec><sec><title>EXPLORATION IN COMPLEX TASKS</title><p>Next, we evaluate the performance of our methods on more complex tasks. To this end, we use three sparse reward cooperative multi-agent tasks depicted in Fig. 7 of Appendix D and analyzed below. Details of implementation and experiment settings are also described in Appendix D.</p><p>Push-Box: A 15 &#215; 15 room is populated with 2 agents and 1 box. Agents need to push the box to the wall in 300 environment steps to get a reward of 1000. However, the box is so heavy that only when two agents push it in the same direction at the same time can it be moved a grid. Agents need to coordinate their positions and actions for multiple steps to earn a reward. The purpose of this task is to demonstrate that EITI and EDTI can explore long-term cooperative strategy.</p></sec><sec><title>Island</title><p>This task is a modified version of the classic Stag Hunt game (<xref ref-type="bibr" rid="b0">Peysakhovich &amp; Lerer, 2018</xref>) where two agents roam a 10 &#215; 10 island populated with 9 treasures and a random walking beast for 300 environment steps. Agents can collect a treasure by stepping on it to get a team reward of 10 or, by attacking the beast within their attack range, capture it for a reward of 300. The beast would also attack the agents when they are too close. The beast and agent have a maximum energy of 8 and 5 respectively, which will be subtracted by 1 every time attacked. Therefore, an agent is too weak to beat the beast alone and they have to cooperate. In order to learn optimal strategy in this task, one method has to keep exploring after sub-optimal external rewards are found.</p></sec><sec><title>Large-Island</title><p>Similar to Island but with more agents (4), more treasures (16), and a beast with more energy (16) and a higher reward (600) for being caught. This task aims to demonstrate feasibility of our methods in cases with more than 2 agents.</p><p>Push-Box requires agents to take coordinated actions at certain positions for multiple steps to get rewarded. Therefore, this task is particularly challenging and all the baselines struggle to earn any reward (<xref ref-type="fig" rid="fig_3">Fig. 4</xref> left and Fig. 8 left). Our methods are considerably more successful because interaction happens when the box is moved - agents remain unmoved when they push the box alone but will move by a grid if push it together. In this way, EITI and EDTI agents are rewarded intrinsically to move the box and thus are able to quickly find the optimal policy.</p><p>In the Island task, collecting treasures is a easily-attainable local optimal. However, efficient trea- sures collecting requires the agents to spread on the island. This leads to a situation where attempting to attack the beast seems a bad choice since it is highly possible that agents will be exposed to the beast's attack alone. They have to give up profitable spreading strategy and take the risk of being killed to discover that if they attack the beast collectively for several timesteps, they will get much more rewards. Our methods help solve this challenge by giving agents intrinsic incentives to appear together in the attack range of the beast, where they have indirect interactions (health is part of the state and it decreases slower when the two are attacked alternatively). Fig. 9 in Appendix D demon- strates that our methods learn to catch the beast quickly, and thus have better performance (Fig. 8 right).</p><p>Finally, outperformance of our methods on Large-Island proves that they can successfully handle cases with more than two agents.</p><p>In summary, both of our methods are able to facilitate effective exploration on all the tasks by exploiting interactions. EITI outperforms EDTI in scenarios where all interaction points align with extrinsic rewards. On other tasks, EDTI performs better than EITI due to its ability to filter out interaction points that can not lead to more values.</p><p>We also study EDTI with only intrinsic rewards, discussion and results are included in Appendix A.</p></sec><sec><title>RELATED WORKS</title><p>Single-agent exploration achieves conspicuous success recently. Provably efficient methods are pro- posed, such as upper confidence bound (UCB) (<xref ref-type="bibr" rid="b0">Jaksch et al., 2010</xref>; <xref ref-type="bibr" rid="b1">Azar et al., 2017</xref>; <xref ref-type="bibr" rid="b0">Jin et al., 2018</xref>) and posterior sampling for reinforcement learning (PSRL) (<xref ref-type="bibr" rid="b0">Strens, 2000</xref>; <xref ref-type="bibr" rid="b1">Osband et al., 2013</xref>; <xref ref-type="bibr" rid="b1">Osband &amp; Van Roy, 2016</xref>; <xref ref-type="bibr" rid="b0">Agrawal &amp; Jia, 2017</xref>). Given that these methods do not scale well to large or continuous settings, another line of research has been focusing on curiosity-driven explo- ration (<xref ref-type="bibr" rid="b0">Schmidhuber, 1991</xref>; <xref ref-type="bibr" rid="b0">Chentanez et al., 2005</xref>; <xref ref-type="bibr" rid="b0">Oudeyer et al., 2007</xref>; <xref ref-type="bibr" rid="b0">Barto, 2013</xref>; <xref ref-type="bibr" rid="b0">Bellemare et al., 2016</xref>; <xref ref-type="bibr" rid="b0">Pathak et al., 2017</xref>; <xref ref-type="bibr" rid="b0">Ostrovski et al., 2017</xref>), and have shown impressive results (<xref ref-type="bibr" rid="b0">Burda et al., 2019</xref>; 2018; <xref ref-type="bibr" rid="b0">Hyoungseok Kim, 2019</xref>). In addition, methods based on variational informa- tion maximization (<xref ref-type="bibr" rid="b0">Houthooft et al., 2016</xref>; <xref ref-type="bibr" rid="b0">Barron et al., 2018</xref>) and mutual information (<xref ref-type="bibr" rid="b0">Rubin et al., 2012</xref>; <xref ref-type="bibr" rid="b0">Still &amp; Precup, 2012</xref>; <xref ref-type="bibr" rid="b0">Salge et al., 2014</xref>; <xref ref-type="bibr" rid="b0">Mohamed &amp; Rezende, 2015</xref>; <xref ref-type="bibr" rid="b0">Hyoungseok Kim, 2019</xref>) have been proposed for single-agent intrinsically motivated exploration.</p><p>Although multi-agent reinforcement learning (MARL) has been making significant progresses in recent years (<xref ref-type="bibr" rid="b0">Foerster et al., 2018</xref>; <xref ref-type="bibr" rid="b0">Lowe et al., 2017</xref>; <xref ref-type="bibr" rid="b0">Wen et al., 2019</xref>; <xref ref-type="bibr" rid="b0">Iqbal &amp; Sha, 2019a</xref>; <xref ref-type="bibr" rid="b0">Sune- hag et al., 2018</xref>; <xref ref-type="bibr" rid="b0">Son et al., 2019</xref>; <xref ref-type="bibr" rid="b0">Rashid et al., 2018</xref>), less attention has been drawn to multi-agent exploration. <xref ref-type="bibr" rid="b0">Dimakopoulou &amp; Van Roy (2018)</xref> and <xref ref-type="bibr" rid="b0">Dimakopoulou et al. (2018)</xref> propose poste- rior sampling methods for exploration of concurrent reinforcement learning in coverage problems, <xref ref-type="bibr" rid="b0">Bargiacchi et al. (2018)</xref> presents a multi-agent upper confidence exploration method for repeated single-stage problems, and <xref ref-type="bibr" rid="b0">Iqbal &amp; Sha (2019b)</xref> investigates methods to combine several decentral- ized curiosity-driven exploration strategies. All these works focus on transition-independent set- tings. Another Bayesian exploration approach has been proposed for learning in stateless repeated games (<xref ref-type="bibr" rid="b0">Chalkiadakis &amp; Boutilier, 2003</xref>). In contrast, this paper focuses on more general multi-agent sequential decision making problems with complex reward dependencies and transition interactions among agents.</p><p>In the literature of MARL, COMA (<xref ref-type="bibr" rid="b0">Foerster et al., 2018</xref>) shares some similarity with our decision- theoretic EDTI approach in that both of them use the idea of counterfactual formulations. However, they are quite different in terms of definition and optimization: (1) conceptually, EDTI measures the influence of one agent on the value functions of other agents, while COMA quantifies individual contribution to the team value; (2) EDTI is defined on counterfactual Q-value over state-action pairs of other agents given its own state-action pair, while COMA uses the counterfactual Q-value just over its own action without considering state information, which is critical for exploration; (3) we explicitly derive the gradients for optimizing EDTI influence for coordinated exploration in the policy gradient framework, which provides more accurate feedback, while COMA uses the counterfactual Q value as a critic. Another line of relevant works (<xref ref-type="bibr" rid="b0">Oliehoek et al., 2012</xref>; <xref ref-type="bibr" rid="b0">de Castro et al., 2019</xref>) propose influence-based abstraction to predict influence sources to help local decision making of agents. In contrast, this paper presents two novel approaches that quantify and maximize the influence between agents for enabling coordinated multi-agent exploration.</p><p>In addition, some previous MARL work has also studied intrinsic rewards. One notably relevant work is <xref ref-type="bibr" rid="b0">Jaques et al. (2018)</xref>, which models the social influence of one agent on other agents' policies. In contrast, EITI measures the influence of one agent on the transition dynamics of other agents. Accompanying this distinction, EITI includes states of agents in the calculation of influence while social influence dos not. Apart from that, the optimization methods are also different - we directly derive the gradients of mutual information and incorporate its optimization in the policy gradient framework, while <xref ref-type="bibr" rid="b0">Jaques et al. (2018)</xref> adds social influence reward to the immediate environmental reward for training policies. <xref ref-type="bibr" rid="b0">Hughes et al. (2018)</xref> proposes an inequality aversion reward for learning in intertemporal social dilemmas. <xref ref-type="bibr" rid="b0">Strouse et al. (2018)</xref> uses mutual information between goal and states or actions as an intrinsic reward to train the agent to share or hide their intentions.</p></sec><sec><title>CLOSING REMARKS</title><p>In this paper, we study the multi-agent exploration problem and propose two influence-based meth- ods that exploits the interaction structure. These methods are based on two interaction measures, MI and Value of Interaction (VoI), which respectively measure the amount and value of one agent's influence on the other agents' exploration processes. These two measures can be best regraded as exploration bonus distribution. We also propose an optimization method in the policy gradient framework, which enables agents to achieve coordinated exploration in a decentralized manner and optimize team performance.</p></sec><sec id="figures"><title>Figures</title><table-wrap id="tab_0"><label>Table 1:</label><caption><title>Table 1:</title><p>Baseline algorithms. The third column is the reward used to train the value func- tion of PPO. u i and u cen are curiosity about individual state s i and global state s, T 1 = log (p(s -i |s, a)/p(s -i |s -i , a -i )), T 2 = 1 &#8722; p(s -i |s -i , a -i )/p(s -i |s, a), and &#8710;Q -i (s, a) = Q -i (s, a) &#8722; Q -i (s -i , a -i ). Social influence (Jaques et al., 2018) and COMA (Foerster et al., 2018) are augmented with curiosity.</p></caption><table><tbody><tr><td /></tr></tbody></table></table-wrap><fig id="fig_0"><object-id>fig_0</object-id><label>Figure 1:</label><caption><title>Figure 1:</title><p>Didactic examples. Left: task Pass. Two agents starting at the upper-left corner are only rewarded when both of them reach the other room through the door, which will open only when at least one of the switches is occupied by one or more agents. Middle: Secret-Room. An extension of Pass with 4 rooms and switches. When the switch 1 is occupied, all the three doors turn open. And the three switches on the right only control the door of its room. The agents need to reach the upper right room to achieve any reward. Right: comparison of our methods with ablations on Pass.</p></caption><graphic /></fig><fig id="fig_1"><object-id>fig_1</object-id><label>Figure 2:</label><caption><title>Figure 2:</title><p>Development of performance of our methods compared to baselines and intrinsic reward terms of EITI, EDTI, and plusV over the training period of 9000 PPO updates segmented into three phases. "Team Reward" shows averaged team reward gained in a episode, with a maximum of 1000. It shows that only EITI, EDTI, and centralized control and Multi can learn the strategy during this stage. "EITI reward", "EDTI reward", and "plusV reward" demonstrate the evolving of correspond- ing intrinsic rewards.</p></caption><graphic /><graphic /><graphic /><graphic /><graphic /><graphic /><graphic /><graphic /><graphic /><graphic /></fig><fig id="fig_2"><object-id>fig_2</object-id><label>Figure 3:</label><caption><title>Figure 3:</title><p>Left: performance comparison between EDTI and EITI on Secret-Room over 7400 PPO updates. Right: EITI and EDTI terms of two agents after 100, 2900, and 7400 updates.</p></caption><graphic /><graphic /><graphic /><graphic /><graphic /><graphic /><graphic /></fig><fig id="fig_3"><object-id>fig_3</object-id><label>Figure 4:</label><caption><title>Figure 4:</title><p>Comparison of our methods against ablations for Push-Box, Island, and Large-Island. Comparison with baselines is shown in Fig. 8 in Appendix D.</p></caption><graphic /></fig></sec></body><back><ref-list id="ref-list-1"><ref id="b0"><element-citation publication-type="journal"><article-title>Optimistic posterior sampling for reinforcement learning: worst-case regret bounds</article-title><source>Advances in Neural Information Processing Systems</source><year>2017</year><fpage>1184</fpage><lpage>1194</lpage><person-group person-group-type="author"><name><surname>References Shipra Agrawal</surname><given-names>Randy Jia</given-names></name></person-group></element-citation></ref><ref id="b1"><element-citation publication-type="journal"><article-title>Minimax regret bounds for reinforce- ment learning</article-title><person-group person-group-type="author"><name><surname>Mohammad Gheshlaghi Azar</surname><given-names>Ian</given-names></name><name><surname>Osband</surname><given-names>R&#233;mi</given-names></name><name><surname>Munos</surname><given-names /></name></person-group></element-citation></ref></ref-list></back></article>