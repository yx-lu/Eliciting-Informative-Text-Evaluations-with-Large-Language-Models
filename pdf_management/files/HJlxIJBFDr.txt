Title:
```
Published as a conference paper at ICLR 2020 SAMPLE EFFICIENT POLICY GRADIENT METHODS WITH RECURSIVE VARIANCE REDUCTION
```
Abstract:
```
Improving the sample efficiency in reinforcement learning has been a long- standing research problem. In this work, we aim to reduce the sample complexity of existing policy gradient methods. We propose a novel policy gradient algo- rithm called SRVR-PG, which only requires O(1/ 3/2 ) 1 episodes to find an - approximate stationary point of the nonconcave performance function J(θ) (i.e., θ such that ∇J(θ) 2 2 ≤ ). This sample complexity improves the existing result O(1/ 5/3 ) for stochastic variance reduced policy gradient algorithms by a factor of O(1/ 1/6 ). In addition, we also propose a variant of SRVR-PG with parameter exploration, which explores the initial policy parameter from a prior probability distribution. We conduct numerical experiments on classic control problems in reinforcement learning to validate the performance of our proposed algorithms. 1 O(·) notation hides constant factors.
```

Figures/Tables Captions:
```
Figure 1: (a)-(c): Comparison of different algorithms. Experimental results are averaged over 10 repetitions. (d)-(f): Comparison of different batch size B on the performance of SRVR-PG.
Table 1: Comparison on sample complexities of different algorithms to achieve ∇J(θ) 2 2 ≤ .
```

Main Content:
```

Section Title: INTRODUCTION
  INTRODUCTION Reinforcement learning (RL) ( Sutton & Barto, 2018 ) has received significant success in solving various complex problems such as learning robotic motion skills ( Levine et al., 2015 ), autonomous driving ( Shalev-Shwartz et al., 2016 ) and Go game ( Silver et al., 2017 ), where the agent progres- sively interacts with the environment in order to learn a good policy to solve the task. In RL, the agent makes its decision by choosing the action based on the current state and the historical rewards it has received so far. After performing the chosen action, the agent's state will change according to some transition probability model and a new reward would be revealed to the agent by the envi- ronment based on the action and new state. Then the agent continues to choose the next action until it reaches a terminal state. The aim of the agent is to maximize its expected cumulative rewards. Therefore, the pivotal problem in RL is to find a good policy which is a function that maps the state space to the action space and thus informs the agent which action to take at each state. To optimize the agent's policy in the high dimensional continuous action space, the most popular approach is the policy gradient method ( Sutton et al., 2000 ) that parameterizes the policy by an unknown parameter θ ∈ R d and directly optimizes the policy by finding the optimal θ. The objective function J(θ) is chosen to be the performance function, which is the expected return under a specific policy and is usually non-concave. Our goal is to maximize the value of J(θ) by finding a stationary point θ * such that ∇J(θ * ) 2 = 0 using gradient based algorithms. Due to the expectation in the definition of J(θ), it is usually infeasible to compute the gradient exactly. In practice, one often uses stochastic gradient estimators such as REINFORCE ( Williams, 1992 ), PGT ( Sutton et al., 2000 ) and GPOMDP ( Baxter & Bartlett, 2001 ) to approximate the gradi- ent of the expected return based on a batch of sampled trajectories. However, this approximation will introduce additional variance and slow down the convergence of policy gradient, which thus requires a huge amount of trajectories to find a good policy. Theoretically, these stochastic gradient (SG) based algorithms require O(1/ 2 ) trajectories ( Robbins & Monro, 1951 ) to find an -approximate stationary point such that E[ ∇J(θ) 2 2 ] ≤ . In order to reduce the variance of policy gradient algorithms,  Papini et al. (2018)  proposed a stochastic variance-reduced policy gradient (SVRPG) Published as a conference paper at ICLR 2020 Algorithms Complexity REINFORCE ( Williams, 1992 ) O(1/ 2 ) PGT ( Sutton et al., 2000 ) O(1/ 2 ) GPOMDP ( Baxter & Bartlett, 2001 ) O(1/ 2 ) SVRPG ( Papini et al., 2018 ) O(1/ 2 ) SVRPG ( Xu et al., 2019 ) O(1/ 5/3 ) SRVR-PG (This paper) O(1/ 3/2 ) algorithm by borrowing the idea from the stochastic variance reduced gradient (SVRG) ( Johnson & Zhang, 2013 ;  Allen-Zhu & Hazan, 2016 ;  Reddi et al., 2016a ) in stochastic optimization. The key idea is to use a so-called semi-stochastic gradient to replace the stochastic gradient used in SG methods. The semi-stochastic gradient combines the stochastic gradient in the current iterate with a snapshot of stochastic gradient stored in an early iterate which is called a reference iterate. In prac- tice, SVRPG saves computation on trajectories and improves the performance of SG based policy gradient methods.  Papini et al. (2018)  also proved that SVRPG converges to an -approximate sta- tionary point θ of the nonconcave performance function J(θ) with E[ ∇J(θ) 2 2 ] ≤ after O(1/ 2 ) trajectories, which seems to have the same sample complexity as SG based methods. Recently, the sample complexity of SVRPG has been improved to O(1/ 5/3 ) by a refined analysis ( Xu et al., 2019 ), which theoretically justifies the advantage of SVRPG over SG based methods. This paper continues on this line of research. We propose a Stochastic Recursive Variance Reduced Policy Gradient algorithm (SRVR-PG), which provably improves the sample complexity of SVRPG. At the core of our proposed algorithm is a recursive semi-stochastic policy gradient inspired from the stochastic path-integrated differential estimator ( Fang et al., 2018 ), which accumulates all the stochastic gradients from different iterates to reduce the variance. We prove that SRVR-PG only takes O(1/ 3/2 ) trajectories to converge to an -approximate stationary point θ of the performance function, i.e., E[ ∇J(θ) 2 2 ] ≤ . We summarize the comparison of SRVR-PG with existing policy gradient methods in terms of sample complexity in  Table 1 . Evidently, the sample complexity of SRVR-PG is lower than that of REINFORCE, PGT and GPOMDP by a factor of O(1/ 1/2 ), and is lower than that of SVRPG ( Xu et al., 2019 ) by a factor of O(1/ 1/6 ). In addition, we integrate our algorithm with parameter-based exploration (PGPE) method ( Sehnke et al., 2008 ;  2010 ), and propose a SRVR-PG-PE algorithm which directly optimizes the prior prob- ability distribution of the policy parameter θ instead of finding the best value. The proposed SRVR-PG-PE enjoys the same trajectory complexity as SRVR-PG and performs even better in some applications due to its additional exploration over the parameter space. Our experimental results on classical control tasks in reinforcement learning demonstrate the superior performance of the proposed SRVR-PG and SRVR-PG-PE algorithms and verify our theoretical analysis.

Section Title: ADDITIONAL RELATED WORK
  ADDITIONAL RELATED WORK We briefly review additional relevant work to ours with a focus on policy gradient based methods. For other RL methods such as value based ( Watkins & Dayan, 1992 ;  Mnih et al., 2015 ) and actor- critic ( Konda & Tsitsiklis, 2000 ;  Peters & Schaal, 2008a ;  Silver et al., 2014 ) methods, we refer the reader to  Peters & Schaal (2008b) ;  Kober et al. (2013) ;  Sutton & Barto (2018)  for a complete review. To reduce the variance of policy gradient methods, early works have introduced unbiased baseline functions ( Baxter & Bartlett, 2001 ;  Greensmith et al., 2004 ;  Peters & Schaal, 2008b ) to reduce the variance, which can be constant, time-dependent or state-dependent.  Schulman et al. (2015b)  proposed the generalized advantage estimation (GAE) to explore the trade-off between bias and variance of policy gradient. Recently, action-dependent baselines are also used in  Tucker et al. (2018) ;  Wu et al. (2018)  which introduces bias but reduces variance at the same time.  Sehnke et al. (2008 ;  2010 ) proposed policy gradient with parameter-based exploration (PGPE) that explores in the parameter space. It has been shown that PGPE enjoys a much smaller variance ( Zhao et al., Published as a conference paper at ICLR 2020 2011 ). The Stein variational policy gradient method is proposed in  Liu et al. (2017) . See  Peters & Schaal (2008b) ;  Deisenroth et al. (2013) ; Li (2017) for a more detailed survey on policy gradient. Stochastic variance reduced gradient techniques such as SVRG ( Johnson & Zhang, 2013 ;  Xiao & Zhang, 2014 ), batching SVRG ( Harikandeh et al., 2015 ), SAGA ( Defazio et al., 2014 ) and SARAH ( Nguyen et al., 2017 ) were first developed in stochastic convex optimization. When the objective function is nonconvex (or nonconcave for maximization problems), nonconvex SVRG ( Allen-Zhu & Hazan, 2016 ;  Reddi et al., 2016a ) and SCSG ( Lei et al., 2017 ; Li & Li, 2018) were proposed and proved to converge to a first-order stationary point faster than vanilla SGD ( Robbins & Monro, 1951 ) with no variance reduction. The state-of-the-art stochastic variance reduced gradient methods for nonconvex functions are the SNVRG ( Zhou et al., 2018 ) and SPIDER ( Fang et al., 2018 ) algorithms, which have been proved to achieve near optimal convergence rate for smooth functions. There are yet not many papers studying variance reduced gradient techniques in RL.  Du et al. (2017)  first applied SVRG in policy evaluation for a fixed policy.  Xu et al. (2017)  introduced SVRG into trust region policy optimization for model-free policy gradient and showed that the resulting algo- rithm SVRPO is more sample efficient than TRPO.  Yuan et al. (2019)  further applied the techniques in SARAH ( Nguyen et al., 2017 ) and SPIDER ( Fang et al., 2018 ) to TRPO ( Schulman et al., 2015a ). However, no analysis on sample complexity (i.e., number of trajectories required) was provided in the aforementioned papers ( Xu et al., 2017 ;  Yuan et al., 2019 ). We note that a recent work by  Shen et al. (2019)  proposed a Hessian aided policy gradient (HAPG) algorithm that converges to the sta- tionary point of the performance function within O(H 2 / 3/2 ) trajectories, which is worse than our result by a factor of O(H 2 ) where H is the horizon length of the environment. Moreover, they need additional samples to approximate the Hessian vector product, and cannot handle the policy in a constrained parameter space. Another related work pointed out by the anonymous reviewer is  Yang & Zhang (2019) , which extended the stochastic mirror descent algorithm ( Ghadimi et al., 2016 ) in the optimization field to policy gradient methods and achieved O(H 2 / 2 ) sample complexity. Af- ter the ICLR conference submission deadline,  Yang & Zhang (2019)  revised their paper by adding a new variance reduction algorithm that achieves O(H 2 / 3/2 ) sample complexity, which is also worse than our result by a factor of O(H 2 ). Apart from the convergence analysis of the general nonconcave performance functions, there has emerged a line of work ( Cai et al., 2019 ;  Liu et al., 2019 ;  Yang et al., 2019 ;  Wang et al., 2019 ) that studies the global convergence of (proximal/trust-region) policy optimization with neural network function approximation, which applies the theory of overparameterized neural networks ( Du et al., 2019b ;a;  Allen-Zhu et al., 2019 ;  Zou et al., 2019 ;  Cao & Gu, 2019 ) to reinforcement learning. Notation v 2 denotes the Euclidean norm of a vector v ∈ R d and A 2 denotes the spectral norm of a matrix A ∈ R d×d . We write a n = O(b n ) if a n ≤ Cb n for some constant C > 0. The Dirac delta function δ(x) satisfies δ(0) = +∞ and δ(x) = 0 if x = 0. Note that δ(x) satisfies +∞ −∞ δ(x)dx = 1. For any α > 0, we define the Rényi divergence ( Rényi et al., 1961 ) between distributions P and Q as D α (P ||Q) = 1 α − 1 log 2 x P (x) P (x) Q(x) α−1 dx, which is non-negative for all α > 0. The exponentiated Rényi divergence is d α (P ||Q) = 2 Dα(P ||Q) .

Section Title: BACKGROUNDS ON POLICY GRADIENT
  BACKGROUNDS ON POLICY GRADIENT Markov Decision Process: A discrete-time Markov Decision Process (MDP) is a tuple M = {S, A, P, r, γ, ρ}. S and A are the state and action spaces respectively. P(s |s, a) is the tran- sition probability of transiting to state s after taking action a at state s. Function r(s, a) : S × A → [−R, R] emits a bounded reward after the agent takes action a at state s, where R > 0 is a constant. γ ∈ (0, 1) is the discount factor. ρ is the distribution of the starting state. A policy at state s is a probability function π(a|s) over action space A. In episodic tasks, fol- lowing any stationary policy, the agent can observe and collect a sequence of state-action pairs τ = {s 0 , a 0 , s 1 , a 1 , . . . , s H−1 , a H−1 , s H }, which is called a trajectory or episode. H is called the trajectory horizon or episode length. In practice, we can set H to be the maximum value among all Published as a conference paper at ICLR 2020 the actual trajectory horizons we have collected. The sample return over one trajectory τ is defined as the discounted cumulative reward R(τ ) = H−1 h=0 γ h r(s h , a h ). Policy Gradient: Suppose the policy, denoted by π θ , is parameterized by an unknown parameter θ ∈ R d . We denote the trajectory distribution induced by policy π θ as p(τ |θ). Then We define the expected return under policy π θ as J(θ) = E τ ∼p(·|θ) [R(τ )|M], which is also called the performance function. To maximize the performance function, we can update the policy param- eter θ by iteratively running gradient ascent based algorithms, i.e., θ k+1 = θ k + η∇ θ J(θ k ), where η > 0 is the step size and the gradient ∇ θ J(θ) is derived as follows: However, it is intractable to calculate the exact gradient in (2.2) since the trajectory distribution p(τ |θ) is unknown. In practice, policy gradient algorithm samples a batch of trajectories {τ i } N i=1 to approximate the exact gradient based on the sample average over all sampled trajectories: At the k-th iteration, the policy is then updated by θ k+1 = θ k + η ∇ θ J(θ k ). According to (2.1), we know that ∇ θ log p(τ i |θ) is independent of the transition probability matrix P . Recall the definition of R(τ ), we can rewrite the approximate gradient as follows ∇ θ J(θ) = 1 N N i=1 H−1 h=0 ∇ θ log π θ (a i h |s i h ) H−1 h=0 γ h r(s i h , a i h ) def = 1 N N i=1 g(τ i |θ), (2.4) where τ i = {s i 0 , a i 0 , s i 1 , a i 1 , . . . , s i H−1 , a i H−1 , s i H } for all i = 1, . . . , N and g(τ i |θ) is an unbiased gradient estimator computed based on the i-th trajectory τ i . The gradient estimator in (2.4) is based on the likelihood ratio methods and is often referred to as the REINFORCE gradient estimator ( Williams, 1992 ). Since E[∇ θ log π θ (a|s)] = 0, we can add any constant baseline b t to the reward that is independent of the current action and the gradient estimator still remains unbiased. With the observation that future actions do not depend on past rewards, another famous policy gradient theorem (PGT) estimator ( Sutton et al., 2000 ) removes the rewards from previous states: g(τ i |θ) = H−1 h=0 ∇ θ log π θ (a i h |s i h ) H−1 t=h γ t r(s i t , a i t ) − b t , (2.5) where b t is a constant baseline. It has been shown ( Peters & Schaal, 2008b ) that the PGT estimator is equivalent to the commonly used GPOMDP estimator ( Baxter & Bartlett, 2001 ) defined as follows: All the three gradient estimators mentioned above are unbiased ( Peters & Schaal, 2008b ). It has been proved that the variance of the PGT/GPOMDP estimator is independent of horizon H while the variance of REINFORCE depends on H polynomially ( Zhao et al., 2011 ;  Pirotta et al., 2013 ). Therefore, we will focus on the PGT/GPOMDP estimator in this paper and refer to them inter- changeably due to their equivalence.

Section Title: THE PROPOSED ALGORITHM
  THE PROPOSED ALGORITHM The approximation in (2.3) using a batch of trajectories often causes a high variance in practice. In this section, we propose a novel variance reduced policy gradient algorithm called stochastic recursive variance reduced policy gradient (SRVR-PG), which is displayed in Algorithm 1. Our SRVR-PG algorithm consists of S epochs. In the initialization, we set the parameter of a reference policy to be θ 0 = θ 0 . At the beginning of the s-th epoch, where s = 0, . . . , S − 1, we set the initial policy parameter θ s+1 0 to be the same as that of the reference policy θ s . The algorithm then samples N episodes {τ i } N i=1 from the reference policy π θ s to compute a gradient estimator v s 0 = 1/N N i=1 g(τ i | θ s ), where g(τ i | θ s ) is the PGT/GPOMDP estimator. Then the policy is immediately update as in Line 6 of Algorithm 1. Within the epoch, at the t-th iteration, SRVR-PG samples B episodes {τ j } B j=1 based on the current policy π θ s+1 t . We define the following recursive semi-stochastic gradient estimator: B B j=1 g(τ j |θ s+1 t ) − 1 B B j=1 g ω (τ j |θ s+1 t−1 ) + v s+1 t−1 , (3.1) where the first term is a stochastic gradient based on B episodes sampled from the current policy, and the second term is a stochastic gradient defined based on the step-wise important weight between the current policy π θ s+1 t and the reference policy π θ s . Take the GPOMDP estimator for example, for a behavior policy π θ1 and a target policy π θ2 , the step-wise importance weighted estimator is defined as follows The difference between the last two terms in (3.1) can be viewed as a control variate to reduce the variance of the stochastic gradient. In many practical applications, the policy parameter space is a subset of R d , i.e., θ ∈ Θ with Θ ⊆ R d being a convex set. In this case, we need to project the updated policy parameter onto the constraint set. Base on the semi-stochastic gradient (3.1), we can update the policy parameter using projected gradient ascent along the direction of v s+1 t : θ s+1 t+1 = P Θ (θ s+1 t + ηv s+1 t ), where η > 0 is the step size and the projection operator associated with Θ is defined as P Θ (θ) = argmin u∈Θ θ − u 2 2 = argmin u∈R d 1 Θ (u) + 1 2η θ − u 2 2 , (3.3) where 1 Θ (u) is the set indicator function on Θ, i.e., 1 Θ (u) = 0 if u ∈ Θ and 1 Θ (u) = +∞ otherwise. η > 0 is any finite real value and is chosen as the step size in our paper. It is easy to see that 1 Θ (·) is nonsmooth. At the end of the s-th epoch, we update the reference policy as θ s+1 = θ s+1 m , where θ s+1 m is the last iterate of this epoch. The goal of our algorithm is to find a point θ ∈ Θ that maximizes the performance function J(θ) subject to the constraint, namely, max θ∈Θ J(θ) = max θ∈R d {J(θ) − 1 Θ (θ)}. The gradient norm ∇J(θ) 2 is not sufficient to characterize the convergence of the algorithm due to additional the constraint. Following the literature on nonsmooth optimization ( Reddi et al., 2016b ;  Ghadimi et al., 2016 ;  Nguyen et al., 2017 ; Li & Li, 2018;  Wang et al., 2018 ), we use the generalized first-order stationary condition: G η (θ) = 0, where the gradient mapping G η is defined as follows We can view G η as a generalized projected gradient at θ. By definition if Θ = R d , we have G η (θ) ≡ ∇J(θ). Therefore, the policy is update is displayed in Line 10 in Algorithm 1, where Published as a conference paper at ICLR 2020 end for 12: θ s+1 = θ s+1 m 13: end for 14: return θ out , which is uniformly picked from {θ s t } t=0,...,m−1;s=0,...,S prox is the proximal operator defined in (3.3). Similar recursive semi-stochastic gradients to (3.1) were first proposed in stochastic optimization for finite-sum problems, leading to the stochastic re- cursive gradient algorithm (SARAH) ( Nguyen et al., 2017 ; 2019) and the stochastic path-integrated differential estimator (SPIDER) ( Fang et al., 2018 ;  Wang et al., 2018 ). However, our gradient es- timator in (3.1) is noticeably different from that in  Nguyen et al. (2017) ;  Fang et al. (2018) ;  Wang et al. (2018) ;  Nguyen et al. (2019)  due to the gradient estimator g ω (τ j |θ s+1 t−1 ) defined in (3.2) that is equipped with step-wise importance weights. This term is essential to deal with the non-stationarity of the distribution of the trajectory τ . Specifically, {τ j } B j=1 are sampled from policy π θ s+1 t while the PGT/GPOMDP estimator g(·|θ s+1 t−1 ) is defined based on policy π θ s+1 t−1 according to (2.6). This in- consistency introduces extra challenges in the convergence analysis of SRVR-PG. Using importance weighting, we can obtain E τ ∼p(τ |θ s+1 t ) [g ω (τ |θ s+1 t−1 )] = E τ ∼p(τ |θ s+1 t−1 ) [g(τ |θ s+1 t−1 )], which eliminates the inconsistency caused by the varying trajectory distribution. It is worth noting that the semi-stochastic gradient in (3.1) also differs from the one used in SVRPG ( Papini et al., 2018 ) because we recursively update v s+1 t using v s+1 t−1 from the previous iteration, while SVRPG uses a reference gradient that is only updated at the beginning of each epoch. More- over, SVRPG wastes N trajectories without updating the policy at the beginning of each epoch, while Algorithm 1 updates the policy immediately after this sampling process (Line 6), which saves computation in practice. We notice that very recently another algorithm called SARAPO ( Yuan et al., 2019 ) is proposed which also uses a recursive gradient update in trust region policy optimization ( Schulman et al., 2015a ). Our Algorithm 1 differs from their algorithm at least in the following ways: (1) our recursive gradient v s t defined in (3.1) has an importance weight from the snapshot gradient while SARAPO does not; (2) we are optimizing the expected return while  Yuan et al. (2019)  optimizes the total advantage over state visitation distribution and actions under KullbackLeibler divergence constraint; and most importantly (3) there is no convergence or sample complexity analysis for SARAPO.

Section Title: MAIN THEORY
  MAIN THEORY In this section, we present the theoretical analysis of Algorithm 1. We first introduce some common assumptions used in the convergence analysis of policy gradient methods. Assumption 4.1. Let π θ (a|s) be the policy parameterized by θ. There exist constants G, M > 0 such that the gradient and Hessian matrix of log π θ (a|s) with respect to θ satisfy ∇ θ log π θ (a|s) ≤ G, ∇ 2 θ log π θ (a|s) 2 ≤ M, for all a ∈ A and s ∈ S.

Section Title: Published as a conference paper at ICLR 2020
  Published as a conference paper at ICLR 2020 The above boundedness assumption is reasonable since we usually require the policy function to be twice differentiable and easy to optimize in practice. Similarly, in  Papini et al. (2018) , the au- thors assume that ∂ ∂θi log π θ (a|s) and ∂ 2 ∂θi∂θj log π θ (a|s) are upper bounded elementwisely, which is actually stronger than our Assumption 4.1. In the following proposition, we show that Assumption4.1 directly implies that the Hessian ma- trix of the performance function ∇ 2 J(θ) is bounded, which is often referred to as the smoothness assumption and is crucial in analyzing the convergence of nonconvex optimization ( Reddi et al., 2016a ;  Allen-Zhu & Hazan, 2016 ). Proposition 4.2. Let g(τ |θ) be the PGT estimator defined in (2.5). Assumption 4.1 implies: Similar properties are also proved in  Xu et al. (2019) . However, in contrast to their results, the smoothness parameter L and the bound on the gradient norm here do not rely on horizon H. When H ≈ 1/(1−γ) and γ is sufficiently close to 1, we can see that the order of the smoothness parameter is O(1/(1 − γ) 3 ), which matches the order O(H 2 /(1 − γ)) in  Xu et al. (2019) . The next assumption requires the variance of the gradient estimator is bounded. Assumption 4.3. There exists a constant ξ > 0 such that Var g(τ |θ) ≤ ξ 2 , for all policy π θ . In Algorithm 1, we have used importance sampling to connect the trajectories between two different iterations. The following assumption ensures that the variance of the importance weight is bounded, which is also made in  Papini et al. (2018) ;  Xu et al. (2019) . Assumption 4.4. Let ω(·|θ 1 , θ 2 ) = p(·|θ 1 )/p(·|θ 2 ). There is a constant W < ∞ such that for each policy pairs encountered in Algorithm 1,

Section Title: CONVERGENCE RATE AND SAMPLE COMPLEXITY OF SRVR-PG
  CONVERGENCE RATE AND SAMPLE COMPLEXITY OF SRVR-PG Now we are ready to present the convergence result of SRVR-PG to a stationary point: Theorem 4.5. Suppose that Assumptions 4.1, 4.3 and 4.4 hold. In Algorithm 1, we choose the step size η ≤ 1/(4L) and epoch size m and mini-batch size B such that Then the generalized projected gradient of the output of Algorithm 1 satisfies Remark 4.6. Theorem 4.5 states that under a proper choice of step size, batch size and epoch length, the expected squared gradient norm of the performance function at the output of SRVR-PG is in the order of Recall that S is the number of epochs and m is the epoch length of SRVR-PG, so Sm is the total number of iterations of SRVR-PG. Thus the first term O(1/(Sm)) characterizes the convergence rate of SRVR-PG. The second term O(1/N ) comes from the variance of the stochastic gradient used in the outer loop, where N is the batch size used in the snapshot gradient v s+1 0 in Line 5 of SRVR-PG. Compared with the O(1/(Sm) + 1/N + 1/B) convergence rate in  Papini et al. (2018) , Published as a conference paper at ICLR 2020 our analysis avoids the additional term O(1/B) that depends on the mini-batch size within each epoch. Compared with  Xu et al. (2019) , our mini-batch size B is independent of the horizon length H. This enables us to choose a smaller mini-batch size B while maintaining the same convergence rate. As we will show in the next corollary, this improvement leads to a lower sample complexity. Corollary 4.7. Suppose the same conditions as in Theorem 4.5 hold. Set step size as η = 1/(4L), the batch size parameters as N = O(1/ ) and B = O(1/ 1/2 ) respectively, epoch length as m = O(1/ 1/2 ) and the number of epochs as S = O(1/ 1/2 ). Then Algorithm 1 outputs a point θ out that satisfies E[ G η (θ out ) 2 2 ] ≤ within O(1/ 3/2 ) trajectories in total. Note that the results in  Papini et al. (2018) ;  Xu et al. (2019)  are for ∇ θ J(θ) 2 2 ≤ , while our result in Corollary 4.7 is more general. In particular, when the policy parameter θ is defined on the whole space R d instead of Θ, our result reduces to the case for ∇ θ J(θ) 2 2 ≤ since Θ = R d and G η (θ) = ∇ θ J(θ). In  Xu et al. (2019) , the authors improved the sample complexity of SVRPG ( Papini et al., 2018 ) from O(1/ 2 ) to O(1/ 5/3 ) by a sharper analysis. According to Corollary 4.7, SRVR-PG only needs O(1/ 3/2 ) number of trajectories to achieve ∇ θ J(θ) 2 2 ≤ , which is lower than the sample complexity of SVRPG by a factor of O(1/ 1/6 ). This improvement is more pronounced when the required precision is very small.

Section Title: IMPLICATION FOR GAUSSIAN POLICY
  IMPLICATION FOR GAUSSIAN POLICY Now, we consider the Gaussian policy model and present the sample complexity of SRVR-PG in this setting. For bounded action space A ⊂ R, a Gaussian policy parameterized by θ is defined as π θ (a|s) = 1 √ 2π exp − (θ φ(s) − a) 2 2σ 2 , (4.1) where σ 2 is a fixed standard deviation parameter and φ : S → R d is a mapping from the state space to the feature space. For Gaussian policy, under the mild condition that the actions and the state feature vectors are bounded, we can verify that Assumptions 4.1 and 4.3 hold, which can be found in Appendix D. It is worth noting that Assumption 4.4 does not hold trivially for all Gaussian distributions. In particular,  Cortes et al. (2010)  showed that for two Gaussian distribu- tions π θ1 (a|s) ∼ N (µ 1 , σ 2 1 ) and π θ2 (a|s) ∼ N (µ 2 , σ 2 2 ), if σ 2 > √ 2/2σ 1 , then the variance of ω(τ |θ 1 , θ 2 ) is bounded. For our Gaussian policy defined in (4.1) where the standard deviation σ 2 is fixed, we have σ > √ 2/2σ trivially hold, and therefore Assumption 4.4 holds for some finite constant W > 0 according to (2.1). Recall that Theorem 4.5 holds for any general models under Assumptions 4.1, 4.3 and 4.4. Based on the above arguments, we know that the convergence analysis in Theorem 4.5 applies to Gaussian policy. In the following corollary, we present the sample complexity of Algorithm 1 for Gaussian policy with detailed dependency on precision parameter , horizon size H and the discount factor γ. Corollary 4.8. Given the Gaussian policy defined in (4.1), suppose Assumption 4.4 holds and we have |a| ≤ C a for all a ∈ A and φ(s) 2 ≤ M φ for all s ∈ S, where C a , M φ > 0 are constants. If we set step size as η = O((1−γ) 3 ), the mini-batch sizes and epoch length as N = O((1−γ) −3 −1 ), B = O((1 − γ) −1 −1/2 ) and m = O((1 − γ) −2 −1/2 ), then the output of Algorithm 1 satisfies E[ G η (θ out ) 2 2 ] ≤ after O(1/((1 − γ) 4 3/2 )) trajectories in total. Remark 4.9. For Gaussian policy, the number of trajectories Algorithm 1 needs to find an - approximate stationary point, i.e., E[ G η (θ out ) 2 2 ] ≤ , is also in the order of O( −3/2 ), which is faster than PGT and SVRPG. Additionally, we explicitly show that the sample complexity does not depend on the horizon H, which is in sharp contrast with the results in  Papini et al. (2018) ;  Xu et al. (2019) . The dependence on 1/(1 − γ) comes from the variance of PGT estimator.

Section Title: EXPERIMENTS
  EXPERIMENTS In this section, we provide experiment results of the proposed algorithm on benchmark reinforce- ment learning environments including the Cartpole, Mountain Car and Pendulum problems. In all Published as a conference paper at ICLR 2020 the experiments, we use the Gaussian policy defined in (4.1). In addition, we found that the proposed algorithm works well without the extra projection step. Therefore, we did not use projection in our experiments. For baselines, we compare the proposed SRVR-PG algorithm with the most relevant methods: GPOMDP ( Baxter & Bartlett, 2001 ) and SVRPG ( Papini et al., 2018 ). For the learning rates η in all of our experiments, we use grid search to directly tune η. For instance, we searched η for the Cartpole problem by evenly dividing the interval [10 −5 , 10 −1 ] into 20 points in the log- space. For the batch size parameters N and B and the epoch length m, according to Corollary 4.7, we choose N = O(1/ ), B = O(1/ 1/2 ) and thus m = O(1/ 1/2 ), where > 0 is a user-defined precision parameter. In our experiments, we set N = C 0 / , B = C 1 / 1/2 and m = C 2 / 1/2 and tune the constant parameters C 0 , C 1 , C 2 using grid search. The detailed parameters used in the experiments are presented in Appendix E. We evaluate the performance of different algorithms in terms of the total number of trajectories they require to achieve a certain threshold of cumulative rewards. We run each experiment repeatedly for 10 times and plot the averaged returns with standard deviation. For a given environment, all experiments are initialized from the same random initialization. Figures 1(a), 1(b) and 1(c) show the results on the comparison of GPOMDP, SVRPG, and our proposed SRVR-PG algorithm across three different RL environments. It is evident that, for all environments, GPOMDP is overshadowed by the variance reduced algorithms SVRPG and SRVR-PG significantly. Furthermore, SRVR-PG outperforms SVRPG in all experiments, which is consistent with the comparison on the sample complexity of GPOMDP, SVRPG and SRVR-PG in  Table 1 . Corollaries 4.7 and 4.8 suggest that when the mini-batch size B is in the order of O( √ N ), SRVR-PG achieves the best performance. Here N is the number of episodes sampled in the outer loop of Al- gorithm 1 and B is the number of episodes sampled at each inner loop iteration. To validate our theoretical result, we conduct a sensitivity study to demonstrate the effectiveness of different batch sizes within each epoch of SRVR-PG on its performance. The results on different environments are displayed in Figures 1(d), 1(e) and 1(f) respectively. To interpret these results, we take the Pendu- lum problem as an example. In this setting, we choose outer loop batch size N of Algorithm 1 to be N = 250. By Corollary 4.8, the optimal choice of batch size in the inner loop of Algorithm 1 is B = C √ N , where C > 1 is a constant depending on horizon H and discount factor γ. Figure 1(f) shows that B = 50 ≈ 3 √ N yields the best convergence results for SRVR-PG on Pendulum, which validates our theoretical analysis and implies that a larger batch size B does not necessarily result in an improvement in sample complexity, as each update requires more trajectories, but a smaller batch size B pushes SRVR-PG to behave more similar to GPOMDP. Moreover, by comparing with the outer loop batch size N presented in Table 2 for SRVR-PG in Cartpole and Mountain Car envi- ronments, we found that the results in Figures 1(d) and 1(e) are again in alignment with our theory. Due to the space limit, additional experiment results are included in Appendix E.

Section Title: CONCLUSIONS
  CONCLUSIONS We propose a novel policy gradient method called SRVR-PG, which is built on a recursively updated stochastic policy gradient estimator. We prove that the sample complexity of SRVR-PG is lower than the sample complexity of the state-of-the-art SVRPG ( Papini et al., 2018 ;  Xu et al., 2019 ) algorithm. We also extend the new variance reduction technique to policy gradient with parameter-based ex- ploration and propose the SRVR-PG-PE algorithm, which outperforms the original PGPE algorithm both in theory and practice. Experiments on the classic reinforcement learning benchmarks validate the advantage of our proposed algorithms.

```
